<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Spark - Tag - 爱吃芒果</title>
        <link>http://example.org/tags/spark/</link>
        <description>Spark - Tag - 爱吃芒果</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Sun, 25 May 2025 11:16:21 &#43;0800</lastBuildDate><atom:link href="http://example.org/tags/spark/" rel="self" type="application/rss+xml" /><item>
    <title>Spark物理执行计划</title>
    <link>http://example.org/posts/spark_physical_plan/</link>
    <pubDate>Sun, 25 May 2025 11:16:21 &#43;0800</pubDate>
    <author>爱吃芒果</author>
    <guid>http://example.org/posts/spark_physical_plan/</guid>
    <description><![CDATA[<h2 id="spark物理执行计划生成方法">Spark物理执行计划生成方法</h2>
<p>Spark具体采用3个步骤来生成物理执行计划，首先根据action操作顺序将应用划分为作业（job），然后根据每个job的逻辑处理流程中的ShuffleDependency依赖关系，将job划分为执行阶段（stage）。最后在每个stage中，根据最后生成的RDD的分区个数生成多个计算任务（task）。</p>]]></description>
</item>
<item>
    <title>Spark逻辑处理流程</title>
    <link>http://example.org/posts/spark_logical_plan/</link>
    <pubDate>Sun, 11 May 2025 15:20:27 &#43;0800</pubDate>
    <author>爱吃芒果</author>
    <guid>http://example.org/posts/spark_logical_plan/</guid>
    <description><![CDATA[<p>Spark应用程序需要先转化为逻辑处理流程，逻辑处理流程主要包括：</p>
<ul>
<li>RDD数据模型</li>
<li>数据操作</li>
<li>数据依赖关系</li>
</ul>
<p>数据操作分为两种，<code>transformation</code>操作并不会触发job的实际执行，<code>action</code>操作创建job并立即执行。类似于java中的stream，采用懒加载的方式。</p>]]></description>
</item>
<item>
    <title>Spark基础知识</title>
    <link>http://example.org/posts/spark-base/</link>
    <pubDate>Sat, 10 May 2025 11:51:10 &#43;0800</pubDate>
    <author>爱吃芒果</author>
    <guid>http://example.org/posts/spark-base/</guid>
    <description><![CDATA[<h2 id="rdd数据模型">RDD数据模型</h2>
<p>RDD （Resilient Distributed DataSet)是spark对计算过程中输入输出数据以及中间数据的抽象，表示不可变、分区的集合数据，可以被并行处理。</p>]]></description>
</item>
<item>
    <title>Spark开发环境搭建</title>
    <link>http://example.org/posts/spark_developement_envrionment/</link>
    <pubDate>Sat, 05 Apr 2025 22:05:09 &#43;0800</pubDate>
    <author>爱吃芒果</author>
    <guid>http://example.org/posts/spark_developement_envrionment/</guid>
    <description><![CDATA[<p>通过如下的方法在idea中配置spark开发环境，最后和一般的java项目一样，使用maven面板的 clean和package进行编译。</p>
<p>我实际使用的编译器为java17，idea会提示配置scala编译器。</p>]]></description>
</item>
<item>
    <title>Spark内存管理</title>
    <link>http://example.org/posts/spark_memory_manager/</link>
    <pubDate>Sun, 09 Mar 2025 22:32:09 &#43;0800</pubDate>
    <author>爱吃芒果</author>
    <guid>http://example.org/posts/spark_memory_manager/</guid>
    <description><![CDATA[<h2 id="关键问题">关键问题</h2>
<ol>
<li>
<p>内存被分成哪些区域，各分区之间的关系是什么，通过什么参数控制</p>
</li>
<li>
<p>内存上报和释放的单位是什么，上报和释放是如何实现的</p>
</li>
<li>
<p>如何避免内存没有释放导致资源泄露</p>
</li>
<li>
<p>如何避免重复上报和漏上报问题</p>]]></description>
</item>
</channel>
</rss>
