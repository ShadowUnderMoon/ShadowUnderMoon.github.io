[{"categories":["Presto","Trino"],"content":" // TaskResouce /v1/task/{taskId} public Response createOrUpdateTask(@PathParam(\"taskId\") TaskId taskId, TaskUpdateRequest taskUpdateRequest, @Context UriInfo uriInfo) { requireNonNull(taskUpdateRequest, \"taskUpdateRequest is null\"); Session session = taskUpdateRequest.getSession().toSession(sessionPropertyManager, taskUpdateRequest.getExtraCredentials()); TaskInfo taskInfo = taskManager.updateTask(session, taskId, taskUpdateRequest.getFragment(), taskUpdateRequest.getSources(), taskUpdateRequest.getOutputIds(), taskUpdateRequest.getTotalPartitions()); if (shouldSummarize(uriInfo)) { taskInfo = taskInfo.summarize(); } return Response.ok().entity(taskInfo).build(); } ","date":"2026-01-25","objectID":"/posts/presto-task-execution/:0:0","tags":["Presto","Trino"],"title":"Prestoä»»åŠ¡æ‰§è¡Œæµç¨‹","uri":"/posts/presto-task-execution/"},{"categories":["Presto","Trino"],"content":" StatementClientV1.advance ExecutingStatementResource::getQueryResult Query::getNextResult ","date":"2026-01-25","objectID":"/posts/presto-client-server-interaction/:0:0","tags":["Presto","Trino","TODO"],"title":"Prestoå®¢æˆ·ç«¯å’ŒæœåŠ¡åŒºäº¤äº’æµç¨‹","uri":"/posts/presto-client-server-interaction/"},{"categories":null,"content":"åœ¨Prestoçš„æ‰§è¡Œæ¨¡å‹ä¸­ï¼ŒSQLçš„æ‰§è¡Œè¢«åˆ’åˆ†ä¸ºå¦‚ä¸‹å‡ ä¸ªå±‚æ¬¡ï¼š æŸ¥è¯¢ï¼šç”¨æˆ·æäº¤ä¸€ä¸ªSQLï¼Œè§¦å‘Prestoçš„ä¸€æ¬¡æŸ¥è¯¢ï¼Œåœ¨ä»£ç ä¸­å¯¹åº”ä¸€ä¸ªQueryInfoã€‚æ¯ä¸ªæŸ¥è¯¢éƒ½æœ‰ä¸€ä¸ªå­—ç¬¦ä¸²å½¢å¼çš„QueryId æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µï¼šPrestoç”ŸæˆæŸ¥è¯¢çš„æ‰§è¡Œè®¡åˆ’æ—¶ï¼Œæ ¹æ®æ˜¯å¦éœ€è¦åšè·¨æŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹çš„æ•°æ®äº¤æ¢æ¥åˆ’åˆ†PlanFragmentã€‚è°ƒåº¦æ‰§è¡Œè®¡åˆ’æ—¶ï¼Œæ¯ä¸ªPlanFragmentå¯¹åº”ä¸€ä¸ªæŸ¥è¯¢æ‰§è¡Œé˜¶æ®µï¼Œåœ¨ä»£ç ä¸­å¯¹åº”ä¸€ä¸ªStageInfoï¼Œå…¶ä¸­æœ‰StageIdï¼ŒStageIdçš„å½¢å¼ä¸ºQueryId + ä»0è‡ªå¢idã€‚æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µä¹‹é—´æœ‰æ•°æ®ä¾èµ–å…³ç³»ï¼Œå³ä¸èƒ½å¹¶è¡Œæ‰§è¡Œï¼Œå­˜åœ¨æ‰§è¡Œä¸Šçš„é¡ºåºå…³ç³»ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒStageIdè¶Šå°ï¼Œè¿™ä¸ªæŸ¥è¯¢æ‰§è¡Œé˜¶æ®µçš„æ‰§è¡Œé¡ºåºè¶Šé åã€‚Prestoçš„æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µç±»ä¼¼äºSparkçš„æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µçš„æ¦‚å¿µï¼Œä»–ä»¬çš„ä¸åŒæ˜¯Prestoä¸åƒSparkæ‰¹å¼å¤„ç†é‚£æ ·ï¼Œéœ€è¦å‰é¢çš„æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µæ‰§è¡Œå®Œå†æ‰§è¡Œåé¢çš„æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µï¼ŒPrestoé‡‡ç”¨çš„æ˜¯æµæ°´çº¿ï¼ˆPipelineï¼‰å¤„ç†æœºåˆ¶ã€‚ ä»»åŠ¡ï¼ˆTaskï¼‰ï¼šä»»åŠ¡æ˜¯Prestoåˆ†å¸ƒå¼ä»»åŠ¡çš„æ‰§è¡Œå•å…ƒï¼Œæ¯ä¸ªæŸ¥è¯¢æ‰§è¡Œé˜¶æ®µå¯ä»¥æœ‰å¤šä¸ªä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡å¯ä»¥å¹¶è¡Œæ‰§è¡Œï¼ŒåŒä¸€ä¸ªæŸ¥è¯¢æ‰§è¡Œé˜¶æ®µä¸­çš„æ‰€æœ‰ä»»åŠ¡çš„æ‰§è¡Œé€»è¾‘å®Œå…¨ç›¸åŒã€‚ä¸€ä¸ªæŸ¥è¯¢æ‰§è¡Œé˜¶æ®µçš„ä»»åŠ¡ä¸ªæ•°å°±æ˜¯æ­¤æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µçš„å¹¶å‘åº¦ã€‚åœ¨Prestoçš„ä»»åŠ¡è°ƒåº¦ä»£ç ä¸­ï¼Œå¯ä»¥çœ‹åˆ°ä»»åŠ¡çš„ä¸ªæ•°æ˜¯æ ¹æ®æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µçš„æ•°æ®åˆ†å¸ƒæ–¹å¼ï¼ˆSourceï¼ŒFixedï¼ŒSingleï¼‰ä»¥åŠæŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹çš„ä¸ªæ•°æ¥å†³å®šçš„ã€‚ // SqlQueryExecution.java private void planDistribution(PlanRoot plan) { // éå†æ‰§è¡Œè®¡åˆ’PlanNodeæ ‘ï¼Œæ‰¾åˆ°æ‰€æœ‰çš„TableScanNodeï¼ˆä¹Ÿå°±æ˜¯è¿æ¥å™¨å¯¹åº”çš„PlanNodeï¼‰ï¼Œè·å–åˆ°ä»–ä»¬çš„SplitSource DistributedExecutionPlanner distributedPlanner = new DistributedExecutionPlanner(splitManager, metadata, dynamicFilterService); StageExecutionPlan outputStageExecutionPlan = distributedPlanner.plan(plan.getRoot(), stateMachine.getSession()); // åˆ›å»ºæœ€åä¸€ä¸ªæŸ¥è¯¢æ‰§è¡Œé˜¶æ®µçš„Outputbufferï¼Œè¿™ä¸ªOutputBufferç”¨äºç»™Presto SQLå®¢æˆ·ç«¯è¾“å‡ºæŸ¥è¯¢çš„æœ€ç»ˆè®¡ç®—ç»“æœ PartitioningHandle partitioningHandle = plan.getRoot().getFragment().getPartitioningScheme().getPartitioning().getHandle(); OutputBuffers rootOutputBuffers = createInitialEmptyOutputBuffers(partitioningHandle) .withBuffer(OUTPUT_BUFFER_ID, BROADCAST_PARTITION_ID) .withNoMoreBufferIds(); // åˆ›å»ºSqlStageExecutionï¼Œå¹¶å°†å…¶å°è£…åœ¨SqlQueryScheduleré‡Œé¢è¿”å› // è¿™é‡Œåªæ˜¯åˆ›å»ºStageï¼Œä½†æ˜¯ä¸ä¼šå»è°ƒåº¦æ‰§è¡Œå®ƒ SqlQueryScheduler scheduler = createSqlQueryScheduler( stateMachine, outputStageExecutionPlan, nodePartitioningManager, nodeScheduler, remoteTaskFactory, stateMachine.getSession(), plan.isSummarizeTaskInfos(), scheduleSplitBatchSize, queryExecutor, schedulerExecutor, failureDetector, rootOutputBuffers, nodeTaskMap, executionPolicy, schedulerStats, dynamicFilterService); queryScheduler.set(scheduler); } ","date":"2026-01-25","objectID":"/posts/presto-query-schedule-task/:0:0","tags":null,"title":"Prestoæ‰§è¡Œè®¡åˆ’çš„è°ƒåº¦","uri":"/posts/presto-query-schedule-task/"},{"categories":null,"content":"è·å–æ•°æ®æºåˆ†ç‰‡ SqlQueryExecution.planDistributioné¦–å…ˆä»æ•°æ®æºè¿æ¥å™¨ä¸­è·å–åˆ°æ‰€æœ‰çš„åˆ†ç‰‡æ•°æ®æºã€‚åˆ†ç‰‡æ˜¯Prestoä¸­åˆ†å—ç»„ç»‡æ•°æ®çš„æ–¹å¼ï¼ŒPrestoè¿æ¥å™¨ä¼šå°†å¾…å¤„ç†çš„æ‰€æœ‰æ•°æ®åˆ’åˆ†ä¸ºè‹¥å¹²åˆ†ç‰‡è®©Prestoè¯»å–ï¼Œè€Œè¿™äº›åˆ†ç‰‡ä¹Ÿä¼šè¢«å®‰æ’åˆ°å¤šä¸ªPrestoæŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹ä¸Šæ¥å¤„ç†ä»¥å®ç°åˆ†å¸ƒå¼é«˜æ€§èƒ½è®¡ç®—ã€‚åˆ†å¸ƒå¼OLAPå¼•æ“å‡ ä¹å…¨éƒ½æœ‰åˆ†ç‰‡çš„æŠ½è±¡è®¾è®¡ï¼Œä¾‹å¦‚Sparkã€Flinkç­‰ã€‚ public class ConnectorAwareSplitSource implements SplitSource { private final CatalogName catalogName; private final ConnectorSplitSource source; } public class FixedSplitSource implements ConnectorSplitSource { private final List\u003cConnectorSplit\u003e splits; private int offset; } public class TpcdsSplitManager implements ConnectorSplitManager public ConnectorSplitSource getSplits(ConnectorTransactionHandle transaction, ConnectorSession session, ConnectorTableLayoutHandle layout, SplitSchedulingStrategy splitSchedulingStrategy) { Set\u003cNode\u003e nodes = nodeManager.getRequiredWorkerNodes(); checkState(!nodes.isEmpty(), \"No TPCDS nodes available\"); int totalParts = nodes.size() * splitsPerNode; int partNumber = 0; // Split the data using split and skew by the number of nodes available. ImmutableList.Builder\u003cConnectorSplit\u003e splits = ImmutableList.builder(); for (Node node : nodes) { for (int i = 0; i \u003c splitsPerNode; i++) { splits.add(new TpcdsSplit(partNumber, totalParts, ImmutableList.of(node.getHostAndPort()), noSexism)); partNumber++; } } return new FixedSplitSource(splits.build()); } ä»¥Tpcdsæ•°æ®æºçš„å®ç°ä¸ºä¾‹ï¼Œé€šè¿‡è°ƒç”¨ConnectorSplitManager.getSplitså¯ä»¥è·å¾—ConnectorSplitSourceï¼Œé‡Œé¢ä¿å­˜äº†æ‰€æœ‰çš„åˆ†ç‰‡ï¼Œä¹Ÿå°±æ˜¯ConnectorSplitï¼Œæœ€ç»ˆè¢«å°è£…åˆ°StageExecutionPlanä¸­ã€‚æ³¨æ„ï¼Œè¿™ä¸€æ­¥åªæ˜¯ç”Ÿæˆæ•°æ®æºåˆ†ç‰‡ï¼Œæ—¢ä¸ä¼šå°†åˆ†ç‰‡å®‰æ’åˆ°æŸä¸ªPrestoæŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹ä¸Šï¼Œä¹Ÿä¸ä¼šçœŸæ­£ä½¿ç”¨åˆ†ç‰‡è¯»å–è¿æ¥å™¨çš„æ•°æ®ã€‚ public interface ConnectorSplit { // è¿™ä¸ªä¿¡æ¯å®šä¹‰äº†åˆ†ç‰‡æ˜¯å¦å¯ä»¥éåˆ†ç‰‡æ‰€åœ¨çš„èŠ‚ç‚¹è®¿é—®åˆ°ï¼Œå¯¹äºè®¡ç®—å­˜å‚¨åˆ†ç¦»çš„æƒ…å†µï¼Œè¿™é‡Œéœ€è¦è¿”å›true boolean isRemotelyAccessible(); // è¿™ä¸ªä¿¡æ¯å®šä¹‰äº†åˆ†ç‰‡å¯ä»¥ä»å“ªäº›èŠ‚ç‚¹è®¿é—®ï¼ˆè¿™äº›èŠ‚ç‚¹ï¼Œå¹¶ä¸éœ€è¦æ˜¯Prestoé›†ç¾¤çš„èŠ‚ç‚¹ï¼Œä¾‹å¦‚å¯¹äºè®¡ç®—å­˜å‚¨åˆ†ç¦»çš„æƒ…å†µï¼Œå¤§æ¦‚ç‡Prestoçš„èŠ‚ç‚¹ä¸æ•°æ®æºåˆ†ç‰‡æ‰€åœ¨çš„èŠ‚ç‚¹ä¸æ˜¯ç›¸åŒçš„ List\u003cHostAddress\u003e getAddresses(); // è¿™é‡Œå…è®¸è¿æ¥å™¨è®¾ç½®ä¸€äº›è‡ªå·±çš„ä¿¡æ¯ Object getInfo(); } ","date":"2026-01-25","objectID":"/posts/presto-query-schedule-task/:0:1","tags":null,"title":"Prestoæ‰§è¡Œè®¡åˆ’çš„è°ƒåº¦","uri":"/posts/presto-query-schedule-task/"},{"categories":null,"content":"åˆ›å»ºSqlQueryScheduler private SqlQueryScheduler( QueryStateMachine queryStateMachine, StageExecutionPlan plan, NodePartitioningManager nodePartitioningManager, NodeScheduler nodeScheduler, RemoteTaskFactory remoteTaskFactory, Session session, boolean summarizeTaskInfo, int splitBatchSize, ExecutorService queryExecutor, ScheduledExecutorService schedulerExecutor, FailureDetector failureDetector, OutputBuffers rootOutputBuffers, NodeTaskMap nodeTaskMap, ExecutionPolicy executionPolicy, SplitSchedulerStats schedulerStats, DynamicFilterService dynamicFilterService) { this.queryStateMachine = requireNonNull(queryStateMachine, \"queryStateMachine is null\"); this.executionPolicy = requireNonNull(executionPolicy, \"schedulerPolicyFactory is null\"); this.schedulerStats = requireNonNull(schedulerStats, \"schedulerStats is null\"); this.summarizeTaskInfo = summarizeTaskInfo; this.dynamicFilterService = requireNonNull(dynamicFilterService, \"dynamicFilterService is null\"); // todo come up with a better way to build this, or eliminate this map ImmutableMap.Builder\u003cStageId, StageScheduler\u003e stageSchedulers = ImmutableMap.builder(); ImmutableMap.Builder\u003cStageId, StageLinkage\u003e stageLinkages = ImmutableMap.builder(); // Only fetch a distribution once per query to assure all stages see the same machine assignments Map\u003cPartitioningHandle, NodePartitionMap\u003e partitioningCache = new HashMap\u003c\u003e(); OutputBufferId rootBufferId = Iterables.getOnlyElement(rootOutputBuffers.getBuffers().keySet()); List\u003cSqlStageExecution\u003e stages = createStages( (fragmentId, tasks, noMoreExchangeLocations) -\u003e updateQueryOutputLocations(queryStateMachine, rootBufferId, tasks, noMoreExchangeLocations), new AtomicInteger(), plan.withBucketToPartition(Optional.of(new int[1])), nodeScheduler, remoteTaskFactory, session, splitBatchSize, partitioningHandle -\u003e partitioningCache.computeIfAbsent(partitioningHandle, handle -\u003e nodePartitioningManager.getNodePartitioningMap(session, handle)), nodePartitioningManager, queryExecutor, schedulerExecutor, failureDetector, nodeTaskMap, stageSchedulers, stageLinkages); SqlStageExecution rootStage = stages.get(0); rootStage.setOutputBuffers(rootOutputBuffers); this.rootStageId = rootStage.getStageId(); this.stages = stages.stream() .collect(toImmutableMap(SqlStageExecution::getStageId, identity())); this.stageSchedulers = stageSchedulers.build(); this.stageLinkages = stageLinkages.build(); this.executor = queryExecutor; } createSqlQuerySchedulerä¼šä¸ºæ‰§è¡Œè®¡åˆ’çš„æ¯ä¸ªPlanFragmentåˆ›å»ºå¯¹åº”çš„SqlStageExecutionã€‚æ¯ä¸ªSqlStageExecutionæ ¹æ®ä¸åŒçš„æ•°æ®åˆ†åŒºç±»å‹ï¼ˆPartitioningHandleï¼‰å¯èƒ½å¯¹åº”ä¸åŒçš„StageSchedulerå®ç°io.prestosql.execution.scheduler.SqlQueryScheduler#createStagesã€‚ public interface StageScheduler extends Closeable { /** * Schedules as much work as possible without blocking. * The schedule results is a hint to the query scheduler if and * when the stage scheduler should be invoked again. It is * important to note that this is only a hint and the query * scheduler may call the schedule method at any time. */ ScheduleResult schedule(); @Override default void close() {} } åˆ°ç›®å‰ä¸ºæ­¢ï¼ŒStageScheduleræœ‰4ä¸ªå®ç°ç±»ï¼Œåˆ†åˆ«å¯¹åº”äº†4ç§ä¸åŒçš„æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µè°ƒåº¦æ–¹å¼ï¼Œæœ€å¸¸ç”¨åˆ°çš„æ˜¯SourcePartitionedSchedulerå’ŒFixedCountSchedulerã€‚ StageSchedulerçš„èŒè´£æ˜¯ç»‘å®šæŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹ä¸ä¸Šæ¸¸æ•°æ®æºåˆ†ç‰‡çš„å…³ç³»ï¼Œåˆ›å»ºä»»åŠ¡å¹¶è°ƒåº¦åˆ°æŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹ä¸Šã€‚å¦‚æœstageä½¿ç”¨æ•°æ®æºè¿æ¥å™¨ä»å­˜å‚¨ç³»ç»Ÿæ‹‰å–æ•°æ®ï¼Œè¿™äº›stageçš„ä»»åŠ¡è°ƒåº¦ä½¿ç”¨çš„æ˜¯SourcePartitionedSchedulerã€‚ ä»æ•°æ®æºè¿æ¥å™¨é‚£é‡Œè·å–ä¸€æ‰¹åˆ†ç‰‡ï¼Œå¹¶å‡†å¤‡è°ƒåº¦è¿™äº›åˆ†ç‰‡ã€‚Prestoçš„é»˜è®¤é…ç½®ä¸ºæ¯æ‰¹æœ€å¤šè°ƒåº¦1000ä¸ªåˆ†ç‰‡ã€‚FixedSplitSourcé¢„å…ˆå‡†å¤‡å¥½æ‰€æœ‰çš„åˆ†ç‰‡ï¼ŒPrestoæ¡†æ¶çš„SplitSource::getNextBlockæ¯æ¬¡ä¼šæ ¹æ®éœ€è¦è·å–ä¸€æ‰¹åˆ†ç‰‡ï¼ŒFixedSplitSourceæ ¹æ®éœ€è¦çš„åˆ†ç‰‡ä¸ªæ•°æ¥è¿”å›ã€‚å‡ ä¹æ‰€æœ‰çš„è¿æ¥å™¨éƒ½æ˜¯ç”¨çš„FixedSplitSourceï¼Œåªæœ‰å°‘æ•°å‡ ä¸ªè¿æ¥å™¨ï¼ˆå¦‚Hiveï¼‰å®ç°äº†è‡ªå·±çš„ConnectorSplitSourceã€‚æ ¹æ®SplitPlacementPolicyä¸ºè¿™ä¸€æ‰¹åˆ†ç‰‡æŒ‘é€‰å¯¹åº”çš„èŠ‚ç‚¹ï¼Œå»ºç«‹ä¸€ä¸ªmapï¼Œkeyæ˜¯èŠ‚ç‚¹ï¼Œ valueæ˜¯åˆ†ç‰‡åˆ—è¡¨ã€‚ private void schedule() { try (SetThreadName ignored = new SetThreadName(\"Query-%s\", queryStateMachine.getQueryId())) { Set\u003cStageId\u003e completedStages = new HashSet\u003c\u003e(); // æ ¹æ®æ‰§è¡Œç­–ç•¥ç¡®å®šæŸ¥è¯¢æ‰§è¡Œé˜¶æ®µçš„è°ƒåº¦é¡ºåºå’Œè°ƒåº¦æ—¶æœºï¼Œé»˜è®¤æ˜¯AllAtOnceExecutionPolicy // ä¼šæŒ‰ç…§æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µæ‰§è¡Œçš„ä¸Šä¸‹æ¸¸å…³ç³»ä¾æ¬¡è°ƒåº¦æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µï¼Œç”Ÿæˆä»»åŠ¡å¹¶å…¨éƒ¨åˆ†å‘ç»™PrestoæŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹ // å¦ä¸€ç§ç­–ç•¥æ˜¯PhasedExeuctionPolicy ExecutionSchedule executionSchedule = executionPolicy.createExecutionSchedule(stages.values()); while (!executionSchedu","date":"2026-01-25","objectID":"/posts/presto-query-schedule-task/:0:2","tags":null,"title":"Prestoæ‰§è¡Œè®¡åˆ’çš„è°ƒåº¦","uri":"/posts/presto-query-schedule-task/"},{"categories":null,"content":"FixedCountScheduler public class FixedCountScheduler implements StageScheduler { public FixedCountScheduler(SqlStageExecution stage, List\u003cInternalNode\u003e partitionToNode) { requireNonNull(stage, \"stage is null\"); this.taskScheduler = stage::scheduleTask; this.partitionToNode = requireNonNull(partitionToNode, \"partitionToNode is null\"); } @Override public ScheduleResult schedule() { OptionalInt totalPartitions = OptionalInt.of(partitionToNode.size()); List\u003cRemoteTask\u003e newTasks = IntStream.range(0, partitionToNode.size()) .mapToObj(partition -\u003e taskScheduler.scheduleTask(partitionToNode.get(partition), partition, totalPartitions)) .filter(Optional::isPresent) .map(Optional::get) .collect(toImmutableList()); return new ScheduleResult(true, newTasks, 0); } } FixedCountSchedulerä¸ºæ¯ä¸ªåˆ†åŒºåˆ†é…äº†å¯¹åº”çš„èŠ‚ç‚¹ï¼Œåç»­taskä¼šè¢«å‘é€åˆ°å¯¹åº”çš„èŠ‚ç‚¹å¤„ç†å¯¹åº”çš„åˆ†åŒºï¼Œscheduleæ–¹æ³•éå†æ‰€æœ‰çš„åˆ†åŒºï¼Œè°ƒç”¨taskSchedulerç”Ÿæˆå¯¹åº”çš„remoteTaskï¼Œè¿™é‡Œçš„taskSchedulerå®é™…ä¸Šæ˜¯SqlStageExecution.scheduleTaskã€‚ ","date":"2026-01-25","objectID":"/posts/presto-query-schedule-task/:0:3","tags":null,"title":"Prestoæ‰§è¡Œè®¡åˆ’çš„è°ƒåº¦","uri":"/posts/presto-query-schedule-task/"},{"categories":null,"content":"ç”ŸæˆRemoteTask æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µçš„æ•°æ®æ¥æºæœ‰ä¸¤ç§ï¼Œä¸€ç§æ˜¯æ•°æ®æºè¿æ¥å™¨ï¼Œä¸€ç§æ˜¯ä¸Šæ¸¸æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µçš„ä»»åŠ¡å¯ä»¥è¾“å‡ºåˆ°OutputBufferçš„æ•°æ®ã€‚å¯¹äºä¸‹æ¸¸çš„æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µæ¥è¯´ï¼Œä¸Šæ¸¸æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µçš„ä»»åŠ¡å¯ä»¥ç§°ä¸ºæ•°æ®ä¸Šæ¸¸ä»»åŠ¡ï¼ˆupstream source taskï¼‰ã€‚è¿™äº›æ•°æ®ä¸Šæ¸¸ä»»åŠ¡æ˜¯é€šè¿‡SqlStageExecution::addExchangeLocationsæ³¨å†Œåˆ°ä¸‹æ¸¸çš„SqlStageExecutionä¸­çš„ï¼Œè®©ä¸‹æ¸¸æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µçŸ¥é“å»å“ªé‡Œå–æ•°æ®ã€‚æ— è®ºæ˜¯å“ªä¸€ç§æ•°æ®æºï¼ŒPrestoéƒ½ç»Ÿä¸€æŠ½è±¡ä¸ºConnectorSplitï¼Œå½“ä¸Šæ¸¸æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µä½œä¸ºæ•°æ®æºæ—¶ï¼ŒPrestoæŠŠå®ƒçœ‹åšæ˜¯ä¸€ç§ç‰¹æ®Šçš„è¿æ¥å™¨ã€‚å®ƒçš„catalog name = $remoteï¼Œå…¶å®å°±æ˜¯ä¸€ä¸ªå‡çš„ç›®å½•ï¼ŒConnectorSplitçš„å®ç°ç±»æ˜¯RemoteSplitã€‚ // SqlStageExecution.java private synchronized RemoteTask scheduleTask(InternalNode node, TaskId taskId, Multimap\u003cPlanNodeId, Split\u003e sourceSplits, OptionalInt totalPartitions) { checkArgument(!allTasks.contains(taskId), \"A task with id %s already exists\", taskId); ImmutableMultimap.Builder\u003cPlanNodeId, Split\u003e initialSplits = ImmutableMultimap.builder(); // æ·»åŠ æ¥è‡ªä¸Šæ¸¸çš„æ•°æ®æºConnectorçš„Split initialSplits.putAll(sourceSplits); // æ·»åŠ æ¥è‡ªä¸Šæ¸¸æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µçš„ä»»åŠ¡çš„æ•°æ®è¾“å‡ºï¼Œæ³¨å†Œä¸ºRemoteSplit sourceTasks.forEach((planNodeId, task) -\u003e { TaskStatus status = task.getTaskStatus(); if (status.getState() != TaskState.FINISHED) { initialSplits.put(planNodeId, createRemoteSplitFor(taskId, status.getSelf())); } }); OutputBuffers outputBuffers = this.outputBuffers.get(); checkState(outputBuffers != null, \"Initial output buffers must be set before a task can be scheduled\"); // åˆ›å»ºHttpRemoteTask RemoteTask task = remoteTaskFactory.createRemoteTask( stateMachine.getSession(), taskId, node, stateMachine.getFragment(), initialSplits.build(), totalPartitions, outputBuffers, nodeTaskMap.createPartitionedSplitCountTracker(node, taskId), summarizeTaskInfo); completeSources.forEach(task::noMoreSplits); // å°†åˆšåˆ›å»ºçš„TaskIdæ·»åŠ åˆ°å½“å‰æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µçš„TaskIdåˆ—è¡¨ä¸­ allTasks.add(taskId); // å°†åˆšåˆ›å»ºçš„ä»»åŠ¡æ·»åŠ åˆ°å½“å‰æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µçš„èŠ‚ç‚¹ä¸ä»»åŠ¡æ˜ å°„çš„mapä¸­ tasks.computeIfAbsent(node, key -\u003e newConcurrentHashSet()).add(task); nodeTaskMap.addTask(node, task); task.addStateChangeListener(new StageTaskListener()); task.addFinalTaskInfoListener(this::updateFinalTaskInfo); if (!stateMachine.getState().isDone()) { // å‘PrestoæŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹å‘è¯·æ±‚ï¼Œå°†åˆšåˆ›å»ºçš„ä»»åŠ¡è°ƒåº¦èµ·æ¥ï¼Œå¼€å§‹æ‰§è¡Œ task.start(); } else { // stage finished while we were scheduling this task task.abort(); } return task; } ä»»åŠ¡ç»‘å®šåˆ†é…åˆ°å½“å‰èŠ‚ç‚¹çš„åˆ†ç‰‡ä¹‹åï¼ŒPrestoä¼šè°ƒç”¨HttpRemoteTask.startï¼Œå°†ä»»åŠ¡åˆ†å‘åˆ°æŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹ä¸Š ","date":"2026-01-25","objectID":"/posts/presto-query-schedule-task/:0:4","tags":null,"title":"Prestoæ‰§è¡Œè®¡åˆ’çš„è°ƒåº¦","uri":"/posts/presto-query-schedule-task/"},{"categories":null,"content":"å¯åŠ¨HttpRemoteTask // HttpRemoteTask public void start() { try (SetThreadName ignored = new SetThreadName(\"HttpRemoteTask-%s\", taskId)) { // to start we just need to trigger an update scheduleUpdate(); dynamicFiltersFetcher.start(); taskStatusFetcher.start(); taskInfoFetcher.start(); } } æ„é€ äº†ä¸€ä¸ªæºå¸¦PlanFgragmentå’Œåˆ†ç‰‡ä¿¡æ¯çš„HTTP Postè¯·æ±‚ï¼Œè¯·æ±‚å¯¹åº”PrestoæŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹çš„URIï¼š/v1/task/{taskId}ï¼ŒPrestoæŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹åœ¨æ”¶åˆ°è¯·æ±‚åï¼Œä¼šè§£æPlanFragmentä¸­çš„æ‰§è¡Œè®¡åˆ’å¹¶åˆ›å»ºSqlTaskExecutionï¼Œå¼€å§‹æ‰§è¡Œä»»åŠ¡ã€‚ ","date":"2026-01-25","objectID":"/posts/presto-query-schedule-task/:0:5","tags":null,"title":"Prestoæ‰§è¡Œè®¡åˆ’çš„è°ƒåº¦","uri":"/posts/presto-query-schedule-task/"},{"categories":["Presto","Trino"],"content":"åœ¨SqlQueryExecution.doPlanQueryä¸­ï¼Œä»åŸæ¥çš„æŠ½è±¡è¯­æ³•æ ‘ç”Ÿæˆé€»è¾‘æ‰§è¡Œè®¡åˆ’ï¼Œç„¶åä¼˜åŒ–æ‰§è¡Œè®¡åˆ’ï¼Œç”Ÿæˆä¼˜åŒ–åçš„æ‰§è¡Œè®¡åˆ’ï¼Œæœ€åå°†é€»è¾‘æ‰§è¡Œè®¡åˆ’åˆ‡åˆ†æˆå¤šé¢—å­æ ‘ã€‚ private PlanRoot doPlanQuery() { // planNodeIdæ˜¯ä¸€ä¸ªä»0é€’å¢çš„intå€¼ PlanNodeIdAllocator idAllocator = new PlanNodeIdAllocator(); LogicalPlanner logicalPlanner = new LogicalPlanner(stateMachine.getSession(), planOptimizers, idAllocator, metadata, typeOperators, new TypeAnalyzer(sqlParser, metadata), statsCalculator, costCalculator, stateMachine.getWarningCollector()); // è¯­ä¹‰åˆ†æ(Analysis)ï¼Œç”Ÿæˆæ‰§è¡Œè®¡åˆ’ // ä¼˜åŒ–æ‰§è¡Œè®¡åˆ’ï¼Œç”Ÿæˆä¼˜åŒ–åçš„æ‰§è¡Œè®¡åˆ’ Plan plan = logicalPlanner.plan(analysis); queryPlan.set(plan); // å°†é€»è¾‘æ‰§è¡Œè®¡åˆ’åˆ†æˆå¤šæ£µå­æ ‘ SubPlan fragmentedPlan = planFragmenter.createSubPlans(stateMachine.getSession(), plan, false, stateMachine.getWarningCollector()); // extract inputs List\u003cInput\u003e inputs = new InputExtractor(metadata, stateMachine.getSession()).extractInputs(fragmentedPlan); stateMachine.setInputs(inputs); stateMachine.setOutput(analysis.getTarget()); boolean explainAnalyze = analysis.getStatement() instanceof Explain \u0026\u0026 ((Explain) analysis.getStatement()).isAnalyze(); return new PlanRoot(fragmentedPlan, !explainAnalyze); } ","date":"2026-01-24","objectID":"/posts/presto-query-plan/:0:0","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢è®¡åˆ’ç”Ÿæˆå’Œä¼˜åŒ–","uri":"/posts/presto-query-plan/"},{"categories":["Presto","Trino"],"content":"è¯­ä¹‰åˆ†æã€ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ï¼Œå¹¶ä¼˜åŒ–æ‰§è¡Œè®¡åˆ’ LogicalPlanner.plançš„èŒè´£å¦‚ä¸‹ï¼š è¯­ä¹‰åˆ†æï¼šéå†SQLæŠ½è±¡è¯­æ³•æ ‘ï¼Œå°†æŠ½è±¡è¯­æ³•æ ‘ä¸­è¡¨è¾¾çš„å«ä¹‰æ‹†è§£ä¸ºå¤šä¸ªmapç»“æ„ï¼Œä»¥ä¾¿åç»­ç”Ÿæˆæ‰§è¡Œè®¡åˆ’æ—¶ï¼Œä¸å†é¢‘ç¹éå†SQLæŠ½è±¡è¯­æ³•æ ‘ã€‚åŒæ—¶è·å–äº†è¡¨å’Œå­—æ®µçš„å…ƒæ•°æ®ï¼Œç”Ÿæˆäº†å¯¹åº”çš„ConnectorTableHandleã€ColumnHandleç­‰ä¸æ•°æ®æºè¿æ¥å™¨ç›¸å…³çš„å¯¹è±¡å®ä¾‹ï¼Œä¸ºäº†ä¹‹åæ‹¿æ¥å³ç”¨æ‰“ä¸‹åŸºç¡€ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ç”Ÿæˆçš„æ‰€æœ‰å¯¹è±¡ï¼Œéƒ½ç»´æŠ¤åœ¨ä¸€ä¸ªå®ä¾‹åŒ–çš„Analysiså¯¹è±¡ï¼ŒAnalysiså¯¹è±¡å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªContextå¯¹è±¡ ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ï¼šç”Ÿæˆä»¥PlanNodeä¸ºèŠ‚ç‚¹çš„é€»è¾‘æ‰§è¡Œè®¡åˆ’ï¼Œå®ƒä¹Ÿæ˜¯ç±»ä¼¼äºæŠ½è±¡è¯­æ³•æ ‘çš„æ ‘å‹ç»“æ„ï¼Œæ ‘èŠ‚ç‚¹å’Œæ ¹çš„ç±»å‹éƒ½æ˜¯PlanNodeã€‚ ä¼˜åŒ–æ‰§è¡Œè®¡åˆ’ï¼Œç”Ÿæˆä¼˜åŒ–åçš„æ‰§è¡Œè®¡åˆ’ï¼šç”¨é¢„å®šä¹‰çš„å‡ ç™¾ä¸ªä¼˜åŒ–å™¨è¿­ä»£ä¼˜åŒ–ä¹‹å‰ç”Ÿæˆçš„PlanNodeæ ‘ï¼Œå¹¶è¿”å›ä¼˜åŒ–åçš„PlanNodeæ ‘ public Plan plan(Analysis analysis, Stage stage, boolean collectPlanStatistics) { // è¯­ä¹‰åˆ†æï¼Œç”Ÿæˆæ‰§è¡Œè®¡åˆ’ PlanNode root = planStatement(analysis, analysis.getStatement()); planSanityChecker.validateIntermediatePlan(root, session, metadata, typeOperators, typeAnalyzer, symbolAllocator.getTypes(), warningCollector); // ä¼˜åŒ–æ‰§è¡Œè®¡åˆ’ï¼Œç”Ÿæˆä¼˜åŒ–åçš„æ‰§è¡Œè®¡åˆ’ if (stage.ordinal() \u003e= OPTIMIZED.ordinal()) { for (PlanOptimizer optimizer : planOptimizers) { root = optimizer.optimize(root, session, symbolAllocator.getTypes(), symbolAllocator, idAllocator, warningCollector); requireNonNull(root, format(\"%s returned a null plan\", optimizer.getClass().getName())); } } if (stage.ordinal() \u003e= OPTIMIZED_AND_VALIDATED.ordinal()) { // make sure we produce a valid plan after optimizations run. This is mainly to catch programming errors planSanityChecker.validateFinalPlan(root, session, metadata, typeOperators, typeAnalyzer, symbolAllocator.getTypes(), warningCollector); } TypeProvider types = symbolAllocator.getTypes(); StatsAndCosts statsAndCosts = StatsAndCosts.empty(); if (collectPlanStatistics) { StatsProvider statsProvider = new CachingStatsProvider(statsCalculator, session, types); CostProvider costProvider = new CachingCostProvider(costCalculator, statsProvider, Optional.empty(), session, types); statsAndCosts = StatsAndCosts.create(root, statsProvider, costProvider); } return new Plan(root, types, statsAndCosts); } ","date":"2026-01-24","objectID":"/posts/presto-query-plan/:0:1","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢è®¡åˆ’ç”Ÿæˆå’Œä¼˜åŒ–","uri":"/posts/presto-query-plan/"},{"categories":["Presto","Trino"],"content":"å°†é€»è¾‘æ‰§è¡Œè®¡åˆ’æ ‘æ‹†åˆ†ä¸ºå¤šæ£µå­æ ‘ å°†é€»è¾‘æ‰§è¡Œè®¡åˆ’æ‹†åˆ†ä¸ºå¤šæ£µå­æ ‘å¹¶ç”ŸæˆsubPlançš„é€»è¾‘ï¼Œè¿™ä¸ªè¿‡ç¨‹ç”¨SimplePlanRewriterçš„å®ç°ç±»Fragmenterå±‚å±‚éå†ä¸Šä¸€æ­¥ç”Ÿæˆçš„PlanNodeæ ‘ï¼Œå°†å…¶ä¸­çš„ExchangeNode[scope=REMOTE]æ›¿æ¢ä¸ºRemoteSourceNodeï¼Œå¹¶ä¸”æ–­å¼€å®ƒä¸å¶å­èŠ‚ç‚¹çš„è¿æ¥ï¼Œè¿™æ ·ä¸€ä¸ªPlanNodeæ ‘å°±è¢«åˆ’åˆ†æˆäº†ä¸¤ä¸ªPlanNodeæ ‘ï¼Œæ¯æ£µæ ‘éƒ½æ˜¯ä¸€ä¸ªSubPlanã€‚åœ¨æŸ¥è¯¢æ‰§è¡Œçš„æ•°æ®æµè½¬ä¸­ï¼Œå­æ ‘æ˜¯çˆ¶æ ‘çš„æ•°æ®äº§å‡ºä¸Šæ¸¸ã€‚ public class SubPlan { private final PlanFragment fragment; private final List\u003cSubPlan\u003e children; } // io.prestosql.sql.planner.PlanFragmenter.Fragmenter#visitExchange public PlanNode visitExchange(ExchangeNode exchange, RewriteContext\u003cFragmentProperties\u003e context) { if (exchange.getScope() != REMOTE) { return context.defaultRewrite(exchange, context.get()); } PartitioningScheme partitioningScheme = exchange.getPartitioningScheme(); if (exchange.getType() == ExchangeNode.Type.GATHER) { context.get().setSingleNodeDistribution(); } else if (exchange.getType() == ExchangeNode.Type.REPARTITION) { context.get().setDistribution(partitioningScheme.getPartitioning().getHandle(), metadata, session); } ImmutableList.Builder\u003cSubPlan\u003e builder = ImmutableList.builder(); for (int sourceIndex = 0; sourceIndex \u003c exchange.getSources().size(); sourceIndex++) { FragmentProperties childProperties = new FragmentProperties(partitioningScheme.translateOutputLayout(exchange.getInputs().get(sourceIndex))); builder.add(buildSubPlan(exchange.getSources().get(sourceIndex), childProperties, context)); } List\u003cSubPlan\u003e children = builder.build(); context.get().addChildren(children); List\u003cPlanFragmentId\u003e childrenIds = children.stream() .map(SubPlan::getFragment) .map(PlanFragment::getId) .collect(toImmutableList()); return new RemoteSourceNode(exchange.getId(), childrenIds, exchange.getOutputSymbols(), exchange.getOrderingScheme(), exchange.getType()); } public RemoteSourceNode( @JsonProperty(\"id\") PlanNodeId id, @JsonProperty(\"sourceFragmentIds\") List\u003cPlanFragmentId\u003e sourceFragmentIds, @JsonProperty(\"outputs\") List\u003cSymbol\u003e outputs, @JsonProperty(\"orderingScheme\") Optional\u003cOrderingScheme\u003e orderingScheme, @JsonProperty(\"exchangeType\") ExchangeNode.Type exchangeType) { super(id); requireNonNull(outputs, \"outputs is null\"); this.sourceFragmentIds = sourceFragmentIds; this.outputs = ImmutableList.copyOf(outputs); this.orderingScheme = requireNonNull(orderingScheme, \"orderingScheme is null\"); this.exchangeType = requireNonNull(exchangeType, \"exchangeType is null\"); } RemoteSourceNodeä¿å­˜äº†å½“å‰stageç›´æ¥ä¾èµ–çš„æ‰€æœ‰stageçš„PlanFragmentIdã€‚ ","date":"2026-01-24","objectID":"/posts/presto-query-plan/:0:2","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢è®¡åˆ’ç”Ÿæˆå’Œä¼˜åŒ–","uri":"/posts/presto-query-plan/"},{"categories":["Presto","Trino"],"content":"å‰è¨€ Trinoæ˜¯ä»Prestoåˆ†ç¦»å‡ºæ¥çš„é¡¹ç›®ï¼Œåœ¨åé¢çš„æ–‡ç« ä¸­ä¸ä¼šä¸¥æ ¼åŒºåˆ†Prestoå’ŒTrinoï¼Œé™¤éæŸäº›ä»£ç åªåœ¨å…¶ä¸­ä¸€ä¸ªé¡¹ç›®ä¸­å­˜åœ¨ï¼Œæ ¹æ®ã€ŠOLAPå¼•æ“åº•å±‚åŸç†ä¸è®¾è®¡å®è·µã€‹çš„æ¨èï¼Œåé¢åŸºæœ¬ä¼šé€šè¿‡trinoé¡¹ç›®çš„v350ç‰ˆæœ¬ä¸ºä¾‹åˆ†æprestoçš„ä¸€äº›æºç çº§çš„å®ç°ï¼Œå¸Œæœ›èƒ½å¤Ÿæ¯”è¾ƒç³»ç»Ÿåœ°ç†è§£OLAPå¼•æ“çš„æ•´ä½“å®ç°ã€‚ å¦å¤–éå¸¸æ¨èé˜…è¯»ã€ŠOLAPå¼•æ“åº•å±‚åŸç†ä¸è®¾è®¡å®è·µã€‹è¿™æœ¬ä¹¦ã€‚ ","date":"2026-01-17","objectID":"/posts/presto-environment-setup/:1:0","tags":["Presto","Trino"],"title":"Prestoç¯å¢ƒæ­å»º","uri":"/posts/presto-environment-setup/"},{"categories":["Presto","Trino"],"content":"Prestoé›†ç¾¤çš„æ‹“æ‰‘ç»“æ„ åœ¨Prestoé›†ç¾¤ä¸­å­˜åœ¨ä¸¤ç§è§’è‰²ï¼š é›†ç¾¤åè°ƒèŠ‚ç‚¹ï¼šè´Ÿè´£é›†ç¾¤çš„ç®¡ç†ï¼Œä»¥åŠæŸ¥è¯¢ä»»åŠ¡çš„æ¥æ”¶ã€SQLæ‰§è¡Œè®¡åˆ’ç”Ÿæˆã€ä¼˜åŒ–ï¼Œå¹¶å°†ä»»åŠ¡å‘å¸ƒåˆ°ä¸åŒçš„æŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹ä¸Šï¼Œæœ€ç»ˆå°†ç»“æœè¿”å›ç»™å®¢æˆ·ç«¯ æŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹ï¼šåªè´Ÿè´£æ‰§è¡Œå…·ä½“çš„æŸ¥è¯¢ä»»åŠ¡ ","date":"2026-01-17","objectID":"/posts/presto-environment-setup/:2:0","tags":["Presto","Trino"],"title":"Prestoç¯å¢ƒæ­å»º","uri":"/posts/presto-environment-setup/"},{"categories":["Presto","Trino"],"content":"ç¼–è¯‘ æ¨èä½¿ç”¨jdk11ç¼–è¯‘v350ç‰ˆæœ¬çš„trinoä»£ç ï¼Œå‘½ä»¤å¦‚ä¸‹ ./mvnw install -pl presto-main -am -DskipTests ./mvnw install -pl presto-tpcds -am -DskipTests # ç¼–è¯‘éœ€è¦çš„æ•°æ®æºæ¨¡å— ./mvnw install -pl presto-cli -am -DskipTests # ç¼–è¯‘å®¢æˆ·ç«¯æ¨¡å— ä½¿ç”¨ideaå°†é¡¹ç›®ä¸­presto-parser/target/generated-sources/antlr4æ ‡è®°ä¸ºGenerated Source Rootï¼Œè¿™æ ·antlr4ç”Ÿæˆçš„ä»£ç å°±å¯ä»¥è¢«ideaè¯†åˆ«åˆ°ã€‚ ","date":"2026-01-17","objectID":"/posts/presto-environment-setup/:3:0","tags":["Presto","Trino"],"title":"Prestoç¯å¢ƒæ­å»º","uri":"/posts/presto-environment-setup/"},{"categories":["Presto","Trino"],"content":"å•æœºè°ƒè¯•ç¯å¢ƒæ­å»º åœ¨é¡¹ç›®ä¸­åˆ›å»ºpresto-server-main/etc/coordinator.propertiesæ–‡ä»¶ï¼Œæ–‡ä»¶å†…å®¹å¦‚ä¸‹ coordinator=true node.id=ffffffff-ffff-ffff-ffff-ffffffff node.environment=test node.internal-address=localhost http-server.http.port=8080 discovery-server.enabled=true discovery.uri=http://localhost:8080 exchange.http-client.max-connections=1000 exchange.http-client.max-connections-per-server=1000 exchange.http-client.connect-timeout=1m exchange.http-client.idle-timeout=1m scheduler.http-client.max-connections=1000 scheduler.http-client.max-connections-per-server=1000 scheduler.http-client.connect-timeout=1m scheduler.http-client.idle-timeout=1m query.client.timeout=5m query.min-expire-age=30m plugin.bundles=\\ ../presto-tpcds/pom.xml node-scheduler.include-coordinator=true node-scheduler.include-coordinatoræŒ‡å®šä¸ºtrueè¡¨ç¤ºé›†ç¾¤åè°ƒèŠ‚ç‚¹å¯ä»¥ä½œä¸ºæŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹ï¼Œè¿™æ ·å°±ä¸éœ€è¦é¢å¤–å¯åŠ¨æŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹ã€‚ åœ¨ideaä¸­å¯åŠ¨æ—¶ï¼Œé€‰æ‹©æ–°å»ºApplicatoiné…ç½® VMå¯åŠ¨å‚æ•°å¦‚ä¸‹ -ea -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -Xmx2G -Dconfig=etc/coordinator.properties -Dlog.levels-file=etc/log.properties -Djdk.attach.allowAttachSelf=true classpathä¸ºpresto-server-mainï¼Œå¯åŠ¨ç±»ä¸ºio.prestosql.server.PrestoServer ","date":"2026-01-17","objectID":"/posts/presto-environment-setup/:4:0","tags":["Presto","Trino"],"title":"Prestoç¯å¢ƒæ­å»º","uri":"/posts/presto-environment-setup/"},{"categories":["Presto","Trino"],"content":"é›†ç¾¤è°ƒè¯•ç¯å¢ƒæ­å»º é›†ç¾¤è°ƒè¯•ç¯å¢ƒä¼šåˆ†åˆ«å¯åŠ¨é›†ç¾¤åè°ƒèŠ‚ç‚¹å’ŒæŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹ï¼š å°†coordinator.propertiesæ–‡ä»¶ä¸­çš„node-scheduler.include-coordinatorè®¾ç½®ä¸ºfalseï¼Œé›†ç¾¤åè°ƒèŠ‚ç‚¹ä¸ä¼šæ‰§è¡Œå…·ä½“çš„æŸ¥è¯¢ä»»åŠ¡ æ¯ä¸ªæŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹çš„é…ç½®ç±»ä¼¼ï¼Œæ¯”å¦‚ï¼Œå¯ä»¥åœ¨é¡¹ç›®ä¸­åˆ›å»ºpresto-server-main/etc/worker1.propertiesï¼Œå†…å®¹å¦‚ä¸‹ coordinator=false node.id=3e6b1e41-9e8e-4ac6-92e4-cd3dfd3df7c9 node.environment=test node.internal-address=localhost http-server.http.port=8081 discovery.uri=http://localhost:8080 exchange.http-client.max-connections=1000 exchange.http-client.max-connections-per-server=1000 exchange.http-client.connect-timeout=1m exchange.http-client.idle-timeout=1m query.client.timeout=5m query.min-expire-age=30m plugin.bundles=\\ ../presto-tpcds/pom.xml node-scheduler.include-coordinator=false å¯åŠ¨é…ç½®å’Œåè°ƒèŠ‚ç‚¹ç±»ä¼¼ï¼Œä¿®æ”¹ç³»ç»Ÿå˜é‡-Dconfig=/etc/worker1.propertiesï¼Œå…¶ä½™éƒ½ä¸å˜ã€‚ ","date":"2026-01-17","objectID":"/posts/presto-environment-setup/:5:0","tags":["Presto","Trino"],"title":"Prestoç¯å¢ƒæ­å»º","uri":"/posts/presto-environment-setup/"},{"categories":["Presto","Trino"],"content":"å¯åŠ¨æˆåŠŸ å¯åŠ¨æˆåŠŸåå¯ä»¥è®¿é—®http://127.0.0.1:8080ï¼ŒæŸ¥çœ‹Prestoçš„WebUIã€‚ è¿è¡Œ./presto-cli/target/presto-cli-350-executable.jarå¯ä»¥æ‰“å¼€å®¢æˆ·ç«¯å‘½ä»¤è¡Œï¼Œæ‰§è¡ŒæŸ¥è¯¢å‘½ä»¤ï¼Œæ¯”å¦‚ï¼š presto\u003e show catalogs; Catalog --------- system tpcds (2 rows) Query 20260117_083946_00002_crpxr, FINISHED, 1 node Splits: 19 total, 19 done (100.00%) 0.74 [0 rows, 0B] [0 rows/s, 0B/s] presto\u003e å› ä¸ºæˆ‘ä»¬åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šäº†tpcdsæ•°æ®æºï¼Œæ‰€ä»¥è¿™é‡Œé¢æœ‰ä¸¤ä¸ªæ•°æ®æºï¼ŒPrestoå°†è‡ªèº«çš„å„ç§æ•°æ®ä½œä¸ºsystemæ•°æ®æºã€‚ ","date":"2026-01-17","objectID":"/posts/presto-environment-setup/:6:0","tags":["Presto","Trino"],"title":"Prestoç¯å¢ƒæ­å»º","uri":"/posts/presto-environment-setup/"},{"categories":["Presto","Trino"],"content":"ä¸ºäº†å°½é‡é¿å…workerå‘ç”ŸOOMä»è€ŒçŸ­æš‚ä¸å¯ç”¨ï¼ŒPrestoæä¾›äº†åœ¨å†…å­˜é«˜è´Ÿè½½ä¸‹æŸ¥æ€ä»»åŠ¡çš„æœºåˆ¶ï¼Œä»£ç å¾ˆç®€å•ï¼Œåœ¨com.facebook.presto.memory.HighMemoryTaskKillerä¸­ã€‚ ","date":"2026-01-14","objectID":"/posts/presto-high-memory-task-killer/:0:0","tags":["Presto","Trino"],"title":"Presto workeré«˜è´Ÿè½½æŸ¥æ€ä»»åŠ¡","uri":"/posts/presto-high-memory-task-killer/"},{"categories":["Presto","Trino"],"content":"å¦‚ä½•åˆ¤æ–­å†…å­˜é«˜è´Ÿè½½ Prestoé€šè¿‡ç›‘å¬gcäº‹ä»¶ï¼Œé€šè¿‡fullgcçš„å†…å­˜å›æ”¶æƒ…å†µä»¥åŠå‘ç”Ÿé¢‘ç‡æ¥åˆ¤æ–­workeræ˜¯å¦å¤„äºé«˜è´Ÿè½½çŠ¶æ€ï¼Œå¹¶æä¾›äº†ä¸¤ç§ç­–ç•¥ï¼š FREE_MEMORY_ON_FREQUENT_FULL_GC ä¸¤æ¬¡fullgcé—´éš”å°äº1sä¸”ä¸¤æ¬¡fullgcå›æ”¶çš„å†…å­˜å‡æ²¡æœ‰è¶…è¿‡xmxçš„1% FREE_MEMORY_ON_FULL_GC å½“å‰å†…å­˜å ç”¨è¶…è¿‡90%ä¸”fullgcå›æ”¶çš„å†…å­˜å°äºxmxçš„1% ä»¥ä¸Šç­–ç•¥ä¸­çš„å‚æ•°éƒ½å¯ä»¥é…ç½® private boolean shouldTriggerTaskKiller(GarbageCollectionNotificationInfo info) { boolean triggerTaskKiller = false; DataSize beforeGcDataSize = info.getBeforeGcTotal(); DataSize afterGcDataSize = info.getAfterGcTotal(); if (taskKillerStrategy == FREE_MEMORY_ON_FREQUENT_FULL_GC) { long currentGarbageCollectedBytes = beforeGcDataSize.toBytes() - afterGcDataSize.toBytes(); Duration currentFullGCTimestamp = new Duration(ticker.read(), TimeUnit.NANOSECONDS); if (isFrequentFullGC(lastFullGCTimestamp, currentFullGCTimestamp) \u0026\u0026 !hasFullGCFreedEnoughBytes(currentGarbageCollectedBytes)) { triggerTaskKiller = true; } lastFullGCTimestamp = currentFullGCTimestamp; lastFullGCCollectedBytes = currentGarbageCollectedBytes; } else if (taskKillerStrategy == FREE_MEMORY_ON_FULL_GC) { if (isLowMemory() \u0026\u0026 beforeGcDataSize.toBytes() - afterGcDataSize.toBytes() \u003c reclaimMemoryThreshold) { triggerTaskKiller = true; } } log.debug(\"Task Killer Trigger: \" + triggerTaskKiller + \", Before Full GC Head Size: \" + beforeGcDataSize.toBytes() + \" After Full GC Heap Size: \" + afterGcDataSize.toBytes()); return triggerTaskKiller; } private boolean isLowMemory() { MemoryUsage memoryUsage = memoryMXBean.getHeapMemoryUsage(); if (memoryUsage.getUsed() \u003e heapMemoryThreshold) { return true; } return false; } æ„Ÿè§‰è¿™é‡Œæ²¡æœ‰å¿…è¦ç”¨memoryMXBeané‡æ–°è·å–å½“å‰å†…å­˜å ç”¨å¹¶åˆ¤æ–­æ˜¯å¦é«˜è´Ÿè½½ï¼Œå®Œå…¨å¯ä»¥ä½¿ç”¨afterGcDataSize.toBytes()ä»£æ›¿ã€‚ ","date":"2026-01-14","objectID":"/posts/presto-high-memory-task-killer/:1:0","tags":["Presto","Trino"],"title":"Presto workeré«˜è´Ÿè½½æŸ¥æ€ä»»åŠ¡","uri":"/posts/presto-high-memory-task-killer/"},{"categories":["Presto","Trino"],"content":"é«˜è´Ÿè½½åå¦‚ä½•æ¢å¤ Prestoé€‰æ‹©æ€æ‰å½“å‰workerä¸Šå ç”¨å†…å­˜æœ€é«˜çš„æŸ¥è¯¢ï¼Œè¿™é‡Œçš„å†…å­˜å ç”¨æ˜¯è¯´å®ƒä¸ŠæŠ¥çš„å†…å­˜å ç”¨ï¼Œè€Œéå®é™…çš„å†…å­˜å ç”¨ï¼Œå®é™…çš„å†…å­˜å ç”¨é™¤éæ‰“dumpå¦åˆ™å¾ˆéš¾è·å¾—ã€‚ private void onGCNotification(Notification notification) { if (GC_NOTIFICATION_TYPE.equals(notification.getType())) { GarbageCollectionNotificationInfo info = new GarbageCollectionNotificationInfo((CompositeData) notification.getUserData()); if (info.isMajorGc()) { if (shouldTriggerTaskKiller(info)) { //Kill task consuming most memory List\u003cSqlTask\u003e activeTasks = getActiveTasks(); ListMultimap\u003cQueryId, SqlTask\u003e activeQueriesToTasksMap = activeTasks.stream() .collect(toImmutableListMultimap(task -\u003e task.getQueryContext().getQueryId(), Function.identity())); Optional\u003cQueryId\u003e queryId = getMaxMemoryConsumingQuery(activeQueriesToTasksMap); if (queryId.isPresent()) { List\u003cSqlTask\u003e activeTasksToKill = activeQueriesToTasksMap.get(queryId.get()); for (SqlTask sqlTask : activeTasksToKill) { TaskStats taskStats = sqlTask.getTaskInfo().getStats(); sqlTask.failed(new PrestoException(EXCEEDED_HEAP_MEMORY_LIMIT, format(\"Worker heap memory limit exceeded: User Memory: %d, System Memory: %d, Revocable Memory: %d\", taskStats.getUserMemoryReservationInBytes(), taskStats.getSystemMemoryReservationInBytes(), taskStats.getRevocableMemoryReservationInBytes()))); } } } } } } ","date":"2026-01-14","objectID":"/posts/presto-high-memory-task-killer/:2:0","tags":["Presto","Trino"],"title":"Presto workeré«˜è´Ÿè½½æŸ¥æ€ä»»åŠ¡","uri":"/posts/presto-high-memory-task-killer/"},{"categories":null,"content":"TCP KeepAlive TCP KeepAliveæœºåˆ¶ç†è§£ä¸å®è·µå°ç»“ä¸­è¯¦ç»†ä»‹ç»äº†TCP KeepAliveçš„æœºåˆ¶ï¼Œè¿™é‡Œé‡ç‚¹æä¸€ä¸‹æ¶‰åŠåˆ°çš„å‚æ•°: tcp_keepalive_time åœ¨TCPä¿æ´»æ‰“å¼€çš„æƒ…å†µä¸‹ï¼Œæœ€åä¸€æ¬¡æ•°æ®äº¤æ¢åˆ°TCPå‘é€ç¬¬ä¸€ä¸ªä¿æ´»æ¢æµ‹åŒ…çš„é—´éš”ï¼Œå³å…è®¸çš„ç©ºé—²æ—¶é•¿ï¼Œé»˜è®¤ä¸º2h tcp_keepalive_probes æœ€å¤§å…è®¸å‘é€ä¿æ´»æ¢æµ‹åŒ…çš„æ¬¡æ•°ï¼Œè¾¾åˆ°æ­¤æ¬¡æ•°åç›´æ¥æ”¾å¼ƒå°è¯•ï¼Œå¹¶å…³é—­è¿æ¥ï¼Œé»˜è®¤ä¸º9æ¬¡ tcp_keepalive_intvl å‘é€ä¿æ´»æ¢æµ‹åŒ…çš„é—´éš”ï¼Œé»˜è®¤ä¸º75s æ‰€ä»¥å¦‚æœå¼€å¯äº†TCP KeepAliveå¹¶ä¿æŒé»˜è®¤å‚æ•°ï¼Œåˆ™ç©ºé—²è¿æ¥ä¼šåœ¨å¤§çº¦ 2h 11minä¹‹åè¢«æ–­å¼€ ","date":"2025-10-19","objectID":"/posts/grpc-keepalive/:1:0","tags":null,"title":"Grpc Keepalive","uri":"/posts/grpc-keepalive/"},{"categories":null,"content":"TCP_USER_TIMEOUT TCP_USER_TIMEOUTè¡¨ç¤ºç­‰å¾…ä¼ è¾“æ•°æ®æœªè¢«ackæˆ–è€…ç”±äºæ²¡æœ‰å‘é€çª—å£å¯¼è‡´æœªè¢«ä¼ è¾“çš„æ•°æ®çš„æœ€é•¿æ—¶é—´ï¼Œè¶…è¿‡è¯¥æ—¶é—´ï¼ŒTCPè¿æ¥ä¼šè¢«å¼ºåˆ¶å…³é—­ï¼Œå¹¶ä¸”è¿”å›ETIMEDOUTç»™åº”ç”¨ã€‚ å¦‚æœå°†TCP_USER_TIMEOUTå’Œ TCP KeepAliveåŒæ—¶ä½¿ç”¨ï¼Œæ ¹æ®cloudflareçš„æ–‡ç« When TCP sockets refuse to dieï¼Œtcp_keepalive_probesä¼šè¢«å¿½ç•¥ï¼Œæ¯æ¬¡å°è¯•å‘é€keepAliveæŠ¥æ–‡æ—¶ï¼Œéƒ½ä¼šæ£€æŸ¥æ˜¯å¦è¶…è¿‡äº†TCP_USER_TIMEOUTï¼Œå¦‚æœè¶…è¿‡åˆ™ç›´æ¥å…³é—­è¿æ¥ã€‚ ç±»ä¼¼çš„ï¼Œå¦‚æœå‘ç”Ÿè¶…æ—¶é‡ä¼ ï¼ŒTCP_USER_TIMEOUTå†³å®šäº†æœ€ç»ˆçš„è¶…æ—¶å…³é—­æ—¶é—´ï¼Œtcp_retries2ä¼šè¢«å¿½ç•¥ï¼Œå¯¹äºé›¶å‘é€çª—å£çš„è¿æ¥ï¼Œå‘é€ç«¯ä¼šå®šæœŸå‘é€çª—å£æ¢æµ‹æŠ¥æ–‡ï¼Œåœ¨å‘é€ä¹‹å‰ä¹‹åæ£€æŸ¥è¶…è¿‡äº†TCP_USER_TIMEOUTã€‚ ","date":"2025-10-19","objectID":"/posts/grpc-keepalive/:2:0","tags":null,"title":"Grpc Keepalive","uri":"/posts/grpc-keepalive/"},{"categories":null,"content":"TCPè¶…æ—¶é‡ä¼  tcpè¶…æ—¶é‡è¯•åŒ…å«å¾ˆå¤šæœºåˆ¶ï¼Œè¿™é‡Œåªè®¨è®ºæœ€åŸºç¡€çš„æƒ…å†µï¼ŒæŸä¸ªTCPæŠ¥æ–‡å‘é€åä¸€ç›´æ²¡æœ‰æ”¶åˆ°å“åº”å¯¼è‡´é‡ä¼ ï¼Œé‡ä¼ ä¼šé‡‡ç”¨æŒ‡æ•°é€€é¿çš„æ–¹å¼ï¼Œä¹Ÿå°±æ˜¯æ¯æ¬¡é‡ä¼ çš„é—´éš”éƒ½æ˜¯ä¸Šæ¬¡çš„2å€ï¼Œé‡ä¼ é—´éš”çš„ä¸‹é™ä¸º200msï¼Œä¸Šé™ä¸º120sï¼Œä¸»è¦æœ‰ä»¥ä¸‹å‚æ•°æ§åˆ¶é‡ä¼ è¿‡ç¨‹ï¼š /proc/sys/net/ipv4/tcp_retries1``ï¼Œé»˜è®¤ä¸º3ï¼Œåœ¨3æ¬¡é‡ä¼ åï¼Œä¼šå°è¯•é€šçŸ¥ipå±‚æ›´æ”¹è·¯ç”± /proc/sys/net/ipv4/tcp_retries2ï¼Œé»˜è®¤ä¸º15ï¼Œç”¨æ¥è®¡ç®—é‡ä¼ è¶…æ—¶æ—¶é—´ï¼Œè¶…è¿‡é‡ä¼ è¶…æ—¶æ—¶é—´ï¼Œå¤§çº¦å¯¹åº”13~30minï¼Œè¿æ¥ä¼šè¢«å…³é—­ tcp_syn_retriesï¼Œé»˜è®¤ä¸º6ï¼Œåˆå§‹SYNæŠ¥æ–‡è¢«é‡ä¼ çš„æœ€å¤§æ¬¡æ•°ï¼Œç”±äºRTOé»˜è®¤ä¸º1sï¼Œå¯¹åº”127så·¦å³çš„é‡ä¼ è€—æ—¶ tcp_syncack_retriesï¼Œé»˜è®¤ä¸º5ï¼Œåˆå§‹SYN/ACKæŠ¥æ–‡è¢«é‡ä¼ çš„æœ€å¤§æ¬¡æ•° è¿™é‡Œå¤šè§£é‡Šä¸€ä¸‹tcp_retriesçš„æ„ä¹‰ï¼Œæ ¹æ® Linux TCP_RTO_MIN, TCP_RTO_MAX and the tcp_retries2 sysctlï¼Œåˆå§‹çš„é‡è¯•æ—¶é—´é—´éš”å°±æ˜¯RTO (Retransmission TimeOut)ï¼Œå¯¹äºæ­£åœ¨å»ºç«‹çš„è¿æ¥( TCP Retransmission May Be Misleading (2023) )ï¼ŒRTOçš„åˆå§‹å€¼ä¸º1sï¼Œå¯¹äºå·²ç»å»ºç«‹å®Œæˆçš„è¿æ¥ï¼ŒRTOä¼šé€šè¿‡RTTçš„å†å²ä¿¡æ¯è®¡ç®—å¾—åˆ°ï¼Œå¯¹äºå†…ç½‘ç¯å¢ƒï¼Œå¯ä»¥è®¤ä¸ºåˆå§‹çš„RTOä¸º200msï¼Œæœ€ç»ˆé‡ä¼ è¶…æ—¶æ€»å…±éœ€è¦15minå·¦å³ï¼Œå¦‚æœç½‘ç»œç¯å¢ƒå¾ˆå·®ï¼Œæœ€é«˜å¯èƒ½éœ€è¦30minå·¦å³ã€‚æ˜¾ç„¶å¯¹äºå®¢æˆ·ç«¯è€Œè¨€ï¼Œè¿™ä¸ªè¶…æ—¶æ—¶é—´å¤ªé•¿äº†ï¼Œå¦‚æœæŒ‡å®šTCP_USER_TIMEOUTåˆ™å¯ä»¥åœ¨æ›´çŸ­çš„æ—¶é—´å†…æ£€æµ‹åˆ°ç½‘ç»œä¸å¯è¾¾ï¼Œä»è€Œå¿«é€Ÿå…³é—­è¿æ¥å¹¶é‡è¿ã€‚ todo: https://codearcana.com/posts/2015/08/28/tcp-keepalive-is-a-lie.html https://groups.google.com/g/grpc-io/c/6VZYCFZpyTI https://github.com/grpc/grpc-java/issues/11517 https://github.com/apache/brpc/issues/1154 https://lukexng.medium.com/grpc-keepalive-maxconnectionage-maxconnectionagegrace-6352909c57b8 https://pandaychen.github.io/2020/09/01/GRPC-CLIENT-CONN-LASTING/ ","date":"2025-10-19","objectID":"/posts/grpc-keepalive/:3:0","tags":null,"title":"Grpc Keepalive","uri":"/posts/grpc-keepalive/"},{"categories":null,"content":" æ˜¯çš„ï¼Œæœ‰ä¸å°‘ä¼˜ç§€çš„åšå®¢ï¼æŠ€æœ¯æ–‡ç« æ¢è®¨äº†åœ¨ gRPC / TCP é•¿è¿æ¥ä¸­ï¼Œåº”å¯¹â€œéšè”½æ–­å¼€â€ï¼ˆsilent dropï¼‰ã€â€œåƒµå°¸è¿æ¥â€ï¼ˆzombie connectionï¼‰é—®é¢˜çš„æ–¹æ¡ˆã€‚ä¸‹é¢åˆ—å‡ ä¸ªç»å…¸ä¸å®ç”¨çš„ï¼Œå¹¶ä¸”æˆ‘è¿˜ä¼šæ‘˜å…³é”®è¦ç‚¹ä¾›ä½ å¿«é€Ÿå¯¹æ¯”ã€‚ ä½ ä¹Ÿå¯ä»¥æ ¹æ®ä½ ç”¨çš„è¯­è¨€ / æ¡†æ¶ï¼ˆGo / Java / C++ / .NET ç­‰ï¼‰å»æ‰¾ç›¸åº”çš„å®è·µç‰ˆæœ¬ã€‚ ","date":"2025-10-12","objectID":"/posts/grpc-client-connection/:0:0","tags":null,"title":"Grpc Client Connection","uri":"/posts/grpc-client-connection/"},{"categories":null,"content":"ğŸ“š æ¨èæ–‡ç« ä¸åšå®¢ æ ‡é¢˜ / é“¾æ¥ æ ¸å¿ƒå†…å®¹ / äº®ç‚¹ é€‚åˆè¯»è€… /ç”¨é€” How gRPC Keepalive Solved Our Zombie Connections è®²è¿°ä¸€ä¸ªçœŸå®æ¡ˆä¾‹ï¼šä»–ä»¬å› ä¸ºç½‘ç»œé»‘æ´ï¼ˆèŠ‚ç‚¹æ–­ç½‘ï¼‰å¯¼è‡´ gRPC å®¢æˆ·ç«¯ä¸€ç›´è®¤ä¸ºè¿æ¥å¯ç”¨ï¼Œæœ€ç»ˆé‡‡ç”¨ gRPC Keepalive æœºåˆ¶å°†æ•…éšœæ£€æµ‹æ—¶é—´ä»åˆ†é’Ÿçº§é™åˆ°ç§’çº§ã€‚ (Medium) é€‚åˆç†è§£ä¸ºä½•éœ€è¦ â€œåº”ç”¨å±‚ keepaliveâ€ Lessons learned from running a large gRPC mesh at Datadog åœ¨å¤§è§„æ¨¡ gRPC Mesh ä¸­çš„ç»éªŒæ€»ç»“ï¼Œå…¶ä¸­è®²åˆ°è¦å¼€å¯ keepalive é€šé“é€‰é¡¹ã€è®¾ç½® MAX_CONNECTION_AGEã€ç›‘æ§ silent connection drop ç­‰ç»†èŠ‚ã€‚ (Datadog) é€‚åˆæ¶æ„çº§è®¾è®¡å‚è€ƒ Why TCP keepalive may be important ä»æ›´åº•å±‚çš„ TCP å±‚é¢è®²ä¸ºä»€ä¹ˆè¦ç”¨ keepaliveï¼Œé»˜è®¤å…³é—­çš„é—®é¢˜ï¼Œä»¥åŠä½¿ç”¨æ—¶çš„é…ç½®è€ƒè™‘ã€‚ (redpill-linpro.com) ç”¨äºç†è§£ TCP æœ¬èº«çš„è¡¥å……æœºåˆ¶ TCP Keepalive is a lie è­¦ç¤ºå¼æ–‡ç« ï¼ŒæŒ‡å‡ºå¯ç”¨ TCP keepalive åä¹Ÿä¸ä¸€å®šèƒ½å¿«é€Ÿæ„ŸçŸ¥è¿æ¥æ–­å¼€çš„å±€é™æ€§ã€‚ (Code Arcana) ç”¨äºç†è§£ â€œkeepalive ä¹Ÿæœ‰ä¸èƒ½è§£å†³çš„é—®é¢˜â€ When TCP sockets refuse to die Cloudflare çš„æ–‡ç« ï¼Œé™¤äº†è®² keepalive ä¹‹å¤–ï¼Œè¿˜æ¨èä½¿ç”¨ TCP_USER_TIMEOUT ç­‰å¥—æ¥å­—é€‰é¡¹æ£€æµ‹ä¸æ´»è·ƒ / å¡ä½çš„è¿æ¥ã€‚ (The Cloudflare Blog) å¯¹åº•å±‚ç½‘ç»œ / æ“ä½œç³»ç»Ÿæ„Ÿå…´è¶£çš„å¼€å‘è€… Need to be careful when you using gRPC keepalive ä¸€ç¯‡ medium åšæ–‡ï¼Œæé†’ä½¿ç”¨ keepalive éœ€è°¨æ…é…ç½®ï¼Œæ¯”å¦‚ä¸èƒ½æŠŠ keepalive æ—¶é—´è®¾å¾—å¤ªæ¿€è¿›ï¼Œå®¹æ˜“è§¦å‘æœåŠ¡å™¨ç«¯é™åˆ¶ã€‚ (Medium) é€‚åˆå®é™…è°ƒä¼˜é˜¶æ®µå‚è€ƒ gRPC is easy to misconfigure æ¢è®¨ä¸€äº›å¸¸è§ gRPC keepalive çš„è¯¯ç”¨ï¼Œä»¥åŠ gRPC é»˜è®¤è®¾ç½®èƒŒåçš„â€œé™·é˜±â€ (æ¯”å¦‚ TCP_USER_TIMEOUT è¢«å¯ç”¨) (Evan Jones) é€‚åˆä¸­çº§ / è¿›é˜¶å¼€å‘è€…ç†è§£å‘ç‚¹ ","date":"2025-10-12","objectID":"/posts/grpc-client-connection/:1:0","tags":null,"title":"Grpc Client Connection","uri":"/posts/grpc-client-connection/"},{"categories":null,"content":"ğŸ” å…³é”®è¦ç‚¹ \u0026 æ€»ç»“ ä»è¿™äº›æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æç‚¼å‡ºä¸€äº›è¡Œä¹‹æœ‰æ•ˆçš„ç­–ç•¥ï¼Œä»¥åŠéœ€è¦æ³¨æ„çš„å‘ã€‚ä¸‹é¢æ˜¯æˆ‘ç»™ä½ çš„æ€»ç»“ã€å¯¹æ¯”ä¸å»ºè®®ï¼š æ–¹æ¡ˆ / æŠ€æœ¯ ä¼˜ç‚¹ å±€é™ / é£é™© å®è·µå»ºè®® gRPC HTTP/2 Keepaliveï¼ˆPINGï¼‰ åœ¨æ²¡æœ‰æµé‡æ—¶ä¸»åŠ¨æ¢æµ‹å¯¹ç«¯æ˜¯å¦å­˜æ´»ï¼›ä¸€æ—¦ PING è¶…æ—¶åˆ™æ–­å¼€é‡è¿ã€‚Datadog åœ¨å…¶ gRPC Mesh ä¸­å°±ç”¨è¿™ä¸ªæ¥ç¼“è§£ silent drop é—®é¢˜ã€‚ (Datadog) å¦‚æœé…ç½®ä¸å½“ï¼ŒPING å¤ªé¢‘ç¹å¯èƒ½è¢«æœåŠ¡å™¨è§†ä¸º Abuseï¼ˆè¿‡å¤š PINGï¼‰è€Œæ–­å¼€è¿æ¥ï¼›ä¹Ÿè¦ä¸æœåŠ¡ç«¯çš„æœ€å° PING é—´éš”ç­–ç•¥åŒ¹é…ã€‚ (Datadog) ç»™å®¢æˆ·ç«¯é€šé“è®¾ç½® keepalive æ—¶é—´ \u0026 è¶…æ—¶ï¼ˆtimeoutï¼‰ï¼Œä¸è¦è®¾å¾—å¤ªå°ã€‚ è®¾ç½® TCP_USER_TIMEOUT å¯ä½¿æ“ä½œç³»ç»Ÿåœ¨å†™å…¥æ•°æ®é•¿æ—¶é—´æœªè¢« ACK æ—¶ä¸»åŠ¨æ–­å¼€ï¼Œä»è€ŒåŠæ—¶è®©ä¸Šå±‚æ„ŸçŸ¥å¼‚å¸¸ã€‚Cloudflare æå€¡æ­¤åŠæ³•ã€‚ (The Cloudflare Blog) æŸäº›æ“ä½œç³»ç»Ÿ / æ ˆå¯èƒ½ä¸æ”¯æŒ / å¯¹è¯¥é€‰é¡¹å…¼å®¹æ€§æœ‰é™ã€‚éœ€è¦ç¡®ä¿ä½ çš„å¹³å°æ”¯æŒã€‚ é…åˆ keepalive ä½¿ç”¨ï¼Œä½¿ â€œæ¢æµ‹å¤±è´¥â€ æ›´å¿«åœ°è¢« TCP æ ˆæ•æ‰ã€‚ å®šæœŸé‡å»ºè¿æ¥ï¼ˆå‰”é™¤è€è¿æ¥ / å¼ºåˆ¶é‡è¿ï¼‰ å³ä½¿è¿æ¥çœ‹ä¼¼æ­£å¸¸ï¼Œä¹Ÿå¯ä»¥å®šæœŸï¼ˆä¾‹å¦‚æ¯ N åˆ†é’Ÿï¼‰ä¸»åŠ¨å…³é—­æ—§è¿æ¥ï¼Œé‡æ–°åš DNS è§£ææˆ– LB é€‰è·¯ã€‚Datadog å°±ç”¨ MAX_CONNECTION_AGEã€‚ (Datadog) ä¼šå¼•å…¥çŸ­æš‚çš„è¿æ¥é‡å»ºå¼€é”€å’Œå¯èƒ½çš„æŠ–åŠ¨ï¼›éœ€è¦å¹³è¡¡é‡è¿é¢‘ç‡ä¸ç¨³å®šæ€§ã€‚ åœ¨æœåŠ¡ç«¯ / å®¢æˆ·ç«¯éƒ½æ”¯æŒçš„æƒ…å†µä¸‹åŠ ä¸Šè¯¥ç­–ç•¥ã€‚ ç½‘ç»œå±‚ / é“¾è·¯å±‚æ£€æµ‹æœºåˆ¶ï¼ˆBFD ç­‰ï¼‰ åœ¨ç½‘ç»œè®¾å¤‡å±‚é¢ï¼ˆäº¤æ¢æœº / è·¯ç”±å™¨ï¼‰å¯ä»¥æ›´å¿«æ£€æµ‹é“¾è·¯æ•…éšœã€‚BFD æ˜¯ä¸€ä¸ªå…¸å‹åè®®ã€‚ (ç»´åŸºç™¾ç§‘) è¿™ä¸ªæ–¹æ¡ˆé€šå¸¸éœ€è¦ç½‘ç»œè®¾å¤‡æ”¯æŒã€é…ç½®èƒ½åŠ›å¼ºï¼›å¯¹äºç«¯åˆ°ç«¯ç”¨æˆ·å±‚ç¨‹åºæ§åˆ¶èƒ½åŠ›æœ‰é™ã€‚ å¦‚æœä½ åœ¨å¯æ§ç½‘ç»œç¯å¢ƒï¼ˆæ•°æ®ä¸­å¿ƒ /ä¸“æœ‰ç½‘ç»œï¼‰ä¸­ï¼Œå¯ä»¥è€ƒè™‘é…åˆ BFD ç­‰é“¾è·¯çº§æ£€æµ‹ã€‚ ç›‘æ§ \u0026 å‘Šè­¦ / å¯è§†åŒ–æ£€æŸ¥ å³ä½¿ä½ æœ‰ keepalive / æ¢æµ‹ï¼Œä¹Ÿè¦ç›‘æ§ socket çŠ¶æ€ï¼ˆæœª ACK çš„å­—èŠ‚ã€é‡è¯•æ¬¡æ•°ç­‰ï¼‰ï¼Œä»¥ä¾¿åŠæ—¶é¢„è­¦ã€‚Datadog ç”¨ ss å‘½ä»¤æŸ¥çœ‹ TCP æœª ACK å­—èŠ‚ã€‚ (Datadog) ç›‘æ§æœ¬èº«æœ‰å¼€é”€ï¼›å¦‚æœç›‘æ§é€»è¾‘æœ‰è¯¯æˆ–æ¼æ£€æµ‹ï¼Œå¯èƒ½ä¸å¤ŸåŠæ—¶ã€‚ å®šæœŸæŠŠè¿™äº›æŒ‡æ ‡çº³å…¥ä½ çš„ç›‘æ§çœ‹æ¿ï¼ˆå¦‚ â€œsocket é‡ä¼ æ¬¡æ•° / é˜Ÿåˆ—é•¿åº¦ / é‡è¯•è¶…æ—¶æ¬¡æ•°â€ï¼‰ å¦‚æœä½ æ„¿æ„çš„è¯ï¼Œæˆ‘å¯ä»¥æ ¹æ®ä½ ç”¨çš„ è¯­è¨€ + æ¡†æ¶ï¼ˆä¾‹å¦‚ Go gRPC / Java gRPC / gRPC-C++ï¼‰ï¼Œç»™ä½ ä¸€ä¸ª â€œåœ¨ä½ çš„æŠ€æœ¯æ ˆé‡Œæœ€ä¼˜çš„è¿æ¥å¥å£®æ€§æ–¹æ¡ˆ + ç¤ºä¾‹é…ç½® + æ¨èåšå®¢â€ çš„æ•´ç†ã€‚è¦å—ï¼Ÿ æ˜¯çš„ï¼Œæœ‰ä¸å°‘åšå®¢ / æŠ€æœ¯æ–‡ç« æˆ–è€…å®˜æ–¹æ–‡æ¡£è®¨è®ºäº† gRPC channel çŠ¶æ€ã€UNAVAILABLE é—®é¢˜ã€è¿æ¥è¯­ä¹‰ (connectivity semantics) å’Œå¦‚ä½•ç›‘æ§é€šé“çŠ¶æ€ã€‚ä¸‹é¢åˆ—å‡ ä¸ªæ¯”è¾ƒæœ‰ä»£è¡¨æ€§çš„ï¼Œä»¥åŠä½ å¯ä»¥è¿›ä¸€æ­¥é˜…è¯»çš„æ–¹å‘ï¼š ","date":"2025-10-12","objectID":"/posts/grpc-client-connection/:2:0","tags":null,"title":"Grpc Client Connection","uri":"/posts/grpc-client-connection/"},{"categories":null,"content":"ğŸ“š æ¨èçš„åšå®¢ / æ–‡ç«  / æŠ€æœ¯èµ„æº åç§° å†…å®¹æ¦‚è¦ ä»·å€¼ / ä¾§é‡ç‚¹ A short introduction to Channelz ä»‹ç» gRPC çš„ channelz è°ƒè¯•å·¥å…·ï¼Œå¦‚ä½•ç”¨å®ƒè§‚å¯Ÿè¿æ¥ã€å­é€šé“ï¼ˆsubchannelsï¼‰ã€æµç­‰è¿è¡Œæ—¶æŒ‡æ ‡ã€‚ (gRPC) å¦‚æœä½ æƒ³åœ¨ç”Ÿäº§ç¯å¢ƒé‡ŒåŠ¨æ€ç›‘æ§é€šé“çŠ¶æ€ï¼Œè¿™ç¯‡æ˜¯å¾ˆå¥½çš„å…¥é—¨ gRPC Connectivity Semantics and API å®˜æ–¹æ–‡æ¡£ï¼Œè¯¦ç»†æè¿° gRPC channel çš„çŠ¶æ€æœºï¼ˆCONNECTING, READY, TRANSIENT_FAILURE, IDLE, SHUTDOWN ç­‰ï¼‰ä»¥åŠçŠ¶æ€å˜è¿çš„è¯­ä¹‰ã€‚ (grpc.github.io) æœ€åŸºç¡€ã€æƒå¨çš„è¯­ä¹‰è¯´æ˜ Lessons learned from running a large gRPC mesh at Datadog Datadog åœ¨å¤§è§„æ¨¡ gRPC ç½‘ç»œä¸­çš„ç»éªŒåˆ†äº«ï¼ŒåŒ…æ‹¬è¿æ¥ç®¡ç†ã€å¤±è”æ¢å¤ã€keepaliveã€è´Ÿè½½å‡è¡¡ç­–ç•¥ç­‰ã€‚ (Datadog) å®æˆ˜å±‚é¢çš„æ€è€ƒï¼Œå¯ä»¥å€Ÿé‰´åœ¨å·¥ç¨‹ç³»ç»Ÿä¸­çš„åšæ³• Keep Python gRPC Client Connection Truly Aliveï¼ˆJeff Li çš„åšå®¢ï¼‰ è®²è¿°åœ¨ Python å®¢æˆ·ç«¯é‡Œ channel è¢«åƒåœ¾å›æ”¶æˆ–é•¿æ—¶é—´ç©ºé—²å¯¼è‡´ â€œUNAVAILABLEâ€ é”™è¯¯çš„æ¡ˆä¾‹ï¼Œä»¥åŠå¦‚ä½•ä¿æŒ channel å­˜æ´»ã€‚ (bluesalt.github.io) å¯¹äº Python å®¢æˆ·ç«¯ç‰¹åˆ«å®ç”¨ Trying to connect an http1.x server â€” gRPC error åˆ†æ â€œUNAVAILABLE: Trying to connect an http1.x serverâ€ é”™è¯¯ï¼ŒæŒ‡å‡ºå¯èƒ½æ˜¯å› ä¸ºé€šè¿‡ HTTP ä»£ç†æˆ–æœåŠ¡å™¨ä¸æ”¯æŒ HTTP/2 å¯¼è‡´çš„å…¼å®¹é—®é¢˜ã€‚ (putridparrot.com) åœ¨å¤æ‚ç½‘ç»œ / ä»£ç† / åè®®ç¯å¢ƒä¸‹å¯èƒ½é‡åˆ°è¿™ç§é”™è¯¯ï¼Œè¿™ç¯‡èƒ½æä¾›è¯Šæ–­æ–¹å‘ Channel State Does Not Changeï¼ˆgrpc-node issueï¼‰ è™½ç„¶æ˜¯ issue è€Œä¸æ˜¯ä¼ ç»Ÿåšå®¢ï¼Œä½†è®¨è®ºå½“ç½‘ç»œæ–­å¼€æ—¶ channel çŠ¶æ€ä¸æ›´æ–°çš„é—®é¢˜ï¼Œä¹Ÿå°±æ˜¯çŠ¶æ€æ£€æµ‹çš„å±€é™æ€§ã€‚ (GitHub) æé†’ä½ â€œçŠ¶æ€â€å¹¶ä¸æ€»æ˜¯èƒ½å³æ—¶åæ˜ çœŸå®ç½‘ç»œæ–­å¼€ å¦‚æœä½ æ„¿æ„çš„è¯ï¼Œæˆ‘å¯ä»¥æŒ‘å‡ ç¯‡æ¯”è¾ƒè´´è¿‘ä½ ä½¿ç”¨çš„è¯­è¨€ / æ¡†æ¶ï¼ˆJava, Go, Python, etc.ï¼‰çš„åšå®¢ç»™ä½ ã€‚è¦å—ï¼Ÿ ","date":"2025-10-12","objectID":"/posts/grpc-client-connection/:3:0","tags":null,"title":"Grpc Client Connection","uri":"/posts/grpc-client-connection/"},{"categories":["Java"],"content":"èƒŒæ™¯çŸ¥è¯† åœ¨JDK 1.2ä¹‹åï¼ŒJavaå°†å¼•ç”¨åˆ†ä¸ºå¼ºå¼•ç”¨ï¼ˆStrongly Referenceï¼‰ã€è½¯å¼•ç”¨ï¼ˆSoft Referenceï¼‰ã€å¼±å¼•ç”¨ï¼ˆWeak Referenceï¼‰å’Œè™šå¼•ç”¨ï¼ˆPhanom Referenceï¼‰ï¼Œè¿™4ç§å¼•ç”¨å¼ºåº¦ä¾æ¬¡é€æ¸å‡å¼±ã€‚ å¼ºå¼•ç”¨æ˜¯æœ€ä¼ ç»Ÿçš„å¼•ç”¨å®šä¹‰ï¼Œæ˜¯æŒ‡åœ¨ç¨‹åºä»£ç ä¸­æ™®éå­˜åœ¨çš„å¼•ç”¨èµ‹å€¼ï¼Œæ— è®ºä»»ä½•æƒ…å†µä¸‹ï¼Œåªè¦å¼ºå¼•ç”¨å…³ç³»è¿˜å­˜åœ¨ï¼Œåƒåœ¾æ”¶é›†å™¨å°±æ°¸è¿œä¸ä¼šå›æ”¶æ‰è¢«å¼•ç”¨çš„å¯¹è±¡ è½¯å¼•ç”¨æ˜¯ç”¨æ¥æè¿°ä¸€äº›è¿˜æœ‰ç”¨ã€ä½†éå¿…é¡»çš„å¯¹è±¡ï¼Œåªè¢«è½¯å¼•ç”¨å…³ç³»ç€çš„å¯¹è±¡ï¼Œåœ¨ç³»ç»Ÿå°†è¦å‘ç”Ÿå†…å­˜æº¢å‡ºå¼‚å¸¸å‰ï¼Œä¼šæŠŠè¿™äº›å¯¹è±¡åˆ—è¿›å›æ”¶èŒƒå›´ä¸­è¿›è¡Œç¬¬äºŒæ¬¡å›æ”¶ï¼Œå¦‚æœè¿™æ¬¡å›æ”¶è¿˜æ²¡æœ‰è¶³å¤Ÿçš„å†…å­˜ï¼Œæ‰ä¼šæŠ›å‡ºå†…å­˜æº¢å‡ºå¼‚å¸¸ å¼±å¼•ç”¨ä¹Ÿæ˜¯ç”¨æ¥æè¿°é‚£äº›éå¿…é¡»å¯¹è±¡ï¼Œä½†æ˜¯å®ƒçš„å¼ºåº¦æ¯”è½¯å¼•ç”¨æ›´å¼±ä¸€äº›ï¼Œè¢«å¼±å¼•ç”¨å…³è”çš„å¯¹è±¡åªèƒ½ç”Ÿå­˜åˆ°ä¸‹ä¸€æ¬¡åƒåœ¾æ”¶é›†å‘ç”Ÿä¸ºæ­¢ã€‚å½“åƒåœ¾æ”¶é›†å™¨å¼€å§‹å·¥ä½œï¼Œæ— è®ºå½“å‰å†…å­˜æ˜¯å¦è¶³å¤Ÿï¼Œéƒ½ä¼šå›æ”¶æ‰åªè¢«å¼±å¼•ç”¨å…³è”çš„å¯¹è±¡ è™šå¼•ç”¨ä¹Ÿç§°ä¸ºå¹½çµå¼•ç”¨æˆ–è€…å¹»å½±å¼•ç”¨ï¼Œå®ƒæ˜¯æœ€å¼±çš„ä¸€ç§å¼•ç”¨å…³ç³»ï¼Œä¸€ä¸ªå¯¹è±¡æ˜¯å¦æœ‰è™šå¼•ç”¨çš„å­˜åœ¨å®Œå…¨ä¸ä¼šå¯¹å…¶ç”Ÿå­˜æ—¶é—´æ„æˆå½±å“ï¼Œä¹Ÿæ— æ³•é€šè¿‡è™šå¼•ç”¨æ¥å–å¾—ä¸€ä¸ªå¯¹è±¡å®ä¾‹ã€‚ä¸ºä¸€ä¸ªå¯¹è±¡è®¾ç½®è™šå¼•ç”¨å…³è”çš„å”¯ä¸€ç›®çš„åªæ˜¯ä¸ºäº†èƒ½åœ¨è¿™ä¸ªå¯¹è±¡è¢«æ”¶é›†å™¨å›æ”¶æ—¶æ”¶åˆ°ä¸€ä¸ªç³»ç»Ÿé€šçŸ¥ ","date":"2025-09-07","objectID":"/posts/direct-memory-in-practice/:1:0","tags":["Java"],"title":"ç›´æ¥å†…å­˜å®æˆ˜","uri":"/posts/direct-memory-in-practice/"},{"categories":["Java"],"content":"å‚è€ƒæ–‡çŒ® æ·±å…¥ç†è§£Javaè™šæ‹Ÿæœº ","date":"2025-09-07","objectID":"/posts/direct-memory-in-practice/:2:0","tags":["Java"],"title":"ç›´æ¥å†…å­˜å®æˆ˜","uri":"/posts/direct-memory-in-practice/"},{"categories":["gRPC"],"content":"Metadataæ˜¯å…³äºæŸä¸ªç‰¹å®šRPCè°ƒç”¨çš„ä¿¡æ¯ï¼Œgrpcå…ƒæ•°æ®é€šè¿‡HTTP/2 headerå®ç°ï¼Œå®ƒä»¥é”®å€¼å¯¹çš„å½¢å¼å­˜åœ¨ï¼Œå…¶ä¸­é”®æ˜¯å­—ç¬¦ä¸²ï¼Œå€¼é€šå¸¸æ˜¯å­—ç¬¦ä¸²ï¼Œä½†ä¹Ÿå¯ä»¥æ˜¯äºŒè¿›åˆ¶æ•°æ®ã€‚é”®ä¸åŒºåˆ†å¤§å°å†™ï¼Œç”±ASCIIå­—æ¯ã€æ•°å­—å’Œç‰¹æ®Šå­—ç¬¦-ã€_ã€.ç»„æˆï¼Œä½†ä¸èƒ½ä»¥grpc-å¼€å¤´ï¼ˆè¯¥å‰ç¼€ä¿ç•™ç»™gRPCä½¿ç”¨ï¼‰ï¼ŒäºŒè¿›åˆ¶æ•°æ®å¯¹åº”çš„keyä»¥-binç»“å°¾ï¼Œè€ŒASCIIç±»å‹çš„é”®ä¸å¸¦-binåç¼€ã€‚ grpcçš„metadataå¯ä»¥ç”±å®¢æˆ·ç«¯å’ŒæœåŠ¡ç«¯åŒæ–¹å‘é€å’Œæ¥æ”¶ï¼ŒHeadersï¼ˆé¦–éƒ¨ä¿¡æ¯ï¼‰ï¼šåœ¨RPCè°ƒç”¨ä¸­ï¼Œå®¢æˆ·ç«¯åœ¨å‘é€åˆå§‹è¯·æ±‚ä¹‹å‰ä¼šå°†headerså‘é€ç»™æœåŠ¡ç«¯ï¼Œè€ŒæœåŠ¡ç«¯åœ¨å‘é€åˆå§‹å“åº”ä¹‹å‰ï¼Œä¹Ÿä¼šå°†headerså‘é€ç»™å®¢æˆ·ç«¯ã€‚Trailersï¼ˆå°¾éƒ¨ä¿¡æ¯ï¼‰ï¼šå½“æœåŠ¡ç«¯å…³é—­æŸä¸ªRPCæ—¶ï¼Œä¼šå‘é€trailersç»™å®¢æˆ·ç«¯ã€‚Headersæ˜¯RPCå¼€å§‹æ—¶çš„metadataï¼Œç±»ä¼¼äºHTTPè¯·æ±‚ã€å“åº”çš„å¤´éƒ¨ï¼ŒTrailersæ˜¯RPCç»“æŸæ—¶çš„metadataï¼Œå¸¸ç”¨äºä¼ é€’çŠ¶æ€ç æˆ–é”™è¯¯ä¿¡æ¯ã€‚grpcä½¿ç”¨HTTP/2ä½œä¸ºåº•å±‚åè®®ï¼Œè¿™ç§æœºåˆ¶å€Ÿç”¨äº†HTTP/2çš„header/traileræ¦‚å¿µã€‚ ç”¨æˆ·è‡ªå®šä¹‰çš„metadataä¸ä¼šè¢«gRPCä½¿ç”¨ï¼Œè¿™å…è®¸å®¢æˆ·ç«¯å‘æœåŠ¡ç«¯æä¾›ä¸è°ƒç”¨æœ‰å…³çš„ä¿¡æ¯ï¼Œåä¹‹äº¦ç„¶ã€‚å¯¹metadataçš„è®¿é—®æ–¹å¼å–å†³äºå…·ä½“çš„ç¼–ç¨‹è¯­è¨€ã€‚ å…ƒæ•°æ®çš„ä½¿ç”¨åœºæ™¯ï¼š éªŒè¯è¯ä¹¦ è·Ÿè¸ªä¿¡æ¯ï¼ˆtracingï¼‰ è‡ªå®šä¹‰headerï¼ˆè´Ÿè½½å‡è¡¡ã€é€Ÿç‡æ§åˆ¶ã€é”™è¯¯ä¿¡æ¯ï¼‰ å…¶ä»–åº”ç”¨ç›¸å…³çš„ä¿¡æ¯ï¼Œä¾èµ–äºåœ¨rpcè´Ÿè½½ä¹‹å‰æˆ–è€…ä¹‹åå‘é€çš„æ•°æ® å…ƒæ•°æ®çš„é™åˆ¶ï¼š éæµé‡æ§åˆ¶ï¼Œè¦æ±‚ç”¨æˆ·è‡ªå®šä¹‰çš„é™åˆ¶ï¼ŒGRPC_ARG_MAX_METADATA_SIZEæ§åˆ¶å•ä¸ªè¯·æ±‚è¢«å…è®¸çš„å…ƒæ•°æ®å¤§å°ï¼Œå¦‚æœè¯·æ±‚çš„å…ƒæ•°æ®å¤§å°è¶…è¿‡é™åˆ¶ï¼Œè¯·æ±‚ä¼šè¢«æ‹’ç» **ä¸ä¸€è‡´çš„metadataé™åˆ¶:**è¯·æ±‚çš„å…ƒæ•°æ®å¤§å°å°äºæœåŠ¡å™¨çš„é™åˆ¶ï¼Œä½†å´è¢«æ‹’ç»ï¼Œæä¾›äº†metadata limit exceededçš„é”™è¯¯ä¿¡æ¯ã€‚ä»£ç†é…ç½®äº†ä¸€ä¸ªæ›´å°çš„å…ƒæ•°æ®é™åˆ¶ï¼Œæ‰€ä»¥å¯¼è‡´è¯·æ±‚å‹æ ¹æ²¡æœ‰åˆ°æœåŠ¡å™¨ï¼Œç›´æ¥è¢«æ‹’ç»ã€‚é€šè¿‡ä¸ºæ‰€æœ‰çš„ä»£ç†é…ç½®å…¨å±€çš„å…ƒæ•°æ®é™åˆ¶ï¼Œå®¢æˆ·ç«¯å’ŒæœåŠ¡ç«¯éƒ½ä¸åº”è¯¥è¶…è¿‡è¿™ä¸ªé™åˆ¶ï¼Œå¦‚æœå®ƒä»¬æœŸæœ›è¯·æ±‚èƒ½å¤Ÿé€šè¿‡è¿™äº›ä»£ç†ã€‚ å¦å¤–ï¼Œå¦‚æœä»£ç†æ–°å¢ä¸€äº›å…ƒæ•°æ®ï¼Œä¹Ÿå¯èƒ½å¯¼è‡´æœ€ç»ˆåˆ°è¾¾æœåŠ¡å™¨çš„è¯·æ±‚å…ƒæ•°æ®è¶…é™ï¼Œå¯¼è‡´è¯·æ±‚è¢«æ‹’ç»ã€‚ **å…¨éƒ¨è¯·æ±‚è¢«æ‹’ç»ï¼š**å¦‚æœæ–°çš„metadataå…ƒç´ è¢«æ·»åŠ åˆ°è¯·æ±‚ï¼Œæ°å¥½è¶…è¿‡äº†å¯¹ç«¯å…è®¸çš„metadataå¤§å°é™åˆ¶ï¼Œè¯·æ±‚ä¼šè¢«æ‹’ç»ï¼Œå¦‚æœæŸä¸ªå®¢æˆ·ç«¯çš„æ‰€æœ‰è¯·æ±‚éƒ½å‘ç”Ÿäº†è¿™ç§æƒ…å†µï¼Œåˆ™æ‰€æœ‰è¯·æ±‚éƒ½ä¼šè¢«æ‹’ç»ï¼Œåœ¨æœ€åæƒ…å†µä¸‹å¯¼è‡´ç³»ç»Ÿå®Œå…¨ä¸å¯ç”¨ã€‚ å¼•å…¥ä¸¤å±‚é™åˆ¶ï¼Œè½¯é™åˆ¶å’Œç¡¬é™åˆ¶ï¼Œåœ¨ä¸¤ä¸ªé™åˆ¶ä¹‹é—´éšæœºæ‹’ç»ä¸€äº›è¯·æ±‚ï¼Œéšç€å…ƒæ•°æ®å¤§å°çš„å¢åŠ è¯·æ±‚è¢«æ‹’ç»çš„æ¦‚ç‡æå‡ï¼Œä»è€Œè®©å®¢æˆ·æœ‰æœºä¼šæ³¨æ„åˆ°å¢é•¿çš„å…ƒæ•°æ®è¶…é™æŠ¥é”™ï¼Œé¿å…å®Œå…¨ä¸å¯ç”¨ã€‚ è¦ç‚¹ï¼š ä¿è¯å…ƒæ•°æ®è½¯é™åˆ¶å’Œç¡¬é™åˆ¶çš„ä¸€è‡´æ€§ï¼Œå¹¶ä¸”æ„è¯†åˆ°å®¢æˆ·ç«¯å’ŒæœåŠ¡ç«¯ä¹‹é—´å…ƒæ•°æ®çš„é™åˆ¶ï¼ŒåŒ…æ‹¬ä»£ç† ç›‘æ§å¢é•¿çš„RESOURCE_EXHAUSTEDé”™è¯¯ï¼Œå¯èƒ½è¡¨ç¤ºå…ƒæ•°æ®è¶…é™ RESOURCE_EXHAUSTED: received metadata size exceeds soft limit (15000 vs 8192); :path:50B :authority:100B ä½¿ç”¨GRPC_ARG_MAX_METADATA_SIZEè®¾ç½®è½¯é™åˆ¶ï¼ˆé»˜è®¤8KBï¼‰ï¼ŒGRPC_ARG_ABSOLUTE_MAX_METADATA_SIZEè®¾ç½®ç¡¬é™åˆ¶ï¼ˆé»˜è®¤16KBï¼‰ ","date":"2025-08-02","objectID":"/posts/grpc-metadata/:0:0","tags":["gRPC"],"title":"gRPCå…ƒæ•°æ®","uri":"/posts/grpc-metadata/"},{"categories":["FoundationDB"],"content":"macä¸Šè¿è¡ŒFoundationDB fdb_javaçš„javaåŒ…æ²¡æœ‰æ‰“åŒ…fdb_cçš„åŠ¨æ€é“¾æ¥åº“ï¼Œæ‰€ä»¥éœ€è¦é€šè¿‡-DFDB_LIBRARY_PATH_FDB_C=/usr/local/lib/libfdb_c.dylibæŒ‡å®šå¯¹åº”çš„åŠ¨æ€é“¾æ¥åº“ä½ç½®ã€‚ ä½ çš„å½“å‰æœåŠ¡å™¨æ˜¯ FoundationDB 7.3.63ï¼Œæœ€å¤§æ”¯æŒ API version æ˜¯ 730ã€‚å› æ­¤ï¼ŒJava å®¢æˆ·ç«¯çš„ jar åŒ…ç‰ˆæœ¬å¿…é¡»ä¹Ÿåœ¨ 7.3.x ç‰ˆæœ¬èŒƒå›´å†…ï¼Œå¦åˆ™å¯èƒ½ä¸å…¼å®¹ï¼ˆå°¤å…¶é«˜ç‰ˆæœ¬å¦‚ 7.4 ä¼šå°è¯•ä½¿ç”¨ API version 740ï¼‰ã€‚ \u003c!-- Maven é…ç½®ç¤ºä¾‹ --\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.apple.foundationdb\u003c/groupId\u003e \u003cartifactId\u003efoundationdb\u003c/artifactId\u003e \u003cversion\u003e7.3.63\u003c/version\u003e \u003c/dependency\u003e â¯ fdbcli --version FoundationDB CLI 7.3 (v7.3.63) source version 5140696da2df47c143ae74c0f4207b65d0d94876 protocol fdb00b073000000 ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:1:0","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"äº‹åŠ¡ äº‹åŠ¡æ˜¯ä¸€ç³»åˆ—çš„æ•°æ®åº“è¯»å†™è¢«ä½œä¸ºä¸€ä¸ªæ•´ä½“å¤„ç†ï¼Œå…·æœ‰ä¸€äº›å…³é”®çš„æ€§è´¨ã€‚é¦–å…ˆæ‰€æœ‰åœ¨äº‹åŠ¡ä¸­çš„è¯»éƒ½çœ‹åˆ°æ•°æ®åº“çš„ç›¸åŒå¿«ç…§ï¼Œä¸ä¼šçœ‹åˆ°å¹¶å‘çš„å…¶ä»–äº‹åŠ¡çš„ä¿®æ”¹ï¼Œå…¶æ¬¡ï¼Œåœ¨ä¸€ä¸ªäº‹åŠ¡ä¸­çš„å†™å…¥è¦ä¸å…¨éƒ¨æˆåŠŸï¼Œè¦ä¸å…¨éƒ¨å¤±è´¥ï¼ˆæ¯”å¦‚ç”±äºè¿æ¥ä¸­æ–­å¯¼è‡´çš„å¤±è´¥ï¼‰ï¼Œæœ€åå½“ä¸€ä¸ªäº‹åŠ¡æˆåŠŸæäº¤æ—¶ï¼Œå†™å…¥è¢«æŒä¹…åŒ–å­˜å‚¨ã€‚äº‹åŠ¡çš„è¿™äº›æ€§è´¨ï¼ˆéš”ç¦»isolationï¼ŒåŸå­atomicityï¼ŒæŒä¹…åŒ–durabilityï¼‰æ„æˆäº†ACIDæ¨¡å‹çš„åŸºæœ¬ä¿éšœã€‚FoundationDBå›¢é˜Ÿè®¤ä¸ºACIDäº‹åŠ¡çš„æ”¯æŒä¸åªæ˜¯ä¸€é¡¹é¢å¤–çš„ç‰¹æ€§ï¼Œè€Œæ˜¯ä½¿ç”¨ç®€å•æœ‰æ•ˆæ–¹å¼æ„å»ºå¥å£®åº”ç”¨çš„æ ¸å¿ƒå› ç´ ã€‚ éœ€è¦æ”¯æŒå¹¶å‘è®¿é—®çš„æ¯ä¸ªåº”ç”¨éƒ½åº”è¯¥åŸºäºå…·æœ‰ACIDç‰¹æ€§çš„äº‹åŠ¡æ„å»ºï¼Œäº‹åŠ¡æ˜¯å¤„ç†å¹¶å‘æœ€ç®€å•å’Œæœ€å¼ºæœ‰åŠ›çš„ç¼–ç¨‹æ¨¡å‹ï¼Œéšç€NoSQLæŠ€æœ¯çš„ä¸æ–­æˆç†Ÿï¼Œäº‹åŠ¡å°†æ‰®æ¼”è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ã€‚ä¸€ä¸ªé€šå¸¸çš„è¯¯è§£æ˜¯äº‹åŠ¡ä»…å¯¹ç”µå­å•†åŠ¡æˆ–è€…é“¶è¡Œä¸šåŠ¡æœ‰ç”¨ï¼Œç„¶è€Œäº‹åŠ¡çš„çœŸæ­£ä»·å€¼åœ¨äºå®ƒä»¬åœ¨åº”ç”¨å¼€å‘ä¸­çš„å·¥ç¨‹æ„ä¹‰ï¼Œè€ŒéæŸä¸ªç‰¹å®šåº”ç”¨çš„å®ç°ç»†èŠ‚ã€‚å¾—ç›ŠäºACIDç‰¹æ€§ï¼Œäº‹åŠ¡å¯ä»¥è¢«ç»„åˆä»¥æ„å»ºæ–°çš„æŠ½è±¡å±‚ï¼Œæ”¯æŒæ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„ï¼Œå¹¶å®æ–½å®Œæ•´çº¦æŸã€‚ç”±æ­¤å¯è§ï¼Œä¸å…¶ä»–æ–¹æ¡ˆç›¸æ¯”ï¼ŒåŸºäºäº‹åŠ¡æ„å»ºåº”ç”¨ç¨‹åºæ›´ä¸ºç®€ä¾¿ï¼Œå¯é ä¸”å…·å¤‡å¾ˆå¼ºçš„å¯æ‰©å±•æ€§ã€‚ä»ç³»ç»Ÿè®¾è®¡çš„è§’åº¦çœ‹ï¼Œä½ å¯èƒ½ä¼šè§‰å¾—ï¼Œç³»ç»Ÿçš„è®¾è®¡æƒè¡¡ç ´äº‹ä½ æ”¾å¼ƒäº‹åŠ¡å¸¦æœ‰çš„ä¼˜åŠ¿ï¼Œä»¥æ¢å–é€Ÿåº¦ã€å¯æ‰©å±•æ€§å’Œå®¹é”™èƒ½åŠ›ï¼Œç„¶è€Œï¼Œè¿™æ ·æ„å»ºçš„ç³»ç»Ÿé€šè¿‡è„†å¼±ã€éš¾ä»¥ç®¡ç†ï¼Œå¹¶ä¸”å‡ ä¹æ— æ³•é€‚åº”ä¸æ–­å˜åŒ–çš„ä¸šåŠ¡éœ€æ±‚ï¼Œæ”¾å¼ƒäº‹åŠ¡æ‰€å¸¦æ¥çš„ä»£ä»·å¾€å¾€å¾—ä¸å¿å¤±ï¼Œå°¤å…¶æ˜¯åœ¨å­˜åœ¨åœ¨å¤§è§„æ¨¡ä¸‹æä¾›äº‹åŠ¡å®Œæ•´æ€§çš„æ›¿ä»£æ–¹æ¡ˆæ—¶ã€‚ å½“å¤šä¸ªå®¢æˆ·ç«¯ã€ç”¨æˆ·æˆ–è€…åº”ç”¨ç¨‹åºçš„ä¸åŒéƒ¨åˆ†åŒæ—¶å¯¹åŒä¸€æ•°æ®è¿›è¡Œè¯»å†™æ—¶ï¼Œå°±ä¼šäº§ç”Ÿå¹¶å‘ã€‚äº‹åŠ¡è®©å¼€å‘è€…æ›´å®¹æ˜“ç®¡ç†å¹¶å‘ï¼Œå®ç°è¿™ç§ç®€åŒ–çš„å…³é”®äº‹åŠ¡ç‰¹æ€§æ˜¯éš”ç¦»æ€§ï¼Œå³ACIDä¸­çš„Iï¼Œå½“ç³»ç»Ÿä¿è¯äº‹åŠ¡å®Œå…¨éš”ç¦»æ—¶ï¼ˆç§°ä¸ºå¯ä¸²è¡ŒåŒ–ï¼‰ï¼Œå¼€å‘è€…å¯ä»¥å°†æ¯ä¸ªäº‹åŠ¡è§†ä¸ºé¡ºåºæ‰§è¡Œï¼Œå°½ç®¡å®ƒä»¬å®é™…ä¸Šå¯èƒ½æ˜¯å¹¶å‘æ‰§è¡Œçš„ï¼Œè¿™æ ·ï¼Œå¼€å‘è€…æ— éœ€å†è´¹å¿ƒæ¨ç†ä¸åŒäº‹åŠ¡æ“ä½œä¹‹é—´å¯èƒ½çš„ç›¸äº’å½±å“ã€‚æœ‰äº›ç³»ç»Ÿä»…åœ¨ä¸€å°éƒ¨åˆ†é¢„å®šä¹‰æ“ä½œä¸Šæä¾›ACIDäº‹åŠ¡æ”¯æŒï¼Œé€šå¸¸å—é™äºæ•°æ®æ¨¡å‹çš„ç»“æ„ï¼Œæ¯”å¦‚ï¼Œä¸€ä¸ªæ–‡æ¡£æ•°æ®åº“å¯èƒ½åªå…è®¸åœ¨äº‹åŠ¡ä¸­æ›´æ–°å•ä¸ªæ–‡æ¡£ã€‚ç„¶è€Œäº‹åŠ¡çš„çœŸæ­£å¼ºå¤§ä¹‹å¤„åœ¨äºåº”ç”¨å¼€å‘è€…å¯ä»¥åœ¨ä»»æ„ä¸€ç»„æ•°æ®ä¸Šè‡ªç”±å®šä¹‰äº‹åŠ¡ï¼Œå¯¹äºä½¿ç”¨é”®å€¼å­˜å‚¨çš„å¼€å‘è€…æ¥è¯´ï¼Œåº”ç”¨èƒ½å¤Ÿå®šä¹‰å¯è¯»å–å’Œå¯å†™å…¥ä»»æ„æ•°é‡é”®å€¼å¯¹çš„äº‹åŠ¡ï¼Œåªæœ‰å½“å¼€å‘è€…å¯ä»¥ä¸å—äººä¸ºé™åˆ¶åœ°è‡ªç”±å®šä¹‰äº‹åŠ¡æ—¶ï¼Œäº‹åŠ¡æ‰èƒ½æˆä¸ºæ„å»ºåº”ç”¨ç¨‹åºçš„åŸºæœ¬æ„ä»¶ã€‚ ç”±åº”ç”¨å®šä¹‰çš„äº‹åŠ¡æ˜¯å¯ç»„åˆçš„ï¼Œéš”ç¦»æ€§å’ŒåŸå­æ€§ç»“åˆç¡®ä¿ä¸€ä¸ªäº‹åŠ¡çš„æ‰§è¡Œä¸ä¼šå½±å“å¦ä¸€ä¸ªäº‹åŠ¡çš„å¯è§è¡Œä¸ºï¼Œè¿™ä¸€ä¿éšœä½¿äº‹åŠ¡ä¹‹é—´å…·æœ‰å¯ç»„åˆæ€§ï¼Œä»è€Œå¯ä»¥å°†å®ƒä»¬ç»„åˆèµ·æ¥æ„å»ºæ–°çš„æŠ½è±¡å±‚ï¼Œè¿™äº›æŠ½è±¡å¯ä»¥å°è£…åœ¨ç³»ç»Ÿçš„ä¸åŒå±‚ä¸­ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå¸¸è§çš„æŠ½è±¡æ˜¯ï¼Œä¸ºäº†å¿«é€ŸæŸ¥æ‰¾æ»¡è¶³æŸä¸ªæ¡ä»¶çš„æ•°æ®é¡¹ï¼Œéœ€è¦åœ¨ç»´æŠ¤ä¸»æ•°æ®çš„åŒæ—¶ç»´æŠ¤ä¸€ä¸ªç´¢å¼•ï¼Œåœ¨ä»»ä½•é”®å€¼å­˜å‚¨ä¸­ï¼Œè¿™ä¸ªåŠŸèƒ½å¯ä»¥é€šè¿‡ä½¿ç”¨ç´¢å¼•å­—æ®µä½œä¸ºé”®ï¼Œå†å­˜å‚¨ä¸€ä»½æ•°æ®å‰¯æœ¬æ¥ç®€å•å®ç°ã€‚ç„¶è€Œï¼Œå¹¶å‘æ›´æ–°å¯èƒ½ä½¿è¿™ä¸ªè®¾è®¡å˜å¾—å¤æ‚ï¼Œæ²¡æœ‰äº‹åŠ¡çš„æƒ…å†µä¸‹ï¼Œéš¾ä»¥ä¿è¯åœ¨æ•°æ®å˜æ›´æ˜¯ï¼Œæ•°æ®æœ‰ç´¢å¼•éƒ½èƒ½è¢«ä¸€è‡´çš„æ›´æ–°ã€‚è€Œé€šè¿‡äº‹åŠ¡ï¼Œç´¢å¼•å±‚å¯ä»¥åœ¨åŒä¸€ä¸ªäº‹åŠ¡ä¸­åŒæ—¶æ›´æ–°æ•°æ®å’Œç´¢å¼•ï¼Œä»è€Œä¿è¯äºŒè€…çš„ä¸€è‡´æ€§ï¼Œæ„å»ºå‡ºä¸€ä¸ªå¼ºå¤§çš„æŠ½è±¡ã€‚ äº‹åŠ¡ä½¿å„å±‚å¯ä»¥ç®€å•é«˜æ•ˆçš„æ„å»ºæŠ½è±¡ï¼Œä»è€Œæä¾›äº†é«˜åº¦å¯æ‰©å±•çš„èƒ½åŠ›ï¼Œä»¥æ”¯æŒå¤šç§æ•°æ®æ¨¡å‹ï¼Œé’ˆå¯¹å±‚æ¬¡åŒ–æ–‡æ¡£ã€åˆ—å¼æ•°æ®æˆ–å…³ç³»å‹æ•°æ®ä¼˜åŒ–çš„æ•°æ®æ¨¡å‹ï¼Œéƒ½å¯ä»¥æ„å»ºåœ¨æœ‰åºé”®å€¼å­˜å‚¨ä¸Šï¼Œå¹¶é€šè¿‡æŠ½è±¡å±‚å®ç°ã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œé«˜å±‚æ¨¡å‹ä¸­çš„ä¸€ä¸ªæ•°æ®å¯¹è±¡é€šå¸¸ä¼šå¯¹åº”åˆ°åº•å±‚çš„å¤šä¸ªé”®å€¼å¯¹ï¼Œäº‹åŠ¡é€šè¿‡å°†å¤šä¸ªé”®å€¼å¯¹çš„æ›´æ–°å°è£…åœ¨ä¸€ä¸ªåŸå­æ“ä½œä¸­ï¼Œä½¿è¿™äº›æ˜ å°„çš„å¯é å®ç°å˜å¾—ååˆ†ç›´æ¥ã€‚ äº‹åŠ¡å¸¦æ¥çš„å¥½å¤„ä¸ä»…ä»…æ˜¯è®©é«˜å±‚æ•°æ®æ¨¡å‹çš„é€‰æ‹©æ›´åŠ çµæ´»ï¼Œå®ƒä»¬è¿˜èƒ½æå‡åœ¨ç»™å®šæ•°æ®æ¨¡å‹ä¸‹çš„æ•°æ®è¡¨ç¤ºæ•ˆç‡ã€‚ä»¥æ–‡æ¡£å‹æ•°æ®åº“ä¸­å¸¸ç”¨çš„â€œåµŒå…¥å¼æ•°æ®â€è®¾è®¡æ¨¡å¼ä¸ºä¾‹ï¼ˆå…³ç³»å‹æ•°æ®åº“èƒŒæ™¯çš„å¼€å‘è€…å¯èƒ½æ›´ç†Ÿæ‚‰åè§„èŒƒåŒ–è¿™ä¸€æœ¯è¯­ï¼‰ï¼Œåœ¨è¿™ç§æ¨¡å¼ä¸­ï¼Œæ•°æ®è¢«åµŒå¥—åœ¨å•ä¸ªæ–‡æ¡£çš„å±‚çº§ç»“æ„ä¸­ï¼ˆé€šå¸¸ä¼šå‡ºç°æ•°æ®é‡å¤ï¼‰ï¼Œè€Œä¸æ˜¯é€šè¿‡å¼•ç”¨å…±äº«å…¶ä»–æ–‡æ¡£ã€‚è¿™ç§åµŒå…¥çš„åšæ³•å¾€å¾€æ˜¯å› ä¸ºç¼ºä¹å¯¹å…¨å±€äº‹åŠ¡çš„æ”¯æŒï¼Œç”±äºå¤§å¤šæ•°æ–‡æ¡£æ•°æ®åº“åªæ”¯æŒå¯¹å•ä¸ªæ–‡æ¡£çš„åŸå­æ“ä½œï¼Œå¼€å‘è€…ä¸å¾—ä¸å°†ç›¸å…³æ•°æ®å‹ç¼©è¿›ä¸€ä¸ªæ–‡æ¡£ä¸­ï¼Œä»¥å®ç°åŸå­æ›´æ–°ï¼Œä½†è¿™æ ·åšçš„ç»“æœæ˜¯ï¼Œæ–‡æ¡£æ›´å¤§ã€æ›´å¤æ‚ã€è®¿é—®å’Œæ›´æ–°çš„æ•ˆç‡é€šè¿‡ä¹Ÿæ›´ä½ã€‚è€Œæœ‰äº†äº‹åŠ¡ï¼Œå°±æ— éœ€å†ä¾èµ–åµŒå…¥æ¥å®ç°åŸå­æ€§ï¼Œå¼€å‘è€…å¯ä»¥ä»¥è®¿é—®æ•ˆç‡ä¸ºå¯¼å‘æ¥å»ºæ¨¡ï¼Œé€‚å½“åœ°å°†æ•°æ®æ‹†åˆ†æˆå¤šä¸ªæ–‡æ¡£ï¼ŒåŒæ—¶ä»å¯é€šè¿‡å•ä¸ªäº‹åŠ¡ä¿è¯è¿™äº›æ•°æ®çš„ä¸€è‡´æ€§ï¼Œæ›´è¿›ä¸€æ­¥ï¼Œè¿™äº›æ•°æ®è¿˜ä¼šè¢«å¤šä¸ªå®¢æˆ·ç«¯å…±äº«å¹¶å¹¶å‘æ›´æ–°ï¼Œè€Œäº‹åŠ¡æœºåˆ¶æä¾›äº†å®‰å…¨ç®¡ç†å…±äº«çŠ¶æ€æ‰€éœ€çš„å¹¶å‘æ§åˆ¶èƒ½åŠ›ã€‚ å¤§å¤šæ•°æ‰¿æ‹…å…³é”®ä¸šåŠ¡åŠŸèƒ½çš„åº”ç”¨ç¨‹åºï¼Œéƒ½ä¼šéšç€æ—¶é—´ç»å†éœ€æ±‚çš„å˜åŒ–ï¼ˆè€Œä¸”ï¼Œå˜åŒ–å¾€å¾€æ˜¯å¢åŠ éœ€æ±‚è€Œä¸æ˜¯å‡å°‘ï¼‰ï¼Œä½ å¯èƒ½è®¤ä¸ºäº‹åŠ¡å®Œæ•´æ€§æ˜¯ä¸€ä¸ªä¸é”™ä½†éå¿…éœ€çš„ç‰¹æ€§ã€‚ç¡®å®ï¼Œè®¸å¤šåº”ç”¨å¯ä»¥é€šè¿‡ä¸ºç‰¹å®šçš„ã€ç®€å•çš„è®¿é—®æ¨¡å¼ç²¾å¿ƒè®¾è®¡æ•°æ®ç»“æ„ï¼Œä»è€Œé¿å…å¯¹å…¨å±€äº‹åŠ¡çš„ä¾èµ–ã€‚ç„¶è€Œï¼Œå½“è¿™äº›åº”ç”¨éœ€è¦æ¼”è¿›æ—¶ï¼Œèƒ½å¤Ÿçµæ´»ä¿®æ”¹æ•°æ®æ¨¡å‹ï¼Œä»¥åŠæ‹¥æœ‰å…¨å±€äº‹åŠ¡çš„èƒ½åŠ›ï¼Œå¾€å¾€å†³å®šäº†ä½ æ˜¯è½»æ¾æ”¹è¿›è¿˜æ˜¯ä¸å¾—ä¸æ¨å€’é‡æ„ã€‚å‡è®¾ä½ è¿è¥ä¸€ä¸ªwebåº”ç”¨ï¼Œç”¨æˆ·æ—¢å¯ä»¥å‘é€å¸–å­ï¼Œä¹Ÿå¯ä»¥æ¥æ”¶æ¥è‡ªå…¶ä»–ç”¨æˆ·çš„å¸–å­ï¼Œæ‰€æœ‰å¸–å­éƒ½ä¿å­˜åœ¨åç«¯æ•°æ®åº“ä¸­ï¼ŒæŸå¤©ç®¡ç†å‘˜å¸Œæœ›ä½ å¼€å‘ä¸€ä¸ªåˆ†æçœ‹æ¿ï¼Œç”¨æ¥è¿›è¡Œä¸€äº›åŸºç¡€çš„æ•°æ®åˆ†æï¼Œå°½ç®¡ä½ çš„æ•°æ®å­˜å‚¨æ”¯æŒè¯»å†™æ“ä½œï¼Œä½†è¿™ä¸ªçœ‹æ¿æœ€åˆåªæ˜¯ç”¨äºåªè¯»æŸ¥è¯¢ï¼Œè€Œä¸”æ•°æ®åªæ˜¯ç”¨æ¥åˆ†æç”¨é€”ï¼Œå³ä½¿ç»“æœå¶å°”æœ‰ç‚¹å»¶è¿Ÿï¼Œä¹Ÿä¸ä¼šæœ‰äººåœ¨ç•Œé¢ä¸Šæ³¨æ„åˆ°ï¼Œå› æ­¤ä½ é€‰æ‹©ä¸å¼•å…¥äº‹åŠ¡ï¼Œå¹¶å¼€å‘äº†å‡ ä¸ªéå¸¸å‡ºè‰²çš„æ•°æ®å¯è§†åŒ–åŠŸèƒ½ï¼Œè®©å…¬å¸èƒ½ä»¥å…¨æ–°æ–¹å¼åˆ†æå’ŒæŸ¥çœ‹å¸–å­ã€‚å¥½æ¶ˆæ¯æ˜¯ï¼Œçœ‹æ¿éå¸¸å—æ¬¢è¿ï¼Œåæ¶ˆæ¯æ˜¯ï¼Œä½ å¼€å§‹ä¸æ–­æ”¶åˆ°æ–°åŠŸèƒ½çš„éœ€æ±‚ï¼Œç”¨æˆ·å¸Œæœ›èƒ½åœ¨çœ‹æ¿ä¸­ç›´æ¥ç¼–è¾‘æˆ–å®¡æ ¸è¢«åˆ†æå‡ºæ¥çš„å¸–å­ï¼Œå¹¿å‘Šéƒ¨é—¨ä¹Ÿå¸Œæœ›é€šè¿‡apiè®¿é—®çœ‹æ¿ä¸­çš„æ•°æ®ï¼Œç”¨äºé©±åŠ¨è®¡è´¹ç³»ç»Ÿï¼Œæ­¤æ—¶ï¼Œçœ‹æ¿ä¸å†æ˜¯ä¸€ä¸ªç®€å•çš„åªè¯»å·¥å…·ï¼Œè€Œä½ çš„æ–°APIåˆ™éœ€è¦ä¸ºå…¶è¿”å›çš„æ•°æ®æä¾›å¼ºä¸€è‡´æ€§ä¿éšœã€‚è¿™ç§éœ€æ±‚æ¼”è¿›çš„æƒ…å†µæ˜¯éå¸¸è‡ªç„¶ä¸”å¸¸è§çš„æ¨¡å¼ï¼Œäº‹åŠ¡çš„å­˜åœ¨ï¼Œå†³å®šäº†ä½ æ˜¯å¦èƒ½è½»æ¾åº”å¯¹è¿™äº›å˜åŒ–ï¼Œé¡ºåˆ©æ·»åŠ æ–°çš„åŠŸèƒ½ï¼Œæˆ–è€…åªèƒ½æ¨ç¿»å¤§é‡å·²æœ‰ä»£ç ï¼Œä»å¤´å¼€å¤´é‡æ„ã€‚ ä½ å¯èƒ½ä¼šå› ä¸ºå¯¹äº‹åŠ¡å­˜åœ¨çš„ä¸¥é‡çš„tradeoffè€Œä¸æ„¿ä½¿ç”¨å®ƒä»¬ï¼Œå°¤å…¶æ˜¯åœ¨NoSQLæ•°æ®åº“æ‰€é¢å‘çš„é«˜æ€§èƒ½åº”ç”¨åœºæ™¯æ›´æ˜¯å¦‚æ­¤ï¼Œç„¶è€Œï¼Œå¦‚æœä½ æ›´æ·±å…¥åœ°åˆ†æè¿™äº›tradeoffå°±ä¼šå‘ç°ï¼Œå¯¹ç”¨æˆ·è€Œè¨€ï¼Œäº‹åŠ¡çš„ä»£ä»·å…¶å®éå¸¸ä½ã€‚ï¼ˆçœŸæ­£çš„ä»£ä»·åœ¨äºæ•°æ®åº“å·¥ç¨‹å¸ˆ-æ„å»ºåˆ†å¸ƒå¼äº‹åŠ¡ç³»ç»Ÿç¡®å®éå¸¸å›°éš¾ï¼‰ã€‚ æˆ‘ä»¬å°šæœªå‘ç°æ”¯æŒäº‹åŠ¡çš„ç³»ç»Ÿåœ¨å¯æ‰©å±•æ€§æˆ–æ€§èƒ½æ–¹é¢å­˜åœ¨ä»»ä½•å®é™…é™åˆ¶ï¼Œå½“NoSQLæ•°æ®åº“è¿åŠ¨å…´èµ·æ—¶ï¼Œæ—©æœŸçš„ä¸€äº›ç³»ç»Ÿï¼ˆå¦‚Google BigTableï¼‰é‡‡ç”¨äº†æç®€çš„è®¾è®¡ï¼Œä¸“æ³¨äºå¯æ‰©å±•æ€§å’Œæ€§èƒ½ï¼Œè¿™äº›ç³»ç»Ÿä¸»åŠ¨èˆå¼ƒäº†å…³ç³»å‹æ•°æ®åº“ä¸­å¸¸è§çš„è®¸å¤šç‰¹æ€§ï¼Œå¹¶å‡è®¾è¿™äº›è¢«èˆå¼ƒçš„åŠŸèƒ½å¯¹äºå¯æ‰©å±•æ€§æˆ–æ€§èƒ½ç›®æ ‡æ¥è¯´æ˜¯å¤šä½™ç”šè‡³æœ‰å®³çš„ã€‚ä½†è¿™äº›å‡è®¾æ˜¯é”™è¯¯çš„ï¼Œç°åœ¨è¶Šæ¥è¶Šæ¸…æ™°çš„è¡¨æ˜ï¼Œæ”¯æŒäº‹åŠ¡æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå·¥ç¨‹å®ç°çš„é—®é¢˜ï¼Œè€Œä¸æ˜¯æ¶æ„è®¾è®¡ä¸Šçš„æ ¹æœ¬æ€§æƒè¡¡ï¼Œç”¨äºç»´æŠ¤äº‹åŠ¡å®Œæ•´æ€§çš„ç®—æ³•åŒæ—¶å¯ä»¥åƒå…¶ä»–åˆ†å¸ƒå¼ç®—æ³•ä¸€æ ·è¿›è¡Œæ‰©å±•ã€‚äº‹åŠ¡ç¡®å®ä¼šå¸¦æ¥ä¸€å®šçš„CPUæˆæœ¬ï¼Œä½†æ ¹æ®æˆ‘ä»¬çš„ç»éªŒï¼Œè¿™éƒ¨åˆ†å¼€é”€é€šå¸¸ä¸åˆ°ç³»ç»Ÿæ€»CPUä½¿ç”¨é‡çš„10%ï¼Œä¸ºäº†è·å¾—äº‹åŠ¡å®Œæ•´æ€§ï¼Œè¿™ç‚¹ä»£ä»·æ˜¯éå¸¸å°çš„ï¼Œå¹¶ä¸”å®Œå…¨å¯ä»¥é€šè¿‡å…¶ä»–æ–¹å¼è½»æ¾è¡¥å¿å›æ¥ã€‚ äº‹åŠ¡ä¿è¯å†™å…¥æ“ä½œçš„æŒä¹…æ€§ï¼Œè¿™ç§ä¿è¯ä¼šå¸¦æ¥ä¸€å®šçš„å†™å…¥å»¶è¿Ÿå¢åŠ ï¼ŒæŒä¹…æ€§æ„å‘³ç€ä¸€æ—¦å†™å…¥æäº¤ï¼Œå³ä½¿ä¹‹åå‘ç”Ÿç¡¬ä»¶æ•…éšœï¼Œæ•°æ®ä¹Ÿä¸ä¼šä¸¢å¤±ï¼Œå› æ­¤ï¼ŒæŒä¹…æ€§æ˜¯å®¹é”™èƒ½åŠ›çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚ä¸æ”¯æŒæŒä¹…æ€§çš„NoSQLç³»ç»Ÿåœ¨å®¹é”™æ€§æ–¹é¢å¿…ç„¶è¾ƒå¼±ï¼Œç”±äºçœŸæ­£çš„å®¹é”™èƒ½åŠ›è‡³å…³é‡è¦ï¼Œä¸ºå®ç°æŒä¹…äº‹åŠ¡æ‰€å¸¦æ¥çš„å†™å…¥å»¶è¿Ÿé€šå¸¸æ˜¯å€¼å¾—ä»˜å‡ºçš„ä»£ä»·ï¼Œå¯¹äºé‚£äº›å¯¹å†™å…¥å»¶è¿Ÿæœ‰ä¸¥æ ¼è¦æ±‚çš„åº”ç”¨ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©å…³é—­æŒä¹…æ€§ï¼ŒåŒæ—¶ä»ä¿ç•™ACIä¸‰é¡¹äº‹åŠ¡ç‰¹æ€§ã€‚ éšç€NoSQLæ•°æ®åº“ è¢«è¶Šæ¥è¶Šå¹¿æ³›åœ°åº”ç”¨äºå„ç§åœºæ™¯ï¼Œæ„å»ºå…¶ä¸Šçš„åº”ç”¨ç¨‹åºä¹Ÿè¶Šæ¥è¶Šå¤šåœ°æ¶‰åŠå¤šä¸ªå®¢æˆ·ç«¯çš„éç®€å•å¹¶å‘æ“ä½œã€‚å¦‚æœæ²¡æœ‰å……åˆ†çš„å¹¶å‘æ§åˆ¶ï¼Œä¼ ç»Ÿå¹¶å‘ä¸­çš„å„ç§é—®é¢˜å°†é‡æ–°å‡ºç°ï¼Œç»™åº”ç”¨å¼€å‘è€…å¸¦æ¥æ²‰é‡çš„è´Ÿæ‹…ã€‚ACIDäº‹åŠ¡é€šè¿‡æä¾›ä¸¥æ ¼å¯ä¸²è¡ŒåŒ–çš„æ“ä½œï¼Œå¸®åŠ©å¼€å‘è€…ç®€åŒ–å¹¶å‘å¤„ç†ï¼Œå¹¶å¯ç»„åˆä½¿ç”¨ä»¥æ„å»ºå¥å£®çš„åº”ç”¨ç³»ç»Ÿã€‚å¦‚æœä½ æ­£åœ¨æ„å»ºä¸€ä¸ªéœ€è¦å¯æ‰©å±•æ€§çš„åº”ç”¨ï¼Œè€Œç³»ç»Ÿåˆä¸æ”¯æŒäº‹åŠ¡ï¼Œæœ€ç»ˆä½ å°†ä¸ºæ­¤ä»˜å‡ºä»£ä»·ã€‚å¹¸è¿çš„æ˜¯ï¼ŒNoSQLæ•°æ®åº“åœ¨å…·å¤‡äº‹åŠ¡æ”¯æŒçš„æƒ…å†µä¸‹ï¼Œä¾ç„¶å¯ä»¥å®ç°è‰¯å¥½å¯¹çš„å¯æ‰©å±•æ€§ã€å®¹é”™æ€§å’Œæ€§èƒ½ã€‚å› æ­¤ï¼Œæ˜¯å¦ä½¿ç”¨äº‹åŠ¡ï¼Œå¹¶ä¸æ˜¯æ¶æ„è®¾è®¡ä¸Šçš„æ ¹æœ¬æ€§æƒè¡¡ï¼Œè€Œæ˜¯ä¸€ä¸ªå·¥ç¨‹ç†æ€§é€‰æ‹©ï¼Œéšç€æŠ€æœ¯çš„ä¸æ–­æˆç†Ÿï¼Œäº‹åŠ¡å°†æˆä¸ºæœªæ¥NoSQLæ•°æ®åº“çš„åŸºç¡€èƒ½åŠ›ä¹‹ä¸€ã€‚ ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:2:0","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"CAPç†è®º æ•°æ®åº“å¯ä»¥åœ¨å‘ç”Ÿç½‘ç»œåˆ†åŒºæ—¶åŒæ—¶æä¾›å¼ºä¸€è‡´æ€§å’Œç³»ç»Ÿå¯ç”¨æ€§ï¼Œäººä»¬æ™®éè®¤ä¸ºè¿™ç§ç»„åˆä¸å¯èƒ½å®ç°ï¼Œä½†è¿™ç§çœ‹æ³•æ˜¯åŸºäºå¯¹CAPå®šç†çš„è¯¯è§£ã€‚ 2000å¹´ï¼ŒEric Breweræå‡ºäº†ä¸€ä¸ªçŒœæƒ³ï¼Œä¸€ä¸ªåˆ†å¸ƒå¼ç³»ç»Ÿä¸èƒ½åŒæ—¶æ»¡è¶³ä¸€è‡´æ€§ã€å¯ç”¨æ€§å’Œåˆ†åŒºå®¹å¿æ€§è¿™ä¸‰ä¸ªæ€§è´¨ï¼š Consistencyï¼ˆä¸€è‡´æ€§ï¼‰ï¼šæ¯æ¬¡è¯»å–éƒ½èƒ½çœ‹åˆ°ä¹‹å‰å®Œæˆçš„å†™æ“ä½œ Availability ï¼ˆå¯ç”¨æ€§ï¼‰ï¼šæ¯æ¬¡è¯»å†™è¯·æ±‚æ€»èƒ½æˆåŠŸè¿”å›å“åº” Partition toleranceï¼ˆåˆ†åŒºå®¹å¿æ€§ï¼‰ï¼šå³ä½¿ç½‘ç»œæ•…éšœå¯¼è‡´éƒ¨åˆ†èŠ‚ç‚¹ä¹‹é—´æ— æ³•é€šä¿¡ï¼Œç³»ç»Ÿä»èƒ½ç»´æŒå…¶æ‰¿è¯ºçš„æ€§è´¨ åœ¨2002å¹´ï¼ŒGilbertå’ŒLynchåœ¨å¼‚æ­¥å’Œéƒ¨åˆ†åŒæ­¥ç½‘ç»œæ¨¡å‹ä¸‹æ­£å¼è¯æ˜äº†è¿™ä¸€ç‚¹ï¼Œè¿™ä¸€ç»“è®ºåæ¥è¢«ç§°ä¸ºCAPå®šç†ã€‚Breweræœ€åˆè®¤ä¸ºå¼€å‘è€…å¿…é¡»åœ¨ä¸‰è€…ä¸­é€‰å‡ºä¸¤é¡¹ï¼ˆCPã€APæˆ–CAï¼‰ï¼Œä½†è¿›ä¸€æ­¥çš„æ€è€ƒå‘ç°CAï¼ˆä¸æ”¯æŒåˆ†åŒºå®¹å¿ï¼‰å¹¶ä¸æ˜¯ä¸€ä¸ªå¯è¡Œçš„é€‰é¡¹ï¼Œå› ä¸ºä»å®šä¹‰å‡ºå‘ï¼Œåœ¨å®é™…å‘ç”Ÿç½‘ç»œåˆ†åŒºæ—¶ï¼Œç³»ç»Ÿå°±å¿…é¡»æ”¾å¼ƒä¸€è‡´æ€§æˆ–å¯ç”¨æ€§ã€‚å› æ­¤ï¼Œç°ä»£å¯¹CAPçš„ç†è§£æ˜¯ï¼Œåœ¨å‘ç”Ÿç½‘ç»œåˆ†åŒºçš„æƒ…å†µä¸‹ï¼Œåˆ†å¸ƒå¼ç³»ç»Ÿå¿…é¡»åœ¨ä¸€è‡´æ€§å’Œå¯ç”¨æ€§ä¹‹é—´åšå‡ºé€‰æ‹©ã€‚ è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªAPå‹çš„æ•°æ®åº“ï¼Œåœ¨è¿™ç§æ•°æ®åº“ä¸­ï¼Œå³ä½¿èŠ‚ç‚¹ä¹‹é—´çš„ç½‘ç»œè¿æ¥ä¸å¯ç”¨ï¼Œè¯»å†™æ“ä½œä¹Ÿæ€»æ˜¯èƒ½å¤ŸæˆåŠŸï¼Œä»è¡¨é¢ä¸Šçœ‹ï¼Œè¿™ä¼¼ä¹æ˜¯éå¸¸ç†æƒ³çš„ç‰¹æ€§ï¼Œç„¶è€Œç¼ºç‚¹å¾ˆæ˜æ˜¾ï¼Œæƒ³è±¡ä¸€ä¸ªç®€å•çš„åˆ†å¸ƒå¼æ•°æ®åº“ç³»ç»Ÿï¼Œå®ƒç”±ä¸¤ä¸ªèŠ‚ç‚¹ç»„æˆï¼Œè€Œç½‘ç»œåˆ†åŒºå¯¼è‡´ä»–ä»¬æ— æ³•é€šä¿¡ï¼Œä¸ºäº†æ»¡è¶³å¯ç”¨æ€§ï¼Œè¿™ä¸¤ä¸ªèŠ‚ç‚¹å¿…é¡»å„è‡ªç»§ç»­æ¥å—å®¢æˆ·ç«¯çš„å†™å…¥æ“ä½œã€‚å½“ç„¶ï¼Œç”±äºç½‘ç»œåˆ†åŒºä½¿é€šä¿¡å˜å¾—ä¸å¯èƒ½ï¼Œä¸€ä¸ªèŠ‚ç‚¹ä¸Šçš„å†™å…¥æ“ä½œæ— æ³•è¢«å¦ä¸€ä¸ªèŠ‚ç‚¹çœ‹åˆ°ï¼Œè¿™æ ·çš„ç³»ç»Ÿå®é™…ä¸Šåªèƒ½ç®—æ˜¯åä¹‰ä¸Šçš„ä¸€ä¸ªæ•°æ®åº“ï¼Œåªè¦åˆ†åŒºç»§ç»­å­˜åœ¨ï¼Œè¯¥ç³»ç»Ÿå°±å®Œå…¨ç­‰åŒäºä¸¤ä¸ªç‹¬ç«‹çš„æ•°æ®åº“ï¼Œå®ƒä»¬çš„å†…å®¹ç”šè‡³ä¸å¿…ç›¸å…³ï¼Œæ›´ä¸ç”¨è¯´ä¸€è‡´äº†ã€‚ å¾ˆå¤šäººè¯¯è§£CAPå®šç†ï¼Œé€šå¸¸æ˜¯å› ä¸ºæé”™äº†å¯ç”¨æ€§åœ¨CAPä¸­çš„å«ä¹‰ï¼Œåœ¨CAPçš„è¯­ä¹‰ä¸­ï¼Œå¯ç”¨æ€§æŒ‡çš„æ˜¯å³ä¾¿å‡ºç°ç½‘ç»œåˆ†åŒºï¼Œç³»ç»Ÿä¸­çš„æ‰€æœ‰èŠ‚ç‚¹éƒ½å¿…é¡»ä»èƒ½å¤„ç†è¯»å†™è¯·æ±‚ï¼Œå¦‚æœç³»ç»Ÿåœ¨åˆ†åŒºæ—¶ï¼Œåªæœ‰éƒ¨åˆ†èŠ‚ç‚¹å¯ä»¥æ­£å¸¸å“åº”è¯»å†™è¯·æ±‚ï¼Œé‚£å®ƒåœ¨CAPçš„æ„ä¹‰ä¸Šå°±ä¸ç®—å¯ç”¨ï¼Œå³ä½¿ç”¨æˆ·è¡¨é¢ä¸Šçœ‹èµ·æ¥è¿˜èƒ½ç”¨ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå³ä¾¿ç³»ç»Ÿå¯¹å®¢æˆ·ç«¯æ¥è¯´è¡¨é¢é«˜å¯ç”¨ï¼ˆæ¯”å¦‚å“åº”é€Ÿåº¦å¿«ï¼ŒæœåŠ¡æ²¡æŒ‚ï¼‰ï¼Œç”šè‡³ç¬¦åˆå…¶SLAæœåŠ¡ç­‰çº§åè®®ï¼Œä½†åªè¦ä¸æ˜¯æ‰€æœ‰çš„èŠ‚ç‚¹éƒ½ä¿æŒè¯»å†™èƒ½åŠ›ï¼Œåœ¨CAPç†è®ºä¸­å°±ä¸èƒ½ç§°ä¸ºå¯ç”¨ã€‚ åƒæ‰€æœ‰éµå¾ªACIDçš„æ•°æ®åº“ä¸€æ ·ï¼Œåœ¨ç½‘ç»œåˆ†åŒºå‘ç”Ÿæ—¶ï¼ŒFoundationDBä¼šä¼˜å…ˆä¿è¯ä¸€è‡´æ€§è€Œä¸æ˜¯å¯ç”¨æ€§ï¼Œä½†è¿™å¹¶æ„å‘³ç€æ•°æ®åº“ä¼šå¯¹å®¢æˆ·ç«¯å®Œå…¨ä¸å¯ç”¨ï¼Œå½“æ‰¿è½½FoundationDBçš„å¤šä¸ªæœºå™¨æˆ–è€…æ•°æ®ä¸­å¿ƒä¹‹é—´å‘ç”Ÿç½‘ç»œåˆ†åŒºï¼Œå¯¼è‡´å®ƒä»¬æ— æ³•é€šä¿¡æ—¶ï¼Œå…¶ä¸­ä¸€äº›æœºå™¨å¯èƒ½æ— æ³•æ‰§è¡Œå†™æ“ä½œï¼Œä¸è¿‡ï¼Œåœ¨å¾ˆå¤šçœŸå®ä¸–ç•Œçš„æƒ…å†µä¸‹ï¼Œæ•´ä¸ªæ•°æ®åº“ç³»ç»Ÿä»¥åŠä½¿ç”¨å®ƒçš„åº”ç”¨ç¨‹åºä»ç„¶å¯ä»¥ä¿æŒæ­£å¸¸è¿è¡Œï¼ŒæŸäº›æœºå™¨å‘ç”Ÿç½‘ç»œåˆ†åŒºçš„å½±å“ï¼Œå¹¶ä¸ä¼šæ¯”è¿™äº›æœºå™¨ç›´æ¥æ•…éšœæ›´ä¸¥é‡ï¼Œè€ŒFoundationDBç”±äºå…¶å®¹é”™è®¾è®¡ï¼Œèƒ½å¾ˆå¥½åœ°åº”å¯¹è¿™ç±»æ•…éšœã€‚ FoundationDBçš„è®¾è®¡ç›®æ ‡æ˜¯å³ä½¿æŸäº›æœºå™¨å®•æœºæˆ–ç½‘ç»œä¸å¯é ï¼Œæ•°æ®åº“æ•´ä½“å’Œå®¢æˆ·ç«¯åº”ç”¨ä¾ç„¶å¯ä»¥ç»§ç»­è¿è¡Œï¼Œè¿™çœ‹èµ·æ¥æ˜¯é«˜å¯ç”¨ï¼Œä½†ä¸æ˜¯CAPç†è®ºä¸­çš„Aï¼Œå› ä¸ºåœ¨ç½‘ç»œåˆ†åŒºæ—¶ï¼Œè¢«å­¤ç«‹çš„æœºå™¨ä¸Šçš„æ•°æ®åº“ä¸å¯ç”¨ã€‚FoundationDBå‘ˆç°çš„æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„é€»è¾‘æ•°æ®åº“ï¼Œå³ä½¿ç‰©ç†ä¸Šæ˜¯åˆ†å¸ƒå¼çš„ï¼Œç½‘ç»œåˆ†åŒºæ—¶ï¼ŒæŒ‘æˆ˜æ˜¯è¦å†³å®šå“ªäº›æœºå™¨è¿˜èƒ½ç»§ç»­å¤„ç†è¯»å†™è¯·æ±‚ï¼Œä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒFoundationDBé…ç½®äº†ä¸€ç»„åè°ƒæœåŠ¡å™¨ï¼ˆcoordination serversï¼‰æ¥åšå‡ºå†³å®šï¼ŒFoundationDBä¼šé€‰æ‹©å æ®å¤šæ•°çš„åˆ†åŒºç»§ç»­å¯¹å¤–æä¾›æœåŠ¡ï¼Œå¦‚æœä¸å­˜åœ¨è¿™æ ·çš„åˆ†åŒºï¼Œæ•°æ®åº“å°±çœŸçš„åœæœºäº†ã€‚åè°ƒæœåŠ¡å™¨ä¹‹é—´é€šè¿‡Paxosç®—æ³•ä¿æŒä¸€ä¸ªå°è§„æ¨¡çš„å…±äº«çŠ¶æ€ï¼Œè¿™ä¸ªçŠ¶æ€å…·æœ‰ä¸€è‡´æ€§å’Œåˆ†åŒºå®¹å¿æ€§ï¼Œç±»ä¼¼äºæ•´ä¸ªæ•°æ®åº“ï¼Œå…±äº«çŠ¶æ€åœ¨CAPæ„ä¹‰ä¸Šä¹Ÿä¸æ˜¯å¯ç”¨çš„ä½†åœ¨å æ®å¤šæ•°çš„åˆ†åŒºä¸­èƒ½å¤Ÿæä¾›è¯»å†™èƒ½åŠ›ã€‚FoundationDBåˆ©ç”¨è¿™ä¸ªå…±äº«çŠ¶æ€æ¥ç»´æŠ¤å’Œæ›´æ–°æ‹“æ‰‘ç»“æ„ï¼ˆreplication topologyï¼‰ï¼Œå½“æœ‰èŠ‚ç‚¹å¤±è´¥æ—¶ï¼Œåè°ƒæœåŠ¡å™¨ä¼šåè°ƒè¿™ä¸ªæ‹“æ‰‘çš„å˜æ›´ï¼Œä½†æ³¨æ„åè°ƒæœåŠ¡å™¨å¹¶ä¸å‚ä¸å…·ä½“äº‹åŠ¡çš„æäº¤æ“ä½œã€‚ ä¸ºäº†è¯´æ˜åè°ƒæœåŠ¡å™¨æ˜¯å¦‚ä½•æ”¯æŒå®¹é”™çš„ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªæ”¯æŒæ•°æ®å‰¯æœ¬çš„æœ€å°FoundationDBé›†ç¾¤ï¼Œå½“ç„¶ï¼Œå½“é›†ç¾¤è§„æ¨¡æ›´å¤§æ—¶ï¼Œåè°ƒæœºåˆ¶æä¾›çš„å®¹é”™æ€§å’Œå¯ç”¨æ€§ä¹Ÿä¼šæ›´å¼ºã€‚æƒ³è±¡ä¸€å®¶å°å‹çš„ç½‘ç»œåˆ›ä¸šå…¬å¸å¸Œæœ›å…¶åº”ç”¨ç¨‹åºåœ¨ä¸€ä¸ªæ•°æ®ä¸­å¿ƒå†…éƒ¨è¿è¡Œï¼Œå¹¶ä¸”å³ä½¿æœ‰ä¸€å°æœºå™¨æ•…éšœä¹Ÿèƒ½ä¿æŒå¯ç”¨ï¼Œå®ƒéƒ¨ç½²äº†ä¸€ä¸ªç”±ä¸‰å°æœºå™¨ç»„æˆçš„é›†ç¾¤ï¼Œ Aã€Bã€Cï¼Œæ¯å°æœºå™¨ä¸Šéƒ½è¿è¡Œä¸€ä¸ªæ•°æ®åº“æœåŠ¡å™¨å’Œä¸€ä¸ªåè°ƒæœåŠ¡å™¨ã€‚æ ¹æ®å¤šæ•°æ´¾è§„åˆ™ï¼Œåœ¨è¿™ä¸ªé›†ç¾¤ä¸­ï¼Œä»»ä½•èƒ½å½¼æ­¤é€šä¿¡çš„ä¸¤å°æœºå™¨éƒ½èƒ½ä¿æŒç³»ç»Ÿçš„å¯ç”¨æ€§ï¼Œè¯¥å…¬å¸å°†FoundationDBé…ç½®ä¸ºåŒå‰¯æœ¬å†—ä½™æ¨¡å¼ï¼Œå³doubleï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä»½æ•°æ®éƒ½ä¼šè¢«å¤åˆ¶ä¸¤æ¬¡ï¼Œåˆ†åˆ«ä¿å­˜åœ¨ä¸åŒçš„æœºå™¨ä¸Šã€‚ç°åœ¨è®¾æƒ³ä¸€ç§æƒ…å†µï¼ŒæŸä¸ªæœºæ¶é¡¶éƒ¨çš„äº¤æ¢æœºå‘ç”Ÿæ•…éšœï¼Œå¯¼è‡´Aä¸ç½‘ç»œéš”ç¦»ï¼Œç”±äºFoundationDBè¦æ±‚å¿…é¡»ä»Bæˆ–è€…Cè·å–ç¡®è®¤ï¼Œæ‰€ä»¥Aæ— æ³•æä¾›æ–°çš„äº‹ç‰©ï¼Œè¿è¡Œåœ¨Aä¸Šçš„æ•°æ®åº“æœåŠ¡å™¨åªèƒ½ä¸Aä¸Šçš„åè°ƒæœåŠ¡å™¨é€šä¿¡ï¼Œå› æ­¤æ— æ³•è·å–å¤šæ•°ç¥¨æ¥å»ºç«‹æ–°çš„å‰¯æœ¬æ‹“æ‰‘ç»“æ„ï¼ˆreplicatioin topologyï¼‰ï¼Œå¯¹äºå“ªäº›åªèƒ½é“¾æ¥åˆ°Açš„å®¢æˆ·ç«¯æ¥è¯´ï¼Œæ•°æ®åº“å¤„äºä¸å¯ç”¨çŠ¶æ€ã€‚ç„¶è€Œå¯¹äºæ‰€æœ‰å…¶ä»–å®¢æˆ·ç«¯æ¥è¯´ï¼Œæ•°æ®åº“æœåŠ¡å™¨ä»ç„¶å¯ä»¥è®¿é—®å¤§å¤šæ•°åè°ƒæœåŠ¡å™¨ï¼ˆBå’ŒCï¼‰ï¼Œå‰¯æœ¬é…ç½®ä¹Ÿç¡®ä¿äº†å³ä½¿æ²¡æœ‰Aï¼Œç³»ç»Ÿä¸­ä»ç„¶ä¿ç•™äº†å®Œæ•´çš„æ•°æ®å‰¯æœ¬ï¼Œå¯¹äºè¿™äº›å®¢æˆ·ç«¯æ¥è¯´ï¼Œæ•°æ®åº“ä»ç„¶å¯ä»¥è¯»å†™ï¼ŒwebæœåŠ¡å™¨ä¹Ÿå¯ä»¥ç»§ç»­æä¾›æœåŠ¡ã€‚ å½“ç½‘ç»œåˆ†åŒºç»“æŸåï¼ŒèŠ‚ç‚¹Aå°†å†æ¬¡èƒ½å¤Ÿä¸å¤§å¤šæ•°åè°ƒæœåŠ¡å™¨è¿›è¡Œé€šä¿¡ï¼Œå¹¶é‡æ–°åŠ å…¥æ•°æ®åº“ï¼Œæ ¹æ®é€šä¿¡ä¸­æ–­æŒç»­çš„æ—¶é—´é•¿çŸ­ï¼ŒAä¼šé€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹å¼ä¹‹ä¸€é‡æ–°åŒæ­¥ï¼Œå¦‚æœä¸­æ–­æ—¶é—´è¾ƒçŸ­ï¼Œå®ƒä¼šæ¥å—åœ¨å®ƒç¦»çº¿æœŸé—´æ‰€å‘ç”Ÿçš„äº‹åŠ¡ï¼Œå¦‚æœä¸­æ–­æ—¶é—´è¾ƒé•¿ï¼Œæœ€ç³Ÿç³•çš„æƒ…å†µæ˜¯ï¼Œå®ƒå°†éœ€è¦é‡æ–°ä¼ è¾“æ•´ä¸ªæ•°æ®åº“çš„å†…å®¹ï¼Œä¸€æ—¦AæˆåŠŸé‡æ–°åŠ å…¥æ•°æ®åº“ï¼Œæ‰€æœ‰æ•°æ®åº“å°†å†æ¬¡èƒ½å¤Ÿä»¥å®¹é”™æ–¹å¼å¤„ç†äº‹åŠ¡ï¼Œä¸ä¸Šæ–‡æåˆ°çš„æœ€å°åŒ–é›†ç¾¤ç›¸æ¯”ï¼Œå®é™…ç”Ÿäº§ç¯å¢ƒé€šå¸¸ä¼šé…ç½®ä¸ºä¸‰é‡å†—ä½™æ¨¡å¼ï¼Œå³tripleï¼Œå¹¶éƒ¨ç½²åœ¨äº”å°æˆ–è€…æ›´å¤šæœºå™¨ä¸Šï¼Œä»è€Œå®ç°ç›¸åº”æ›´é«˜çš„å¯ç”¨æ€§ã€‚ ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:3:0","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"ä¸€è‡´æ€§ åœ¨åˆ†å¸ƒå¼æ•°æ®åº“ä¸­è¯­å¢ƒä¸­ï¼Œä¸€è‡´æ€§è¿™ä¸ªæ¦‚å¿µç»å¸¸è¢«ä½“ç§¯ï¼Œç„¶è€Œï¼Œåœ¨ACIDå±æ€§å’ŒCAPå®šç†ä¸­ï¼Œä¸€è‡´æ€§æŒ‡çš„æ˜¯ä¸åŒçš„å«ä¹‰ï¼Œè¿™ä¸¤ä¸ªæ„æ€ç»å¸¸è¢«æ··æ·†ã€‚ACIDä¸­çš„ä¸€è‡´æ€§æŒ‡çš„æ˜¯æ•°æ®å§‹ç»ˆç¬¦åˆåº”ç”¨ç¨‹åºçš„å®Œæ•´æ€§çº¦æŸï¼Œä¾‹å¦‚ï¼ŒæŸæ¡æ•°æ®ä¸å…¶ç´¢å¼•ä¹‹é—´ä¿æŒä¸€è‡´çš„çº¦æŸï¼Œè¿™æŒ‡çš„æ˜¯äº‹åŠ¡æ‰§è¡Œåæ•°æ®åº“çš„çŠ¶æ€å¿…é¡»æ˜¯åˆæ³•çš„ï¼Œæ¯”å¦‚ä¸»å¤–é”®ã€å”¯ä¸€æ€§çº¦æŸã€ä½™é¢ä¸èƒ½ä¸ºè´Ÿç­‰ã€‚CAPä¸­çš„ä¸€è‡´æ€§æŒ‡çš„æ˜¯ä¸€è‡´æ€§æ¨¡å‹ï¼Œæè¿°çš„æ˜¯ä¸€ä¸ªå®¢æˆ·ç«¯çš„å†™æ“ä½œåœ¨ä»€ä¹ˆæ¡ä»¶ä¸‹å¯¹å…¶ä»–å®¢æˆ·ç«¯å¯è§ï¼Œä¸€ä¸ªä¾‹å­æ˜¯æœ€ç»ˆä¸€è‡´æ€§æ¨¡å‹ï¼Œåœ¨è¿™ç§æ¨¡å‹ä¸­ï¼Œå†™å…¥çš„æ•°æ®åœ¨è¶³å¤Ÿé•¿çš„æ—¶é—´åä¼šåœ¨æ‰€æœ‰å‰¯æœ¬ä¸­ä¿æŒä¸€è‡´ï¼ŒCAPçš„ä¸€è‡´æ€§æ›´æ¥è¿‘äºå‰¯æœ¬ä¸€è‡´æ€§æˆ–è€…å¯è§æ€§ä¿è¯ã€‚å®Œæ•´æ€§çº¦æŸå±äºåº”ç”¨å±‚é¢çš„èŒƒç•´ï¼Œè€Œä¸€è‡´æ€§æ¨¡å‹å±äºæ•°æ®åº“å†…éƒ¨å®ç°çš„èŒƒç•´ï¼Œä¸¤è€…éƒ½å¾ˆé‡è¦ï¼Œå› ä¸ºå¦‚ä½•ä¸æ”¯æŒå…¶ä¸­ä»»æ„ä¸€ç§ï¼Œå°±ä¼šå¯¼è‡´æ•°æ®æŸåã€‚ åº”ç”¨ç¨‹åºé€šå¸¸æ ¹æ®å…¶æ‰€å±é¢†åŸŸå®šä¹‰å®Œæ•´æ€§çº¦æŸï¼Œè¿™äº›çº¦æŸå¯ä»¥è¡¨ç°ä¸ºå¯¹æŸä¸ªæ•°æ®å€¼çš„ç±»å‹çº¦æŸï¼Œé™åˆ¶æŸäº›å­—æ®µåªèƒ½å­˜å‚¨ç‰¹å®šç±»å‹çš„æ•°æ®ï¼ˆå³domain integrityï¼‰ï¼Œå¤šä¸ªæ•°æ®å€¼ä¹‹é—´çš„å…³ç³»ï¼ˆreference integrityï¼‰ï¼Œæˆ–è€…æ¥è‡ªåº”ç”¨é¢†åŸŸçš„ä¸šåŠ¡è§„åˆ™ã€‚åœ¨å…³ç³»å‹æ•°æ®åº“ç®¡ç†ç³»ç»Ÿä¸­ï¼Œå®Œæ•´æ€§çº¦æŸé€šå¸¸åœ¨è®¾è®¡å…³ç³»æ¨¡å¼æ—¶ä½¿ç”¨SQLè¿›è¡Œå®šä¹‰ï¼Œåœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œå®Œæ•´æ€§çº¦æŸçš„å®šä¹‰å’Œå¼ºåˆ¶æ‰§è¡Œä¸å…³ç³»æ¨¡å‹ç´§å¯†ç»‘å®šï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§å¤šæ•°NoSQLæ•°æ®åº“æ ¹æœ¬ä¸æ”¯æŒå®Œæ•´æ€§çº¦æŸï¼Œè€Œæ˜¯å°†ç»´æŠ¤æ•°æ®å®Œæ•´æ€§çš„è´£ä»»å®Œå…¨è½¬ç§»ç»™åº”ç”¨ç¨‹åºå¼€å‘è€…ã€‚FoundationDBé‡‡ç”¨ç¬¬ä¸‰ç§æ³•å¸ˆï¼Œç”±äºå®Œæ•´æ€§çº¦æŸæ˜¯ç”±åº”ç”¨é¢†åŸŸå®šä¹‰çš„ï¼ŒFoundationDBçš„æ ¸å¿ƒå¹¶ä¸ä¼šç›´æ¥å¼ºåˆ¶æ‰§è¡Œè¿™äº›çº¦æŸï¼Œç„¶è€Œï¼ŒFoundationDBæ‰€æä¾›çš„äº‹åŠ¡å…·å¤‡åŸå­æ€§å’Œéš”ç¦»æ€§ä¸¤ä¸ªä¿è¯ï¼Œä½¿å¾—åº”ç”¨ç¨‹åºå¼€å‘è€…èƒ½å¤Ÿç›´æ¥æ ¹æ®é¢†åŸŸéœ€æ±‚ç»´æŠ¤å®Œæ•´æ€§çº¦æŸã€‚ç®€è€Œè¨€ä¹‹ï¼Œåªè¦æ¯ä¸ªäº‹åŠ¡åœ¨æ‰§è¡Œæ—¶éƒ½èƒ½ä¿æŒæ‰€éœ€çš„çº¦æŸï¼ŒFoundationDBå°±èƒ½ä¿è¯å¤šä¸ªå®¢æˆ·ç«¯åŒæ—¶æ‰§è¡Œäº‹åŠ¡æ—¶ä¹Ÿèƒ½ä¿æŒè¿™äº›çº¦æŸã€‚è¿™ç§æ–¹æ³•å…è®¸æ„å»ºå¤šç§æ•°æ®æ¨¡å‹ï¼ŒåŒ…æ‹¬é¢å‘æ–‡æ¡£æˆ–å…³ç³»å‹æ¨¡å‹ï¼Œå®ƒä»¬éƒ½å¯ä»¥ä½œä¸ºä¸Šå±‚ç³»ç»Ÿï¼Œè‡ªè¡Œç»´æŠ¤å„è‡ªçš„å®Œæ•´æ€§çº¦æŸã€‚ ä¸€è‡´æ€§æ¨¡å‹ç”¨äºå®šä¹‰æ•°æ®åº“åœ¨å¹¶å‘å†™å…¥çš„æƒ…å†µä¸‹ï¼Œå†™å…¥å¯¹è¯»å–è€…ä½•æ—¶å¯è§çš„ä¿è¯ï¼Œæ ¹æ®ä¿è¯å¼ºåº¦ï¼Œä¸åŒçš„ä¸€è‡´æ€§æ¨¡å‹åˆ†å¸ƒåœ¨ä¸€ä¸ªä»å¼±åˆ°å¼ºçš„è°±ç³»ä¸­ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œä¸€è‡´æ€§è¶Šå¼ºï¼Œå¼€å‘äººå‘˜å°±è¶Šå®¹æ˜“ç†è§£å’Œæ¨ç†æ•°æ®åº“è¡Œä¸ºï¼Œä»è€ŒåŠ å¿«å¼€å‘è¿›åº¦ã€‚ä¾‹å¦‚ï¼Œå› æœä¸€è‡´æ€§ï¼ˆcasual consistencyï¼‰ä¿è¯è¯»å–è€…èƒ½çœ‹åˆ°æ‰€æœ‰å› æœä¸Šå·²æäº¤çš„å†™å…¥ï¼ˆä¹Ÿå°±æ˜¯ä¸å½“å‰è¯»å–æ“ä½œæœ‰ä¾èµ–å…³ç³»çš„å†™æ“ä½œï¼Œè¯»å–æ—¶æ˜¯å¯è§çš„ï¼‰ï¼Œæœ€ç»ˆä¸€è‡´æ€§ï¼ˆeventual consistencyï¼‰ï¼Œä»…ä¿è¯åœ¨è¶³å¤Ÿçš„æ—¶é—´ï¼Œè¯»å–è€…æœ€ç»ˆèƒ½çœ‹åˆ°å†™å…¥çš„æ•°æ®ï¼Œä½†ä¸èƒ½ä¿è¯ä½•æ—¶å¯è§ï¼Œè®¸å¤šæ—©æœŸçš„NoSQLç³»ç»Ÿé‡‡ç”¨çš„å°±æ˜¯æœ€ç»ˆä¸€è‡´æ€§æ¨¡å‹ã€‚FoundationDBæä¾›ä¸¥æ ¼ä¸²è¡ŒåŒ–ï¼ˆstrict serializabilityï¼‰ï¼Œè¿™æ˜¯æœ€å¼ºçš„ä¸€è‡´æ€§æ¨¡å‹ï¼Œä»¥æä¾›æœ€å¤§ç¨‹åº¦çš„å¼€å‘ä¾¿åˆ©æ€§ï¼Œè¿™æ„å‘³ç€å®ƒåœ¨å¹¶å‘åœºæ™¯ä¸‹è¡¨ç°å¾—åƒæ‰€æœ‰äº‹åŠ¡æ˜¯æŒ‰æŸä¸ªé¡ºåºä¸€ä¸ªæ¥ç€ä¸€ä¸ªæ‰§è¡Œï¼Œæ²¡æœ‰ä»»ä½•äº¤é”™ï¼Œä»è€Œå¤§å¤§ç®€åŒ–äº†åº”ç”¨ç¨‹åºçš„é€»è¾‘ã€‚ ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:4:0","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"å¯æ‰©å±•æ€§ å¯æ‰©å±•æ€§è¢«å¹¿æ³›è®¤ä¸ºæ˜¯æˆåŠŸåº”ç”¨ç¨‹åºå¿…å¤‡çš„å±æ€§ä¹‹ä¸€ï¼Œå®é™…ä¸Šï¼Œå¯æ‰©å±•æ€§æ˜¯ä¸ç³»ç»Ÿæ€§èƒ½å¯†åˆ‡ç›¸å…³çš„ä¸‰ä¸ªå±æ€§ä¹‹ä¸€ï¼š é«˜æ€§èƒ½ï¼ˆHigh Performanceï¼‰ï¼šåœ¨ç‰¹å®šé…ç½®ä¸‹å®ç°æœ€é«˜æ€§èƒ½çš„èƒ½åŠ› å¯æ‰©å±•æ€§ï¼ˆscalabilityï¼‰ï¼šåœ¨ä¸åŒè§„æ¨¡ä¸‹é«˜æ•ˆæä¾›æœåŠ¡çš„èƒ½åŠ› å¼¹æ€§ï¼ˆelasticityï¼‰ï¼šèƒ½å¤Ÿå¿«é€Ÿé€‚åº”è§„æ¨¡çš„å¢å‡çš„èƒ½åŠ› ç‰¹åˆ«æ˜¯é«˜æ€§èƒ½å’Œå¯æ‰©å±•æ€§ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå¸¸å¸¸è¢«äººä»¬è¯¯è§£ï¼Œä¾‹å¦‚èš‚èšç¾¤æ¬åœŸå¾ˆæœ‰æ‰©å±•æ€§ï¼Œä½†æ€§èƒ½ä¸é«˜ï¼Œè€Œæ¨åœŸæœºæ¬åœŸæ€§èƒ½å¾ˆé«˜ï¼Œä½†ä¸å…·å¤‡å¯æ‰©å±•æ€§ã€‚è¿™ä¸‰ä¸ªå±æ€§å¯¹ä½ çš„ä¸šåŠ¡éƒ½è‡³å…³é‡è¦ï¼š é«˜æ€§èƒ½æ„å‘³ç€åœ¨ä¸šåŠ¡æµé‡ç¿»å€æ˜¯ä½ æ— éœ€é‡æ„æ¶æ„ å¯æ‰©å±•æ€§æ„å‘³ç€ä½ çš„æ”¯æŒå¯ä»¥ä»å°èµ·æ­¥ï¼Œå¹¶éšç€ä¸šåŠ¡å¢é•¿è€Œå¢é•¿ å¼¹æ€§æ„å‘³ç€ä½ å¯ä»¥æŒç»­æ ¹æ®éœ€æ±‚ä¼˜é›…åœ°æ‰©å±•æ‰©å±•æˆ–è€…æ”¶ç¼©ç³»ç»Ÿè§„æ¨¡ FoundationDBçš„æ„å»ºç›®æ ‡æ˜¯ä¼˜åŒ–ä¸€ç³»åˆ—å…³é”®çš„æ€§èƒ½æŒ‡æ ‡ï¼Œè¿™ç§æ–¹æ³•æ˜¯å®ƒåœ¨ä¼—å¤šåˆ†å¸ƒå¼æ•°æ®åº“ä¸­çš„ä¸€ä¸ªé‡è¦å·®å¼‚ç‚¹ï¼Œéœ€è¦å…¶ä»–ç³»ç»Ÿæ›´å…³æ³¨ç®€åŒ–è‡ªèº«äº§å“çš„å¼€å‘å·¥ä½œï¼Œè€Œä¸æ˜¯æå‡äº§å“çš„æ€§èƒ½ï¼Œè€Œæˆ‘ä»¬åœ¨ç³»ç»Ÿçš„æ¯ä¸€ä¸ªå±‚é¢éƒ½ä¼šè¯„ä¼°æ½œåœ¨è®¾è®¡åœ¨çœŸå®ç¯å¢ƒä¸­çš„æ•ˆç‡ã€‚æˆ‘ä»¬è‡ªè¡Œæ„å»ºäº†é’ˆå¯¹CPUã€å†…å­˜æ§åˆ¶å™¨ã€ç£ç›˜ã€ç½‘ç»œå’ŒSDDçš„åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œå¹¶è¿›è¡Œå»ºæ¨¡å’Œmointorï¼Œç”šè‡³ä¸ºäº†æœ€å¤§åŒ–æ€§èƒ½è€Œç‰ºç‰²å¼€å‘è¿‡ç¨‹çš„ç®€æ´æ€§ã€‚åœ¨æˆ‘ä»¬è¿½æ±‚é«˜æ€§èƒ½æ˜¯ï¼Œå…³æ³¨çš„ä¸åªæ˜¯å„ç§ç®—æ³•åœ¨ç†è®ºä¸Šçš„å¯æ‰©å±•æ€§ï¼Œæˆ‘ä»¬è¿½æ±‚ä¹Ÿç¡®å®å®ç°äº†ç°å®ä¸–ç•Œä¸­çš„é«˜æ€§èƒ½è¡¨ç°-æ¯”å¦‚æ¯ç§’å¤„ç†æ•°ç™¾ä¸‡ä¸ªæ“ä½œã€‚ FoundatioinDBæä¾›äº†å‡ºè‰²çš„å¯æ‰©å±•æ€§ï¼Œä»åœ¨å•å°æœºå™¨ä¸Šä»…å ç”¨ä¸€ä¸ªæ ¸å¿ƒçš„ä¸€éƒ¨åˆ†èµ„æºï¼Œåˆ°åœ¨ä¸€ä¸ªé›†ç¾¤ä¸­å……åˆ†åˆ©ç”¨æ•°åå°é«˜æ€§èƒ½å¤šæ ¸æœºå™¨çš„å…¨éƒ¨ç®—åŠ›ï¼Œéƒ½èƒ½å¤Ÿçµæ´»é€‚åº”ã€‚ FoundationDBæ”¯æŒæ ¹æ®ä¸æ–­å˜åŒ–çš„éœ€æ±‚åŠ¨æ€æ·»åŠ æˆ–ç§»é™¤ç¡¬ä»¶èµ„æºï¼Œè€Œä¸ä¼šä¸­æ–­æœåŠ¡æˆ–é™ä½æœåŠ¡è´¨é‡ã€‚æ¯å½“æ•°æ®å†™å…¥æ•°æ®åº“æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å°†æ¯ä¸€æ¡æ•°æ®å¤åˆ¶åˆ°å¤šå°ç‹¬ç«‹çš„è®¡ç®—æœºä¸Šï¼Œè¿™ç§å‰¯æœ¬æœºåˆ¶ä¸ä»…èƒ½ç«‹å³è¿›è¡Œè´Ÿè½½å‡è¡¡ï¼Œè¿˜èƒ½åœ¨æ›´é•¿æ—¶é—´å†…è‡ªåŠ¨åœ¨è®¡ç®—æœºä¹‹é—´è¿ç§»æ•°æ®ä»¥æŒç»­å¹³è¡¡è´Ÿè½½ï¼Œæ ¹æ®è¯·æ±‚è´Ÿè½½å’Œæ•°æ®è§„æ¨¡ï¼ŒFoundationDBä¼šæ— ç¼åœ°åœ¨åˆ†å¸ƒå¼æœåŠ¡å™¨ä¹‹é—´é‡æ–°åˆ†å¸ƒæ•°æ®ï¼Œå®ƒå…·æœ‰å®Œå…¨çš„å¼¹æ€§ï¼Œé¢å¯¹çƒ­ç‚¹æ•°æ®èƒ½æ¬§åœ¨æ¯«ç§’çº§ä½œä¸ºå“åº”ï¼Œé¢å¯¹é‡å¤§ä½¿ç”¨å˜åŒ–èƒ½åœ¨æ•°åˆ†é’Ÿå†…å®Œæˆè°ƒæ•´ã€‚ ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:5:0","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"FoundationDBçš„æ¶æ„ ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:6:0","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"FoundationDBé›†ç¾¤çš„ç»„æˆ FoundationDBè®©ä½ çš„æ¶æ„æ›´å…·æœ‰çµæ´»æ€§ï¼Œä¸”æ˜“äºè¿ç»´ï¼Œä½ çš„åº”ç”¨ç¨‹åºå¯ä»¥å°†æ•°æ®ç›´æ¥å‘é€åˆ°FoundationDBï¼Œæˆ–å‘é€åˆ°ä¸€ä¸ªå±‚Layerä¸­ï¼Œè¿™æ˜¯ç”¨æˆ·ç¼–å†™çš„æ¨¡å—ï¼Œå¯æä¾›æ–°çš„æ•°æ®æ¨¡å‹ã€ä¸ç°æœ‰ç³»ç»Ÿçš„å…¼å®¹æ€§ï¼Œç”šè‡³ä½œä¸ºå®Œæ•´çš„æ¡†æ¶ä½¿ç”¨ï¼Œæ— è®ºé‚£ç§æ–¹å¼ï¼Œæ‰€æœ‰æ•°æ®éƒ½ä¼šé€šè¿‡ä¸€ä¸ªæœ‰åºåœ°ã€äº‹åŠ¡æ€§çš„é”®å€¼APIå­˜å‚¨åˆ°åŒä¸€ä¸ªåœ°æ–¹ã€‚FoundationDBçš„æ¶æ„é‡‡ç”¨äº†è§£è€¦å¼è®¾è®¡ï¼Œå…¶ä¸­æ¯ä¸ªè¿›ç¨‹è¢«åˆ†é…ä¸åŒçš„å¼‚æ„è§’è‰²ï¼ˆä¾‹å¦‚Coordinatorã€Storage Serverã€Masterç­‰ï¼‰ï¼Œé›†ç¾¤åœ¨è¿è¡Œæ—¶ä¼šå°è¯•å°†ä¸åŒçš„è§’è‰²åˆ†åˆ«æ‹›å‹Ÿä¸ºç‹¬ç«‹çš„è¿›ç¨‹ï¼Œç„¶è€Œä¸ºäº†æ»¡è¶³é›†ç¾¤çš„æ‹›å‹Ÿç›®æ ‡ï¼Œä¹Ÿæœ‰å¯èƒ½å°†å¤šä¸ªæ— çŠ¶æ€è§’è‰²åˆå¹¶éƒ¨ç½²åœ¨åŒä¸€ä¸ªè¿›ç¨‹ä¸Šï¼Œé€šè¿‡ä¸ºä¸åŒè§’è‰²æ°´å¹³æ‰©å±•è¿›ç¨‹æ•°é‡ï¼Œå¯ä»¥å®ç°æ•°æ®åº“çš„æ‰©å±•ã€‚ Coordinator æ‰€æœ‰å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨é€šè¿‡ä¸€ä¸ªcluster fileè¿æ¥åˆ°FoundationDBé›†ç¾¤ï¼Œè¯¥æ–‡ä»¶ä¸­åŒ…å«Coordinatorçš„IP:PORTä¿¡æ¯ï¼Œå®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨éƒ½ä¼šä½¿ç”¨Coordinatorä¸ClusterControllerå»ºç«‹è¿æ¥ï¼Œå¦‚æœå½“å‰æ²¡æœ‰é›†ç¾¤æ§åˆ¶å™¨ï¼ŒæœåŠ¡å™¨ä¼šå°è¯•ç«é€‰ç§°ä¸ºæ§åˆ¶å™¨ï¼Œå¹¶åœ¨é€‰ä¸¾å®Œæˆåå‘å…¶æ³¨å†Œï¼Œå®¢æˆ·ç«¯åˆ™é€šè¿‡é›†ç¾¤æ§åˆ¶å™¨è·å–æœ€æ–°çš„GRVä»£ç†ï¼ˆGRV proxiesï¼‰å’Œæäº¤ä»£ç†ï¼ˆcommit proxiesï¼‰åˆ—è¡¨ã€‚ Cluster Controller é›†ç¾¤æ§åˆ¶å™¨æ˜¯ä¸€ä¸ªç”±å¤šæ•°åè°ƒè€…é€‰ä¸¾äº§ç”Ÿçš„å•ä¾‹è§’è‰²ï¼Œå®ƒæ˜¯æ•´ä¸ªé›†ç¾¤ä¸­æ‰€æœ‰è¿›ç¨‹çš„å…¥å£ï¼Œè´Ÿè´£ä»¥ä¸‹ä»»åŠ¡ï¼šåˆ¤æ–­æŸä¸ªè¿›ç¨‹æ˜¯å¦æ•…éšœã€å‘ŠçŸ¥å„ä¸ªè¿›ç¨‹åº”æ‰¿æ‹…çš„è§’è‰²ï¼Œåœ¨æ‰€æœ‰è¿›ç¨‹ä¹‹é—´ä¼ é€’ç³»ç»Ÿä¿¡æ¯ Master Masterè´Ÿè´£åè°ƒå†™å­ç³»ç»Ÿï¼ˆwrite sub-systemï¼‰ä»ä¸€ä¸ªä»£ï¼ˆgenerationï¼‰è¿‡æ¸¡åˆ°ä¸‹ä¸€ä¸ªä»£çš„è¿‡ç¨‹ï¼Œå†™å­ç³»ç»ŸåŒ…æ‹¬Masterã€GRVä»£ç†ã€æäº¤ä»£ç†ã€è§£æå™¨ä»¥åŠäº‹åŠ¡æ—¥å¿—ï¼Œè¿™ä¸‰ä¸ªè§’è‰²ï¼ˆæŒ‡Masterã€GRV proxyã€commit Proxyï¼‰è¢«è§†ä¸ºä¸€ä¸ªæ•´ä½“ï¼Œå¦‚æœå…¶ä¸­ä»»ä½•ä¸€ä¸ªå¤±è´¥ï¼Œå°†ä¼šåŒæ—¶é‡æ–°æ‹›å‹Ÿè¿™ä¸‰ä¸ªè§’è‰²çš„æ›¿ä»£è¿›ç¨‹ã€‚Masterè¿˜è´Ÿè´£ä¸ºä¸€æ‰¹mutationï¼ˆå˜æ›´æ“ä½œï¼‰æä¾›æäº¤ç‰ˆæœ¬å·ï¼Œå°†å…¶åˆ†å‘ç»™commit proxiesï¼Œåœ¨æ—©æœŸçš„ç‰ˆæœ¬ä¸­ï¼ŒRatekeeperå’ŒData Distributoræ˜¯ä¸Masterè¿è¡Œåœ¨åŒä¸€ä¸ªè¿›ç¨‹ä¸­ï¼Œä½†ä»6.2ç‰ˆæœ¬å¼€å§‹ï¼Œå®ƒä»¬éƒ½æˆä¸ºäº†é›†ç¾¤ä¸­çš„å•ä¾‹è§’è‰²ï¼Œå…¶ç”Ÿå‘½å‘¨æœŸä¸å†ä¾èµ–äºMaster GRV Proxies GRVä»£ç†è´Ÿè´£æä¾›è¯»å–ç‰ˆæœ¬ï¼ˆread versionï¼‰ï¼Œå¹¶ä¸Ratekeeperé€šä¿¡ä»¥æ§åˆ¶è¯»å–ç‰ˆæœ¬çš„å‘æ”¾é€Ÿç‡ï¼Œä¸ºäº†æä¾›ä¸€ä¸ªè¯»å–ç‰ˆæœ¬ï¼ŒGRVä»£ç†ä¼šå‘æ‰€æœ‰MasteræŸ¥è¯¢å½“å‰ä¸ºæ­¢æœ€å¤§çš„å·²æäº¤ç‰ˆæœ¬ï¼ˆcommited versionï¼‰ï¼ŒåŒæ—¶æ£€æŸ¥äº‹åŠ¡æ—¥å¿—ï¼ˆtransaction logï¼‰æ˜¯å¦ä»åœ¨æ­£å¸¸è¿è¡Œï¼ˆæœªåœæ­¢ï¼‰ï¼ŒRatekeeperä¼šäººä¸ºåœ°å‡æ…¢GRVä»£ç†æä¾›è¯»å–ç‰ˆæœ¬çš„é€Ÿç‡ï¼Œä»¥æ§åˆ¶ç³»ç»Ÿè´Ÿè½½ã€‚ Commit Proxies æäº¤ä»£ç†è´Ÿè´£æäº¤äº‹åŠ¡ã€å‘Masteræ±‡æŠ¥å·²æäº¤çš„ç‰ˆæœ¬ï¼Œå¹¶è¿½è¸ªæ¯ä¸ªé”®èŒƒå›´æ‰€å¯¹åº”çš„å­˜å‚¨æœåŠ¡å™¨ï¼Œæäº¤äº‹åŠ¡çš„è¿‡ç¨‹åŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼š å‘Masterè·å–ä¸€ä¸ªæäº¤ç‰ˆæœ¬ï¼ˆcommit versionï¼‰ ä½¿ç”¨è§£æå™¨ï¼ˆresolversï¼‰åˆ¤æ–­å½“å‰äº‹åŠ¡æ˜¯å¦ä¸ä¹‹å‰å·²æäº¤çš„äº‹åŠ¡å†²çª å°†äº‹åŠ¡å†™å…¥äº‹åŠ¡æ—¥å¿—ï¼Œä»¥ä¿è¯å…¶æŒä¹…æ€§ ä»¥\\xffå­—èŠ‚å¼€å¤´çš„é”®ç©ºé—´æ˜¯ä¿ç•™çš„ç³»ç»Ÿå…ƒæ•°æ®åŒºåŸŸï¼Œæ‰€æœ‰å†™å…¥è¯¥åŒºåŸŸçš„mutationï¼ˆå˜æ›´æ“ä½œï¼‰éƒ½ä¼šé€šè¿‡è§£æå™¨åˆ†å‘åˆ°æ‰€æœ‰æäº¤ä»£ç†ï¼Œè¿™ä¸ªç³»ç»Ÿå…ƒæ•°æ®åŒ…å«ä¸€ä¸ªé”®èŒƒå›´åˆ°å­˜å‚¨æœåŠ¡å™¨çš„æ˜ å°„å…³ç³»ï¼Œå³æ¯ä¸ªé”®åŒºé—´æ˜¯ç”±å“ªäº›å­˜å‚¨æœåŠ¡å™¨è´Ÿè´£å­˜å‚¨çš„ã€‚æäº¤ä»£ç†ä¼šæŒ‰éœ€å°†è¿™äº›æ˜ å°„ä¿¡æ¯æä¾›ç»™å®¢æˆ·ç«¯ï¼Œå®¢æˆ·ç«¯ä¼šç¼“å­˜è¿™ä»½æ˜ å°„è¡¨ï¼Œå¦‚æœå®¢æˆ·ç«¯å‘æŸä¸ªå­˜å‚¨æœåŠ¡å™¨è¯·æ±‚ä¸€ä¸ªè¯¥æœåŠ¡å™¨æ²¡æœ‰çš„æ•°æ®é”®ï¼Œä¼šæ¸…æ¥šç¼“å­˜å¹¶ä»æäº¤ä»£ç†è·å–ä¸€ä»½æœ€æ–°çš„æœåŠ¡å™¨åˆ—è¡¨ã€‚ Transaction Logs äº‹åŠ¡æ—¥å¿—å°†å˜æ›´æŒä¹…åŒ–åˆ°ç£ç›˜ï¼Œä»¥å®ç°å¿«é€Ÿçš„æäº¤å»¶è¿Ÿï¼Œè¿™äº›æ—¥å¿—ä¼šæŒ‰ç‰ˆæœ¬é¡ºåºä»æäº¤ä»£ç†ï¼ˆcommit proxyï¼‰æ¥æ”¶æäº¤è¯·æ±‚ï¼Œåªæœ‰åœ¨æ•°æ®è¢«å†™å…¥å¹¶é€šè¿‡fsyncåŒæ­¥åˆ°ç£ç›˜ä¸Šçš„è¿½åŠ å¼å˜æ›´æ—¥å¿—åï¼Œæ‰ä¼šå‘æäº¤ä»£ç†è¿”å›å“åº”ã€‚åœ¨æ•°æ®å†™å…¥ç£ç›˜ä¹‹å‰ï¼Œç³»ç»Ÿå°±ä¼šå°†å˜æ›´è½¬å‘ç»™è´Ÿè´£è¯¥å˜æ›´çš„å­˜å‚¨æœåŠ¡å™¨ï¼Œä¸€æ—¦å­˜å‚¨æœåŠ¡å™¨å°†å˜æ›´æŒä¹…åŒ–ï¼Œå®ƒä»¬å°±ä¼šä»æ—¥å¿—ä»å¼¹å‡ºè¯¥å˜æ›´ï¼Œè¿™é€šè¿‡å‘ç”Ÿåœ¨å˜æ›´æœ€åˆæäº¤åˆ°æ—¥å¿—åçš„å¤§çº¦6så·¦å³ï¼Œæˆ‘ä»¬åªä¼šåœ¨è¿›ç¨‹é‡å¯æ—¶ä»æ—¥å¿—çš„ç£ç›˜ä¸­è¯»å–æ•°æ®ï¼Œå¦‚æœæŸä¸ªå­˜å‚¨æœåŠ¡å™¨å‘ç”Ÿæ•…éšœï¼Œé‚£ä¹ˆåŸæœ¬è¦å‘é€åˆ°è¯¥æœåŠ¡å™¨çš„å˜æ›´å°†ä¼šåœ¨æ—¥å¿—ä¸­å †ç§¯ï¼Œä¸€æ—¦æ•°æ®åˆ†å¸ƒæœºåˆ¶å°†è¯¥æ•…éšœæœåŠ¡å™¨è´Ÿè´£çš„æ•°æ®é‡æ–°åˆ†é…ç»™å…¶ä»–å­˜å‚¨æœåŠ¡å™¨ï¼Œæˆ‘ä»¬å°±ä¼šä¸¢å¼ƒåŸæœ¬ä¸ºè¯¥å¤±è´¥æœåŠ¡å™¨ä¿ç•™çš„æ—¥å¿—æ•°æ®ã€‚ Resolvers è§£æå™¨è´Ÿè´£åˆ¤æ–­äº‹åŠ¡ä¹‹é—´æ˜¯å¦å­˜åœ¨å†²çªï¼Œå¦‚æœæŸä¸ªäº‹åŠ¡è¯»å–äº†æŸä¸ªé”®ï¼Œè€Œè¿™ä¸ªé”®åœ¨è¯¥äº‹åŠ¡çš„è¯»å–ç‰ˆæœ¬ä¸æäº¤ç‰ˆæœ¬ä¹‹é—´è¢«å†™å…¥è¿‡ï¼Œåˆ™è¯¥äº‹åŠ¡å°±ä¼šå‘ç”Ÿå†²çªã€‚è§£æå™¨é€šè¿‡å°†æœ€è¿‘5så†…å·²æäº¤çš„å†™æ“ä½œä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œå¹¶å°†æ–°äº‹åŠ¡çš„è¯»å–è¯·æ±‚ä¸è¿™æ‰¹æäº¤è¿›è¡Œæ¯”è¾ƒï¼Œä»è€Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨å†²çªã€‚ Storage Servers é›†ç¾¤ä¸­ç»å¤§å¤šæ•°è¿›ç¨‹éƒ½æ˜¯å­˜å‚¨æœåŠ¡å™¨ï¼Œå­˜å‚¨æœåŠ¡å™¨è¢«åˆ†é…äº†ä¸€å®šèŒƒå›´çš„é”®åŒºé—´ï¼Œå¹¶è´Ÿè´£å­˜å‚¨è¯¥åŒºé—´å†…çš„æ‰€æœ‰æ•°æ®ï¼Œå®ƒä»¬ä¼šåœ¨å†…å­˜ä¸­ä¿ç•™æœ€è¿‘5sçš„å˜æ›´ï¼ŒåŒæ—¶åœ¨ç£ç›˜ä¸Šä¿ç•™ä¸€ä»½æˆªè‡³5så‰çš„æ•°æ®å‰¯æœ¬ï¼Œå®¢æˆ·ç«¯å¿…é¡»åœ¨æœ€è¿‘5så†…çš„ç‰ˆæœ¬ä¸Šè¿›è¡Œè¯»å–æ“ä½œï¼Œå¦åˆ™ä¼šæ”¶åˆ°transaction_too_oldé”™è¯¯ã€‚å½“å‰çš„SSDå­˜å‚¨å¼•æ“ä½¿ç”¨åŸºäºSQLiteçš„Bæ ‘æ¥å­˜å‚¨æ•°æ®ï¼Œè€Œå†…å­˜å­˜å‚¨å¼•æ“å°†æ•°æ®ä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œå¹¶ç»´æŠ¤ä¸€ä¸ªè¿½åŠ å¼æ—¥å¿—ï¼Œåªæœ‰åœ¨è¿›ç¨‹é‡å¯æ—¶ï¼Œæ‰ä¼šä»ç£ç›˜ä¸­è¯»å–æ—¥å¿—ã€‚åœ¨FoundationDB 7.0ç‰ˆæœ¬ï¼ŒBæ ‘å­˜å‚¨å¼•æ“å°†è¢«å…¨æ–°çš„Redwoodå¼•æ“æ‰€å–ä»£ã€‚ Data Distributor æ•°æ®åˆ†å‘å™¨è´Ÿè´£ç®¡ç†å­˜å‚¨æœåŠ¡å™¨çš„ç”Ÿå‘½å‘¨æœŸï¼Œå†³å®šæ¯ä¸ªå­˜å‚¨æœåŠ¡å™¨è´Ÿè´£å“ªäº›æ•°æ®åŒºé—´ï¼Œå¹¶ç¡®ä¿æ•°æ®åœ¨æ‰€æœ‰å­˜å‚¨æœåŠ¡å™¨ä¹‹é—´å‡åŒ€åˆ†å¸ƒï¼Œæ•°æ®åˆ†å‘å™¨åœ¨é›†ç¾¤ä¸­æ˜¯ä¸€ä¸ªå•ä¾‹è§’è‰²ï¼Œç”±é›†ç¾¤æ§åˆ¶å™¨è´Ÿè´£é€‰ä¸¾å’Œç›‘æ§ã€‚ Ratekeeper é€Ÿç‡æ§åˆ¶å™¨è´Ÿè´£ç›‘æ§ç³»ç»Ÿè´Ÿè½½ï¼Œå½“é›†ç¾¤æ¥è¿‘é¥±å’Œæ—¶ï¼Œå®ƒä¼šé€šè¿‡é™ä½ä»£ç†ï¼ˆproxyï¼‰åˆ†å‘è¯»å–ç‰ˆæœ¬çš„é€Ÿç‡æ¥å‡æ…¢å®¢æˆ·ç«¯äº‹åŠ¡çš„é€Ÿç‡ï¼ŒRatekeeperæ˜¯é›†ç¾¤ä¸­çš„ä¸€ä¸ªå•ä¾‹è§’è‰²ï¼Œç”±é›†ç¾¤æ§åˆ¶å™¨è´Ÿè´£é€‰ä¸¾å’Œç›‘æ§ã€‚ Client å®¢æˆ·ç«¯é€šè¿‡é“¾æ¥ç‰¹å®šè¯­è¨€çš„ç»‘å®šï¼ˆå³å®¢æˆ·ç«¯åº“ï¼‰æ¥ä¸FoundationDBé›†ç¾¤é€šä¿¡ï¼Œè¿™äº›è¯­è¨€ç»‘å®šæ”¯æŒåŠ è½½å¤šä¸ªç‰ˆæœ¬çš„Cåº“ï¼Œå…è®¸å®¢æˆ·ç«¯èƒ½å¤Ÿä¸è¾ƒæ—§ç‰ˆæœ¬çš„FoundationDBé›†ç¾¤è¿›è¡Œé€šä¿¡ï¼Œç›®å‰ï¼ŒFoundationDBå®˜æ–¹æ”¯æŒçš„è¯­è¨€ç»‘å®šåŒ…æ‹¬ï¼šCã€Goã€Pythonã€Javaå’ŒRubyã€‚ ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:6:1","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"äº‹åŠ¡å¤„ç† åœ¨FoundationDBä¸­ï¼Œæ•°æ®åº“äº‹åŠ¡ç”±å®¢æˆ·ç«¯è®¿é—®æŸä¸ªGRV (Get Read Version)ä»£ç†å¼€å§‹ï¼Œä»¥è·å–ä¸€ä¸ªè¯»ç‰ˆæœ¬ï¼Œè¿™ä¸ªè¯»ç‰ˆæœ¬ä¿è¯å¤§äºå®¢æˆ·ç«¯å·²çŸ¥çš„ä»»ä½•æäº¤ç‰ˆæœ¬ï¼Œå³ä½¿è¿™äº›ç‰ˆæœ¬æ˜¯é€šè¿‡FoundationDBä¹‹å¤–çš„å…¶ä»–é€”å¾„è·å¾—çš„ï¼ˆæ¯”å¦‚ä»æ—¥å¿—ã€å…¶ä»–å®¢æˆ·ç«¯ã€å¤–éƒ¨ç³»ç»Ÿç­‰å¾—çŸ¥ï¼‰ï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿å®¢æˆ·ç«¯åœ¨äº‹åŠ¡ä¸­èƒ½å¤Ÿçœ‹åˆ°æ­¤å‰å·²ç»æäº¤çš„å˜æ›´ç»“æœã€‚éšåï¼Œå®¢æˆ·ç«¯å¯ä»¥å‘å­˜å‚¨æœåŠ¡å™¨å‘èµ·å¤šæ¬¡è¯»å–è¯·æ±‚ï¼Œä»¥è·å–è¯¥è¯»å–ç‰ˆæœ¬ä¸‹çš„æ•°æ®ã€‚å†™æ“ä½œåˆ™ä¼šæš‚å­˜äºå®¢æˆ·ç«¯çš„å±€åœ°å†…å­˜ä¸­ï¼Œå¹¶ä¸ä¼šç«‹å³å‘é€åˆ°é›†ç¾¤ã€‚åœ¨é»˜è®¤è¯·æ±‚ä¸‹ï¼Œå¦‚æœåœ¨åŒä¸€ä¸ªäº‹åŠ¡ä¸­è¯»å–äº†æŸä¸ªå·²å†™å…¥çš„é”®ï¼Œå®¢æˆ·ç«¯ä¼šè¿”å›åˆšåˆšå†™å…¥çš„å€¼ã€‚åœ¨æäº¤é˜¶æ®µï¼Œå®¢æˆ·ç«¯ä¼šå°†äº‹åŠ¡æ•°æ®ï¼ˆåŒ…æ‹¬æ‰€æœ‰è¯»å–å’Œå†™å…¥æ“ä½œï¼‰å‘é€ç»™æŸä¸ªæäº¤ä»£ç†ï¼ˆcommit proxyï¼‰ï¼Œå¹¶ç­‰å¾…å…¶è¿”å›äº‹åŠ¡æ˜¯å¦æˆåŠŸæäº¤çš„ç»“æœï¼Œå¦‚æœäº‹åŠ¡ä¸å…¶ä»–äº‹åŠ¡å‘ç”Ÿå†²çªè€Œæ— æ³•æäº¤ï¼Œå®¢æˆ·ç«¯å¯ä»¥é€‰æ‹©ä»å¤´å¼€å§‹é‡è¯•ã€‚å¦‚æœäº‹åŠ¡æäº¤æˆåŠŸï¼Œæäº¤ä»£ç†ä¼šå°†è¯¥äº‹åŠ¡çš„æäº¤ç‰ˆæœ¬è¿”å›ç»™å®¢æˆ·ç«¯ä»¥åŠmasterï¼Œä»¥ä¾¿GRVä»£ç†è·å–åˆ°æœ€æ–°çš„æäº¤ç‰ˆæœ¬ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªæäº¤ç‰ˆæœ¬ç”±masteråˆ†é…ï¼Œä¸”ä¸€å®šå¤§äºæœ€åˆçš„è¯»å–ç‰ˆæœ¬ã€‚FoundationDBçš„æ¶æ„å°†å®¢æˆ·ç«¯çš„è¯»å–æ“ä½œå’Œå†™å…¥ï¼ˆå³äº‹åŠ¡æäº¤ï¼‰æ“ä½œåˆ†ç¦»ï¼Œä»è€Œåˆ†åˆ«å®ç°æ‰©å±•æ€§ï¼Œè¯»å–æ“ä½œç”±å®¢æˆ·ç«¯ç›´æ¥å‘é€ç»™å­˜å‚¨æœåŠ¡å™¨ï¼Œéšç€å­˜å‚¨æœåŠ¡å™¨æ•°å¢åŠ å¯ä»¥çº¿æ€§æ‰©å±•ï¼Œå†™å…¥æ“ä½œåˆ™é€šè¿‡å¢åŠ Commit Proxyã€Resolverå’ŒLog Serverç­‰ç»„ä»¶çš„å®ä¾‹æ¥å®ç°æ‰©å±•ã€‚ ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:6:2","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"ç¡®å®šè¯»å–ç‰ˆæœ¬ å½“å®¢æˆ·ç«¯å‘GRVä»£ç†è¯·æ±‚è¯»å–ç‰ˆæœ¬æ—¶ï¼ŒGRVä»£ç†ä¼šå‘masteræŸ¥è¯¢æœ€æ–°çš„å·²æäº¤ç‰ˆæœ¬ï¼Œå¹¶æ£€æŸ¥ä¸€ç»„ç¬¦åˆå¤åˆ¶ç­–ç•¥çš„äº‹åŠ¡æ—¥å¿—æ˜¯å¦å¤„äºå­˜æ´»çŠ¶æ€ï¼ŒéšåGRVä»£ç†ä¼šå°†å½“å‰æœ€å¤§å·²æäº¤ç‰ˆæœ¬ä½œä¸ºè¯»å–ç‰ˆæœ¬è¿”å›ç»™å®¢æˆ·ç«¯ã€‚GRVä»£ç†ä¹‹æ‰€ä»¥éœ€è¦å‘masterè¯·æ±‚æœ€æ–°çš„æäº¤ç‰ˆæœ¬ï¼Œæ˜¯å› ä¸ºmasteræ˜¯é›†ä¸­ç»´æŠ¤æ‰€æœ‰æäº¤ä»£ç†ä¸­æœ€å¤§æäº¤ç‰ˆæœ¬çš„è§’è‰²ã€‚è€ŒGRVä»£ç†æ£€æŸ¥ä¸€ç»„ç¬¦åˆå¤åˆ¶ç­–ç•¥çš„äº‹åŠ¡æ—¥å¿—æ˜¯å¦å­˜æ´»ï¼Œæ˜¯ä¸ºäº†ç¡®ä¿è‡ªå·±ä¸æ˜¯ä¸€ä¸ªå·²ç»è¢«æ–°ä¸€ä»£GRVä»£ç†å–ä»£çš„æ—§ä»£ç†ï¼Œå› ä¸ºGRVä»£ç†æ˜¯æ— çŠ¶æ€çš„è§’è‰²ï¼Œæ¯æ¬¡æ¢å¤åéƒ½ä¼šé‡æ–°é€‰ä¸¾ï¼Œå¦‚æœå‘ç”Ÿè¿‡ä¸€æ¬¡æ¢å¤ï¼Œè€Œæ—§çš„GRVä»£ç†è¿›ç¨‹ä»ç„¶å­˜æ´»ï¼Œå®ƒä»å¯èƒ½å¯¹å¤–å‘æ”¾è¯»å–ç‰ˆæœ¬ï¼Œè¿™æ ·ä¼šå¯¼è‡´åªè¯»äº‹åŠ¡è¯»å–åˆ°è¿‡æ—¶æ•°æ®ï¼ˆè€Œè¯»å†™äº‹åŠ¡ä¼šå› ä¸ºç‰ˆæœ¬å†²çªè€Œè¢«ä¸­æ­¢ï¼‰ã€‚é€šè¿‡éªŒè¯æ»¡è¶³å¤åˆ¶ç­–ç•¥çš„ä¸€ç»„äº‹åŠ¡æ—¥å¿—ä»ç„¶åœ¨çº¿ï¼ŒGRVä»£ç†å¯ä»¥ç¡®è®¤ç³»ç»Ÿå¹¶æœªç»å†æ¢å¤ï¼Œå› æ­¤åªè¯»äº‹åŠ¡èƒ½å¤Ÿè¯»å–åˆ°æœ€æ–°çš„æ•°æ®ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå®¢æˆ·ç«¯ä¸èƒ½ç›´æ¥æƒ³masterè¯·æ±‚è¯»å–ç‰ˆæœ¬ï¼Œè¿™æ˜¯å› ä¸ºmasteræ— æ³•æ¨ªå‘æ‹“å±•ï¼Œæ‰¿æ‹…å¤ªå¤šå·¥ä½œä¼šæˆä¸ºæ€§èƒ½ç“¶é¢ˆï¼Œå°½ç®¡åˆ†å‘è¯»å–ç‰ˆæœ¬çš„å¼€é”€æœ¬èº«å¹¶ä¸é«˜ï¼Œä½†è¿™é¡¹å·¥ä½œä»ç„¶éœ€è¦masterä»Ratekeeperè·å–äº‹åŠ¡é…é¢ã€æ‰¹å¤„ç†è¯·æ±‚ï¼Œå¹¶å¯èƒ½ç»´æŠ¤æ•°åƒä¸ªå®¢æˆ·ç«¯çš„ç½‘ç»œè¿æ¥ã€‚å› æ­¤ï¼Œè¿™é¡¹å·¥ä½œè¢«å§”æ‰˜ç»™å¯æ‰©å±•çš„GRVä»£ç†æ¥å¤„ç†ã€‚ ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:6:3","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"äº‹åŠ¡æäº¤ ä¸€ä¸ªå®¢æˆ·ç«¯äº‹åŠ¡çš„æäº¤è¿‡ç¨‹åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼Œå®¢æˆ·ç«¯å°†äº‹åŠ¡å‘é€ç»™ä¸€ä¸ªæäº¤ä»£ç†ï¼Œæäº¤ä»£ç†æƒ³masterè¯·æ±‚ä¸€ä¸ªæäº¤ç‰ˆæœ¬ï¼Œmasterè¿”å›ä¸€ä¸ªæ¯”ä»¥å¾€ä»»ä½•æäº¤ç‰ˆæœ¬éƒ½æ›´å¤§çš„ç‰ˆæœ¬å·ï¼Œæäº¤ä»£ç†å°†äº‹åŠ¡çš„è¯»å–å†²çªåŒºé—´å’Œå†™å…¥å†²çªåŒºé—´è¿åŒæäº¤ç‰ˆæœ¬ä¸€èµ·å‘é€ç»™è§£æå™¨ï¼Œè§£æå™¨æ ¹æ®æäº¤ç‰ˆæœ¬å¯¹äº‹åŠ¡æ’åºï¼Œå¹¶åˆ¤æ–­åœ¨æ­¤é¡ºåºä¸‹æ‰§è¡Œæ˜¯å¦æœ‰å†²çªï¼Œä»è€Œç¡®å®šè¯¥äº‹åŠ¡æ˜¯å¦ä¸ä¹‹å‰çš„äº‹åŠ¡å­˜åœ¨å†²çªï¼Œå¦‚æœå­˜åœ¨å†²çªï¼Œæäº¤ä»£ç†ä¼šå‘å®¢æˆ·å•è¿”å›not_commitedé”™è¯¯ï¼Œå¦‚æœä¸å­˜åœ¨å†²çªï¼Œæäº¤ä»£ç†ä¼šå°†è¯¥äº‹åŠ¡çš„å˜æ›´å’Œæäº¤ç‰ˆæœ¬å‘é€åˆ°äº‹åŠ¡æ—¥å¿—ï¼Œä¸€æ—¦è¿™äº›å˜æ›´åœ¨æ—¥å¿—ä¸­è¢«æŒä¹…åŒ–ï¼Œæäº¤ä»£ç†å°±å‘å®¢æˆ·ç«¯è¿”å›æˆåŠŸå“åº”ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæäº¤ä»£ç†ä¼šå°†æ¯ä¸ªè§£æå™¨æ‰€è´Ÿè´£çš„é”®åŒºé—´ï¼ˆkey rangesï¼‰åˆ†åˆ«å‘é€ç»™å¯¹åº”çš„è§£æå™¨ï¼Œå¦‚æœä»»æ„ä¸€ä¸ªè§£æå™¨æ£€æµ‹åˆ°å†²çªï¼Œæ•´ä¸ªäº‹åŠ¡å°±ä¸ä¼šè¢«æäº¤ã€‚è¿™ä¸ªæœºåˆ¶æœ‰ä¸€ä¸ªç¼ºé™·ï¼Œå¦‚æœåªæœ‰æŸä¸€ä¸ªè§£æå™¨å‘ç°äº†å†²çªï¼Œå…¶ä»–è§£æå™¨æ²¡æœ‰ï¼Œåè€…ä¼šé”™è¯¯çš„ä»»åŠ¡äº‹åŠ¡å·²æäº¤ï¼Œå¹¶åœ¨ä¹‹åå¯¹æœ‰é‡å å†™å…¥å†²çªåŒºé—´çš„äº‹åŠ¡æ‰§è¡Œé”™è¯¯çš„å†²çªåˆ¤æ–­ï¼Œå¯èƒ½å¯¼è‡´æœ¬åº”å¯æäº¤çš„äº‹åŠ¡è¢«é”™è¯¯ä¸­æ­¢ï¼Œä¸è¿‡åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä¸€ä¸ªè®¾è®¡è‰¯å¥½çš„å·¥ä½œè´Ÿè½½é€šå¸¸åªæœ‰æå°‘æ•°äº‹åŠ¡å‘ç”Ÿå†²çªï¼Œå› æ­¤è¿™ç§å†²çªæ”¾å¤§ç°è±¡å¯¹æ€§èƒ½å½±å“å¾ˆå°ï¼Œå¦å¤–ï¼Œæ¯ä¸ªäº‹åŠ¡åœ¨è§£æå™¨ä¸­ä»…ä¿ç•™5sçš„å†²çªä¿¡æ¯ï¼Œ5såè§£æå™¨ä¼šæ¸…æ¥šæ—§äº‹åŠ¡çš„å†²çªåŒºé—´ï¼Œè¿™ä¹Ÿè¿›ä¸€æ­¥é™ä½äº†å‡ºç°æ­¤ç±»è¯¯åˆ¤å†²çªçš„å¯èƒ½æ€§ã€‚ ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:6:4","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"åå°å·¥ä½œ åœ¨äº‹åŠ¡å¤„ç†æµç¨‹èƒŒåå­˜åœ¨ä¸€äº›åå°å·¥ä½œï¼š Ratekeeperï¼šä»GRVä»£ç†ã€æäº¤ä»£ç†ã€äº‹åŠ¡æ—¥å¿—ã€å­˜å‚¨æœåŠ¡å™¨æ”¶é›†ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶è®¡ç®—é›†ç¾¤é¢„æœŸçš„äº‹åŠ¡é€Ÿç‡ Data distributionï¼šç›‘æ§æ‰€æœ‰çš„å­˜å‚¨æœåŠ¡å™¨å¹¶æ‰§è¡Œè´Ÿè½½å‡è¡¡æ“ä½œåœ¨æ‰€æœ‰å­˜å‚¨æœåŠ¡å™¨ä¸­å‡åŒ€åˆ†å¸ƒæ•°æ® Storage Serverï¼šä»äº‹åŠ¡æ—¥å¿—æ‹‰å–å˜æ›´ï¼Œå°†å®ƒä»¬å†™å…¥å­˜å‚¨æœåŠ¡å™¨çš„ç£ç›˜ commit Proxiesï¼šå½“æ²¡æœ‰å®¢æˆ·ç«¯äº§ç”Ÿäº‹åŠ¡æ—¶ï¼Œå‘¨æœŸæ€§çš„å‘é€ç©ºçš„æäº¤åˆ°äº‹åŠ¡æ—¥å¿—ä»¥ä¿æŒæäº¤ç‰ˆæœ¬çš„å¢é•¿ ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:6:5","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"äº‹åŠ¡ç³»ç»Ÿæ¢å¤ äº‹åŠ¡ç³»ç»Ÿå®ç°äº†FoundtionDBé›†ç¾¤çš„å†™æµæ°´çº¿ï¼Œå®ƒçš„æ€§èƒ½ä¸¥é‡å½±å“äº‹åŠ¡æäº¤å»¶è¿Ÿï¼Œå…¸å‹çš„æ¢å¤æ—¶é—´å¤§æ¦‚åœ¨å‡ ç™¾æ¯«ç§’ï¼Œä½†æ›´é•¿æ—¶é—´ï¼ˆé€šå¸¸å‡ ç§’é’Ÿï¼‰ä¹Ÿæœ‰å¯èƒ½å‘ç”Ÿã€‚æ¯å½“äº‹åŠ¡ç³»ç»Ÿä¸­å‘ç”Ÿæ•…éšœæ—¶ï¼Œå°†æ‰§è¡Œæ¢å¤æµç¨‹ä»è€Œæ¢å¤äº‹åŠ¡ç³»ç»Ÿåˆ°ä¸€ä¸ªæ–°çš„é…ç½®ï¼Œæ¯”å¦‚è¯´ä¸€ä¸ªå¹²å‡€çš„çŠ¶æ€ã€‚ç‰¹åˆ«çš„ï¼Œmasterè¿›ç¨‹ç›‘æ§GRVä»£ç†ã€æäº¤ä»£ç†ã€Resolverã€äº‹åŠ¡æ—¥å¿—çš„å¥åº·ï¼Œå¦‚æœè¢«ç›‘æ§çš„è¿›ç¨‹ä¸­ä»»ä½•ä¸€ä¸ªå‘ç”Ÿæ•…éšœï¼Œmasterè¿›ç¨‹å°†ä¼šä¸­æ­¢ï¼Œé›†ç¾¤æ§åˆ¶å™¨æ£€æµ‹åˆ°è¿™ä¸€äº‹ä»¶ï¼Œå°†ä¼šé€‰ä¸¾æ–°çš„Masterï¼Œæ–°çš„Masterå°†åè°ƒç»„ç»‡æ¢å¤è¿‡ç¨‹å¹¶ä¸”é€‰ä¸¾å‡ºä¸€ä¸ªæ–°çš„äº‹åŠ¡ç³»ç»Ÿå®ä¾‹ï¼Œé€šè¿‡è¿™ç§æ–¹å¼ï¼Œäº‹åŠ¡å¤„ç†è¢«åˆ’åˆ†ä¸ºå¤šä¸ªepochï¼ˆæ—¶æœŸï¼‰ï¼Œæ¯ä¸ªepochä»£è¡¨äº‹åŠ¡ç³»ç»Ÿçš„ä¸€ä»£å®ä¾‹ï¼Œå¹¶ç”±ä¸€ä¸ªå”¯ä¸€çš„Masterè¿›ç¨‹è´Ÿè´£åè°ƒã€‚å¯¹æ¯ä¸ªepochï¼ŒMasteré€šè¿‡å‡ æ­¥å‘èµ·æ¢å¤æµç¨‹ï¼Œé¦–å…ˆï¼ŒMasterä»Coordinatorè¯»å–ä¹‹å‰çš„äº‹åŠ¡ç³»ç»ŸçŠ¶æ€ï¼Œå¹¶ä¸”é”å®šåè°ƒçŠ¶æ€ï¼Œä»¥é˜²æ­¢å…¶ä»–Masterè¿›ç¨‹åŒæ—¶è¿›è¡Œæ¢å¤æ“ä½œã€‚ç„¶åMasteræ¢å¤åŸå…ˆçš„äº‹åŠ¡ç³»ç»ŸçŠ¶æ€ï¼ŒåŒ…æ‹¬æ‰€æœ‰äº‹åŠ¡æ—¥å¿—æœåŠ¡å™¨çš„ä¿¡æ¯ï¼Œåœæ­¢è¿™äº›äº‹åŠ¡æ—¥å¿—æœåŠ¡å™¨æ¥å—äº‹åŠ¡ï¼Œå¹¶ä¸”é€‰ä¸¾æ–°çš„ä¸€ç»„GRVä»£ç†ã€æäº¤ä»£ç†ã€è§£æå™¨å’Œäº‹åŠ¡æ—¥å¿—æœåŠ¡å™¨ã€‚åœ¨ä¹‹å‰çš„äº‹åŠ¡æ—¥å¿—æœåŠ¡å™¨åœæ­¢å¹¶ä¸”é€‰ä¸¾å‡ºæ–°çš„äº‹åŠ¡ç³»ç»Ÿä¹‹åï¼ŒMasterä¼šå°†åŒ…å«å½“å‰äº‹åŠ¡ç³»ç»Ÿä¿¡æ¯çš„åè°ƒçŠ¶æ€å†™å…¥åè°ƒè€…ï¼Œæœ€åï¼ŒMasterå¼€å§‹æ¥å—æ–°çš„äº‹åŠ¡æäº¤è¯·æ±‚ã€‚ å› ä¸ºGRVä»£ç†ã€æäº¤ä»£ç†å’Œè§£æå™¨æ˜¯æ— çŠ¶æ€çš„ï¼Œå®ƒä»¬çš„æ¢å¤ä¸éœ€è¦é¢å¤–çš„å·¥ä½œï¼Œç›¸åçš„ï¼Œäº‹åŠ¡æ—¥å¿—æœåŠ¡å™¨ä¿å­˜æäº¤äº‹åŠ¡çš„æ—¥å¿—ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿ä¹‹å‰æ‰€ä»¥æäº¤çš„äº‹åŠ¡éƒ½æ˜¯æŒä¹…åŒ–çš„ï¼Œå¹¶ä¸”å¯è¢«å­˜å‚¨æœåŠ¡å™¨è·å–ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹äºä»»ä½•æäº¤ä»£ç†å·²å‘å®¢æˆ·ç«¯è¿”å›æäº¤æˆåŠŸçš„äº‹åŠ¡ï¼Œå®ƒä»¬çš„å˜æ›´æ—¥å¿—å¿…é¡»å·²ç»è¢«æŒä¹…åŒ–åˆ°å¤šä¸ªæ—¥å¿—æœåŠ¡å™¨ä¸­ï¼ˆä¾‹å¦‚ï¼Œåœ¨å‰¯æœ¬è¦æ±‚ä¸º3çš„æƒ…å†µä¸‹ï¼Œæ—¥å¿—éœ€ä¿ç•™äº3ä¸ªæœåŠ¡å™¨ä¸Šï¼‰ã€‚ æœ€åï¼Œæ¢å¤è¿‡ç¨‹ä¼šå°†äº‹åŠ¡ç³»ç»Ÿæ—¶é—´å¿«é€Ÿæ¨è¿›90sï¼Œè¿™ä¼šå¯¼è‡´æ‰€æœ‰æ­£åœ¨è¿›è¡Œçš„å®¢æˆ·ç«¯äº‹åŠ¡å¼•è¯»å–ç‰ˆæœ¬è¿‡æ—§è€Œè¢«ä¸­æ­¢ï¼Œå¹¶è¿”å›transaction_too_oldé”™è¯¯ã€‚åœ¨é‡è¯•è¿‡ç¨‹ä¸­ï¼Œè¿™äº›å®¢æˆ·ç«¯äº‹åŠ¡å°†è¿æ¥åˆ°æ–°ä¸€ä»£çš„äº‹åŠ¡ç³»ç»Ÿï¼Œå¹¶é‡æ–°å‘èµ·æäº¤ã€‚ commit_result_unknowné”™è¯¯è¡¨ç¤ºï¼šåœ¨äº‹åŠ¡æäº¤è¿‡ç¨‹ä¸­å¦‚æœå‘ç”Ÿäº†æ¢å¤ï¼Œä¹Ÿå°±æ˜¯è¯´æäº¤ä»£ç†å·²ç»å°†å˜æ›´å‘é€ç»™äº‹åŠ¡æ—¥å¿—ï¼Œé‚£ä¹ˆå®¢æˆ·ç«¯å¯èƒ½ä¼šæ”¶åˆ°commit_result_unknowné”™è¯¯ï¼Œå¹¶éšåé‡è¯•è¯¥äº‹åŠ¡ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒFoundationDBå…è®¸ç¬¬ä¸€æ¬¡æäº¤å’Œé‡è¯•æäº¤éƒ½æˆåŠŸï¼Œå› ä¸ºcommit_result_unknownçš„å«ä¹‰æ˜¯è¿™ä¸ªäº‹åŠ¡å¯èƒ½æäº¤äº†ï¼Œå¯èƒ½æ²¡æœ‰æäº¤ï¼Œå› æ­¤å¼ºçƒˆå»ºè®®äº‹åŠ¡å…·æœ‰å¹‚ç­‰æ€§ï¼Œè¿™æ ·æ‰èƒ½æ­£ç¡®å¤„ç†commit_result_unknownåœºæ™¯ä¸‹å¯èƒ½å‡ºç°çš„é‡å¤æäº¤ã€‚ ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:6:6","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"äº‹åŠ¡æƒå¨ ç”±äºFoundationDBå…è®¸å¤šä¸ªå®¢æˆ·ç«¯åŒæ—¶è¿›è¡Œæ“ä½œï¼Œå®ƒå†…éƒ¨ä½¿ç”¨ä¸€ç»„åˆ†å¸ƒå¼èŠ‚ç‚¹æ¥åä½œå®ŒæˆèŠ‚ç‚¹çš„å†²çªæ£€æµ‹ï¼Œè¿™äº›èŠ‚ç‚¹å…±åŒæ„æˆäº†æ‰€è°“äº†äº‹åŠ¡æƒå¨ï¼ˆtransactional authorityï¼‰ã€‚åœ¨äº‹åŠ¡æ‰§è¡Œæ—¶ï¼ŒFoundationDBä½¿ç”¨çš„æ˜¯ä¹è§‚å¹¶å‘æ§åˆ¶ï¼ˆOptimistic Concurrency Controlï¼ŒOCCï¼‰ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä¸ä¼šåƒä¼ ç»Ÿæ•°æ®åº“é‚£æ ·åœ¨æ›´æ–°æŸä¸ªkeyå‰åŠ é”ï¼Œæ›´æ–°åè§£é”ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯ï¼Œå®¢æˆ·ç«¯æäº¤äº‹åŠ¡åï¼Œç³»ç»Ÿä¼šå»æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–äº‹åŠ¡åœ¨æ­¤æœŸé—´å¯¹ç›¸åŒçš„keyåšè¿‡å†²çªæ€§çš„ä¿®æ”¹ï¼Œå¦‚æœå­˜åœ¨å†²çªï¼Œäº‹åŠ¡ä¼šè¢«äº‹åŠ¡æƒå¨æ‹’ç»ï¼Œå®¢æˆ·ç«¯éœ€è¦é‡è¯•è¿™ç¬”äº‹åŠ¡ã€‚ ä¸ºäº†ä¿æŒé«˜æ€§èƒ½ï¼Œäº‹åŠ¡æƒå¨æ˜¯ç”±å¤šä¸ªç‹¬ç«‹çš„äº‹åŠ¡æœåŠ¡å™¨ï¼ˆtransaction serversï¼‰å®ç°çš„ï¼Œæ¯å°æœåŠ¡å™¨è´Ÿè´£å¤„ç†éƒ¨åˆ†ä¼ å…¥çš„äº‹åŠ¡æµï¼ŒFoundationDBå°†äº‹åŠ¡å¤„ç†æ‹†åˆ†ä¸ºè‹¥å¹²ç‹¬ç«‹çš„åŠŸèƒ½æ¨¡å—ï¼Œå¹¶èƒ½å¤Ÿåˆ†åˆ«å¯¹è¿™äº›æ¨¡å—è¿›è¡Œæ‹“å±•ï¼Œè¿™äº›åŠŸèƒ½æ¨¡å—åŒ…æ‹¬ï¼š å¯¹ä¼ å…¥çš„äº‹åŠ¡è¿›è¡Œæ‰¹å¤„ç† æ£€æµ‹äº‹åŠ¡å†²çª è®°å½•äº‹åŠ¡æ—¥å¿— æŒä¹…åŒ–å­˜å‚¨æ•°æ® åœ¨äººä»¬çš„ç›´è§‰ä¸­ï¼Œé€šå¸¸ä¼šè®¤ä¸ºäº‹åŠ¡å†²çªæ£€æµ‹é˜¶æ®µå¯èƒ½æ˜¯æ€§èƒ½ç“¶é¢ˆï¼Œå¹¸è¿çš„æ˜¯ï¼Œè¿™ä¸ªé˜¶æ®µå®é™…ä¸Šæ˜¯å¯ä»¥æ‰©å±•çš„ï¼ŒFoundationDBä½¿ç”¨äº†å…ˆè¿›çš„æ•°æ®å¹¶è¡Œï¼ˆdata-parallelï¼‰å’Œå¤šçº¿ç¨‹ç®—æ³•å¯¹å†²çªæ£€æµ‹è¿›è¡Œä¼˜åŒ–ï¼Œå› æ­¤è¯¥é˜¶æ®µåªå ç”¨ç³»ç»Ÿæ€»èµ„æºçš„å¾ˆå°ä¸€éƒ¨åˆ†ï¼Œæ­£æ˜¯ç”±äºè¿™ç§ä¼˜åŒ–ï¼Œä½¿å¾—å°‘é‡çš„äº‹åŠ¡æœåŠ¡å™¨å°±èƒ½æ”¯æ’‘ä¸€ä¸ªå¤§å‹çš„å­˜å‚¨æœåŠ¡å™¨é›†ç¾¤ã€‚ ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:6:7","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"Layeræ¦‚å¿µ å½“æˆ‘ä»¬å¼€å§‹æ„å»ºFoundationDBæ—¶ï¼Œæˆ‘ä»¬æ²¡æœ‰å»è®¾æƒ³å®ƒå¯ä»¥å…·å¤‡å“ªäº›åŠŸèƒ½ï¼Œè€Œæ˜¯åè¿‡æ¥é—®è‡ªå·±ï¼Œæœ‰å“ªäº›åŠŸèƒ½å¯ä»¥è¢«å»æ‰ï¼Œæˆ‘ä»¬çš„ç»“è®ºæ˜¯å‡ ä¹æ‰€æœ‰åŠŸèƒ½éƒ½å¯ä»¥å»æ‰ï¼Œæˆ‘ä»¬é€‰æ‹©ç®€åŒ–æ ¸å¿ƒè®¾è®¡ï¼ŒæŠŠé‡ç‚¹æ”¾åœ¨å°†æ ¸å¿ƒç³»ç»Ÿåšå¾—å°½å¯èƒ½å¼ºå¤§å’Œå¯é ä¸Šï¼Œå…¶ä»–åŠŸèƒ½ä½œä¸ºåˆ†å±‚ï¼ˆlayerï¼‰æ„å»ºåœ¨æ ¸å¿ƒä¹‹ä¸Šã€‚ å½“ä½ åœ¨ä»Šå¤©é€‰æ‹©ä¸€ä¸ªæ•°æ®åº“æ—¶ï¼Œå®é™…ä¸Šä½ å¹¶ä¸æ˜¯é€‰æ‹©äº†ä¸€é¡¹æŠ€æœ¯ï¼Œè€Œæ˜¯åŒæ—¶é€‰æ‹©äº†ä¸‰å±‚æŠ€æœ¯ï¼šå­˜å‚¨å¼•æ“ã€æ•°æ®æ¨¡å‹ã€API/æŸ¥è¯¢è¯­è¨€ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œé€‰æ‹©PostgreSQLï¼Œä½ å®é™…ä¸Šé€‰æ‹©äº†PostgreSQLçš„å­˜å‚¨å¼•æ“ã€å…³ç³»å‹æ•°æ®æ¨¡å‹ä»¥åŠSQLæŸ¥è¯¢è¯­è¨€ï¼Œé€‰æ‹©MongoDBï¼Œä½ é€‰æ‹©çš„æ˜¯MongoDBçš„åˆ†å¸ƒå¼å­˜å‚¨å¼•æ“ã€æ–‡æ¡£å‹æ•°æ®æ¨¡å‹ä»¥åŠMongoDBçš„APIã€‚åœ¨è¿™äº›ç³»ç»Ÿä¸­ï¼Œå„å±‚çš„åŠŸèƒ½æ˜¯äº¤ç»‡åœ¨ä¸€èµ·çš„ï¼Œä¾‹å¦‚ï¼Œå®ƒä»¬éƒ½æä¾›äº†ç´¢å¼•åŠŸèƒ½ï¼Œè€Œç´¢å¼•è¿™ä¸ªæ¦‚å¿µåœ¨å­˜å‚¨å¼•æ“ã€æ•°æ®æ¨¡å‹å’ŒAPIä¸‰å±‚ä¸­éƒ½å­˜åœ¨å…¶è¡¨ç°å½¢å¼ï¼Œä¸è®ºæ˜¯æ–‡æ¡£æ•°æ®åº“ã€åˆ—å¼å­˜å‚¨ã€è¡Œå­˜å‚¨ã€JSONæ•°æ®åº“è¿˜æ˜¯é”®å€¼å­˜å‚¨ï¼Œå„ç§æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹éƒ½æœ‰å…¶åˆç†æ€§ï¼Œè€Œç°å®ä¸­ï¼Œä¸€ä¸ªåº”ç”¨çš„ä¸åŒéƒ¨åˆ†å¯èƒ½é€‚åˆä¸åŒçš„æ•°æ®æ¨¡å‹ï¼Œè¿™å°±å¸¦æ¥äº†ä¸€ä¸ªè‰°éš¾çš„é€‰æ‹©ï¼Œæ˜¯ä¸ºäº†æ”¯æŒæ–°çš„æ•°æ®æ¨¡å‹å¼•å…¥ä¸€æ•´å¥—å…¨æ–°çš„æ•°æ®åº“è¿˜æ˜¯ç¡¬å¡è¿›å·²æœ‰çš„æ•°æ®åº“ï¼Œå‹‰å¼ºé€‚é…ï¼Ÿ FoundationDBå°†æ•°æ®å­˜å‚¨æŠ€æœ¯å’Œæ•°æ®æ¨¡å‹è§£è€¦å¼€æ¥ï¼Œå®ƒçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªæœ‰åºçš„é”®å€¼å­˜å‚¨å¼•æ“ï¼Œå¯ä»¥é«˜æ•ˆåœ°é€‚é…å¹¶æ˜ å°„åˆ°å„ç§ä¸°å¯Œçš„æ•°æ®æ¨¡å‹ä¸Šï¼Œä»¥ç´¢å¼•ä¸ºä¾‹ï¼ŒFoundationDBçš„æ ¸å¿ƒå±‚å®Œå…¨ä¸æä¾›ç´¢å¼•åŠŸèƒ½ï¼Œä¹Ÿæ°¸è¿œä¸ä¼šæä¾›ã€‚ç›¸åï¼Œç´¢å¼•åŠŸèƒ½ä¹Ÿç”±ä¸Šå±‚ç»„ä»¶ï¼ˆLayerï¼‰æ¥å®ç°ï¼Œé€šè¿‡å­˜å‚¨ä¸¤ç±»é”®å€¼å¯¹ï¼šä¸€ç±»ç”¨äºå­˜å‚¨åŸå§‹æ•°æ®ï¼Œå¦ä¸€ç±»ç”¨äºç»´æŠ¤ç´¢å¼•ã€‚ä¾‹å¦‚ï¼Œé”®å€¼å¯¹people/alice/eye_color=blueå­˜å‚¨äº†Aliceçš„çœ¼ç›é¢œè‰²ï¼ŒåŒæ—¶ç»´æŠ¤ä¸€ä¸ªç´¢å¼•é”®å€¼å¯¹eye_color/blue/alice=trueè¡¨ç¤ºæŒ‰çœ¼ç›é¢œè‰²ç´¢å¼•åˆ°Aliceã€‚ç°åœ¨è¦æŸ¥æ‰¾æ‰€æœ‰çœ¼ç›æ˜¯è“è‰²çš„äººï¼Œåªéœ€æŸ¥æ‰¾æ‰€æœ‰ä»¥eye_color/blueå¼€å¤´çš„é”®å³å¯ï¼Œç”±äºFoundationDBæ ¸å¿ƒä¼šä¿æŒæ‰€æœ‰é”®çš„æœ‰åºæ€§ï¼Œå¹¶ä¸”è¿™äº›é”®æœ‰å…±åŒçš„å‰ç¼€ï¼Œæ‰€ä»¥è¿™ä¸ªæ“ä½œå¯ä»¥é€šè¿‡ä¸€æ¬¡é«˜æ•ˆçš„èŒƒå›´è¯»å–ï¼ˆrange-readï¼‰å®ç°ï¼Œå½“ç„¶ï¼Œä»»ä½•ä¸€ä¸ªæ”¯æŒæœ‰æ•ˆé”®å€¼çš„æ•°æ®åº“ç†è®ºä¸Šéƒ½èƒ½é‡‡ç”¨ç±»ä¼¼çš„æ–¹å¼ï¼Œä½†çœŸæ­£çš„é­”åŠ›åœ¨äºFoundationDBæä¾›äº†çœŸæ­£çš„ACIDäº‹åŠ¡æ”¯æŒï¼Œç´¢å¼•å±‚å¯ä»¥åœ¨åŒä¸€ä¸ªäº‹åŠ¡ä¸­åŒæ—¶æ›´æ–°æ•°æ®å’Œç´¢å¼•ï¼Œä»è€Œç¡®ä¿ä¸¤è€…çš„ä¸€è‡´æ€§ï¼Œè¿™ç§äº‹åŠ¡ä¿éšœçš„é‡è¦æ€§ä¸å®¹ä½ä¼°ï¼Œæ­£å› ä¸ºæœ‰äº†äº‹åŠ¡æœºåˆ¶ï¼Œæ‰ä½¿å¾—æ„å»ºä¸Šå±‚åŠŸèƒ½ï¼ˆå¦‚ç´¢å¼•ã€è§†å›¾ã€å…³ç³»æ¨¡å‹ç­‰ï¼‰å˜å¾—ç®€å•ã€å¯é å’Œé«˜æ•ˆã€‚ ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:7:0","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"æ€§èƒ½ ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:8:0","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"å¯æ‹“å±•æ€§ FoundationDBèƒ½å¤Ÿå¾ˆå¥½åœ°æ¨ªå‘æ‰©å±•ï¼Œé›†ç¾¤ä¸­CPUæ ¸å¿ƒæ•°å¢åŠ æ—¶ï¼Œå®ƒçš„æ€§èƒ½ä¹Ÿä¼šæˆæ¯”ä¾‹çš„æé«˜ï¼Œæ— è®ºé›†ç¾¤æ˜¯å°è§„æ¨¡è¿˜æ˜¯å¤§è§„æ¨¡ã€‚ ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:8:1","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"å»¶è¿Ÿ FoundationDBåœ¨å„ç§ä¸åŒç±»å‹çš„å·¥ä½œè´Ÿè½½ä¸‹éƒ½èƒ½ä¿æŒè¾ƒä½çš„å»¶è¿Ÿï¼Œå³ä½¿åœ¨æ¥è¿‘é›†ç¾¤é¥±å’Œï¼ˆèµ„æºæ¥è¿‘ç”¨å°½ï¼‰æ—¶ï¼Œå»¶è¿Ÿä¹Ÿåªæ˜¯æ¸©å’Œåœ°å¢åŠ ã€‚å¯¹äºFoundationDBå®¢æˆ·ç«¯æ¥è¯´ï¼Œæœ€æ˜¾è‘—çš„å»¶è¿Ÿå‡ºç°åœ¨äº‹åŠ¡çš„å‡†å¤‡å’Œæäº¤è¿‡ç¨‹ä¸­ï¼Œå†™æ“ä½œæœ¬èº«ä¸ä¼šå¼•å…¥å»¶è¿Ÿï¼Œç›´åˆ°äº‹åŠ¡æäº¤ã€‚ä¸€ä¸ªäº‹åŠ¡ä¸­çœŸæ­£æ¶‰åŠå»¶è¿Ÿçš„æ“ä½œæœ‰ä¸‰ç§ï¼š äº‹åŠ¡å¼€å§‹ï¼šç¬¬ä¸€æ¬¡è¯»å–æ—¶éœ€è¦è·å–ä¸€è‡´æ€§ç‰ˆæœ¬å·ï¼Œè¿™é€šå¸¸éœ€è¦å‡ æ¯«ç§’ï¼Œåœ¨é«˜å†™è´Ÿè½½æ—¶å»¶è¿Ÿå¯èƒ½ä¼šé›†ä¸­åœ¨è¿™é‡Œ è¯»å–æ“ä½œï¼šå•æ¬¡è¯»å–çš„å»¶è¿Ÿé€šå¸¸ä½äº1æ¯«ç§’ï¼Œä½†å¦‚æœè¯»å–æ—¶ä¸²è¡Œè¿›è¡Œï¼Œå»¶è¿Ÿä¼šç´¯ç§¯ï¼Œä½¿ç”¨éé˜»å¡å¹¶å‘è¯»å–å¯ä»¥å‡å°‘æ€»å»¶è¿Ÿå’Œå†²çª æäº¤æ“ä½œï¼šå¯¹äºå†™äº‹åŠ¡ï¼Œæäº¤ä¼šåœ¨ç¡®ä¿æ•°æ®æŒä¹…åŒ–ä¸”å®Œæˆå¤åˆ¶åæˆåŠŸï¼Œå¹³å‡å»¶è¿Ÿä¸º3msï¼Œåªæœ‰ä¸€å°éƒ¨åˆ†ä¼šå½±å“äº‹åŠ¡å†²çªæ£€æµ‹ã€‚ ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:8:2","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"åå FoundationDBæä¾›äº†è‰¯å¥½çš„ååé‡ï¼Œé€‚ç”¨äºå„ç§è¯»å†™å·¥ä½œè´Ÿè½½ï¼Œå¹¶æ”¯æŒä¸¤ç§å®Œå…¨æŒä¹…åŒ–çš„å­˜å‚¨å¼•æ“é€‰é¡¹ã€‚Memoryå¼•æ“é€‚ç”¨äºæ•°æ®é›†å®Œå…¨å¯ä»¥æ”¾å…¥å†…å­˜çš„åœºæ™¯ï¼Œåªåœ¨å†™å…¥æ—¶ä½¿ç”¨äºŒçº§å­˜å‚¨ï¼ˆå¦‚ç£ç›˜ï¼‰æ¥ä¿è¯æŒä¹…æ€§ï¼Œè¯»å–æ“ä½œå…¨éƒ¨æ¥è‡ªå†…å­˜ã€‚SSDå¼•æ“é€‚ç”¨äºæ•°æ®é›†æ— æ³•å®Œå…¨æ”¾å…¥å†…å­˜çš„åœºæ™¯ï¼Œä¸€éƒ¨åˆ†çš„è¯»å–ä¼šæ¥è‡ªäºŒæ¬¡å­˜å‚¨ï¼ˆSSDï¼‰ã€‚å› ä¸ºSATAæ¥å£çš„SSDåªæ¯”å†…å­˜æ…¢å¤§çº¦50å€ï¼Œæ‰€ä»¥åªè¦ç¼“å­˜å‘½ä¸­ç‡åˆç†ï¼Œå°†SSDå’Œå†…å­˜ç»“åˆä½¿ç”¨ä»ç„¶å¯ä»¥å®ç°æ¥è¿‘å†…å­˜çº§åˆ«çš„ååé‡ï¼Œä¸ä¹‹ç›¸æ¯”ï¼Œä¼ ç»Ÿæœºæ¢°ç¡¬ç›˜æ¯”å†…å­˜æ…¢å¤§çº¦5000å€ï¼Œä¸€æ—¦ç¼“å­˜å‘½ä¸­ç‡ä½äº100%ï¼Œååé‡ä¼šæ€¥å‰§ä¸‹é™ã€‚ FoundationDBåªæœ‰åœ¨é«˜å¹¶å‘çš„å·¥ä½œè´Ÿè½½ä¸‹æ‰èƒ½è¾¾åˆ°æœ€å¤§ååé‡ï¼Œäº‹å®ä¸Šï¼Œåœ¨ç»™å®šçš„å¹³å‡å»¶è¿Ÿä¸‹ï¼Œå¹¶å‘æ€§æ˜¯ååé‡çš„ä¸»è¦å†³å®šå› ç´ ã€‚ ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:8:3","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"å¹¶å‘ FoundationDBè¢«è®¾è®¡ä¸ºåœ¨æ¥è‡ªå¤§é‡å®¢æˆ·ç«¯çš„é«˜å¹¶å‘ä¸‹å®ç°é«˜æ€§èƒ½ï¼Œåœ¨ä¸€ä¸ªç»™å®šç³»ç»Ÿä¸­ï¼Œå¹³å‡ååé‡å’Œå»¶è¿Ÿä¹‹é—´çš„å…³ç³»ç”±æ’é˜Ÿè®ºä¸­çš„Littleå®šå¾‹ï¼ˆåˆ©ç‰¹å°”æ³•åˆ™ï¼‰æè¿°ï¼Œè¿™ä¸€å®šå¾‹åœ¨FoundationDBä¸­çš„å®é™…åº”ç”¨æ˜¯ $$ throughput = outstanding \\ requests / latency $$ è¿™ä¸ªå…¬å¼æ„å‘³ç€ï¼šåœ¨å›ºå®šçš„å¹³å‡å»¶è¿Ÿä¸‹ï¼Œåªæœ‰è¶³å¤Ÿå¤šçš„å¹¶å‘è¯·æ±‚æ‰èƒ½æœ€å¤§åŒ–ååé‡ æ¯”å¦‚ä¸€ä¸ªFoundationDBé›†ç¾¤çš„æäº¤å»¶è¿Ÿå¯èƒ½æ˜¯2msï¼Œä½†å®ƒçš„å¤„ç†èƒ½åŠ›è¿œè¶…æ¯ç§’500æ¬¡æäº¤ï¼Œäº‹å®ä¸Šæ¯ç§’å‡ ä¸‡ä¸ªæäº¤éƒ½æ˜¯è½»æ¾å¯ä»¥å®ç°çš„ï¼Œè¦å®ç°è¿™ç§ååé‡ï¼Œå¿…é¡»åŒæ—¶å¹¶å‘å¤„ç†ä¸Šç™¾ä¸ªè¯·æ±‚ï¼Œæ²¡æœ‰è¶³å¤Ÿçš„æŒ‚èµ·è¯·æ±‚ï¼ˆå³å¹¶å‘ä¸è¶³ï¼‰æ˜¯æ€§èƒ½ä½ä¸‹çš„æœ€ä¸»è¦åŸå› ã€‚ ","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:8:4","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["FoundationDB"],"content":"åœ¨FoundationDBä¸­å¤„ç†å¤§value https://github.com/sunesimonsen/fdb-blobsé€šè¿‡å°†æ•°æ®åˆ‡åˆ†æˆchunkå®ç°äº†å¯¹ä»»æ„å¤§å°blobçš„å¤„ç†ï¼Œä¸‹é¢åˆ†æä¸€ä¸‹å®ç°ï¼š type Store struct { db fdb.Database blobsDir directory.DirectorySubspace removedDir directory.DirectorySubspace uploadsDir directory.DirectorySubspace chunkSize int chunksPerTransaction int systemTime SystemTime idGenerator IdGenerator } // Creates and returns a new blob with the content of the given reader r. func (store *Store) Create(r io.Reader) (*Blob, error) { // åœ¨uploadè·¯å¾„ä¸‹ç”Ÿæˆç±»ä¼¼uuidçš„è·¯å¾„,å¹¶å°†ä¸Šä¼ æ—¶é—´å’Œå®é™…æ•°æ®åˆ†åˆ«å†™åˆ° /uploadStartedAt å’Œ /byteså­è·¯å¾„ä¸‹ // æ•°æ®ä¼šè‡ªåŠ¨åˆ†ç‰‡åˆ°/byte/chunk_numberï¼Œè¿”å›å¯¹åº”çš„è·¯å¾„ /id token, err := store.Upload(r) if err != nil { return nil, err } id, err := transact(store.db, func(tr fdb.Transaction) (Id, error) { return store.CommitUpload(tr, token) }) if err != nil { return nil, err } return store.Blob(id) } // Uploads the content of the given reader r into a temporary location and // returns a token for commiting the upload on a transaction later. func (store *Store) Upload(r io.Reader) (UploadToken, error) { // äº§ç”Ÿä¸€ä¸ªéšæœºçš„idï¼Œç±»ä¼¼äºuuid id := store.idGenerator.NextId() // åˆ›å»ºä¸€ä¸ªuploadDir: uploadDir/idï¼Œå®é™…ä¸Šåœ¨foundationdbåº•å±‚æ²¡æœ‰è·¯å¾„çš„æ¦‚å¿µï¼Œä½†å¾ˆå¤šæ—¶å€™æˆ‘ä»¬éƒ½ä¼šä½¿ç”¨è·¯å¾„æ¥åŒºåˆ†ä¸åŒé”® uploadDir, err := store.uploadsDir.Create(store.db, []string{string(id)}, nil) token := UploadToken{dir: uploadDir} if err != nil { return token, err } err = updateTransact(store.db, func(tr fdb.Transaction) error { unixTimestamp := store.systemTime.Now().Unix() // å†™å…¥ (uploadDir/id/uploadStartedAt, unixTimeStamp)ï¼Œè®°å½•ä¸Šä¼ æ—¶é—´ç‚¹ tr.Set(uploadDir.Sub(\"uploadStartedAt\"), encodeUInt64(uint64(unixTimestamp))) return nil }) if err != nil { return token, err } err = store.write(uploadDir, r) // è¿”å›çš„tokenæ˜¯å†™å…¥æ•°æ®çš„pathï¼Œå³uploadDir/id return token, err } func (store *Store) write(blobDir subspace.Subspace, r io.Reader) error { // chunkSizeï¼Œæ¯ä¸ªchunkçš„å¤§å°ï¼Œæœ€åä¸€ä¸ªchunkçš„å¤§å°å¯ä»¥å°äºè¿™ä¸ªï¼Œé»˜è®¤ä¸º10kb chunk := make([]byte, store.chunkSize) var written uint64 var chunkIndex int bytesSpace := blobDir.Sub(\"bytes\") // å¾ªç¯ï¼Œå¦‚æœæ•°æ®é‡å¤§ï¼Œå¯èƒ½éœ€è¦å¤šä¸ªäº‹åŠ¡æ‰èƒ½å†™å…¥ for { finished, err := transact(store.db, func(tr fdb.Transaction) (bool, error) { // chunksPerTransaction æ¯ä¸ªäº‹åŠ¡ä¸­çš„å†™å…¥çš„chunkæ•°é‡ for i := 0; i \u003c store.chunksPerTransaction; i++ { n, err := io.ReadFull(r, chunk) // å†™å…¥æ•°æ® (uploadDir/id/bytes/chunkIndex, chunk_data) tr.Set(bytesSpace.Sub(chunkIndex), chunk[0:n]) chunkIndex++ written += uint64(n) if err == io.ErrUnexpectedEOF || err == io.EOF { return true, nil } if err != nil { return false, err } } return false, nil }) if finished { break } if err != nil { return err } } return updateTransact(store.db, func(tr fdb.Transaction) error { // å†™å…¥len tr.Set(blobDir.Sub(\"len\"), encodeUInt64(written)) // å†™å…¥chunkSize tr.Set(blobDir.Sub(\"chunkSize\"), encodeUInt64(uint64(store.chunkSize))) return nil }) } // Commits an upload with the given token on a transaction. This creates a blob // from the upload and returns its id. // è¿™ä¸ªæ•´ä¸ªå°±æ˜¯ä¸€ä¸ªäº‹åŠ¡ func (store *Store) CommitUpload(tr fdb.Transaction, token UploadToken) (Id, error) { if token.dir == nil { return \"\", errors.New(\"invalid upload token, tokens needs to be produced by the upload method\") } uploadDir := token.dir uploadPath := uploadDir.GetPath() id := uploadPath[len(uploadPath)-1] dstPath := append(store.blobsDir.GetPath(), id) // å°†å¯¹äºä» uploadDirç›®å½•ç§»åŠ¨åˆ°blobsç›®å½•ï¼Œè¿™é‡Œåº”è¯¥æ²¡æœ‰åº•å±‚æ•°æ®çš„ç§»åŠ¨ï¼Œåªæ˜¯ç›®å½•å…ƒæ•°æ®çš„ç§»åŠ¨ï¼Œç›®å½•å…ƒæ•°æ®å¯èƒ½è®°å½•åœ¨fdbçš„System metadataä¸­ï¼ŒDirectoryçš„æŠ½è±¡æˆ–è®¸å¾ˆå¥½ç”¨ blobDir, err := uploadDir.MoveTo(tr, dstPath) if err != nil { return Id(id), err } unixTimestamp := store.systemTime.Now().Unix() // è®¾ç½®blobçš„åˆ›å»ºæ—¶é—´ tr.Set(blobDir.Sub(\"createdAt\"), encodeUInt64(uint64(unixTimestamp))) return Id(id), nil } // Returns a blob instance for the given id. func (store *Store) Blob(id Id) (*Blob, error) { blobDir, err := store.openBlobDir(id) if err != nil { return nil, err } data, err := readTransact(store.db, func(tr fdb.ReadTransaction) ([]byte, error) { return tr.Get(blobDir.Sub(\"chunkSize\")).Get() }) if err != nil { return nil, err } chunkSize := int(decodeUInt64(data)) blob := \u0026Blob{ db: store.db, dir: blobDir, chunkSize: chunkSize, chunksPerTransaction: store.chunksPerTransact","date":"2025-07-26","objectID":"/posts/foundationdb-in-practice/:9:0","tags":["FoundationDB"],"title":"FoundationDB in Practice","uri":"/posts/foundationdb-in-practice/"},{"categories":["Java"],"content":"ä¸»è¦å…¶å®æ˜¯å­¦ä¹ Javaæ—¥å¿—é€šå…³ç³»åˆ—æ–‡ç« ï¼Œå¤§ä½¬å¤ªå¼ºäº† æ—¥å¿—å‘å±•åˆ°ä»Šå¤©ï¼Œè¢«æŠ½è±¡æˆäº†ä¸‰å±‚ï¼Œæ¥å£å±‚ã€å®ç°å±‚ã€é€‚é…å±‚ã€‚ æ¥å£å±‚ï¼šæˆ–è€…å«æ—¥å¿—é—¨é¢ï¼ˆfacadeï¼‰ï¼Œå°±æ˜¯interfaceï¼Œåªå®šä¹‰æ¥å£ï¼Œç­‰åˆ«äººå®ç° å®ç°å±‚ï¼šçœŸæ­£å¹²æ´»çš„ï¼Œèƒ½å¤ŸæŠŠæ—¥å¿—å†…å­˜è®°å½•ä¸‹æ¥çš„å·¥å…·ï¼Œä½†è¯·æ³¨æ„å®ƒä¸æ˜¯ä¸Šé¢æ¥å£å®ç°ï¼Œå› ä¸ºå®ƒä¸æ„ŸçŸ¥ä¹Ÿä¸ç›´æ¥å®ç°æ¥å£ï¼Œä»…ä»…æ˜¯ç‹¬ç«‹çš„å®ç° é€‚é…å±‚ï¼šä¸€èˆ¬ç§°ä¸ºadapterï¼Œå› ä¸ºæ¥å£å±‚å’Œé€‚é…å±‚å¹¶ééƒ½å‡ºè‡ªä¸€å®¶ä¹‹æ‰‹ï¼Œå®ƒä»¬ä¹‹é—´æ— æ³•ç›´æ¥åŒ¹é… é€‚é…å±‚æœ‰å¯ä»¥åˆ†ä¸ºç»‘å®šï¼ˆbindingï¼‰å’Œæ¡¥æ¥ï¼ˆbridgingï¼‰ä¸¤ç§èƒ½åŠ›ï¼š ç»‘å®šï¼ˆbindingï¼‰ï¼šå°†æ¥å£å±‚ç»‘å®šåˆ°æŸä¸ªå®ç°å±‚ï¼ˆå®ç°ä¸€ä¸ªæ¥å£å±‚ï¼Œå¹¶è°ƒç”¨å®ç°å±‚çš„æ–¹æ³•ï¼‰ æ¡¥æ¥ï¼ˆbridgingï¼‰ï¼šå°†æ¥å£å±‚æ¡¥æ¥åˆ°å¦ä¸€ä¸ªæ¥å£å±‚ï¼ˆå®ç°ä¸€ä¸ªæ¥å£å±‚ï¼Œå¹¶è°ƒç”¨å¦ä¸€ä¸ªæ¥å£å±‚çš„æ¥å£ï¼‰ï¼Œä¸»è¦ä½œç”¨æ˜¯æ–¹ä¾¿ç”¨æˆ·ä½æˆæœ¬çš„åœ¨å„æ¥å£å±‚å’Œé€‚é…å±‚ä¹‹é—´è¿ç§» ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:0:0","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"Javaæ—¥å¿—çš„å†å² ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:1:0","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"Log4j (1999) å¤§ç¥Cekiå¼€å‘ï¼Œæä¾›äº†ä»¥ä¸‹èƒ½åŠ› æ—¥å¿—å¯ä»¥è¾“å‡ºåˆ°æ§åˆ¶å°ã€æ–‡ä»¶ã€æ•°æ®åº“ï¼Œç”šè‡³è¿œç¨‹æœåŠ¡å™¨å’Œç”µå­é‚®ä»¶ï¼ˆè¢«ç§°ä½œAppenderï¼‰ æ—¥å¿—è¾“å‡ºæ ¼å¼ï¼ˆè¢«ç§°ä½œLayoutï¼‰å…è®¸å®šåˆ¶ï¼Œæ¯”å¦‚é”™è¯¯æ—¥å¿—å’Œæ™®é€šæ—¥å¿—ä½¿ç”¨ä¸åŒçš„å±•ç°æ–¹å¼ æ—¥å¿—è¢«åˆ†ä¸º5ä¸ªçº§åˆ«ï¼ˆè¢«ç§°ä¸ºLevelï¼‰ï¼Œä»ä½åˆ°é«˜ä¾æ¬¡æ˜¯debugã€infoã€warnã€errorã€fatalï¼Œè¾“å‡ºå‰ä¼šæ ¡éªŒé…ç½®çš„å…è®¸çº§åˆ«ï¼Œå°äºæ­¤çº§åˆ«çš„æ—¥å¿—å°†è¢«å¿½ç•¥ã€‚é™¤å¤–ä¹‹å¤–è¿˜æœ‰allã€offä¸¤ä¸ªç‰¹æ®Šçº§åˆ«ï¼Œè¡¨ç¤ºå®Œå…¨å¼€æ”¾å’Œå®Œå…¨å…³é—­æ—¥å¿— å¯ä»¥åœ¨å·¥ç¨‹ä¸­éšæ—¶æŒ‡å®šä¸åŒçš„è®°å½•å™¨ï¼ˆè¢«ç§°ä¸ºLoggerï¼‰ï¼Œå¯ä»¥ä¸ºä¹‹é…ç½®ç‹¬ç«‹çš„è®°å½•ä½ç½®ã€æ—¥å¿—çº§åˆ« æ”¯æŒé€šè¿‡propertiesæˆ–è€…xmlæ–‡ä»¶è¿›è¡Œé…ç½® ä¸åŒLog4jæœ‰æ¯”è¾ƒæ˜æ˜¾çš„æ€§èƒ½çŸ­æ¿ï¼Œåœ¨Logbackå’ŒLog4j 2æ¨å‡ºåé€æ¸å¼å¾®ï¼Œæœ€ç»ˆApacheåœ¨2015å¹´å®£å¸ƒç»ˆæ­¢å¼€å‘Log4jå¹¶å…¨é¢è¿ç§»è‡³Log4j2 ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:1:1","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"JUL (2002.2) Sunå…¬å¸åœ¨2002å¹´å‘å¸ƒï¼Œè¿™å¥—ç³»ç»Ÿç§°ä¸ºJava Logging APIï¼ŒåŒ…è·¯å¾„æ˜¯java.util.loggingï¼Œç®€ç§°JULã€‚ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:1:2","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"JCL (2002.8) å¯¹äºç‹¬ç«‹ä¸”è½»é‡çš„é¡¹ç›®æ¥è¯´ï¼Œå¼€å‘è€…å¯ä»¥æ ¹æ®å–œå¥½ä½¿ç”¨æŸä¸ªæ—¥å¿—æ–¹æ¡ˆå³å¯ï¼Œä½†æ›´å¤šçš„æƒ…å†µæ˜¯ä¸€å¥—ä¸šåŠ¡ç³»ç»Ÿä¾èµ–äº†å¤§é‡çš„ä¸‰æ–¹å·¥å…·ï¼Œè€Œä¼—å¤šä¸‰æ–¹å·¥å…·ä¼šå„è‡ªä½¿ç”¨ä¸åŒçš„æ—¥å¿—å®ç°ï¼Œå½“å®ƒä»¬è¢«é›†æˆåœ¨ä¸€èµ·æ—¶ï¼Œå¿…ç„¶å¯¼è‡´æ—¥å¿—è®°å½•çš„æ··ä¹±ã€‚ ä¸ºæ­¤Apacheåœ¨2002å¹´æ¨å‡ºäº†ä¸€å¥—æ¥å£Jakarta Common Loggingï¼Œç®€ç§°JCLï¼Œå®ƒçš„ä¸»è¦ä½œè€…ä»ç„¶æ˜¯Cekiï¼Œè¿™å¥—æ¥å£ä¸»åŠ¨æ”¯æŒäº†Log4jã€JULç­‰ä¼—å¤šæ—¥å¿—å·¥å…·ï¼Œå¼€å‘è€…å¦‚æœæƒ³æ‰“å°æ—¥å¿—ï¼Œåªéœ€è°ƒç”¨JCLçš„æ¥å£å³å¯ï¼Œè‡³äºæœ€ç»ˆä½¿ç”¨çš„æ—¥å¿—å®ç°åˆ™ç”±æœ€ä¸Šå±‚çš„ä¸šåŠ¡ç³»ç»Ÿå†³å®šï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¿™å…¶å®å°±æ˜¯å…¸å‹çš„æ¥å£ä¸å®ç°åˆ†ç¦»çš„è®¾è®¡ã€‚ ç®€å•ä»‹ç»ä¸€ä¸‹JCLè‡ªå¸¦çš„å‡ ä¸ªé€‚é…å±‚ã€å®ç°å±‚ï¼š Jdk14Loggerï¼šç”¨äºç»‘å®šJULï¼ˆå› ä¸ºJULä»JDK 1.4å¼€å§‹æä¾›ï¼‰çš„é€‚é…å±‚ Log4JLoggerï¼šç”¨äºç»‘å®šLog4jçš„é€‚é…å±‚ NoOpLogï¼šJCLè‡ªå¸¦çš„æ—¥å¿—å®ç°ï¼Œä½†å®ƒæ˜¯ç©ºå®ç°ï¼Œä¸åšä»»ä½•äº‹æƒ… SimpleLogï¼šJCLè‡ªå¸¦çš„æ—¥å¿—å®ç°ï¼Œè®©ç”¨æˆ·å“ªæ€•ä¸ä¾èµ–å…¶ä»–å·¥å…·ä¹Ÿèƒ½æ‰“å°æ—¥å¿—ï¼Œåªæ˜¯åŠŸèƒ½éå¸¸ç®€å• å½“æ—¶é¡¹ç›®å‰ç¼€å–åJakartaï¼Œæ˜¯å› ä¸ºå®ƒå±äºApacheå’ŒSunå…±åŒæ¨å‡ºçš„Jakartaé¡¹ç›®ï¼Œç°åœ¨JCLä½œä¸ºApache Commonsçš„å­é¡¹ç›®ï¼Œå«Apache Commons Loggingï¼Œä¸æˆ‘ä»¬å¸¸ç”¨çš„Commons langã€CLommons Collectionsç­‰æ˜¯å¸ˆå…„å¼Ÿï¼Œä½†JCLçš„ç®€å†™å‘½åè¢«ä¿ç•™ä¸‹æ¥ï¼Œå¹¶æ²¡æœ‰æ”¹æˆACLã€‚ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:1:3","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"Slf4j (2005) Log4jçš„ä½œè€…Cekiçœ‹åˆ°äº†å¾ˆå¤šLog4jå’ŒJCLçš„ä¸è¶³ï¼Œä½†åˆæ— åŠ›æ¨åŠ¨é¡¹ç›®å¿«é€Ÿè¿­ä»£ï¼ŒåŠ ä¸Šå¯¹Apacheç®¡ç†ä¸æ»¡ï¼Œè®¤ä¸ºè‡ªå·±å¤±å»äº†å¯¹Log4jé¡¹ç›®çš„æ§åˆ¶æƒï¼Œäºæ˜¯åœ¨2005å¹´é€‰æ‹©è‡ªç«‹é—¨æˆ·ï¼Œå¹¶å¾ˆå¿«æ¨å‡ºäº†ä¸€æ¬¾æ–°ä½œå“Simple Logging Facade for Javaï¼Œç®€ç§°Slf4jã€‚ Slf4jä¹Ÿæ˜¯ä¸€ä¸ªæ¥å£å±‚ï¼Œæ¥å£è®¾è®¡ä¸JCLéå¸¸æ¥è¿‘ï¼ˆæ¯•ç«Ÿæœ‰å¸ˆæ‰¿å…³ç³»ï¼‰ï¼Œç›¸æ¯”JCLæœ‰ä¸€ä¸ªé‡è¦çš„åŒºåˆ«æ˜¯æ—¥å¿—å®ç°å±‚çš„ç»‘å®šæ–¹å¼ï¼šJCLæ˜¯åŠ¨æ€ç»‘å®šï¼Œå³åœ¨è¿è¡Œæ—¶æ‰§è¡Œæ—¥å¿—è®°å½•æ—¶åˆ¤å®šåˆé€‚çš„æ—¥å¿—å®ç°ï¼Œè€ŒSlf4jé€‰æ‹©çš„æ˜¯é™æ€ç»‘å®šï¼Œåº”ç”¨ç¼–è¯‘æ—¶å·²ç»ç¡®å®šæ—¥å¿—å®ç°ï¼Œæ€§èƒ½è‡ªç„¶æ›´å¥½ï¼Œè¿™å°±æ˜¯å¸¸è¢«æåˆ°çš„classloaderé—®é¢˜ã€‚ åœ¨æ¨å‡ºSlf4jçš„æ—¶å€™ï¼Œå¸‚é¢ä¸Šå·²ç»æœ‰å¦ä¸€å¥—æ¥å£JCLï¼Œä¸ºäº†å°†é€‰æ‹©æƒäº¤ç»™ç”¨æˆ·ï¼ŒSlf4jæ¨å‡ºäº†ä¸¤ä¸ªæ¡¥æ¥å±‚ï¼š jcl-over-slf4jï¼šå®¢æˆ·è°ƒç”¨JCLæ¥å£ï¼Œåº•å±‚è½¬åˆ°Slf4jæ¥å£ slf4j-jclï¼šå®¢æˆ·è°ƒç”¨slf4jæ¥å£ï¼Œåº•å±‚è½¬åˆ°JCLæ¥å£ è¿™é‡Œè§£é‡Šä¸€ä¸‹slf4j-log4j12è¿™ä¸ªåå­—ï¼Œå®ƒè¡¨ç¤ºslf4j + log4j 1.2ï¼ˆLog4jçš„æœ€åä¸€ä¸ªç‰ˆæœ¬ï¼‰çš„é€‚é…å±‚ã€‚ç±»ä¼¼çš„ï¼Œslf4j-jdk14è¡¨ç¤ºslf4j + jdk1.4ï¼ˆå°±æ˜¯JULï¼‰çš„é€‚é…å±‚ã€‚ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:1:4","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"Logback (2006) Cekiåœ¨2006å¹´åˆæ¨å‡ºä¸€æ¬¾æ—¥å¿—è®°å½•å®ç°æ–¹æ¡ˆï¼šLogbackï¼Œæ— è®ºæ˜¯æ˜“ç”¨åº¦ã€åŠŸèƒ½ã€è¿˜æ˜¯æ€§èƒ½ï¼ŒLogbackéƒ½ä¼˜äºLog4jï¼Œåœ¨åŠ ä¸Šå¤©ç„¶æ”¯æŒSlf4jè€Œä¸éœ€è¦é¢å¤–çš„é€‚é…å±‚ï¼Œè‡ªç„¶æ‹¥è¶¸è€…ä¼—ï¼Œç›®å‰Logbackå·²ç»æˆä¸ºJavaç¤¾åŒºæœ€è¢«å¹¿æ³›æ¥å—çš„æ—¥å¿—å®ç°å±‚ã€‚ ç›¸è¾ƒäºLog4jï¼ŒLogbackæä¾›äº†å¾ˆå¤šæˆ‘ä»¬ç°åœ¨çœ‹èµ·æ¥ç†æ‰€å½“ç„¶çš„æ–°ç‰¹æ€§ï¼š æ”¯æŒæ—¥å¿—æ–‡ä»¶åˆ‡å‰²æ»šåŠ¨è®°å½•ï¼Œæ”¯æŒå¼‚æ­¥å†™å…¥ é’ˆå¯¹å†å²æ—¥å¿—ï¼Œæ—¢æ”¯æŒæŒ‰æ—¶é—´æˆ–æŒ‰ç¡¬ç›˜å ç”¨è‡ªåŠ¨æ¸…ç†ï¼Œä¹Ÿæ”¯æŒè‡ªåŠ¨å‹ç¼©ä»¥èŠ‚çœç¡¬ç›˜ç©ºé—´ æ”¯æŒåˆ†æ”¯è¯­æ³•ï¼Œé€šè¿‡\u003cif\u003eã€\u003cthen\u003eã€\u003celse\u003eå¯ä»¥æŒ‰æ¡ä»¶é…ç½®ä¸åŒçš„æ—¥å¿—è¾“å‡ºé€»è¾‘ï¼Œæ¯”å¦‚åˆ¤æ–­ä»…åœ¨å¼€å‘ç¯å¢ƒè¾“å‡ºæ›´è¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯ å¤§é‡çš„æ—¥å¿—è¿‡æ»¤å™¨ï¼Œç”šè‡³å¯ä»¥åšåˆ°é€šè¿‡ç™»é™†ç”¨æˆ·Sessionè¯†åˆ«æ¯ä¸€ä½ç”¨æˆ·å¹¶è¾“å‡ºç‹¬ç«‹çš„æ—¥å¿—æ–‡ä»¶ å¼‚å¸¸å †æ ˆæ”¯æŒæ‰“å°jarä¿¡æ¯ï¼Œè®©æˆ‘ä»¬ä¸ä½†çŸ¥é“è°ƒç”¨å‡ºè‡ªå“ªä¸ªæ–‡ä»¶å“ªä¸€è¡Œï¼Œä¹Ÿå¯ä»¥çŸ¥é“è¿™ä¸ªæ–‡ä»¶æ¥è‡ªå“ªä¸ªjaråŒ… Logbackä¸»è¦ç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼š logback-coreï¼šè®°å½•ã€è¾“å‡ºæ—¥å¿—çš„æ ¸å¿ƒå®ç° logback-classicï¼šé€‚é…å±‚ï¼Œå®Œæ•´å®ç°äº†Slf4jæ¥å£ logback_accessï¼šç”¨äºå°†Logbacké›†æˆåˆ°Servletå®¹å™¨ï¼ˆTomcatã€Jettyï¼‰ä¸­ï¼Œè®©è¿™äº›å®¹å™¨çš„HTTPè®¿é—®æ—¥å¿—ä¹Ÿå¯ä»¥ç»ç”±å¼ºå¤§çš„Logbackè¾“å‡º ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:1:5","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"Log4j 2(2012) Apacheåœ¨2012å¹´æ¨å‡ºApache Log4j 2ï¼Œæœ‰ä»¥ä¸‹äº®ç‚¹ï¼š æ’ä»¶åŒ–ç»“æ„ï¼Œç”¨æˆ·å¯ä»¥è‡ªå·±å¼€å‘æ’ä»¶ï¼Œå®ç°Appenderã€Loggerã€Filterå®Œæˆæ‰©å±• åŸºäºLMAX Disruptorçš„å¼‚æ­¥è¯è¾“å‡ºï¼Œåœ¨å¤šçº¿ç¨‹åœºæ™¯ä¸‹ç›¸æ¯”Logbackæœ‰10å€å·¦å³çš„æ€§èƒ½æå‡ Log4j 2ä¸»è¦ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š log4j-coreï¼šæ ¸å¿ƒå®ç°ï¼ŒåŠŸèƒ½ç±»ä¼¼äºlogback-core log4j-apiï¼šæ¥å£å±‚ï¼ŒåŠŸèƒ½ç±»ä¼¼äºSlf4jï¼Œé‡Œé¢åªåŒ…å«Log4j 2çš„æ¥å£å®šä¹‰ Log4j 2æä¾›JCLå’ŒSlf4jä¹‹å¤–çš„ç¬¬ä¸‰ä¸ªæ¥å£å±‚ï¼ˆlog4j-apiï¼Œè™½ç„¶åªæ˜¯è‡ªå·±çš„æ¥å£ï¼‰ï¼Œä¸è¿‡ç›®å‰å¤§å®¶ä¸€èˆ¬æŠŠLog4j 2çœ‹åšå®ç°å±‚çœ‹å¾…ï¼Œå¹¶å¼•å…¥JCLæˆ–Slf4jä½œä¸ºæ¥å£å±‚ï¼Œç‰¹åˆ«æ˜¯JCLï¼Œåœ¨æ—¶éš”è¿‘10å¹´åï¼Œä¸2023å¹´æ¨å‡ºäº†1.3.0ç‰ˆï¼Œå¢åŠ äº†å¯¹Log4j 2çš„é€‚é…ã€‚ è™½ç„¶Log4j 2å‘å¸ƒå·²ç»å¾ˆä¹…äº†ï¼Œä½†å®ƒä¾ç„¶æ— æ³•æ’¼åŠ¨Logbakcçš„æ±Ÿæ¹–åœ°æ–¹ Log4j 2è™½ç„¶é¡¶ç€Log4jçš„åå·ï¼Œä½†å´æ˜¯ä¸€å¥—å®Œå…¨é‡å†™çš„æ—¥å¿—ç³»ç»Ÿï¼Œæ— æ³•åªé€šè¿‡ä¿®æ”¹Log4jç‰ˆæœ¬å·å®Œæˆå‡çº§ï¼Œå†å²ç”¨æˆ·å‡çº§æ„æ„¿ä½ Log4j 2æ¯”Logbackæ™šé¢ä¸–6å¹´ï¼Œå´æ²¡æœ‰æä¾›è¶³å¤Ÿä¸¤çœ¼åŠå·®å¼‚åŒ–çš„èƒ½åŠ›ï¼Œè€ŒSlf4j+Logbackè¿™å¥—ç»„åˆå·²ç»éå¸¸ä¼˜ç§€ï¼Œå…ˆå‘ä¼˜åŠ¿æ˜æ˜¾ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:1:6","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"spring-jcl Spring Bootç›®å‰ä½¿ç”¨sprint-jcl + Logbackè¿™å¥—æ–¹æ¡ˆï¼ŒSpringæ›¾åœ¨å®ƒçš„å®˜æ–¹blogä¸­æåˆ°ï¼Œå¦‚æœå¯ä»¥é‡æ¥ï¼ŒSpringä¼šé€‰æ‹©Slf4jè€Œä¸æ˜¯JCLä½œä¸ºé»˜è®¤æ—¥å¿—æ¥å£ã€‚Springæƒ³è¦æ”¯æŒSLF4jï¼Œåˆè¦ä¿è¯å‘å‰å…¼å®¹JCLï¼Œäºæ˜¯ä»5.0 ï¼ˆSpring Boot 2.0ï¼‰å¼€å§‹æä¾›äº†spring-jclè¿™ä¸ªåŒ…ï¼Œå®ƒé¡¶ç€Springçš„åå·ï¼Œä»£ç ä¸­æŠ¥åå´ä¸JCLä¸€è‡´ï¼ˆorg.apache.commons.loggingï¼‰ï¼Œä½œç”¨è‡ªç„¶ä¹Ÿä¸JCLä¸€è‡´ï¼Œä½†å®ƒé¢å¤–é€‚é…äº†Slf4jã€‚å¹¶å°†Slf4jæ”¾åœ¨æŸ¥æ‰¾çš„ç¬¬ä¸€é¡ºä½ã€‚ å¦‚æœä½ æ˜¯åŸºäºSpring Initializeæ–°åˆ›å»ºçš„åº”ç”¨ï¼Œå¯ä»¥ä¸å¿…ç®¡è¿™ä¸ªåŒ…ï¼Œå®ƒå·²ç»åœ¨èƒŒåé»˜é»˜å·¥ä½œäº†ï¼Œå¦‚æœä½ åœ¨é¡¹ç›®å¼€å‘è¿‡ç¨‹ä¸­é‡åˆ°åŒ…å†²çªï¼Œæˆ–è€…éœ€è¦è‡ªå·±é€‰æ‹©æ—¥å¿—æ¥å£å’Œå®ç°ï¼Œåˆ™å¯ä»¥æŠŠspring-jclå½“åšjclå¯¹å¾…ï¼Œå¤§èƒ†æ’é™¤å³å¯ã€‚ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:1:7","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"æ€»ç»“ ä¸¤ä¸ªæ¥å£ï¼ˆJCLã€Slf4jï¼‰å’Œå››ä¸ªå®ç°(Log4jã€JULã€Logbackã€Log4j2)ï¼Œå†åŠ ä¸Šæ— æ•°çš„é€‚é…å±‚ï¼Œç»„æˆäº†ä¸€å¼ å›¾ã€‚ ç›¸åŒé¢œè‰²çš„æ¨¡å—å…·æœ‰ç›¸åŒçš„groupId JCLçš„é€‚é…å±‚æ˜¯ç›´æ¥åœ¨å®ƒè‡ªå·±çš„åŒ…ä¸­æä¾›çš„ è¦æƒ³ä½¿ç”¨Logbackï¼Œå°±ä¸€å®šç»•ä¸å¼€Slf4jï¼ˆå¼•ç”¨å®ƒçš„é€‚é…å±‚ä¹Ÿç®—ï¼‰ã€‚åŒæ ·çš„ï¼Œè¦æƒ³ä½¿ç”¨Log4j 2ï¼Œé‚£å®ƒçš„log4j-apiä¹Ÿç»•ä¸å¼€ã€‚ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:1:8","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"Slf4j + Logbackæ•´åˆå’Œæ’åŒ… å¯¹Javaæ—¥å¿—é€šå…³ï¼ˆäºŒï¼‰ - Slf4j+Logback æ•´åˆåŠæ’åŒ…çš„å­¦ä¹  é€‰æ‹©Slf4j + Logbackçš„åŸå› ï¼š Slf4jçš„APIç›¸è¾ƒJCLæ›´ä¸°å¯Œï¼Œä¸”å¾—åˆ°Intellij IDEAç¼–è¾‘å™¨çš„å®Œæ•´æ”¯æŒï¼Œè¿™æ˜¯æ ¸å¿ƒä¼˜åŠ¿ Slf4jæ”¯æŒæ—¥å¿—å†…å®¹æƒ°æ€§æ±‚å€¼ï¼Œç›¸æ¯”JCLæ€§èƒ½æ›´å¥½ åœ¨å‰è¾¹é€‰å®šSlf4jçš„å‰æä¸‹ï¼ŒåŒä¸€å‚ç‰Œä¸”è¡¨ç°ä¼˜å¼‚çš„Logbackè‡ªç„¶ä¸­æ ‡ Slf4j + Logbackæ˜¯ç›®å‰å¤§éƒ¨åˆ†å¼€å‘è€…çš„é€‰æ‹©ï¼ˆ2021å¹´Slf4j 76%,Logback 48%ï¼‰ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:2:0","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"åŸºç¡€ä¾èµ–é¡¹ æ ¹æ®å‰é¢çš„çŸ¥è¯†ï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“çŸ¥é“ä»¥ä¸‹ä¸‰ä¸ªåŒ…æ˜¯å¿…é¡»çš„ï¼š Slf4jæ˜¯åŸºæœ¬çš„æ—¥å¿—é—¨é¢ï¼Œå®ƒçš„æ ¸å¿ƒAPiåœ¨org.slf4j:slf4j-apiä¸­ Logbackçš„æ ¸å¿ƒå®ç°å±‚åœ¨ch.qos.logback:logback-coreä¸­ Logbacké’ˆå¯¹Slf4jçš„é€‚é…å±‚åœ¨ch.qos.logback:logback-classicä¸­ å…¶ä¸­logback-classicä¼šç›´æ¥ä¾èµ–å…¶ä»–ä¸¤é¡¹ï¼Œè€Œä¸”å®ƒä¾èµ–çš„ä¸€å®šæ˜¯å®ƒèƒ½å¤Ÿæ”¯æŒçš„æœ€åˆé€‚ç‰ˆæœ¬ï¼Œæ‰€ä»¥ä¸ºäº†é¿å…æ­§ä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨é¡¹ç›®ä¸­ä»…æ˜¾å¼ä¾èµ–logback-classicå³å¯ã€‚ å¦å¤–è¦æ³¨æ„ï¼ŒSlf4jå’ŒLogbackçš„ç‰ˆæœ¬å¹¶ä¸å®Œå…¨å‘å‰å…¼å®¹ï¼Œä»–ä»¬ä¹‹é—´ä¹Ÿæœ‰å¯¹åº”å…³ç³» slf4jç‰ˆæœ¬å…¼å®¹æ€§ slfj4j 2.0.xç‰ˆæœ¬æœ‰ä¸å°çš„æ”¹åŠ¨ï¼Œä¸å†ä¸»åŠ¨æŸ¥æ‰¾org.slf4j.impl.StaticLoggerBinderï¼Œè€Œæ˜¯æ”¹ç”¨JDK ServiceLoaderï¼ˆä¹Ÿå°±æ˜¯SPIï¼ŒService Provider Interfaceï¼‰çš„æ–¹å¼æ¥åŠ è½½å®ç°ï¼Œè¿™æ˜¯JDK 8ä¸­çš„ç‰¹æ€§ï¼Œæ‰€ä»¥Slf4jå¯¹JDKçš„ä¾èµ–æ˜¾è€Œæ˜“è§ã€‚ Logbackç‰ˆæœ¬å…¼å®¹æ€§ å› ä¸ºSlf4jæŠ€æœ¯æ–¹æ¡ˆå˜åŒ–ï¼Œå¯¼è‡´logback-classicä¹Ÿéœ€è¦åˆ†åˆ«ä½œé€‚é…ï¼Œå¦‚æœä½¿ç”¨äº†ä¸åŒ¹é…çš„ç‰ˆæœ¬å°†ä¼šæŠ¥å¼‚å¸¸ å…¶ä¸­logback 1.3.xå’Œ1.4.xæ˜¯å¹¶è¡Œç»´æŠ¤ç‰ˆæœ¬ï¼Œæ¯æ¬¡æ›´æ–°éƒ½ä¼šåŒæ—¶å‘å¸ƒ1.3.nå’Œ1.4.nï¼Œç”¨æˆ·éœ€è¦æ ¹æ®é¡¹ç›®çš„JDKç‰ˆæœ¬è¿›è¡Œé€‰æ‹©ï¼Œä¸è¿‡ç›®å‰Logbackå·²ç»å…¨é¢å‡çº§1.5.xï¼Œä¸”1.3.xå’Œ1.4.xä¸å†ç»´æŠ¤ã€‚ æ€»ç»“ ä»å‰é¢çš„ç‰ˆæœ¬å…¼å®¹æ€§æˆ‘ä»¬å¯ä»¥çŸ¥é“ï¼š å¦‚æœä½¿ç”¨JDK8ï¼Œå»ºè®®é€‰æ‹©Slf4j 2.0 + Logback 1.3 å¦‚æœä½¿ç”¨JDK 11åŠä»¥ä¸Šï¼Œå»ºè®®é€‰æ‹©Slf4j 2.0 + Logback 1.5 ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:2:1","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"é€‚é…Spring Boot Spring Booté€šè¿‡spring-boot-starter-loggingåŒ…ç›´æ¥ä¾èµ–äº†Logbackï¼ˆç„¶åå†é—´æ¥ä¾èµ–äº†Slf4jï¼‰ï¼Œå®ƒé€šè¿‡org.springframework.boot.logging.LoggingSystemæŸ¥æ‰¾æ—¥å¿—æ¥å£å¹¶è‡ªåŠ¨é€‚é…ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨Spring Bootæ—¶ä¸€èˆ¬å¹¶ä¸éœ€è¦å…³ç³»æ—¥å¿—ä¾èµ–ï¼Œåªç®¡ä½¿ç”¨å³å¯ã€‚ä½†å› ä¸ºslf4j 2.0.xå’Œ1.7.xå®ç°ä¸ä¸€è‡´ï¼Œå¯¼è‡´Spring Bootä¹Ÿä¼šæŒ‘ç‰ˆæœ¬ã€‚ æ ¹æ®è¿™ä¸ªè¡¨æ ¼ï¼Œä»¥åŠå‰ä¸€èŠ‚æ€»ç»“çš„ç‰ˆæœ¬å…¼å®¹å…³ç³»ï¼Œæœ€ç»ˆå¯ä»¥å¾—å‡ºç»“è®ºï¼š å¦‚æœä½¿ç”¨Spring Boot 2åŠä»¥ä¸‹ï¼Œå»ºè®® Slf4j 1.7.x + Logback 1.2.x å¦‚æœä½¿ç”¨Spring Boot 3ï¼Œå»ºè®®é€‰æ‹©Slf4j 2.0.x + Logback 1.4.xï¼ˆå¦‚æœSpringåšå¥½äº†Logback 1.4.xçš„é€‚é…ï¼Œä¼°è®¡å°±æ¨è1.5.xäº†ï¼‰ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:2:2","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"æ¡¥æ¥å…¶ä»–å®ç°å±‚ æˆ‘ä»¬è¿˜è¦ä¿è¯é¡¹ç›®ä¾èµ–çš„äºŒæ–¹ã€ä¸‰æ–¹åŒ…èƒ½å¤Ÿæ­£å¸¸æ‰“å°æ—¥å¿—ï¼Œè€Œå®ƒä»¬å¯èƒ½ä¾èµ–çš„æ˜¯JCL/Log4j/Log4j2/JULï¼Œæˆ‘ä»¬å¯ä»¥ç»Ÿä¸€å¼•å…¥é€‚é…å±‚åšå¥½æ¡¥æ¥ï¼š é€šè¿‡org.slf4j:jcl-over-slf4jå°†JCLæ¡¥æ¥åˆ°Slf4jä¸Š é€šè¿‡org.slf4j:log4j-over-slf4jå°†Log4jæ¡¥æ¥åˆ°Slf4jä¸Š é€šè¿‡org.slf4j:jul-to-slf4jå°†JULæ¡¥æ¥åˆ°Slf4jä¸Š é€šè¿‡org.apache.logging.log4j:log4j-to-slf4jå°†Log4j2æ¡¥æ¥åˆ°Slf4jä¸Š æ³¨æ„ï¼Œæ‰€æœ‰org.slf4jçš„åŒ…ç‰ˆæœ¬è¦å®Œå…¨ä¸€è‡´ï¼Œæ‰€ä»¥å¦‚æœå¼•å…¥è¿™äº›æ¡¥æ¥åŒ…ï¼Œè¦ä¿è¯å®ƒä»¬çš„ç‰ˆæœ¬ä¸å‰é¢é€‰æ‹©çš„slf4j-apiç‰ˆæœ¬å¯¹åº”ï¼Œä¸ºæ­¤Slf4jä»2.0.8å¼€å§‹æä¾›bomåŒ…ï¼Œçœå»ç»´æŠ¤æ¯ä¸ªåŒ…ç‰ˆæœ¬çš„çƒ¦æ¼ï¼ˆè‡³äºä½ç‰ˆæœ¬å°±åªèƒ½äººè‚‰ä¿æŒç‰ˆæœ¬ä¸€è‡´æ€§äº†ï¼‰ã€‚log4j-to-slf4jè¿™ä¸ªåŒ…ï¼Œå¯¹Sfl4j1å’Œslf2j2éƒ½èƒ½æ”¯æŒã€‚ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:2:3","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"å»é™¤æ— ç”¨ä¾èµ– æ¡¥æ¥å±‚ä½¿ç”¨ä¸è¢«æ¡¥æ¥åŒ…ä¸€æ ·çš„åŒ…ç»“æ„ï¼Œå†å°†è°ƒç”¨è½¬åˆ°å¦ä¸€ä¸ªæ¥å£ä¸Šï¼Œæ‰€ä»¥å¦‚æœåŒæ—¶å¼•å…¥æ¡¥æ¥å±‚ä»¥åŠè¢«æ¡¥æ¥çš„åŒ…ï¼Œå¤§æ¦‚ç‡ä¼šå¼•èµ·åŒ…å†²çªã€‚ ç”±äºå¾ˆå¤šå·¥å…·ä¼šåœ¨ä¸ç»æ„é—´å¼•å…¥æ—¥å¿—æ¥å£å±‚ã€å®ç°å±‚ï¼Œæ‰€ä»¥æˆ‘ä»¬æœ‰å¿…è¦ä»æ•´ä¸ªåº”ç”¨çº§åˆ«ç€çœ¼ï¼ŒæŠŠé‚£äº›æ— ç”¨çš„æ¥å£å±‚ã€å®ç°å±‚æ’é™¤æ‰ï¼ŒåŒ…æ‹¬JCLã€Log4jå’ŒLog4j 2 æ’æ‰JCLï¼šcommons-logging:commons-logging æ’æ‰Log4jï¼šlog4j:log4j æ’æ‰Log4j2ï¼šorg.apache.logging.log4j:log4j-core ä»¥åŠï¼Œå¦‚æœé¡¹ç›®é—´æ¥å¼•å…¥äº†å…¶ä»–çš„æ¡¥æ¥åŒ…ï¼Œä¹Ÿå¯èƒ½ä¼šå¼•èµ·å’Œå†²çªï¼Œéœ€è¦æ’æ‰ã€‚å¯ä»¥ä½¿ç”¨maven-enforcer-pluginæ’ä»¶æ ‡è¯†é‚£äº›åŒ…æ˜¯è¦è¢«æ’æ‰çš„ï¼Œå®ƒåªæ˜¯ä¸€ä¸ªæ ¡éªŒï¼Œå®é™…ä¸Šä½ ä»ç„¶éœ€è¦åœ¨æ¯ä¸ªå¼•å…¥äº†é”™è¯¯åŒ…çš„ä¾èµ–ä¸­è¿›è¡Œæ’é™¤ã€‚ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:2:4","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"æ³¨æ„äº‹é¡¹ æ ¹æ®å‰é¢çš„ä»‹ç»ï¼Œæˆ‘ä»¬åœ¨å®é™…é¡¹ç›®ä¸­å°†ä¸»è¦åšä¸¤ç±»äº‹æƒ…ï¼š å¼•å…¥æœŸæœ›çš„åŒ…ï¼Œå¹¶æŒ‡å®šç‰ˆæœ¬ æ’é™¤ä¸€äº›åŒ…ï¼ˆæŒ‡å®šç©ºç‰ˆæœ¬ï¼‰ ä»¥ä¸Šä¸¤ä¸ªåŠ¨ä½œï¼Œä¸€èˆ¬æˆ‘ä»¬éƒ½æ˜¯åœ¨çˆ¶POMçš„\u003cdependencyManagement\u003eä¸­å®Œæˆçš„ï¼Œä½†è¿™åªæ˜¯ç®¡ç†åŒ…ç‰ˆæœ¬ï¼Œåœ¨é¡¹ç›®æ²¡æœ‰å®é™…å¼•ç”¨ä¹‹å‰ï¼Œå¹¶ä¸ä¼šçœŸçš„åŠ è½½ã€‚ åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä¼šæŒ‰ç…§è¿™ä¸ªæ€è·¯æ¥å¤„ç†ï¼š æœ‰ä¸€ä¸ªæ¨¡å—Aï¼Œä¾èµ–Log4jæ‰“å°æ—¥å¿—ï¼Œæ‰€ä»¥å®ƒä¾èµ–äº†log4j:log4jåŒ… æˆ‘ä»¬åœ¨çˆ¶POMä¸­æŠŠlog4j:log4jæ’æ‰äº†ï¼Œæ­¤æ—¶æ¨¡å—Aè°ƒç”¨Log4jä¼šæŠ¥é”™ æˆ‘ä»¬åœ¨çˆ¶POMä¸­å¼•å…¥log4j-over-slf4jï¼Œç›®æ ‡æ˜¯æŠŠ`Log4jåˆ‡æ¢åˆ°Slf4jï¼Œè®©æ¨¡å—Aä¸æŠ¥é”™ çœ‹èµ·æ¥å¾ˆå®Œç¾ï¼Œé¡¹ç›®ä¹Ÿèƒ½æ­£å¸¸å¯åŠ¨ï¼Œä½†å½“æ¨¡å—Aéœ€è¦æ‰“å°æ—¥å¿—æ˜¯ï¼Œæˆ‘ä»¬å´è¿˜æ˜¯å¾—åˆ°äº†ä¸€ä¸ªé”™è¯¯log4j:WARN No appenders could be found for loggerã€‚è¿™æ˜¯å› ä¸ºlog4j-over-slf4jå¹¶æ²¡æœ‰çœŸçš„è¢«å¼•å…¥æˆ‘ä»¬çš„é¡¹ç›®ä¸­ï¼ˆå¾ˆå°‘æœ‰å“ªä¸€ä¸ªäºŒæ–¹åŒ…ä¼šå¼•è¿™ä¸ªä¸œè¥¿ï¼Œä¼šè¢«éª‚çš„ï¼‰ã€‚ è§£å†³æ–¹æ¡ˆä¹Ÿå¾ˆç®€å•ï¼Œå°†log4j-over-slf4jé€šè¿‡dependencies\u003eå¼•å…¥å³å¯ï¼Œåœ¨çˆ¶POMåšè¿™ä¸ªä¹Ÿè¡Œï¼Œåœ¨å®é™…æœ‰ä¾èµ–çš„å­POMä¹Ÿè¡Œã€‚ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:2:5","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"Slf4ä»‹ç» å­¦ä¹ å¤§ä½¬Javaæ—¥å¿—é€šå…³ï¼ˆä¸‰ï¼‰ - Slf4j ä»‹ç» ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:3:0","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"åˆ›å»ºLoggerå®ä¾‹ å·¥å‚å‡½æ•° è¦ä½¿ç”¨Slf4jï¼Œéœ€è¦å…ˆåˆ›å»ºä¸€ä¸ªorg.slf4j.Loggerå®ä¾‹ï¼Œå¯ä»¥ä½¿ç”¨å®ƒçš„å·¥å‚å‡½æ•°org.slf4j.LoggerFactory.getLogger()ï¼Œå‚æ•°å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–Classï¼š å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè¿™ä¸ªå­—ç¬¦ä¸²ä¼šä½œä¸ºè¿”å›Loggerå®ä¾‹çš„åå­— å¦‚æœæ˜¯Classï¼Œä¼šè°ƒç”¨å®ƒçš„getNameè·å–Classçš„å…¨è·¯å¾„ï¼Œä½œä¸ºLoggerå®ä¾‹çš„åå­— public class ExampleService { // ä¼ Classï¼Œä¸€èˆ¬éƒ½æ˜¯ä¼ å½“å‰çš„Class private static final Logger log = LoggerFactory.getLogger(ExampleService.class); // ç›¸å½“äº private static final logger log = LoggerFactory.getLogger(\"com.example.service.ExampleService\"); // ä½ ä¹Ÿå¯ä»¥æŒ‡å®šä»»æ„å­—ç¬¦ä¸² private static final Logger log = LoggerFactory.getLogger(\"service\") } è¿™ä¸ªå­—ç¬¦ä¸²æ ¼å¼çš„å®ä¾‹åå­—å¯ä»¥ç§°ä¹‹ä¸ºLoggerNameï¼Œç”¨äºåœ¨æ—¥å¿—å®ç°å±‚åŒºåˆ†å¦‚ä½•æ‰“å°æ—¥å¿—ã€‚ Lombook Lombokæä¾›äº†é’ˆå¯¹å„ç§æ—¥å¿—ç³»ç»Ÿçš„æ”¯æŒï¼Œæ¯”å¦‚ä½ åªéœ€è¦@lombok.extern.slf4j.Slf4jæ³¨è§£å°±å¯ä»¥å¾—åˆ°ä¸€ä¸ªé™æ€çš„logå­—æ®µï¼Œä¸ç”¨åœ¨æ‰‹åŠ¨è°ƒç”¨å·¥å‚å‡½æ•°ï¼Œé»˜è®¤çš„LoggerNameå³æ˜¯è¢«æ³¨è§£çš„Classï¼ŒåŒæ—¶ä¹Ÿæ”¯æŒå­—ç¬¦ä¸²æ ¼å¼çš„topicå­—æ®µæŒ‡å®šLoggerNameã€‚ @Slf4j public class ExampleService { // æ³¨è§£ @Slf4j ä¼šå¸®ä½ ç”Ÿæˆä¸‹è¾¹è¿™è¡Œä»£ç  // private staitc final org.slf4j.Logger log = org.slf4j.LoggerFactory.getLogger(ExampleService.class) } ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:3:1","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"æ—¥å¿—çº§åˆ« é€šè¿‡org.slf4j.event.Levelæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€å…±æœ‰äº”ä¸ªç­‰çº§ï¼ŒæŒ‰ä¼˜å…ˆçº§ä»ä½åˆ°é«˜ä¾æ¬¡ä¸ºï¼š TRACEï¼šä¸€èˆ¬ç”¨äºè®°å½•è°ƒç”¨é“¾è·¯ï¼Œæ¯”å¦‚æ–¹æ³•è¿›å…¥æ—¶æ‰“å°xxx start DEBUGï¼šç±»ä¼¼äºtraceï¼Œå¯ä»¥ç”¨æ¥æ‰“å°æ–¹æ³•çš„å‡ºå…¥å‚ INFOï¼šé»˜è®¤çº§åˆ«ï¼Œä¸€èˆ¬ç”¨äºè®°å½•ä»£ç æ‰§è¡Œæ—¶çš„å…³é”®ä¿¡æ¯ WARNï¼šå½“ä»£ç æ‰§è¡Œé‡åˆ°é¢„æœŸå¤–åœºæ™¯ï¼Œä½†å®ƒä¸å½±å“åç»­æ‰§è¡Œæ—¶ä½¿ç”¨ ERRORï¼šå‡ºç°å¼‚å¸¸ï¼Œä»¥åŠä»£ç æ— æ³•å…œåº•æ—¶ä½¿ç”¨ æ—¥å¿—çš„å®ç°å±‚ä¼šå†³å®šé‚£ä¸ªç­‰çº§çš„æ—¥å¿—å¯ä»¥è¾“å‡ºï¼Œè¿™ä¹Ÿæ˜¯æˆ‘ä»¬æ‰“å°æ—¥å¿—æ˜¯éœ€è¦åŒºåˆ†ç­‰çº§çš„åŸå› ï¼Œåœ¨ä¿è¯é‡è¦çš„æ—¥å¿—ä¸ä¸¢å¤±çš„åŒæ—¶ï¼Œä»…åœ¨æœ‰éœ€è¦æ—¶æ‰æ‰“å°ç”¨äºDebugçš„æ—¥å¿—ã€‚ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:3:2","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"æ‰“å°æ¥å£ é€šè¿‡org.slf4j.Loggerå¯ä»¥çœ‹åˆ°æœ‰éå¸¸å¤šçš„æ—¥å¿—æ‰“å°æ¥å£ï¼Œæ‰“è¿‡å®šä¹‰çš„æ ¼å¼éƒ½ç±»å‹ï¼Œä»¥infoä¸ºä¾‹ï¼Œä¸€å…±æœ‰ä¸¤å¤§ç±» public boolean info(...) public boolean isInfoEnabled(...) infoæ–¹æ³• è¿™ä¸ªæ–¹æ³•æœ‰å¤§é‡çš„é‡è½½ï¼Œä¸è¿‡ä½¿ç”¨é€»è¾‘æ˜¯ä¸€è‡´çš„ @Slf4j public class ExampleService { public void querySomething(Long a, String b) { // åªä¼ å…¥ä¸€ä¸ªStringå‚æ•°ï¼ŒStringå†…å®¹éœ€è¦è‡ªå·±æ‹¼ log.info(\"request\") log.info(\"request a=\" + a + \", b = \" + b); // ä½¿ç”¨æ ¼å¼æ¨¡æ¿ï¼Œé€šè¿‡{}æ’å…¥å ä½ç¬¦ // Intellij IDEAä¼šæ ¡éªŒåç»­å‚æ•°æ•°é‡æ˜¯å¦ä¸å ä½ç¬¦ä¸€è‡´ï¼Œä¸ä¸€è‡´æ—¶ä¼šç»™å‡ºè­¦å‘Š log.info(\"request a={}\"); // è­¦å‘Šï¼šå‚æ•°æ¯”å ä½ç¬¦å°‘ log.info(\"request a={}\", a); log.info(\"request a={}, b={}\", a, b); // æ‰€æœ‰å‚æ•°çš„æœ€åå¯ä»¥è·Ÿä¸€ä¸ªå¼‚å¸¸ï¼Œå®ƒä¸å ç”¨å ä½ç¬¦ Exception e = new RuntimeException(); log.info(\"request a={}\", a, e); } } isInfoEnabledæ–¹æ³• é€šè¿‡isInfoEnabledæ–¹æ³•å¯ä»¥è·å–å½“å‰Loggerå®ä¾‹æ˜¯å¦å¼€å¯äº†å¯¹åº”çš„æ—¥å¿—çº§åˆ«ï¼Œæ¯”å¦‚æˆ‘ä»¬å¯èƒ½è§è¿‡ç±»ä¼¼è¿™æ ·çš„ä»£ç ï¼š if (log.isInfoEnabled()) { log.info(...) } ä½†å…¶å®æ—¥å¿—å®ç°å±‚æœ¬èº«å°±ä¼šåˆ¤æ–­å½“å‰Loggerå®ä¾‹çš„è¾“å‡ºç­‰çº§ï¼Œä½äºæ­¤ç­‰çº§çš„æ—¥å¿—å¹¶ä¸ä¼šè¾“å‡ºï¼Œæ‰€ä»¥ä¸€èˆ¬ä¸å¤ªéœ€è¦è¿™æ ·çš„åˆ¤æ–­ï¼Œä½†å¦‚æœä½ çš„è¾“å‡ºéœ€è¦é¢å¤–æ¶ˆè€—èµ„æºï¼Œé‚£ä¹ˆè¿˜æ˜¯å…ˆåˆ¤æ–­ä¸€ä¸‹ä¸ºå¥½ if (log.isIInfoEnabled()) { // æœ‰è¿œç¨‹è°ƒç”¨ String resource = rpcService.call(); log.info(\"resource={}\", resource) // è¦è§£æå¤§å¯¹è±¡ Object result = ...; // ä¸€ä¸ªå¤§å¯¹è±¡ log.info(\"result={}\", JSON.toJSONString(result)); } Marker æœ‰äº›æ¥å£æ”¯æŒorg.slf4j.Markerç±»å‹çš„å…¥å‚ï¼Œæ¯”å¦‚log.info(Marker, ...) æˆ‘ä»¬å¯ä»¥é€šè¿‡å·¥å‚å‡½æ•°åˆ›å»ºMarkerå¹¶ä½¿ç”¨ï¼Œæ¯”å¦‚ Marker marker = MarkerFactory.getMarker(\"foobar\"); log.info(marker, \"test a={}\", 1); è¿™ä¸ªMarkeræ˜¯ä¸€ä¸ªæ ‡è®°ï¼Œå®ƒä¼šä¼ é€’ç»™æ—¥å¿—å®ç°å±‚ï¼Œç”±å®ç°å±‚å†³å®šMarkerçš„å¤„ç†æ–¹å¼ï¼Œæ¯”å¦‚ å°†Markeré€šè¿‡%markeræ‰“å°å‡ºæ¥ ä½¿ç”¨MarkerFilterè¿‡æ»¤å‡ºï¼ˆæˆ–è¿‡æ»¤æ‰ï¼‰å¸¦æœ‰æŸä¸ªMarkerçš„æ—¥å¿—ï¼Œæ¯”å¦‚æŠŠéœ€è¦Sunfireç›‘æ§çš„æ—¥å¿—éƒ½è¿‡æ»¤å‡ºæ¥å†™åˆ°ä¸€ä¸ªå•ç‹¬çš„æ—¥å¿—æ–‡ä»¶ä¸­ã€‚ MDC MDCçš„å…¨ç§°æ˜¯Mapped Diagnostic Contextï¼Œç›´è¯‘ä¸ºæ˜ å°„è°ƒè¯•ä¸Šä¸‹æ–‡ï¼Œè¯´äººè¯å°±æ˜¯ç”¨æ¥å­˜å‚¨æ‰©å±•å­—æ®µçš„åœ°æ–¹ï¼Œè€Œä¸”å®ƒæ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œæ¯”å¦‚OpenTelemetryçš„traceIdå°±ä¼šè¢«å­˜åˆ°MDCä¸­ è€Œä¸”MDCçš„ä½¿ç”¨ä¹Ÿå¾ˆç®€å•ï¼Œå°±åƒæ˜¯ä¸€ä¸ªMap\u003cString, String\u003eå®ä¾‹ï¼Œå¸¸ç”¨æ–¹æ³•put/get/remove/clearéƒ½æœ‰ // å’ŒMap\u003cString, String\u003e ç›¸ä¼¼çš„æ¥å£å®šä¹‰ MDC.put(\"key\", \"value\"); String valeu = MDC.get(\"key\"); MDC.remove(\"key\"); MDC.clear(); // è·å–MDCä¸­çš„æ‰€æœ‰å†…å®¹ Map\u003cString, String\u003e context = MDC.getCopyOfContextMap(); ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:3:3","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"Logbackä»‹ç» å­¦ä¹ Javaæ—¥å¿—é€šå…³ï¼ˆå››ï¼‰ - Logback ä»‹ç» ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:4:0","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"é…ç½®å…¥å£ Logbackæ”¯æŒXMLã€Groovyçš„é…ç½®æ–¹å¼ï¼Œä»¥XMLæ¥è¯´ï¼Œå®ƒä¼šé»˜è®¤æŸ¥æ‰¾resourcesç›®å½•ä¸‹çš„logback-test.xmlç”¨äºæµ‹è¯•/logback.xmlæ–‡ä»¶ã€‚ è€Œå¦‚æœä½ æ˜¯ä½¿ç”¨Spring Bootï¼Œé‚£ä¹ˆä½ è¿˜å¯ä»¥ä½¿ç”¨logback-spring.xmlæ–‡ä»¶è¿›è¡Œé…ç½®ï¼Œè¿™ä¸¤è€…çš„åŒºåˆ«æ˜¯ logback-spring.xmlæ˜¯ç”±Sprint Bootæ‰¾åˆ°ï¼Œæ’å…¥è‡ªå·±çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å¹¶åšè¿›ä¸€æ­¥å¤„ç†åå†ä¼ é€’ç»™Logbackçš„ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­ä½¿ç”¨\u003cspringProfile\u003eåŒºåˆ†ç¯å¢ƒé…ç½®ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨\u003cspringProperty\u003eæ‹¿åˆ°Springä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆæ¯”å¦‚spring.application.nameï¼‰ã€‚ logback.xmlæ˜¯ç”±logbackè‡ªå·±æ‰¾åˆ°ï¼Œè‡ªç„¶ä¸ä¼šæœ‰Spring Bootç›¸å…³çš„èƒ½åŠ›ã€‚ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:4:1","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"é…ç½®æ–‡ä»¶ä»‹ç» æ¥ä¸‹æ¥æˆ‘ä»¬ä»¥logback-spring.xmlä¸ºä¾‹è¿›è¡Œä»‹ç»ï¼Œä¸€ä¸ªLogbacké…ç½®æ–‡ä»¶ä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªæ ‡ç­¾ï¼š configurationï¼šæœ€å¤–å±‚çš„çˆ¶æ ‡ç­¾ï¼Œå…¶ä¸­æœ‰å‡ ä¸ªå±æ€§é…ç½®ï¼Œä½†é¡¹ç›®ä¸­è¾ƒå°‘ä½¿ç”¨ propertyï¼šå®šä¹‰å˜é‡ appenderï¼šè´Ÿè´£æ—¥å¿—è¾“å‡ºï¼ˆä¸€èˆ¬æ˜¯å†™åˆ°æ–‡ä»¶ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å®ƒè®¾ç½®è¾“å‡ºæ–¹æ¡ˆ loggerï¼šç”¨æ¥è®¾ç½®æŸä¸ªLoggerNmaeçš„æ‰“å°çº§åˆ« rootï¼šloggerçš„å…œåº•é…ç½®ï¼Œä»è€Œæˆ‘ä»¬ä¸å¿…é…ç½®æ¯ä¸ªLoggerName conversionRuleï¼šå®šä¹‰è½¬æ¢è§„åˆ™ springPropertyå’Œproperty å‰æ–‡æåˆ°\u003cspringProperty\u003eç”¨æ¥æ’å…¥Springä¸Šä¸‹æ–‡ï¼Œé‚£\u003cproperty\u003eå°±æ˜¯Logbackè‡ªå·±å®šä¹‰å˜é‡çš„æ ‡ç­¾ \u003cspringProperty scope=\"context\" name=\"APP_NAME\" source=\"spring.application.name\"/\u003e \u003cproperty name=\"LOG_PATH\" value=\"${user.home}/${APP_NAME}/logs\"/\u003e \u003cproperty name=\"APP_LOG_FILE\" value=\"${LOG_PATH}/application.log\"/\u003e \u003cproperty name=\"APP_LOG_PATTERN\" value=\"%date{yyyy-MM-dd HH:mm:ss.SSS}|%-5level|%X{trace_id}|%thread|%logger{20}|%message%n%exception\"/\u003e æˆ‘ä»¬é¦–å…ˆä½¿ç”¨\u003cspringProperty\u003eæ’å…¥APP_NAMEè¿™ä¸ªå˜é‡æ¥è¡¨ç¤ºåº”ç”¨åï¼Œè™½ç„¶ç”¨å®ƒæ‹¼å‡ºLOG_PATHå˜é‡ï¼Œç¤ºä¾‹ä¸­è¿˜ç”¨åˆ°äº†${user.home}è¿™ä¸ªLogbackå†…å»ºæ”¯æŒçš„ä¸Šä¸‹æ–‡å˜é‡ï¼ŒAPP_LOG_FILEæ˜¯logæ–‡ä»¶è·¯å¾„ï¼ŒAPP_LOG_PATTERNæ˜¯æ—¥å¿—æ ¼å¼ã€‚ appender \u003cappender name=\"APPLICATION\" class=\"ch.qos.logback.core.rolliing.RollingFileAppender\"\u003e \u003cfile\u003e${APP_LOG_FILE}\u003c/file\u003e \u003cencoder\u003e \u003cpattern\u003e${APP_LOG_PATTERN}\u003c/pattern\u003e \u003c/encoder\u003e \u003crollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\"\u003e \u003cfileNamePolicy\u003e${APP_LOG_FILE}.%d{yyyy-MM-dd}.%i\u003c/fileNamePolicy\u003e \u003cmaxHistory\u003e30\u003c/maxHistory\u003e \u003cmaxFileSize\u003e200MB\u003c/maxFileSize\u003e \u003ctotalSizeCap\u003e10GB\u003c/totalSizeCap\u003e \u003c/rollingPolicy\u003e \u003c/appender\u003e \u003cappender name=\"APPLICATION-async\" class=\"ch.qos.logback.classic.AsyncAppender\"\u003e \u003cqueueSize\u003e256\u003c/queueSize\u003e \u003cdiscardingThresold\u003e0\u003c/discardingThresold\u003e \u003cnerverBlock\u003etrue\u003c/nerverBlock\u003e \u003cappend-ref ref=\"APPLICATION\"/\u003e \u003c/appender\u003e ch.qos.logback.core.rolling.RollingFileAppenderè´Ÿè´£å°†æ—¥å¿—æ»šåŠ¨æ‰“å°ï¼Œé¿å…å•æ–‡ä»¶ä½“ç§¯è¿‡å¤§ï¼Œå…·ä½“çš„æ»šåŠ¨ç­–ç•¥åœ¨\u003crollingPolicy\u003eä¸­æŒ‡å®šã€‚ ch.qos.logback.classic.AsyncAppenderè´Ÿè´£å°†æ—¥å¿—å¼‚æ­¥æ‰“å°ï¼Œé¿å…å¤§é‡æ‰“å°æ—¥å¿—æ—¶é˜»å¡çº¿ç¨‹ loggerå’Œroot \u003clogger\u003eç”¨æ¥è®¾ç½®æŸä¸ªLoggerNameçš„æ‰“å°çº§åˆ«ï¼Œæ¯”å¦‚ \u003clogger level=\"INFO\" additivity=\"false\" name=\"com.foo.bar\"\u003e \u003cappender-ref ref=\"APPLICATION-async\"/\u003e \u003c/logger\u003e \u003croot level=\"INFO\"\u003e \u003cappender-ref ref=\"APPLICATION-async\"/\u003e \u003c/root\u003e ä¸Šé¢çš„é…ç½®æŒ‡å®šæ‰€æœ‰LoggerNameä¸ºcom.foo.barçš„æ—¥å¿—ä»¥INFOçº§åˆ«è¿›è¡Œæ‰“å°ï¼Œæ­¤é…ç½®ç»‘å®šçš„è¾“å‡ºå™¨ï¼ˆappenderï¼‰ä¸ºAPPLICATION-asyncã€‚ å…¶ä¸­LoggerNameä¼šä»¥.ä¸ºåˆ†éš”ç¬¦é€çº§å‘ä¸ŠåŒ¹é…ï¼Œæ¯”å¦‚å®é™…LoggerNameä¸ºcom.foo.bar.service.ExampleServiceï¼Œé‚£ä¹ˆå®ƒçš„æŸ¥æ‰¾è¿‡ç¨‹ä¾æ¬¡ä¸ºï¼š com.foo.bar.service.ExampleSerivce com.foo.bar.service com.foo.barï¼ˆæ­¤æ—¶å‘½ä¸­äº†æˆ‘ä»¬æä¾›çš„loggerï¼Œå¦å¤–å› ä¸ºé…ç½®äº†additivity=â€œfalseâ€ï¼Œæ‰€ä»¥åœæ­¢ç»§ç»­å‘ä¸‹æŸ¥æ‰¾ com.foo com \u003croot\u003e è€Œ\u003croot\u003eå°±æ˜¯å…œåº•é…ç½®äº†ï¼Œå½“LoggerNameæ²¡åŒ¹é…åˆ°ä»»ä½•ä¸€é¡¹\u003clogger\u003eæ—¶ï¼Œå°±ä¼šä½¿ç”¨\u003croot\u003eï¼Œæ‰€ä»¥å®ƒæ²¡æœ‰additivityå’Œnameå±æ€§çš„ã€‚ä¸€èˆ¬å®é™…ä¸šåŠ¡åœºæ™¯ä¸­ï¼Œæ‰€æœ‰\u003clogger\u003eéƒ½å»ºè®®åŠ ä¸Šadditivity=falseï¼Œå¦åˆ™æ—¥å¿—å°±ä¼šå› æŸ¥æ‰¾åˆ°å¤šä¸ª\u003clogger\u003eï¼ˆæˆ–\u003croot\u003eï¼‰è€Œæ‰“å°å¤šä»½ã€‚ springProfile Springè¿˜æä¾›äº†\u003cspringProfile\u003eæ ‡ç­¾ï¼Œç”¨æ¥æ ¹æ®Spring Profilesï¼ˆå³spring.profiles.activeçš„å€¼ï¼‰åŠ¨æ€è°ƒæ•´æ—¥å¿—ä¿¡æ¯ï¼Œæ¯”å¦‚æˆ‘ä»¬å¸Œæœ›çº¿ä¸Šç¯å¢ƒä½¿ç”¨INFOçº§åˆ«ï¼Œè€Œé¢„å‘ã€æ—¥å¿—ä½¿ç”¨TRACEçº§åˆ«ã€‚ \u003cspringProfile name=\"production\"\u003e \u003croot level=\"INFO\"\u003e \u003cappender-ref ref=\"APPLICATION-async\"/\u003e \u003c/root\u003e \u003c/springProfile\u003e \u003cspringProfile name=\"staging.testing\"\u003e \u003croot level=\"TRACE\"\u003e \u003cappender-ref ref=\"APPLICATION-async\"/\u003e \u003c/root\u003e \u003c/springProfile\u003e ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:4:2","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"Java API é™¤äº†ä½¿ç”¨XMLé…ç½®æ–‡ä»¶å¤–ï¼ŒLogbackè¿˜æä¾›äº†å¤§é‡çš„Java APIä»¥æ”¯æŒæ›´å¤æ‚çš„ä¸šåŠ¡è¯‰æ±‚ï¼Œæˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªéå¸¸å®ç”¨çš„åœºæ™¯æ¥ç®€å•ä»‹ç»ä¸€ä¸‹ï¼š åœºæ™¯ä¸€ï¼šä½¿ç”¨log.info(\"obj={}\", obj)æ—¶ï¼Œå¦‚ä½•å°†objç»Ÿä¸€è½¬JSON Stringåè¾“å‡º åœºæ™¯äºŒï¼šæ—¥å¿—ä¸­æ¶‰åŠåˆ°çš„æ‰‹æœºå·ã€èº«ä»½è¯å·ï¼Œå¦‚ä½•è„±æ•åå†è®°å½•æ—¥å¿— åœºæ™¯ä¸‰ï¼šLogbacké…ç½®åŸºäºXMLï¼Œå¦‚ä½•ä¸æ”¹ä»£ç ä¸å‘å¸ƒï¼Œä¹Ÿå¯ä»¥åŠ¨æ€ä¿®æ”¹æ—¥å¿—çº§åˆ« å…¶ä¸­å‰ä¸¤ä¸ªé—®é¢˜éƒ½å¯ä»¥é€šè¿‡MessageConverterå®ç°ï¼Œç¬¬ä¸‰ä¸ªé—®é¢˜å¯ä»¥å€ŸåŠ©LoggerContextåŠLoggerå®ç°ã€‚ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:4:3","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"MDBä¸­çš„traceId æ‰‹åŠ¨è®°å½•traceIdï¼Œæ¯”å¦‚ log.info(\"traceId={}, blah blah blah\", Span.current().getSpanContext().getTraceId()); å…¶å®OpenTelemetryå·²ç»è‡ªåŠ¨å°†traceIdåŠ åˆ°äº†MDCï¼Œå¯¹åº”çš„Keyæ˜¯trace_idï¼Œä½¿ç”¨%mdc{trace_id}å³å¯æ‰“å°å‡ºtraceIã€‚ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:4:4","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"æœ€ä½³å®è·µ å­¦ä¹ Javaæ—¥å¿—é€šå…³ï¼ˆäº”ï¼‰ - æœ€ä½³å®è·µ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:5:0","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"æ€»æ˜¯ä½¿ç”¨æ¥å£å±‚ æ— è®ºæ˜¯å†™ä»£ç è¿˜æ˜¯å®ç°ä¸€ä¸ªä¸‰æ–¹å·¥å…·ï¼Œè¯·åªä½¿ç”¨æ¥å£å±‚è®°å½•æ—¥å¿— å¦‚æœéœ€è¦å‘å¤–æä¾›ä¸‰æ–¹å·¥å…·ï¼Œè®°å¾—åœ¨ä¾èµ–ä¸­å°†æ—¥å¿—çš„å®ç°å±‚åŠé€‚é…å±‚æ ‡è®°ä¸ºoptionalï¼Œæ¯”å¦‚ï¼š \u003cdependency\u003e \u003cgroupId\u003ech.qos.logback\u003c/groupId\u003e \u003cartifactId\u003elogback-core\u003c/artifactId\u003e \u003cversion\u003e${logback.version}\u003c/version\u003e \u003cscope\u003eruntime\u003c/scope\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e \u003cscope\u003eruntime\u003c/scope\u003eï¼šruntimeçš„åŒ…ç¼–è¯‘æ—¶ä¼šè¢«å¿½ç•¥ï¼ˆè®¤ä¸ºè¿è¡Œç¯å¢ƒå·²ç»æœ‰å¯¹åº”åŒ…äº†ï¼‰ \u003coptional\u003etrue\u003c/optional\u003eï¼šä¾èµ–ä¸ä¼šä¼ é€’ï¼Œmavenä¸ä¼šè‡ªåŠ¨å®‰è£…æ­¤åŒ… ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:5:1","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"ä¸è¦æ‰“å°åˆ†å‰²çº¿ ä¸è¦æ‰“å°ç±»ä¼¼è¿™ç§åªåŒ…å«åˆ†å‰²çº¿çš„å†…å®¹ï¼Œlog.info(\"=========start========\")ï¼Œå› ä¸ºåœ¨èŒ«èŒ«çš„æ—¥å¿—ä¸­ï¼Œè¿™å¥æ—¥å¿—çš„ä¸‹ä¸€æ¡å¾ˆå¯èƒ½æ¥è‡ªå…¶ä»–å¼‚æ­¥ä»»åŠ¡ï¼Œå¦‚æœä½¿ç”¨SLSæ”¶é›†ç”šè‡³æ¥è‡ªå¦ä¸€å°æœºå™¨ï¼Œè¿™æ¡åˆ†å‰²çº¿æ ¹æœ¬èµ·ä¸åˆ°ä»»ä½•ä½œç”¨ã€‚ æ­£ç¡®çš„æ–¹å¼æ˜¯é€šè¿‡å…³é”®å­—è¿›è¡Œæ ‡è®°ï¼Œæ¯”å¦‚ï¼šlog.info(\"FooBarProcessor start, request={}\", request)ï¼Œä¹‹åå°±å¯ä»¥é€šè¿‡å…³é”®å­—FooBarProcessorå¿«é€Ÿè¿‡æ»¤ï¼Œè¿™å¯¹äºgrepå’ŒSLSéƒ½é€‚ç”¨ã€‚ å¦å¤–ï¼Œå¯ä»¥ç”¨Markerè®©æ—¥å¿—è¯­ä¹‰æ›´æ¸…æ™°ï¼Œåªæ˜¯éº»çƒ¦äº†ç‚¹ï¼Œçœ‹ä¸ªäººå–œå¥½ã€‚ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:5:2","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"é¿å…å› å†™æ—¥å¿—è€ŒæŠ›é”™ æ¯”å¦‚æ²¡æœ‰åˆ¤ç©ºå°±ç›´æ¥è°ƒç”¨äº†å®ƒçš„æ–¹æ³•ï¼š Object result = rpcResource.call(); log.info(\"result.userId={}\", result.getUserId()); ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:5:3","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"ä¸¤ä¸ªFastJsonå‚æ•° IgnoreErrorGetter Fastjsonçš„åºåˆ—åŒ–å…¶å®æ˜¯ä¾èµ–äºç±»ä¸­å„ä¸ªgetterï¼Œå¦‚æœæŸä¸ªgetteræŠ›å¼‚å¸¸åˆ™ä¼šé˜»æ–­æ•´ä¸ªåºåˆ—åŒ–ï¼Œä½†å…¶å®æœ‰äº›getterå¼‚å¸¸å¹¶éä¸¥é‡é—®é¢˜ï¼Œæ­¤æ—¶å°±å¯ä»¥ä½¿ç”¨SerializerFeature.IgnoreErrorGetterå‚æ•°å¿½ç•¥getterä¸­æŠ›å‡ºçš„å¼‚å¸¸ï¼š public class Foo { private Long userId; @Deprecated private Long accountId; public long getAccountId() { throw new RuntimeException(\"accountId deprecated\"); } } log.info(\"foo={}\", JSON.toJSONString(foo, SerializerFeature.IgnoreErrorGetter)); IgnoreNonFieldGetter æ¯”å¦‚æœ‰ä¸ªResultåŒ…è£…ç±»å¦‚ä¸‹ï¼Œæ³¨æ„isErroræ–¹æ³•ï¼Œå½“è¢«Fastjsonåºåˆ—åŒ–æ—¶ï¼Œä¼šè¾“å‡º\"error\":falseï¼Œå¦‚æœå¸Œæœ›å¿½ç•¥æ‰ç±»ä¼¼è¿™ç§æ²¡æœ‰å®ä½“å­—æ®µå¯¹åº”çš„getteræ–¹æ³•ï¼Œå°±å¯ä»¥è¿½åŠ SerializerFeature.IgnoreNonFieldGetterå‚æ•° @Data public class Result\u003cT\u003e { private boolean success; private T data; public boolean isError() { return !success; } } log.info(\"result={}\", JSON.toJSONString(result, SerializerFeature.IgnoreNonFieldGetter)) è¿™ä¸ªå‚æ•°å¯¹äºæ‰“å°ResultåŒ…è£…ç±»éå¸¸æœ‰å¸®åŠ©ï¼Œå¦‚æœæ‰“å°å‡º\"error\": falseï¼Œé‚£å½“ä½ å¸Œæœ›ä½¿ç”¨errorä½œä¸ºå…³é”®å­—æŸ¥è¯¢é”™è¯¯æ—¶ï¼Œå°±ä¼šåŒ¹é…åˆ°å¾ˆå¤šåŒ…å«errorå´å¹¶éé”™è¯¯çš„æ— æ•ˆæ•°æ®ã€‚ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:5:4","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"ä¸è¦é—æ¼å¼‚å¸¸å †æ ˆ å¼‚å¸¸å€¼å‚æ•°æ˜¯ä¸å ç”¨å­—ç¬¦æ¨¡ç‰ˆçš„ï¼Œå¦‚æœä½ çš„å‚æ•°æ•°é‡ä¸åŒ¹é…ï¼Œå¾ˆå¯èƒ½æ‰“å°ç»“æœä¸é¢„æœŸä¸ç¬¦ Exception e = new RuntimeException(\"blahblashblash\"); log.error(\"exception={}\", e); å› ä¸ºæ­¤æ—¶eå’Œå¯¹åº”çš„{}ä½ç½®åŒ¹é…ï¼ŒSlf4jä¼šå°è¯•å°†å¼‚å¸¸è½¬ä¸ºå­—ç¬¦ä¸²æ‹¼åˆ°æ—¥å¿—æ¨¡ç‰ˆä¸­ï¼Œæœ€ç»ˆè¿™å¥ç›¸å½“äº log.error(\"exception={}\", e.toString()); æœ€ç»ˆä½ åªèƒ½å¾—åˆ°exception=blahblashblashï¼Œè€Œå †æ ˆå°±ä¸¢æ‰äº† æ­£ç¡®çš„åšæ³•æ˜¯è¦ä¿è¯å¼‚å¸¸å‚æ•°ä¸å ç”¨å­—ç¬¦æ¨¡ç‰ˆ // ç”¨ e.getMessage()æ‹¼åˆ°æ—¥å¿—ä¿¡æ¯åï¼Œé€šè¿‡æœ‰ç‹¬ç«‹çš„eç”¨äºæ‰“å°å †æ ˆ log.error(\"exception={}\", e.getMessage(), e); æœ€ç»ˆä¼šè¾“å‡º exception=blashblashlbahs æ¢è¡Œåä¼šæœ‰å †æ ˆä¿¡æ¯ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:5:5","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"é™åˆ¶æ—¥å¿—è¾“å‡ºé•¿åº¦ é™åˆ¶æ—¥å¿—æ–‡æœ¬æœ€å¤§é•¿åº¦ æœ‰æ—¶å€™ä¸€ä¸ªPOJOéå¸¸å¤§ï¼Œå½“æˆ‘ä»¬é€šè¿‡ log.info(\"result={}\", JSON.toJSONString(result)) æ‰“å°æ—¥å¿—æ—¶ï¼Œæ•´æ¡æ—¥å¿—å°±ä¼šå˜å¾—éå¸¸é•¿ï¼Œä¸ä½†å¯¹æ€§èƒ½æœ‰å½±å“ï¼Œä¸»è¦æ˜¯è¿™ä¹ˆå¤§çš„ç»“æœå¯¹å®é™…é—®é¢˜æ’æŸ¥ä¹Ÿä¸è§å¾—æœ‰å¸®åŠ©ã€‚ å¯ä»¥é€šè¿‡Format modifiersé™åˆ¶æ¶ˆæ¯æœ€å¤§é•¿åº¦ï¼Œå¹¶å°†è¶…å‡ºçš„éƒ¨åˆ†ä¸¢å¼ƒ %.-2000message é™åˆ¶å †æ ˆçš„å±‚çº§ å…¶å®Logbackå¤©ç„¶æ”¯æŒï¼Œæ¯”å¦‚%exception{50}å°±å¯ä»¥åªæ‰“å°50å±‚ï¼ŒåŒæ—¶Logbacké’ˆå¯¹å¼‚å¸¸å †æ ˆæœ‰æ›´å¤šçš„æ§åˆ¶èƒ½åŠ› ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:5:6","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"å°†å †æ ˆåˆå¹¶ä¸ºä¸€è¡Œ æœ‰äº›åŒå­¦å¸Œæœ›å°†å †æ ˆåœ¨ä¸€è¡Œè¾“å‡ºï¼Œä¿è¯é€šè¿‡ç®¡é“è¿›è¡Œå¤šå±‚grepæ—¶æåˆ°æœŸæœ›çš„è®°å½•ã€‚ å…¶å®é€šè¿‡Logbacké…ç½®å°±å¯ä»¥æ”¯æŒè¿™ä¸ªèƒ½åŠ›ï¼Œä¸»è¦ç”¨åˆ°%replace %replace(%exception){'[\\r\\n\\t]+', ' '}%nopex %replace(p){r, t}ï¼Œå°†ç»™åˆ°çš„pï¼Œä½¿ç”¨æ­£åˆ™rè¿›è¡ŒåŒ¹é…ï¼Œå‘½ä¸­çš„æ›¿æ¢ä¸ºtï¼Œæ‰€ä»¥ä¸Šè¾¹å°±æ˜¯ï¼Œå°†%exceptionä¸­çš„[\\r\\n\\t]æ›¿æ¢ä¸ºå››ä¸ªç©ºæ ¼ %nopexï¼šå¦‚æœä¸åŠ ï¼ŒLogabckä¼šè‡ªåŠ¨åœ¨æ—¥å¿—æœ€åè¿½åŠ %exceptionï¼Œå¯¼è‡´å¼‚å¸¸å †æ ˆè¢«æ‰“ä¸¤é ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:5:7","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"ä¸å»ºè®®ä½¿ç”¨%methodå’Œ%line åœ¨Logbackçš„é…ç½®ä¸­ï¼Œå¯ä»¥é€šè¿‡%methodå’Œ%lineè¾“å‡ºæ–¹æ³•åå’Œè¡Œå·ï¼Œä½†è¿™ä¸¤é¡¹éƒ½ä¾èµ–äºå½“å‰çš„å †æ ˆè½¨è¿¹ï¼ˆStackTraceï¼‰ï¼Œè€Œè·å–å †æ ˆè½¨è¿¹çš„ä»£ä»·éå¸¸é«˜ï¼Œæ—¥å¿—å°±ä¼šå ç”¨å¤§é‡çš„CPUï¼Œæ‰€ä»¥ä¸€èˆ¬æƒ…å†µä¸‹ä¸å»ºè®®åœ¨æ—¥å¿—ä¸­è¾“å‡ºè¿™äº›å­—æ®µã€‚ å¦‚æœå¯¹æ–¹æ³•åæœ‰è¾“å‡ºè¦æ±‚ï¼Œå¯ä»¥ç›´æ¥ç¡¬ç¼–ç åˆ°è¾“å‡ºå­—ç¬¦ä¸²ä¸­ï¼Œæ¯”å¦‚ log.info(\"queryUserInfo, request={}, result={}\", request, result); ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:5:8","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"ä¸è¦å°†æ—¥å¿—è¾“å‡ºåˆ°Console æˆ‘ä»¬å¹³æ—¶è°ƒç”¨System.out.printlnæ—¶ï¼Œé»˜è®¤è¾“å‡ºä½ç½®å°±æ˜¯æ§åˆ¶å°ï¼ŒLogbackä¹Ÿæä¾›äº†ch.qos.logback.core.ConsoleAppenderç”¨äºå°†æ—¥å¿—è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œä½†æ§åˆ¶å°åŸºæœ¬æ²¡äººçœ‹ï¼Œè¿˜æµªè´¹æœºå™¨èµ„æºã€‚ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:5:9","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"æ— ç”¨çš„LogUtil æœ‰æ—¶ä¼šé‡åˆ°å…¶ä»–äººé¢å¤–å†™çš„æ—¥å¿—å·¥å…·ç±»ï¼Œæ¯”å¦‚LogUtilï¼Œå°†Slf4jæˆ–Logbackå·²æœ‰èƒ½åŠ›é‡æ–°å®ç°ä¸€éï¼Œä½†å®é™…ä¸Šé€šè¿‡æ­£ç¡®é…ç½®ï¼Œç›´æ¥ä½¿ç”¨Slf4jæä¾›çš„å¼ºå¤§APIæ˜¯æ›´å¥½çš„åšæ³•ã€‚ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:5:10","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Java"],"content":"ç†Ÿè¯»ã€Šæ—¥å¿—è§„çº¦ã€‹ ã€Šé˜¿é‡Œå·´å·´Javaå¼€å‘æ‰‹å†Œã€‹æœ‰ä¸“é—¨ä¸€ç« æ˜¯ã€Šæ—¥å¿—è§„çº¦ã€‹ï¼Œå»ºè®®ç†Ÿè¯»ï¼Œå…¶å®æ•´ä¸ªã€Šé˜¿é‡Œå·´å·´Javaå¼€å‘æ‰‹å†Œã€‹éƒ½åº”è¯¥ç†Ÿè¯»ï¼ŒèŠ±ä¸äº†å¤šå°‘æ—¶é—´ã€‚ ","date":"2025-07-22","objectID":"/posts/java-log-in-practice/:5:11","tags":["Java"],"title":"Javaæ—¥å¿—å®æˆ˜","uri":"/posts/java-log-in-practice/"},{"categories":["Presto"],"content":"ç®€å•æ‹‰å–æ•°æ®æŸ¥è¯¢çš„å®ç°åŸç† SELECT ss_item_sk, ss_sales_price FROM store_sales; ","date":"2025-07-20","objectID":"/posts/trino-filter-project/:1:0","tags":["Presto"],"title":"Prestoæ•°æ®è¿‡æ»¤å’ŒæŠ•å½±","uri":"/posts/trino-filter-project/"},{"categories":["Presto"],"content":"æ‰§è¡Œè®¡åˆ’çš„ç”Ÿæˆå’Œä¼˜åŒ– åˆå§‹é€»è¾‘æ‰§è¡Œè®¡åˆ’ TableScanèŠ‚ç‚¹ï¼šè´Ÿè´£ä»æ•°æ®æºè¿æ¥å™¨æ‹‰å–æ•°æ® OutputèŠ‚ç‚¹ï¼šé€»è¾‘æ‰§è¡Œè®¡åˆ’çš„æ ¹èŠ‚ç‚¹ï¼Œè¡¨ç¤ºè¾“å‡ºè®¡ç®—ç»“æœï¼Œå…¶è‡ªèº«æ²¡æœ‰è®¡ç®—é€»è¾‘ public class TableScanNode extends PlanNode { // TableHandleè¡¨ç¤ºçš„æ˜¯ä¸å½“å‰TableScanNodeå¯¹åº”çš„æ˜¯æ•°æ®æºå­˜å‚¨ä¸­çš„é‚£ä¸ªç±» private final TableHandle table; // outputSymbols: TableScanNodeè¾“å‡ºçš„symbolsåˆ—è¡¨ï¼Œåœ¨Prestoä¸­ä½¿ç”¨Symbolè¡¨ç¤ºè¦è¾“å‡ºå“ªäº›åˆ— private final List\u003cSymbol\u003e outputSymbols; // assignmentsï¼šå¯¹äºoutputSymbolsä¸­çš„æ¯ä¸ªSymbolï¼Œæ˜ç¡®å…¶æ¥æºäºæ•°æ®æºConnectorçš„é‚£ä¸ªColumnï¼ˆç”¨ColumnHandleè¡¨ç¤ºï¼‰ private final Map\u003cSymbol, ColumnHandle\u003e assignments; // symbol -\u003e column private final TupleDomain\u003cColumnHandle\u003e enforcedConstraint; private final boolean forDelete; æŸ¥è¯¢æ‰§è¡Œæ—¶ï¼Œåªè¦å°†TableHandleã€ColumnHandläº¤ç»™æ•°æ®æºè¿æ¥å™¨ï¼Œå®ƒå°±çŸ¥é“æ‹‰å–é‚£äº›è¡¨ã€å“ªäº›åˆ—çš„æ•°æ®ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºæœ¬æŠ½è±¡ï¼Œåœ¨è€ƒè™‘åˆ°å„ç§ä¸‹æ¨ä¼˜åŒ–æ—¶ï¼Œè¿™ä¸¤ä¸ªæ¦‚å¿µå°†å‘æŒ¥æ›´å¤§çš„ä½œç”¨ public class OutputNode extends PlanNode { // ä¸Šæ¸¸PlanNodeèŠ‚ç‚¹ private final PlanNode source; // SELECTè¯­å¥æœ€ç»ˆè¦è¾“å‡ºç»“æœçš„åˆ—åç§° private final List\u003cString\u003e columnNames; // OutputNodeè¾“å‡ºçš„Symbolè¡¨ç¤ºï¼Œä¸ColumnNames--å¯¹åº” private final List\u003cSymbol\u003e outputs; ä¼˜åŒ–åçš„é€»è¾‘æ‰§è¡Œè®¡åˆ’ ä¸ºäº†æå‡ä»æ•°æ®æºæ‹‰å–æ•°æ®çš„å¹¶å‘åº¦ï¼Œå¯å°†TableScanNodeè®¾è®¡ä¸ºå¤šä¸ªä»»åŠ¡å¹¶åˆ†åˆ«æ”¾ç½®åœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šä»¥å¹¶å‘æ‹‰å–æ•°æ®ï¼Œå¯¹äºOutputNodeæ¥è¯´ï¼Œå…¶å¹¶è¡Œåº¦åªèƒ½æ˜¯1ï¼Œå› ä¸ºå®ƒéœ€è¦å°†ä¸Šæ¸¸TableScanNodeæ‹‰å–çš„æ•°æ®åˆå¹¶åˆ°ä¸€èµ·ï¼Œç»™åˆ°æŸ¥è¯¢å‘èµ·è€…ã€‚ç”±äºæ‰§è¡ŒTableScanNodeä¸OutputNodeçš„å¹¶è¡Œåº¦ä¸åŒï¼Œåœ¨OutputNodeä¸TableScanNodeä¸­é—´éœ€è¦æ’å…¥ä¸€ä¸ªExchangeNodeæ¥å®ç°æ•°æ®äº¤æ¢ï¼Œæ”¹å˜å¹¶è¡Œåº¦ã€‚ æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µåˆ’åˆ† Prestoçš„AddExchangeä¼˜åŒ–å™¨åŸºäºé€»è¾‘æ‰§è¡Œè®¡åˆ’çš„ExchangeNodeåˆ’åˆ†é€»è¾‘æ‰§è¡Œè®¡åˆ’çš„PlanFragmentï¼Œæ¯ä¸ªPlanFragmentå¯¹åº”ç”Ÿæˆä¸€ä¸ªæŸ¥è¯¢æ‰§è¡Œé˜¶æ®µï¼Œã€‚ç®€è€Œè¨€ä¹‹ï¼Œå‡¡æ˜¯ä¸Šæ¸¸èŠ‚ç‚¹å’Œä¸‹æ¸¸èŠ‚ç‚¹è¦æ±‚çš„æ•°æ®åˆ†å¸ƒä¸ä¸€è‡´ï¼Œå°±éœ€è¦åšä¸€æ¬¡æ•°æ®äº¤æ¢ï¼ˆæ— è®ºæ˜¯REPARTITIONè¿˜æ˜¯GATHERæ–¹å¼ï¼‰ï¼Œä¸¤ä¾§éœ€è¦åˆ’åˆ†åˆ°ä¸åŒçš„æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µã€‚é€»è¾‘æ‰§è¡Œè®¡åˆ’ç”±AddExchangesä¼˜åŒ–å™¨åˆ’åˆ†ä¸º2ä¸ªPlanFragmentï¼Œå¯¹åº”åˆ†å¸ƒå¼æ‰§è¡Œæ—¶çš„2ä¸ªæŸ¥è¯¢æ‰§è¡Œé˜¶æ®µã€‚ stage1ï¼šä»æ•°æ®æºè¿æ¥å™¨æ‹‰å–æ•°æ®ï¼Œstage1çš„è¾“å‡ºæ•°æ®ä¼šæ”¾åˆ°OutputBufferä¸­ï¼Œç­‰å¾…ä¸‹æ¸¸stage0çš„ä»»åŠ¡æ¥æ‹‰å–ã€‚ stage0ï¼šä»ä¸Šæ¸¸stage1æ‹‰å–æ•°æ®ï¼Œè¾“å‡ºç»“æœç»™åˆ°é›†ç¾¤åè°ƒèŠ‚ç‚¹ï¼Œstage0è¾“å‡ºçš„æ•°æ®ä¼šæ”¾åˆ°OutputBufferä¸­ï¼Œç­‰å¾…é›†ç¾¤åè°ƒèŠ‚ç‚¹æ¥å– ","date":"2025-07-20","objectID":"/posts/trino-filter-project/:1:1","tags":["Presto"],"title":"Prestoæ•°æ®è¿‡æ»¤å’ŒæŠ•å½±","uri":"/posts/trino-filter-project/"},{"categories":["Presto"],"content":"åˆ†å¸ƒå¼è°ƒåº¦ä¸æ‰§è¡Œçš„è®¾è®¡å®ç° stage1çš„ä»»åŠ¡æ‰§è¡Œæ—¶ä¼šæŒ‰ç…§PlanFragmentçš„PlanNodeå­æ ‘ï¼Œåˆ›å»ºä¸¤ä¸ªç®—å­ï¼ŒTableScanOperatorå’ŒTaskOutputOperatorã€‚TableScanOperatorè´Ÿè´£ä»å­˜å‚¨ç³»ç»Ÿä¸­æ‹‰å–æ•°æ®åè¾“å‡ºç»™TaskOutputOperatorï¼ŒTaskOutputOperatorè´Ÿè´£å°†å¾…è¾“å‡ºçš„æ•°æ®æ”¾åˆ°å½“å‰ä»»åŠ¡çš„OutputBufferä¸­ï¼Œç­‰å¾…ä¸‹æ¸¸Stage0çš„ä»»åŠ¡æ¥æ‹‰å–ã€‚ stage0çš„ä»»åŠ¡æ‰§è¡Œæ—¶ä¼šæŒ‰ç…§PlanFragmentçš„PlanNodeå­æ ‘ï¼Œåˆ›å»ºä¸¤ä¸ªç®—å­ï¼ŒExchangeOperatorå’ŒTaskOutputOperatorï¼ŒExchangeOperatorè´Ÿè´£ä»ä¸Šæ¸¸stage1çš„ä»»åŠ¡ä¸­æ‹‰å–æ•°æ®ï¼Œè€ŒTaskOutputOperatoråˆ™è´Ÿè´£å°†å¾…è¾“å‡ºçš„æ•°æ®æ”¾ç½®åˆ°å½“å‰ä»»åŠ¡çš„OutputBufferä¸­ç­‰å¾…é›†ç¾¤åè°ƒèŠ‚ç‚¹æ¥æ‹‰å–ã€‚ ","date":"2025-07-20","objectID":"/posts/trino-filter-project/:1:2","tags":["Presto"],"title":"Prestoæ•°æ®è¿‡æ»¤å’ŒæŠ•å½±","uri":"/posts/trino-filter-project/"},{"categories":["Presto"],"content":"æ•°æ®è¿‡æ»¤ä¸æ•°æ®æŠ•å½±çš„å®ç°åŸç† SELECT i_category_id, upper(i_category) AS upper_category, concat(i_category, i_color) AS cname, i_category_id * 3 AS m_category icategory_id + i_brand_id AS a_num FROM item WHERE i_item_sk IN (13631, 13283) OR i_category_id BETWEEN 2 AND 10; ","date":"2025-07-20","objectID":"/posts/trino-filter-project/:2:0","tags":["Presto"],"title":"Prestoæ•°æ®è¿‡æ»¤å’ŒæŠ•å½±","uri":"/posts/trino-filter-project/"},{"categories":["Presto"],"content":"æ‰§è¡Œè®¡åˆ’çš„ç”Ÿæˆå’Œä¼˜åŒ– åˆå§‹é€»è¾‘æ‰§è¡Œè®¡åˆ’ public class FilterNode extends PlanNode { private final PlanNode source; // æ•°æ®è¿‡æ»¤çš„è¡¨è¾¾å¼ï¼Œç±»å‹æ˜¯Expression private final Expression predicate; public class ProjectNode extends PlanNode { private final PlanNode source; // Assignmentså¯ä»¥ç†è§£ä¸ºMap\u003cSymbol, Expression\u003eï¼Œè¡¨ç¤ºçš„æ˜¯æ‰€æœ‰è¦è¾“å‡ºçš„è¡¨è¾¾å¼ private final Assignments assignments; ä¼˜åŒ–åçš„é€»è¾‘æ‰§è¡Œè®¡åˆ’ æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µåˆ’åˆ† ä¹Ÿæ˜¯ä»ExchangeNodeåˆ’åˆ†æˆä¸¤ä¸ªstageã€‚ ","date":"2025-07-20","objectID":"/posts/trino-filter-project/:2:1","tags":["Presto"],"title":"Prestoæ•°æ®è¿‡æ»¤å’ŒæŠ•å½±","uri":"/posts/trino-filter-project/"},{"categories":["Presto"],"content":"åˆ†å¸ƒå¼è°ƒåº¦ä¸æ‰§è¡Œçš„è®¾è®¡å®ç° å€¼å¾—å¼•èµ·æ³¨æ„çš„æ˜¯ï¼Œæ‰§è¡Œè®¡åˆ’æ ‘ä¸­çš„PlanNodeå¹¶ä¸æ˜¯æ€»ä¸ç®—å­ä¸€ä¸€å¯¹åº”ï¼Œä¾‹å¦‚ProjectNodeã€FilterNodeå®é™…ä¸Šä¸å­˜åœ¨å¯¹åº”çš„ç®—å­ï¼Œè¿™ä¸¤ç§PlanNodeéƒ½å¯¹åº”äº†FilterAndProjectOperatorï¼Œè€ŒTableScanNodeå¯èƒ½å¯¹åº”å¤šä¸ªç®—å­ï¼šTableScanOperatorã€ScanFilterAndProjectOperatorï¼Œä»PlanNodeæ ‘ç”ŸæˆOperatorChainçš„é˜¶æ®µï¼Œå°†æ ¹æ®ä¸åŒçš„PlanNodeç»„åˆç”Ÿæˆä¸åŒçš„ç®—å­ã€‚ è™½ç„¶æ•°æ®è¿‡æ»¤å’Œæ•°æ®æŠ•å½±åœ¨æŸ¥è¯¢çš„åŠŸèƒ½ä¸Šçœ‹æ˜¯ä¸åŒçš„ï¼Œä½†æ˜¯åœ¨OLAPå¼•æ“çš„åº•å±‚è®¾è®¡ä¸Šéƒ½å¯ä»¥å½’ä¸ºè¡¨è¾¾å¼çš„æ‰§è¡Œã€‚ æ•°æ®è¿‡æ»¤ï¼šç»™å®šæŸè¡Œæ•°æ®ä½œä¸ºè¿‡æ»¤è¡¨è¾¾å¼çš„è¾“å…¥ï¼Œæ‰§è¡Œæ­¤è¡¨è¾¾å¼å¹¶è¾“å‡ºå¸ƒå°”å‹ç»“æœï¼Œåªæœ‰ç»“æœæ˜¯trueæ—¶æ‰ä¼šè¾“å‡ºæ­¤æ•°æ®è¡Œ æ•°æ®æŠ•å½±ï¼šç»™å®šæŸè¡Œæ•°æ®ä½œä¸ºæŠ•å½±è¡¨è¾¾å¼çš„è¾“å…¥ï¼Œæ‰§è¡Œæ­¤è¡¨è¾¾å¼å¹¶è¾“å‡ºè¡¨è¾¾å¼æ‰§è¡Œç»“æœ æˆ‘ä»¬ä»è¡¨è¾¾å¼çš„è§†è§’çœ‹çœ‹ä»Prestoæ¥æ”¶åŒ…å«è¡¨è¾¾å¼çš„sqlåˆ°æŸ¥è¯¢æ‰§è¡Œå®Œæˆï¼Œéƒ½ç»å†äº†å“ªäº›è¿‡ç¨‹ï¼š SQLè§£æåŠè¯­ä¹‰åˆ†æé˜¶æ®µï¼šSQLå­—ç¬¦ä¸²è½¬æ¢ä¸ºæŠ½è±¡è¯­æ³•æ ‘ï¼Œå…¶ä¸­è¡¨è¾¾å¼ç”¨ExpressionèŠ‚ç‚¹åŠå…¶å­ç±»è¡¨ç¤º ç”Ÿæˆé€»è¾‘æ‰§è¡Œè®¡åˆ’é˜¶æ®µï¼šå°†æŠ½è±¡è¯­æ³•æ ‘è½¬æ¢ä¸ºPlanNodeæ ‘ï¼Œå…¶ä¸­FilterNodeçš„predicateå­—æ®µå±æ€§å¯¹åº”æ•°æ®è¿‡æ»¤çš„è¡¨è¾¾å¼ï¼ŒProjectNodeçš„assignmentå­—æ®µå±æ€§å¯¹åº”æ‰€æœ‰æ•°æ®æŠ•å½±å­—æ®µçš„è¡¨è¾¾å¼ ä»»åŠ¡æ‰§è¡Œæ—¶çš„ä»£ç ç”Ÿæˆé˜¶æ®µï¼šé¦–å…ˆExpressionè½¬æ¢ä¸ºRowExpressionï¼ˆRowExpressionæ‰€è¡¨ç¤ºçš„è¡¨è¾¾å¼ä¸Expressionæ‰€è¡¨ç¤ºçš„è¡¨è¾¾å¼åœ¨é€»è¾‘ä¸Šå®Œå…¨ç­‰ä»·ï¼‰ï¼Œç„¶ååˆ©ç”¨ä»£ç ç”ŸæˆæŠ€æœ¯å°†RowExpressionè½¬æ¢ä¸ºå¯ç›´æ¥æ‰§è¡Œçš„ä»£ç ã€‚æ•°æ®è¿‡æ»¤è¡¨è¾¾å¼çš„ä»£ç ç”Ÿæˆé€»è¾‘åŠå…¶å‘¨è¾¹é€»è¾‘è¢«è¿›ä¸€æ­¥å°è£…ä¸ºPageFilterè¿™ä¸ªjavaæ¥å£çš„å®ç°ç±»ï¼Œæ•°æ®æŠ•å½±è¡¨è¾¾å¼çš„ä»£ç ç”Ÿæˆé€»è¾‘åŠå…¶å‘¨è¾¹é€»è¾‘è¢«è¿›ä¸€æ­¥å°è£…ä¸ºPageProjectionè¿™ä¸ªjavaæ¥å£çš„å®ç°ç±»ã€‚æœ€ç»ˆè‡ªåŠ¨ç”Ÿæˆçš„PageFilterã€PageProjectionçš„å®ç°å°†è¢«å°è£…åˆ°ä¸€ä¸ªPageProcessorçš„å®ä¾‹ä¸­ã€‚ ä»»åŠ¡æ‰§è¡Œæ—¶çš„éå†æ•°æ®é˜¶æ®µï¼šç”±ScanFilterAndProjectOperatoræˆ–FilterAndProjectOperatoré©±åŠ¨PageProcessorä¸­çš„PageFilterã€PageProjectionéå†ä»å­˜å‚¨æ‹‰å–çš„æ‰€æœ‰æ•°æ®è¡Œï¼Œé€è¡Œå®Œæˆè¡¨è¾¾å¼çš„æ‰§è¡Œï¼Œä¸ºåç»­ç®—å­è¾“å‡ºè®¡ç®—å®Œæˆåå°è£…çš„Pageæ•°æ®ã€‚ è¡¨ç¤ºè¡¨è¾¾å¼çš„ä¸¤ç§å½¢å¼ Expressionå’ŒRowExpressionæ˜¯ä¸¤ç§è¡¨è¾¾å¼ã€‚ Expressionæ˜¯åœ¨SQLæŠ½è±¡è¯­æ³•æ ‘ã€é€»è¾‘æ‰§è¡Œè®¡åˆ’PlanNodeæ ‘ä¸­ç”¨äºè¡¨ç¤ºè¡¨è¾¾å¼çš„å½¢å¼ï¼Œå®ƒçš„åŸºç±»Expressionç»§æ‰¿è‡ªæŠ½è±¡è¯­æ³•æ ‘çš„åŸºç±»NodeèŠ‚ç‚¹ï¼ŒExpresssionæœ‰å‡ åä¸ªå­ç±»æ¥è¡¨ç¤ºæ›´å…·ä½“çš„æŸä¸ªè¡¨è¾¾å¼é€»è¾‘ï¼Œæ¯”å¦‚ï¼š SymbolReferenceè¡¨ç¤ºå¯¹æŸä¸ªå­—æ®µçš„å¼•ç”¨ FunctionCallè¡¨ç¤ºå¯¹å‡½æ•°çš„è°ƒç”¨ ArithmeticBinaryExpressionè¡¨ç¤ºåŸºæœ¬çš„äºŒå…ƒç®—æ•°è¿ç®—ï¼Œæ¯”å¦‚ x + y, x * y Castè¡¨ç¤ºå°†æŸä¸ªå­—é¢æ„æ€çš„å­—ç¬¦ä¸²è½¬æ¢ä¸ºå…·ä½“çš„ç±»å‹ï¼Œæ¯”å¦‚CAST(3 AS BIGINT) åœ¨å°†è¦åˆ©ç”¨ä»£ç ç”ŸæˆæŠ€æœ¯ç”Ÿæˆç‰¹å®šçš„PageFilterã€PageProjectionå®ç°ç±»æ—¶ï¼ŒRowExpressionå°†å‡ åç§Expressionçš„è¡¨ç¤ºç®€åŒ–ä¸º6ç§ç›¸åŒå«ä¹‰çš„è¡¨è¾¾ï¼Œå®ƒä»¬åˆ†åˆ«å¦‚ä¸‹ï¼š ConstantExpression è¡¨ç¤ºæŸä¸ªå­—é¢å¸¸é‡ï¼Œä¾‹å¦‚ i_category_id * 3ä¸­çš„3 InputReferenceExpression è¡¨ç¤ºå¯¹æŸä¸ªå­—æ®µçš„å¼•ç”¨ CallExpression è¡¨ç¤ºå‡½æ•°æˆ–è€…è¿ç®—ç¬¦è°ƒç”¨ SpecialForm å¯ç”¨äºè¡¨ç¤ºå¤šç§è´Ÿè´£çš„é€»è¾‘ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºINã€BETWEENã€IFã€ANDã€OR LambdaDefinitionExpression è¡¨ç¤ºSQLä¸­Lambdaå‡½æ•°å®šä¹‰ VariableReferenceExpression è¡¨ç¤ºå¯¹å˜é‡çš„å¼•ç”¨ ä¹‹æ‰€ä»¥è¦å…ˆç”¨ExpressionåŠå…¶å­ç±»ä½œä¸ºè¡¨è¾¾å¼çš„ç»“æ„ï¼Œå…¶ååœ¨ç®—å­æ‰§è¡Œæ—¶åˆå°†è¡¨è¾¾å¼è½¬æ¢ä¸ºRowExpressionç»“æ„ï¼Œæ˜¯å› ä¸ºåœ¨ç®—å­æ‰§è¡Œé˜¶æ®µè‡ªåŠ¨ç”Ÿæˆæ•°æ®å¤„ç†ä»£ç æ—¶ï¼ŒRowExpressionç§ç±»æ›´å°‘ï¼Œå¯¹åº”çš„è¡¨è¾¾å¼æ ‘å½¢ç»“æ„æ›´å®¹æ˜“éå†ã€‚ PageProcessorã€PageFilterã€PageProjection public class PageProcessor { public static final int MAX_BATCH_SIZE = 8 * 1024; static final int MAX_PAGE_SIZE_IN_BYTES = 4 * 1024 * 1024; static final int MIN_PAGE_SIZE_IN_BYTES = 1024 * 1024; private final ExpressionProfiler expressionProfiler; private final DictionarySourceIdFunction dictionarySourceIdFunction = new DictionarySourceIdFunction(); private final Optional\u003cPageFilter\u003e filter; private final List\u003cPageProjection\u003e projections; private int projectBatchSize; ç»™å®šæŸä¸ªpageçš„æ•°æ®ï¼ŒPageProcessorå¯ä»¥åˆ©ç”¨å®ƒæŒæœ‰çš„PageFilterã€PageProjectionå®ä¾‹å®Œæˆè¿™ä¸ªPageæ•°æ®çš„è¿‡æ»¤å’ŒæŠ•å½±æ“ä½œã€‚ public interface PageFilter { boolean isDeterministic(); // å‚ä¸è¿‡æ»¤çš„è¾“å…¥Pageçš„æ‰€æœ‰åˆ—çš„channel ID InputChannels getInputChannels(); SelectedPositions filter(ConnectorSession session, Page page); ä¸ºäº†é¿å…æ•°æ®è¿‡æ»¤å®Œæˆåç«‹å³åˆ›å»ºæ–°çš„Pageå†å»åšæ•°æ®æŠ•å½±è®¡ç®—ï¼ˆè¿™æ ·å¯èƒ½å¯¼è‡´åœ¨å†…å­˜ä¸­ç‰©åŒ–ä¸­é—´è¿‡ç¨‹çš„æ•°æ®å¼€é”€æ¯”è¾ƒå¤§ï¼‰ï¼ŒPrestoåˆ©ç”¨SelectedPositionsè¿™æ ·çš„æ•°æ®ç»“æ„è®°å½•æ•°æ®è¿‡æ»¤å®Œæˆåè¢«ç­›é€‰å‡ºæ¥çš„è¡Œçš„IDï¼ˆrow idï¼‰åˆ—è¡¨ï¼ŒPageProjectionçš„æ•°æ®æŠ•å½±æ“ä½œåªå¯¹SelectedPositionsä¸­è®°å½•çš„æ•°æ®è¿‡æ»¤ä¿ç•™ä¸‹æ¥çš„è¡ŒåšæŠ•å½±æ“ä½œï¼Œå¹¶æœ€ç»ˆåˆ›å»ºPageç»“æœæ•°æ®ã€‚ public interface PageProjection { Type getType(); boolean isDeterministic(); // ç»™å®šæŸä¸ªPageï¼ŒInputChannelsæŒ‡å®šçš„blocksç»™åˆ°PageProjectionï¼Œè®¡ç®—å¹¶è¾“å‡ºProjectionåçš„ä¸€ä¸ªBlock InputChannels getInputChannels(); Work\u003cBlock\u003e project(ConnectorSession session, DriverYieldSignal yieldSignal, Page page, SelectedPositions selectedPositions); } åˆ©ç”¨ä»£ç ç”ŸæˆæŠ€æœ¯ç”Ÿæˆè¡¨è¾¾å¼æ‰§è¡Œä»£ç  é¦–å…ˆéœ€è¦ç”Ÿæˆè¡¨è¾¾å¼æ‰§è¡Œä»£ç ï¼Œæ•°æ®è¿‡æ»¤ä»£ç çš„ç”Ÿæˆæµç¨‹æ˜¯ä»PageFunctionCompiler::compileFilteråˆ°RowExpressionCompiler::compileï¼Œæ•°æ®æŠ•å½±ä»£ç çš„ç”Ÿæˆæµç¨‹æ˜¯ä»PageFunctionCompiler::compileProjectionåˆ°RowExpressionCompiler::compileï¼Œå®ƒä»¬æœ€åéƒ½æ˜¯åˆ©ç”¨RowExpressionCompilerä¸­çš„RowExpressionVisitorä»¥æ·±åº¦ä¼˜å…ˆçš„æ–¹å¼éå†ç”¨RowExpressionæ ‘å½¢ç»“æ„è¡¨ç¤ºçš„è¡¨è¾¾å¼æ‰§è¡Œé€»è¾‘ï¼Œä»¥ç”Ÿæˆè¡¨è¾¾å¼çš„æ‰§è¡Œä»£ç ã€‚å¯ä»¥ç†è§£ä¸ºç”Ÿæˆäº†ä¸€ä¸ªå¯è°ƒç”¨çš„å‡½æ•°ã€‚å®é™…ä¸Šç»è¿‡airlift bytecodeä»£ç åº“çš„åŒ…è£…ï¼Œä»£ç ç”ŸæˆæŠ€æœ¯å·²ç»è¢«å¤§å¤§ç®€åŒ–äº†ï¼Œå¯è¯»æ€§ä¹Ÿå¾ˆé«˜ã€‚å…¶æ¬¡ï¼ŒPageFilterã€PageProjectionåœ¨éå†å¾…è®¡ç®—Pageæ•°æ®çš„æ¯ä¸€è¡Œæ—¶ï¼Œä¼šè°ƒç”¨ä¸Šè¿°æµç¨‹ç”Ÿæˆçš„è¡¨è¾¾å¼æ‰§è¡Œä»£ç ï¼Œå¹¶ä¼ å…¥Pageä¸­éœ€è¦å‚ä¸è¡¨è¾¾å¼è®¡ç®—çš„æ‰€æœ‰åˆ—ï¼ˆå¯¹åº”ä¸ºBlockï¼‰ã€‚ æ•´ä½“çš„æ€æƒ³æ˜¯å°†æ ‘å½¢ç»“æ„çš„RowExpressionç»“æ„æ‹å¹³ä¸ºå±‚å±‚åµŒå¥—çš„æ‰§è¡Œé€»è¾‘ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ä¼šç›´æ¥å°†è¡¨è¾¾å¼çš„æ‰§è¡Œé€»è¾‘è‡ªåŠ¨ç”Ÿæˆåœ¨ä¸€ä¸ªå‡½æ•°çš„æ–¹æ³•ä½“ä¸­ï¼Œä»¥å‡å°‘å‡½æ•°è°ƒç”¨çš„å¼€é”€ã€‚ è¡¨è¾¾å¼è®¡ç®—çš„ä¼˜åŒ– å¸¸é‡æŠ˜å ï¼ˆConstant Foldï¼‰ï¼šä¾‹å¦‚SELECT c1 + 2 + 3 FROM è¿™æ ·çš„æŸ¥è¯¢ï¼Œä¸ºäº†é¿å…åœ¨æŸ¥è¯¢æ‰§è¡Œæ—¶è¿­ä»£æ¯è¡Œæ•°æ®éƒ½éœ€è¦åš2 + 3è¿™æ ·çš„å¸¸é‡è®¡ç®—ï¼Œæ‰€ä»¥åœ¨æ‰§è¡Œè®¡åˆ’ç”Ÿæˆé˜¶æ®µç›´æ¥å°†å…¶ä¼˜åŒ–ä¸ºSELECT c1 + 5 FROM ç›¸åŒå­è¡¨è¾¾å¼ä¼˜åŒ–ï¼ˆCommon Sub Expressionï¼‰ï¼šä¾‹å¦‚SELECT strpos(upper(a)), 'FOO') \u003e 0 OR strpos(upper(a), \"BAR\") \u003e 0 FROMè¿™æ ·çš„æŸ¥è¯¢ï¼Œå¯è¯†åˆ«å‡ºupper(a)è¿™ä¸ªè¡¨è¾¾å¼è¢«å¤šæ¬¡ç”¨åˆ°ï¼Œä¸éœ€è¦é‡å¤è®¡ç®—ï¼Œå¯ç¼“å­˜å™¨ç»“æœä»¥å®ç°å¤šæ¬¡å¤ç”¨ã€‚ å­è¡¨è¾¾å¼è®¡ç®—åŠ¨æ€é‡æ’åºï¼šä¾‹å¦‚ï¼Œåœ¨åšæ•°æ®è¿‡æ»¤æ—¶ï¼Œå¦‚æœå‘ç°å…ˆæ‰§è¡ŒæŸäº›å­è¡¨è¾¾å¼èƒ½æ˜¾è‘—å‡å°‘è¿‡æ»¤åçš„æ•°æ®é‡ï¼Œé‚£ä¹ˆå¯åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­åŠ¨","date":"2025-07-20","objectID":"/posts/trino-filter-project/:2:2","tags":["Presto"],"title":"Prestoæ•°æ®è¿‡æ»¤å’ŒæŠ•å½±","uri":"/posts/trino-filter-project/"},{"categories":["Presto"],"content":"æ¥æ”¶SQLæŸ¥è¯¢è¯·æ±‚ å®¢æˆ·ç«¯é¦–å…ˆå‘é€HTTPè¯·æ±‚v1/statementå°†å¾…æŸ¥è¯¢çš„sqlå‘é€ç»™é›†ç¾¤åè°ƒèŠ‚ç‚¹ï¼Œ äº¤ç”±io.prestosql.dispatcher.QueuedStatementResource#postStatementå¤„ç†ï¼Œç”ŸæˆQueryIdï¼Œç„¶åå°†æŸ¥è¯¢åŠ å…¥å¾…æ‰§è¡Œé˜Ÿåˆ—ä¸­ï¼Œè¿”å›å“åº”æŠ¥æ–‡ç»™å®¢æˆ·ç«¯ï¼Œå“åº”æŠ¥æ–‡ä¸­åŒ…å«äº†ä¸‹ä¸€æ¬¡å®¢æˆ·ç«¯åº”è¯¥è®¿é—®çš„URIï¼Œå³/v1/statement/queuedã€‚ public Response postStatement( String statement, @Context HttpServletRequest servletRequest, @Context HttpHeaders httpHeaders, @Context UriInfo uriInfo) { if (isNullOrEmpty(statement)) { throw badRequest(BAD_REQUEST, \"SQL statement is empty\"); } String remoteAddress = servletRequest.getRemoteAddr(); Optional\u003cIdentity\u003e identity = Optional.ofNullable((Identity) servletRequest.getAttribute(AUTHENTICATED_IDENTITY)); MultivaluedMap\u003cString, String\u003e headers = httpHeaders.getRequestHeaders(); // åˆ›å»ºè¯·æ±‚çš„session context SessionContext sessionContext = new HttpRequestSessionContext(headers, remoteAddress, identity, groupProvider); // åˆ›å»ºQueryå¯¹è±¡ï¼Œæ„é€ QueryId Query query = new Query(statement, sessionContext, dispatchManager); queries.put(query.getQueryId(), query); // let authentication filter know that identity lifecycle has been handed off servletRequest.setAttribute(AUTHENTICATED_IDENTITY, null); return createQueryResultsResponse(query.getQueryResults(query.getLastToken(), uriInfo), compressionEnabled); } äº§ç”Ÿçš„QueryIdçš„æ ¼å¼ä¸ºYYYYMMdd_HHmmss_index_coordIdï¼Œåˆ†åˆ«è¡¨ç¤ºå½“å‰æ—¶é—´æˆ³ã€æŸ¥è¯¢è®¡æ•°å’Œåè°ƒèŠ‚ç‚¹idã€‚ // io.prestosql.execution.QueryIdGenerator#createNextQueryId public synchronized QueryId createNextQueryId() { // only generate 100,000 ids per day if (counter \u003e 99_999) { // wait for the second to rollover while (MILLISECONDS.toSeconds(nowInMillis()) == lastTimeInSeconds) { Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS); } counter = 0; } // if it has been a second since the last id was generated, generate a new timestamp long now = nowInMillis(); if (MILLISECONDS.toSeconds(now) != lastTimeInSeconds) { // generate new timestamp lastTimeInSeconds = MILLISECONDS.toSeconds(now); lastTimestamp = TIMESTAMP_FORMAT.print(now); // if the day has rolled over, restart the counter if (MILLISECONDS.toDays(now) != lastTimeInDays) { lastTimeInDays = MILLISECONDS.toDays(now); counter = 0; } } return new QueryId(format(\"%s_%05d_%s\", lastTimestamp, counter++, coordinatorId)); } ä¹‹åå®¢æˆ·ç«¯ä¼šå‘èµ·ç¬¬äºŒæ¬¡HTTPè¯·æ±‚queued/{queryId}/{slug}/{token}ï¼Œslugå’Œtokenåªæ˜¯Prestoç”¨æ¥éªŒè¯æ¥æ”¶åˆ°çš„è¯·æ±‚æ˜¯å¦æ˜¯åˆæ³•çš„ï¼Œslugåœ¨ç”Ÿæˆåå°±ä¸ä¼šå˜ï¼Œtokenåœ¨ç”ŸæˆnextURIçš„é€»è¾‘ä¸­æ¯æ¬¡ä¼š+1ï¼ŒPrestoæ­¤æ—¶æ‰ä¼šçœŸæ­£å°†æŸ¥è¯¢æäº¤åˆ°DispatchManager::createQueryInternalæ¥æ‰§è¡Œã€‚ @ResourceSecurity(PUBLIC) @GET @Path(\"queued/{queryId}/{slug}/{token}\") @Produces(APPLICATION_JSON) public void getStatus( @PathParam(\"queryId\") QueryId queryId, @PathParam(\"slug\") String slug, @PathParam(\"token\") long token, @QueryParam(\"maxWait\") Duration maxWait, @Context UriInfo uriInfo, @Suspended AsyncResponse asyncResponse) { Query query = getQuery(queryId, slug, token); // wait for query to be dispatched, up to the wait timeout ListenableFuture\u003c?\u003e futureStateChange = addTimeout( query.waitForDispatched(), () -\u003e null, WAIT_ORDERING.min(MAX_WAIT_TIME, maxWait), timeoutExecutor); // when state changes, fetch the next result ListenableFuture\u003cQueryResults\u003e queryResultsFuture = Futures.transform( futureStateChange, ignored -\u003e query.getQueryResults(token, uriInfo), responseExecutor); // transform to Response ListenableFuture\u003cResponse\u003e response = Futures.transform( queryResultsFuture, queryResults -\u003e createQueryResultsResponse(queryResults, compressionEnabled), directExecutor()); bindAsyncResponse(asyncResponse, response, responseExecutor); } private \u003cC\u003e void createQueryInternal(QueryId queryId, Slug slug, SessionContext sessionContext, String query, ResourceGroupManager\u003cC\u003e resourceGroupManager) { Session session = null; PreparedQuery preparedQuery = null; try { // æ£€æŸ¥æŸ¥è¯¢é•¿åº¦æ˜¯å¦è¶…è¿‡é™åˆ¶ï¼Œè¶…è¿‡æŠ¥é”™ if (query.length() \u003e maxQueryLength) { int queryLength = query.length(); query = query.substring(0, maxQueryLength); throw new PrestoException(QUERY_TEXT_TOO_LARGE, format(\"Query text length (%s) exceeds the maximum length (%s)\", queryLength, maxQueryLength)); } // åˆ›å»ºæŸ¥è¯¢ä¼šè¯ session = sessionSupplier.createSession(queryId, sessionContext); // æ£€æŸ¥æŸ¥è¯¢æ‰§è¡Œæƒé™ acce","date":"2025-07-09","objectID":"/posts/presto_query_submission/:0:1","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢æäº¤æµç¨‹","uri":"/posts/presto_query_submission/"},{"categories":["Presto"],"content":"è¯æ³•ä¸è¯­æ³•åˆ†æå¹¶ç”ŸæˆæŠ½è±¡è¯­æ³•æ ‘ SqlParserä½¿ç”¨Antlr4ä½œä¸ºè§£æå·¥å…·ï¼Œé€šè¿‡è¯æ³•å’Œè¯­æ³•è§£æé€šè¿‡SQLå­—ç¬¦ä¸²ç”ŸæˆæŠ½è±¡è¯­æ³•æ ‘ã€‚æŠ½è±¡è¯­æ³•æ ‘æ˜¯ç”¨ä¸€ç§æ ‘å½¢ç»“æ„è¡¨ç¤ºSQLæƒ³è¦è¡¨è¿°çš„è¯­ä¹‰ï¼Œå°†ä¸€æ®µSQLå­—ç¬¦ä¸²ç»“æ„åŒ–ï¼Œä»¥æ”¯æŒSQLæ‰§è¡Œå¼•æ“æ ¹æ®æŠ½è±¡è¯­æ³•æ ‘ç”ŸæˆSQLæ‰§è¡Œè®¡åˆ’ã€‚åœ¨Prestoä¸­ï¼ŒNodeè¡¨ç¤ºæ ‘çš„èŠ‚ç‚¹çš„æŠ½è±¡ï¼Œæ ¹æ®è¯­ä¹‰ä¸åŒï¼ŒSQLæŠ½è±¡è¯­æ³•æ ‘ä¸­æœ‰å¤šç§ä¸åŒç±»å‹çš„èŠ‚ç‚¹ï¼Œéƒ½ç»§æ‰¿äº†NodeèŠ‚ç‚¹ã€‚ æŠ½è±¡è¯­æ³•æ ‘åœ¨æˆ‘çœ‹æ¥åªæ˜¯å°†å­—ç¬¦ä¸²è½¬æˆç»“æ„æ›´åŠ ä¸¥è°¨çš„æ ‘çŠ¶ç»“æ„ï¼Œå’Œsqlçš„æ–‡æœ¬è¡¨è¿°ç±»ä¼¼ï¼Œé™¤äº†å°†æ–‡æœ¬è½¬æˆæ ‘çŠ¶ç»“æ„å¤–ï¼Œæ²¡æœ‰åšé¢å¤–çš„æ“ä½œï¼Œæ¯”å¦‚FunctionCallå¹¶æ²¡æœ‰çœŸçš„ç”Ÿæˆè°ƒç”¨çš„å‡½æ•°ï¼Œè€Œåªæ˜¯æè¿°ï¼Œè¿™ä¸€æ­¥æœ€ç»ˆä¼šç”Ÿæˆä¸€ä¸ªç”¨Statementè¡¨ç¤ºçš„æ ¹çš„æŠ½è±¡è¯­æ³•æ ‘ã€‚ public class FunctionCall extends Expression { // å‡½æ•°å private final QualifiedName name; private final Optional\u003cWindow\u003e window; private final Optional\u003cExpression\u003e filter; private final Optional\u003cOrderBy\u003e orderBy; private final boolean distinct; private final Optional\u003cNullTreatment\u003e nullTreatment; // å‡½æ•°å‚æ•° private final List\u003cExpression\u003e arguments; ","date":"2025-07-09","objectID":"/posts/presto_query_submission/:0:2","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢æäº¤æµç¨‹","uri":"/posts/presto_query_submission/"},{"categories":["Presto"],"content":"åˆ›å»ºå¹¶æäº¤QueryExecution dispatchQueryFactory.createDispatchQueryæ–¹æ³•åœ¨æ‰§è¡Œä¸­ï¼Œä¼šæ ¹æ®Statementçš„ç±»å‹ç”ŸæˆQueryExecutionï¼Œå¯¹äºDMLæ“ä½œ(Data Manipulatioin language)ç”ŸæˆSqlQueryExecutionï¼Œå¯¹äºDDL (Data Definition language)ç”ŸæˆDataDefinitionExecutionï¼Œç„¶åè¿›ä¸€æ­¥åŒ…è£…æˆLocalDispatchQueryï¼Œæäº¤åˆ°resourceGroupManagerç­‰å¾…è¿è¡Œã€‚ resourceGroupåº”è¯¥æ˜¯Prestoç”¨æ¥æ”¯æŒèµ„æºéš”ç¦»çš„æ–¹æ³•ï¼Œå¦‚æœæŸä¸ªç»„å†…cpuæˆ–è€…å†…å­˜èµ„æºä¸è¶³ï¼Œåˆ™ä¼šç­‰å¾…èµ„æºå¯ç”¨æˆ–è€…è¶…æ—¶å¤±è´¥ã€‚io.prestosql.execution.resourcegroups.InternalResourceGroup#canRunMoreä¼šæ ¹æ®èµ„æºç»„å¯¹cpuå’Œå†…å­˜çš„é™åˆ¶å†³å®šå½“å‰æŸ¥è¯¢æ˜¯å¦å¯ä»¥æ‰§è¡Œï¼Œio.prestosql.dispatcher.LocalDispatchQuery#waitForMinimumWorkersç­‰å¾…è·å–éœ€è¦çš„workeræ•°é‡ã€‚ ç»è¿‡ä¸€ç³»åˆ—çš„å¼‚æ­¥æ“ä½œï¼ˆå„ç§Futureï¼‰ï¼Œä»£ç æ‰§è¡Œåˆ°SqlQueryExecution.startæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸²èµ·æ¥æ‰§è¡Œè®¡åˆ’çš„ç”Ÿæˆå’Œè°ƒåº¦ã€‚ ","date":"2025-07-09","objectID":"/posts/presto_query_submission/:0:3","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢æäº¤æµç¨‹","uri":"/posts/presto_query_submission/"},{"categories":["Presto"],"content":"æ‰§è¡Œè®¡åˆ’çš„æ‰§è¡Œ ","date":"2025-07-09","objectID":"/posts/presto_query_submission/:1:0","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢æäº¤æµç¨‹","uri":"/posts/presto_query_submission/"},{"categories":["Presto"],"content":"ç®—å­Operator public interface Operator extends AutoCloseable { ListenableFuture\u003c?\u003e NOT_BLOCKED = Futures.immediateFuture(null); OperatorContext getOperatorContext(); /** * Returns a future that will be completed when the operator becomes * unblocked. If the operator is not blocked, this method should return * {@code NOT_BLOCKED}. */ default ListenableFuture\u003c?\u003e isBlocked() { return NOT_BLOCKED; } /** * Returns true if and only if this operator can accept an input page. */ boolean needsInput(); /** * Adds an input page to the operator. This method will only be called if * {@code needsInput()} returns true. */ void addInput(Page page); /** * Gets an output page from the operator. If no output data is currently * available, return null. */ Page getOutput(); /** * After calling this method operator should revoke all reserved revocable memory. * As soon as memory is revoked returned future should be marked as done. * \u003cp\u003e * Spawned threads cannot modify OperatorContext because it's not thread safe. * For this purpose implement {@link #finishMemoryRevoke()} * \u003cp\u003e * Since memory revoking signal is delivered asynchronously to the Operator, implementation * must gracefully handle the case when there no longer is any revocable memory allocated. * \u003cp\u003e * After this method is called on Operator the Driver is disallowed to call any * processing methods on it (isBlocked/needsInput/addInput/getOutput) until * {@link #finishMemoryRevoke()} is called. */ default ListenableFuture\u003c?\u003e startMemoryRevoke() { return NOT_BLOCKED; } /** * Clean up and release resources after completed memory revoking. Called by driver * once future returned by startMemoryRevoke is completed. */ default void finishMemoryRevoke() { } /** * Notifies the operator that no more pages will be added and the * operator should finish processing and flush results. This method * will not be called if the Task is already failed or canceled. */ void finish(); /** * Is this operator completely finished processing and no more * output pages will be produced. */ boolean isFinished(); /** * This method will always be called before releasing the Operator reference. */ @Override default void close() throws Exception { } } TableScanOperatorï¼šç”¨äºè¯»å–æ•°æ®æºè¿æ¥å™¨çš„æ•°æ® AggregationOperatorï¼šç”¨äºèšåˆè®¡ç®—ï¼Œå†…éƒ¨å¯ä»¥æŒ‡å®šä¸€ä¸ªæˆ–å¤šä¸ªèšåˆå‡½æ•°ï¼Œå¦‚sumï¼Œavgç­‰ TaskOutputOperatorï¼šæŸ¥è¯¢æ‰§è¡Œé˜¶æ®µä¹‹é—´çš„ä»»åŠ¡åšæ•°æ®äº¤æ¢ç”¨çš„ï¼Œä¸Šæ¸¸æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µçš„ä»»åŠ¡é€šè¿‡æ­¤ç®—å­å°†è®¡ç®—ç»“æœè¾“å‡ºåˆ°å½“å‰ä»»åŠ¡æ‰€åœ¨èŠ‚ç‚¹çš„OutputBuffer ExchangeOperator: ç”¨äºæŸ¥è¯¢æ‰§è¡Œé˜¶æ®µä¹‹é—´è¿›è¡Œä»»åŠ¡çš„æ•°æ®äº¤æ¢ï¼Œä¸‹æ¸¸æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µçš„ExchangeClientä»ä¸Šæ¸¸OutputBufferè·å–æ•°æ® ","date":"2025-07-09","objectID":"/posts/presto_query_submission/:1:1","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢æäº¤æµç¨‹","uri":"/posts/presto_query_submission/"},{"categories":["Presto"],"content":"Pageã€Blockã€Slice Sliceè¡¨ç¤ºä¸€ä¸ªå€¼ï¼ˆSingle Valueï¼‰ï¼ŒBlockè¡¨ç¤ºä¸€åˆ—ï¼Œç±»ä¼¼äºParquetä¸­çš„åˆ—ï¼ˆColumnï¼‰ï¼ŒPageè¡¨ç¤ºå¤šè¡Œè®°å½•ã€‚ä½†æ˜¯ä»–ä»¬æ˜¯ä»¥å¤šåˆ—å¤šä¸ªblockçš„æ–¹å¼ç»„ç»‡åœ¨ä¸€èµ·ï¼Œç±»ä¼¼Parquetä¸­çš„Row Groupï¼Œè¿™ç§ç»„ç»‡æ–¹å¼ï¼Œä¸åŒè¡Œç›¸åŒåˆ—çš„å­—æ®µéƒ½é¡ºåºç›¸ä¸´ï¼Œå¯¹åº”æ•°æ®æ›´å®¹æ˜“æŒ‰åˆ—è¯»å–ä¸è®¡ç®—ã€‚ ","date":"2025-07-09","objectID":"/posts/presto_query_submission/:1:2","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢æäº¤æµç¨‹","uri":"/posts/presto_query_submission/"},{"categories":["Presto"],"content":"TaskExecutor/Driver TaskExecutoræ˜¯Prestoä»»åŠ¡çš„æ‰§è¡Œæ± ï¼Œå®ƒä»¥å•ä¾‹çš„æ–¹å¼åœ¨PrestoæŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹ä¸Šå¯åŠ¨ï¼Œå†…éƒ¨ç»´æŠ¤äº†ä¸€ä¸ªJavaçº¿ç¨‹æ± -ExecutorServiceç”¨äºæäº¤è¿è¡Œä»»åŠ¡ï¼Œæ‰€ä»¥æ— è®ºæŸä¸ªPrestoæŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹ä¸Šæœ‰å¤šå°‘ä¸ªä»»åŠ¡åœ¨è¿è¡Œï¼ŒTaskExecutoréƒ½åªæœ‰ä¸€ä¸ªã€‚ Driveræ˜¯ä»»åŠ¡çš„ç®—å­é“¾æ‰§è¡Œçš„é©±åŠ¨å™¨ï¼Œç”±å®ƒæ¥æ¨åŠ¨æ•°æ®ç©¿æ¢­äºç®—å­ã€‚åœ¨å…·ä½“çš„å®ç°ä¸Šï¼Œç«å±±æ¨¡å‹æ˜¯è‡ªé¡¶è€Œä¸‹çš„æ‹‰å–ï¼ˆPullï¼‰æ•°æ®ï¼ŒPresto Driverå’Œç«å±±æ¨¡å‹ä¸ä¸€æ ·ï¼Œå®ƒæ˜¯è‡ªåº•è€Œä¸Šçš„æ¨é€ï¼ˆPushï¼‰æ•°æ®ã€‚ private ListenableFuture\u003c?\u003e processInternal(OperationTimer operationTimer) for (int i = 0; i \u003c activeOperators.size() - 1 \u0026\u0026 !driverContext.isDone(); i++) { Operator current = activeOperators.get(i); Operator next = activeOperators.get(i + 1); // skip blocked operator if (getBlockedFuture(current).isPresent()) { continue; } // å¦‚æœå½“å‰ç®—å­æœªå®Œæˆä»»åŠ¡ï¼Œä¸”ä¸‹ä¸€ä¸ªç®—å­æœªè¢«é˜»å¡ä¸”éœ€è¦è¾“å…¥ if (!current.isFinished() \u0026\u0026 getBlockedFuture(next).isEmpty() \u0026\u0026 next.needsInput()) { // ä»å½“å‰ç®—å­ä¸­è·å–è®¡ç®—è¾“å‡ºçš„Page Page page = current.getOutput(); current.getOperatorContext().recordGetOutput(operationTimer, page); // å¦‚æœèƒ½æ‹¿åˆ°ä¸€ä¸ªPageï¼Œå°†å…¶ç»™åˆ°ä¸‹ä¸€ä¸ªç®—å­ if (page != null \u0026\u0026 page.getPositionCount() != 0) { next.addInput(page); next.getOperatorContext().recordAddInput(operationTimer, page); movedPage = true; } if (current instanceof SourceOperator) { movedPage = true; } } // if current operator is finished... if (current.isFinished()) { // let next operator know there will be no more data next.finish(); next.getOperatorContext().recordFinish(operationTimer); } } ","date":"2025-07-09","objectID":"/posts/presto_query_submission/:1:3","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢æäº¤æµç¨‹","uri":"/posts/presto_query_submission/"},{"categories":["Presto"],"content":"åˆ†æ‰¹è¿”å›æŸ¥è¯¢è®¡ç®—ç»“æœç»™SQLå®¢æˆ·ç«¯ Prestoé‡‡ç”¨çš„æ˜¯æµæ°´çº¿çš„å¤„ç†æ–¹å¼ï¼Œæ•°æ®åœ¨Prestoçš„è®¡ç®—è¿‡ç¨‹ä¸­æ˜¯æŒç»­æµåŠ¨çš„ï¼Œæ˜¯åˆ†æ‰¹æ‰§è¡Œå’Œè¿”å›çš„ï¼Œåœ¨æŸä¸ªä»»åŠ¡å†…ä¸éœ€è¦å‰é¢çš„ç®—å­è®¡ç®—å®Œæ‰€æœ‰æ•°æ®å†è¾“å‡ºç»“æœç»™åé¢çš„ç®—å­ï¼Œåœ¨æŸä¸ªæŸ¥è¯¢å†…ä¹Ÿä¸éœ€è¦å‰é¢çš„æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µçš„æ‰€æœ‰ä»»åŠ¡éƒ½è®¡ç®—å®Œæ‰€æœ‰æ•°æ®å†è¾“å‡ºç»“æœç»™åé¢çš„æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µã€‚å› æ­¤ï¼Œä»SQLå®¢æˆ·ç«¯æ¥çœ‹ï¼ŒPrestoä¹Ÿæ”¯æŒåˆ†æ‰¹è¿”å›æŸ¥è¯¢è®¡ç®—ç»“æœç»™SQLå®¢æˆ·ç«¯ã€‚Prestoè¿™ç§æµæ°´çº¿çš„æœºåˆ¶ï¼Œä¸Flinkéå¸¸ç±»ä¼¼ï¼Œä»–ä»¬éƒ½ä¸åƒSparkæ‰¹å¼å¤„ç†é‚£æ ·ï¼Œéœ€è¦å‰é¢çš„æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µæ‰§è¡Œå®Œå†æ‰§è¡Œåç±³å¨œçš„æŸ¥è¯¢æ‰§è¡Œé˜¶æ®µã€‚ å¦‚æœä½ ç”¨è¿‡MYSQLè¿™ç±»å…³ç³»å‹æ•°æ®åº“ï¼Œä¸€å®šå¬è¯´è¿‡æ¸¸æ ‡ï¼ˆcursorï¼‰ï¼Œä¹Ÿç”¨è¿‡å„ç§ç¼–ç¨‹è¯­è¨€çš„JDBCé©±åŠ¨çš„getNextï¼Œé€šè¿‡è¿™æ ·çš„æ–¹å¼æ¥æ¯æ¬¡è·å–SQLæ‰§è¡Œç»“æœçš„ä¸€éƒ¨åˆ†æ•°æ®ã€‚Prestoä¹Ÿæä¾›äº†ç±»ä¼¼çš„æœºåˆ¶ï¼Œå®ƒä¼šç»™SQLå®¢æˆ·ç«¯ä¸€ä¸ªQueryResultï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªnextUriã€‚å¯¹äºæŸä¸ªæŸ¥è¯¢ï¼Œæ¯æ¬¡è¯·æ±‚QueryResultï¼Œéƒ½ä¼šå¾—åˆ°ä¸€ä¸ªæ–°çš„nextUriï¼Œå®ƒçš„ä½œç”¨ç±»ä¼¼äºæ¸¸æ ‡ã€‚ StatementClientV1::advance ExecutingStatementResource::getQueryResults Query::getNextResult ","date":"2025-07-09","objectID":"/posts/presto_query_submission/:1:4","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢æäº¤æµç¨‹","uri":"/posts/presto_query_submission/"},{"categories":["Presto"],"content":"æ‰§è¡Œè®¡åˆ’ç”Ÿæˆçš„è®¾è®¡å®ç° ","date":"2025-07-09","objectID":"/posts/presto_query_submission/:2:0","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢æäº¤æµç¨‹","uri":"/posts/presto_query_submission/"},{"categories":["Presto"],"content":"ä»SQLåˆ°æŠ½è±¡è¯­æ³•æ ‘ æ‰€æœ‰æä¾›SQLæŸ¥è¯¢çš„æ•°æ®åº“ã€æŸ¥è¯¢å¼•æ“éƒ½éœ€è¦è¯­æ³•åˆ†æèƒ½åŠ›ï¼Œå®ƒæŠŠéç»“æ„åŒ–çš„SQLå­—ç¬¦ä¸²è½¬æ¢æˆä¸€ç³»åˆ—çš„è¯æ³•ç¬¦å·ï¼ˆTokenï¼‰ï¼Œç„¶åé€šè¿‡ç‰¹å®šè§„åˆ™è§£æè¯æ³•ç¬¦å·ï¼Œè¿”å›ä¸€ä¸ªè¯­æ³•åˆ†ææ ‘ï¼ˆParseTreeï¼‰ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªç»“æ„åŒ–çš„æ ‘çŠ¶ç»“æ„ï¼Œè¿™å°±æ˜¯æŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰ã€‚ è®¿é—®è€…æ¨¡å¼çš„ä¼˜ç‚¹ï¼š é¿å…ä¿®æ”¹å…ƒç´ ç±»ï¼Œéµå¾ªå¼€é—­åŸåˆ™ åˆ†ç¦»æ“ä½œé€»è¾‘ æ”¯æŒæ–°çš„æ“ä½œæ‹“å±• å¤šæ€åˆ†æ´¾ ä¸åŒçš„Nodeç±»é€šè¿‡acceptæ–¹æ³•é€‚é…è®¿é—®è€…æ¨¡å¼ï¼Œå…¥å‚æ˜¯å½“å‰è®¿é—®è€…å¯¹è±¡ä»¥åŠä¸€ä¸ªä¸Šä¸‹æ–‡å¯¹è±¡ã€‚å½“å‰çš„èŠ‚ç‚¹ä¼šæ¥å—è®¿é—®è¿™ï¼Œå¹¶è°ƒç”¨è®¿é—®è€…çš„ç‰¹å®šæ–¹æ³•ï¼Œè¿™é‡Œä»¥æŠ½è±¡è¯­æ³•æ ‘çš„èŠ‚ç‚¹ä¸ºä¾‹ï¼ŒNodeç±»è°ƒç”¨visitNodeæ–¹æ³•ï¼ŒStatementç±»è°ƒç”¨visitStatementæ–¹æ³• public abstract class Node { protected \u003cR, C\u003e R accept(AstVisitor\u003cR, C\u003e visitor, C context) { return visitor.visitNode(this, context); } } public abstract class Statement extends Node { @Override public \u003cR, C\u003e R accept(AstVisitor\u003cR, C\u003e visitor, C context) { return visitor.visitStatement(this, context); } } å¯¹ä¸€ä¸ªæŠ½è±¡è¯­æ³•æ ‘ä½¿ç”¨è®¿é—®è€…æ¨¡å¼çš„æ—¶å€™ï¼Œæ— è®ºæ ¹èŠ‚ç‚¹æ˜¯ä»€ä¹ˆç±»å‹ï¼Œä»…éœ€è¦ä½¿ç”¨node.accept(visitor)è¯­å¥ï¼ŒèŠ‚ç‚¹ä¼šå¸®æˆ‘ä»¬å¯¼èˆªåˆ°å¯¹åº”çš„è®¿é—®è€…æ–¹æ³•ã€‚ private Node invokeParser(String name, String sql, Function\u003cSqlBaseParser, ParserRuleContext\u003e parseFunction, ParsingOptions parsingOptions) { try { // ä»sqlå­—ç¬¦ä¸²æ„å»ºå­—èŠ‚æµï¼Œå¿½ç•¥å¤§å°å†™ SqlBaseLexer lexer = new SqlBaseLexer(new CaseInsensitiveStream(CharStreams.fromString(sql))); CommonTokenStream tokenStream = new CommonTokenStream(lexer); SqlBaseParser parser = new SqlBaseParser(tokenStream); initializer.accept(lexer, parser); // Override the default error strategy to not attempt inserting or deleting a token. // Otherwise, it messes up error reporting parser.setErrorHandler(new DefaultErrorStrategy() { @Override public Token recoverInline(Parser recognizer) throws RecognitionException { if (nextTokensContext == null) { throw new InputMismatchException(recognizer); } else { throw new InputMismatchException(recognizer, nextTokensState, nextTokensContext); } } }); // PostProcessorç”¨æ¥æ˜¾ç¤ºæ•æ‰æŸç§è¯­æ³•é”™è¯¯å¹¶ç»™å‡ºç²¾ç¡®çš„é”™è¯¯æç¤º // ä½¿ç”¨è§‚å¯Ÿè€…æ¨¡å¼ï¼Œåœ¨è¯­æ³•åˆ†æçš„è¿‡ç¨‹ä¸­è§¦å‘å›è°ƒ parser.addParseListener(new PostProcessor(Arrays.asList(parser.getRuleNames()), parser)); lexer.removeErrorListeners(); lexer.addErrorListener(LEXER_ERROR_LISTENER); parser.removeErrorListeners(); parser.addErrorListener(PARSER_ERROR_HANDLER); ParserRuleContext tree; try { // first, try parsing with potentially faster SLL mode parser.getInterpreter().setPredictionMode(PredictionMode.SLL); // ç”Ÿæˆè¯­æ³•åˆ†ææ ‘å¯¹è±¡ tree = parseFunction.apply(parser); } catch (ParseCancellationException ex) { // if we fail, parse with LL mode tokenStream.seek(0); // rewind input stream parser.reset(); parser.getInterpreter().setPredictionMode(PredictionMode.LL); tree = parseFunction.apply(parser); } // ä½¿ç”¨è®¿é—®è€…æ¨¡å¼ç”Ÿæˆä»¥Nodeä¸ºçˆ¶ç±»çš„æŠ½è±¡è¯­æ³•æ ‘ç»“æ„ return new AstBuilder(parsingOptions).visit(tree); } catch (StackOverflowError e) { throw new ParsingException(name + \" is too large (stack overflow while parsing)\"); } } é€šè¿‡è®¿é—®è€…æ¨¡å¼çš„ä¸€ç³»åˆ—è½¬æ¢ï¼ŒAntlrçš„è¯­æ³•åˆ†ææ ‘ç»“æ„ç»ˆäºè¢«è½¬æ¢æˆPrestoå†…éƒ¨çš„æŠ½è±¡è¯­æ³•æ ‘ç»“æ„ã€‚è¯­æ³•æ ‘èŠ‚ç‚¹ä¸æ˜¯æ‰å¹³çš„ç»“æœï¼Œé‡Œé¢è¿˜å­˜åœ¨å¾ˆå¤šæŠ½è±¡ç±»ã€‚ Statementï¼šå®šä¹‰äº†æœ€ä¸Šå±‚çš„SQLè¯­å¥ï¼ŒPrestoæœ€å¸¸ç”¨çš„å°±æ˜¯OLAPæŸ¥è¯¢èƒ½åŠ›ï¼Œå³DQLè¯­å¥ï¼ˆæ•°æ®æŸ¥è¯¢è¯­è¨€ï¼Œå¯¹åº”Queryï¼‰ï¼Œè¿˜æœ‰å…¶ä»–ç±»å‹çš„SQLè¯­å¥ï¼Œå¦‚INSERT, EXPALIN, CREATE Relationï¼šè¡¨ç¤ºå…³ç³»ä»£æ•°ä¸­çš„å…³ç³»ç±»å‹ï¼Œå¯ä»¥æ˜¯ä¸€å¼ è¡¨æˆ–è§†å›¾ï¼Œä¸€ä¸ªæŸ¥è¯¢ï¼Œä¹Ÿå¯ä»¥æ˜¯ä»»æ„å¤æ‚çš„å…³è”æ“ä½œ QeuryBodyï¼šè¡¨ç¤ºDQLç±»å‹è¯­æ³•æ ‘çš„SQLç»“æ„ï¼Œå¯ä»¥æ˜¯é›†åˆæ“ä½œç»„æˆçš„å¤æ‚SQLï¼Œä¹Ÿå¯ä»¥æ˜¯åŸºç¡€çš„QuerySpecificationç»“æ„ï¼Œæ­¤å¤–è¿˜æ”¯æŒç‰¹æ®Šçš„TableæŸ¥è¯¢å’ŒValueså¸¸é‡ï¼Œä¸€ä¸ªæŸ¥è¯¢æœ¬èº«ä¹Ÿæ˜¯ä¸€ç§å…³ç³»ï¼Œæ‰€ä»¥å®ƒå½’åœ¨äº†Relationä¸‹ Expressionï¼šè¡¨ç¤ºè¡¨è¾¾å¼çš„ç»“æ„ã€‚è¡¨è¾¾å¼æ˜¯æ¯”è¾ƒç‰¹æ®Šçš„ä¸€ç±»ç»“æ„ï¼Œå®ƒæ˜¯ä¸€ç§è®¡ç®—é€»è¾‘ï¼ˆæˆ–è€…è¯´æ˜¯è®¡ç®—è¡¨è¾¾å¼ï¼‰ï¼Œç”±æ“ä½œç¬¦å·å’Œæ“ä½œæ•°ç»„æˆï¼Œæ¯”å¦‚a+1 , a\u003eb QueryBodyæ˜¯æŸ¥è¯¢çš„ä¸»ä½“ç»“æ„ï¼ŒQuerySpecificationæŒ‡å®šäº†æœ€å¸¸è§çš„æŸ¥è¯¢ä¸»ä½“ï¼ŒRelationåˆ™æ˜¯å…³ç³»ï¼Œè¡¨ç¤ºæ•°æ®çš„æ¥æºï¼Œæœ€åå‡ ä¹æ‰€æœ‰å…ƒç´ éƒ½ä¼šä¾èµ–è¡¨è¾¾å¼ï¼Œä»åˆ—åå¼•ç”¨ã€è¡¨åã€WHEREè¯­å¥ä¸­çš„è¿‡æ»¤è°“è¯ç­‰éƒ½åœ¨è¡¨è¾¾å¼çš„èŒƒç•´å†…ã€‚ ","date":"2025-07-09","objectID":"/posts/presto_query_submission/:2:1","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢æäº¤æµç¨‹","uri":"/posts/presto_query_submission/"},{"categories":["Presto"],"content":"è¯­ä¹‰åˆ†æ ç»è¿‡è¯­æ³•åˆ†ææµç¨‹ï¼Œä¸€ä¸ªSQLä»æœ€åˆçš„å­—ç¬¦ä¸²è½¬æ¢æˆPrestoå¼•æ“å†…éƒ¨çš„æŠ½è±¡è¯­æ³•æ ‘ç»“æ„ã€‚åˆ°äº†è¯­ä¹‰åˆ†æé˜¶æ®µï¼Œä¼šç»“åˆå…ƒæ•°æ®å¯¹æŠ½è±¡è¯­æ³•æ ‘åšè¿›ä¸€æ­¥çš„åˆ†æï¼Œä¸»è¦æ˜¯ç»“åˆä¸Šä¸‹æ–‡éªŒè¯SQLçš„æ­£ç¡®æ€§ä»¥åŠè®°å½•æŠ½è±¡è¯­æ³•æ ‘ç›¸å…³å…ƒæ•°æ®ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯Prestoåœ¨è¯­ä¹‰åˆ†æé˜¶æ®µæ²¡æœ‰ç”Ÿæˆæ–°çš„æ ‘ç»“æ„ï¼Œåˆ†ææ‰€å¾—åˆ°çš„å…ƒæ•°æ®ä¿¡æ¯å­˜æ”¾åœ¨ä¸€ä¸ªAnalysisç±»ä¸­ï¼Œé€šè¿‡å¤šä¸ªMapç»“æ„è®°å½•æ¯ä¸ªè¯­æ³•æ ‘èŠ‚ç‚¹ï¼ˆKeyï¼‰çš„å…ƒæ•°æ®ä¿¡æ¯ï¼ˆValueï¼‰ã€‚åç»­æµç¨‹é€šè¿‡æŸ¥æ‰¾Analysisç»“æ„æ¥è·å–å¯¹åº”çš„å…ƒæ•°æ®ã€‚ æ•°æ®æºä¿¡æ¯è¯»å–ï¼šåŒ…æ‹¬è¡¨ã€è§†å›¾ã€ç‰©åŒ–è§†å›¾çš„è¡¨ä¿¡æ¯TableHandleï¼Œåˆ—ä¿¡æ¯ColumnHandleä»¥åŠå¯¹åº”çš„è¡¨å…ƒæ•°æ®TableMetadataï¼Œåˆ—å…ƒæ•°æ®ColumnMetadataã€‚æŸ¥è¯¢è¿™äº›å…ƒæ•°æ®ä¸€æ–¹é¢æ˜¯ä¸ºäº†å°†å…¶åº”ç”¨äºåç»­çš„æ‰§è¡Œè®¡åˆ’ç”Ÿæˆï¼Œå¦ä¸€ä¸ªæ–¹é¢ç”¨äºè¯­ä¹‰æ£€æŸ¥ã€‚ è¯­ä¹‰æ£€æŸ¥ï¼šæœ‰å¾ˆå¤šSQLè§„åˆ™æ˜¯éœ€è¦ä¸Šä¸‹æ–‡ä¿¡æ¯å…±åŒç»´æŠ¤çš„ï¼ŒåŒ…æ‹¬å¯¹å¤šèŠ‚ç‚¹çš„å†…å®¹è¿›è¡Œè”åˆæ¯”å¯¹ã€‚æ¯”å¦‚SELECTè¯­å¥çš„éèšåˆå‡½æ•°åˆ—æ˜¯å¦å’ŒGROUP BYè¯­å¥åŒ¹é…ï¼Œè¯¥çº¦æŸåœ¨è¯­ä¹‰åˆ†æä¸­ç”±AggregationAnalyzerè¿›è¡Œæ£€æŸ¥ï¼Œè¿™äº›æ˜¯è¯­æ³•åˆ†æåšä¸äº†çš„ã€‚ è¯­ä¹‰ç†è§£ï¼šåŒ…æ‹¬æ¶ˆé™¤æ­§ä¹‰ä»¥åŠè¯­æ³•ç³–å±•å¼€ç­‰æ“ä½œ a.bï¼ŒæŒ‡çš„æ˜¯aè¡¨çš„båˆ—è¿˜æ˜¯Row Typeç±»å‹aåˆ—çš„bå­—æ®µï¼Œè¯­æ³•è§„åˆ™å¯èƒ½ä¼šäº§ç”ŸäºŒä¹‰æ€§ï¼Œè¯­ä¹‰åˆ†æé˜¶æ®µéœ€è¦è§£å†³è¿™äº›é—®é¢˜ select * from xxxï¼Œå°†*å±•å¼€æˆæ‰€æœ‰åˆ— å…ƒæ•°æ®åˆ†æï¼šæ‰€æœ‰ä¸‹æ¸¸éœ€è¦çš„ä¿¡æ¯éƒ½ä¼šåœ¨è¯­ä¹‰åˆ†æé˜¶æ®µè®°å½•ä¸‹æ¥ï¼Œè®°å½•åœ¨Analysiså¯¹è±¡ä¸­ æ•°æ®åŸŸï¼ˆscopeï¼‰åˆ†æï¼šæ¯ä¸ªå…³ç³»èŠ‚ç‚¹å½“å‰å¯ç”¨çš„åˆ—ä¿¡æ¯ï¼Œé€šè¿‡è‡ªåº•å‘ä¸Šçš„æ–¹å¼è®¡ç®—å‡ºæ¥ èšåˆå‡½æ•°åˆ†æï¼šæ”¶é›†æ¯ä¸ªQuerySpecificationç»“æ„çš„èšåˆå‡½æ•° ç±»å‹åˆ†æï¼šå¯¹è¡¨è¾¾å¼ç»“æ„Expressionçš„æ¯ä¸ªèŠ‚ç‚¹è¿›è¡Œè‡ªåº•å‘ä¸Šçš„ç±»å‹æ¨å¯¼ï¼Œå…¶ä¸­åŒ…å«éšå¼è½¬æ¢ä¿¡æ¯çš„è®°å½• å‡½æ•°è§£æï¼šè¡¨è¾¾å¼çš„å†…å®¹åŒ…å«å‡½æ•°è°ƒç”¨å’Œæ“ä½œç¬¦ï¼Œæœ¬è´¨ä¸Šéƒ½å¯ä»¥çœ‹åšæ˜¯å‡½æ•°è°ƒç”¨ï¼Œå¼•æ“éœ€è¦åˆ†æä¸æŠ½è±¡è¯­æ³•æ ‘å¯¹åº”çš„å‡½æ•°ç»“æ„FunctionCallæ˜¯å¦å­˜åœ¨ï¼Œå‚æ•°ç±»å‹æ˜¯å¦åŒ¹é…ç­‰ è¯­ä¹‰åˆ†æé˜¶æ®µæœ‰ä¸¤ä¸ªæ ¸å¿ƒçš„åˆ†æå™¨StatementAnalyzerå’ŒExpressionAnalyzerï¼Œä¸€ä¸ªç”¨äºåˆ†æStatementå¯¹è±¡ï¼Œä¸€ä¸ªç”¨äºåˆ†æExpressionè¡¨è¾¾å¼å¯¹è±¡ã€‚ä¸Šæ–‡æåˆ°çš„ç±»å‹åˆ†æã€å‡½æ•°è§£æå°±æ˜¯åœ¨ExpressionAnalyzerä¸­å®Œæˆçš„ã€‚ä»–ä»¬çš„å…³æ³¨ç‚¹ä¸ä¸€æ ·ï¼Œè¿™ä¹Ÿæ˜¯è¯­æ³•æ ‘èŠ‚ç‚¹ä¸ºä»€ä¹ˆä¼šæœ‰ä¸åŒçš„æŠ½è±¡ç±»ã€‚ æ•°æ®åŸŸåˆ†æ Scopeæ˜¯StatementAnalyzerè®¿é—®è€…çš„è¿”å›å€¼ï¼Œå®ƒè¡¨ç¤ºå½“å‰æŠ½è±¡è¯­æ³•æ ‘èŠ‚ç‚¹èƒ½è¢«å¼•ç”¨çš„åˆ—ä¿¡æ¯ï¼Œè¿™é‡Œç§°ä¸ºæ•°æ®åŸŸã€‚å®ƒçš„ç”¨é€”æ˜¯è®°å½•å¯ç”¨çš„åˆ—ä¿¡æ¯ï¼Œåœ¨è¯­ä¹‰åˆ†æä¸­ç”¨æ¥éªŒè¯åˆ—åå­—ç¬¦ä¸²æ˜¯å¦å­˜åœ¨äºæ•°æ®åŸŸä¸­ï¼ˆåˆæ³•æ€§æ ¡éªŒï¼‰ã€‚ åˆ—ä¿¡æ¯ç”¨Fieldè¡¨ç¤ºï¼Œå®ƒä¸ä»…åŒ…å«äº†åˆ—åNameï¼Œç±»å‹ä¿¡æ¯Typeï¼Œè¿˜åŒ…å«äº†æ‰€å±å…³ç³»çš„åˆ«årelationAliasï¼Œæ¯”å¦‚JoinèŠ‚ç‚¹ï¼Œå¯ä»¥å¼•ç”¨ä¸¤ä¸ªå­å…³ç³»çš„åˆ—ï¼Œè¿™ä¸ªæ—¶å€™éœ€è¦åŠ ä¸Šå­è¡¨çš„åç§°æ‰èƒ½æ„æˆå”¯ä¸€æ ‡è¯†ç¬¦ã€‚ RealtionTypeè¡¨ç¤ºå½“å‰èŠ‚ç‚¹çš„å…³ç³»ï¼Œåº•å±‚æ˜¯Fieldåˆ—è¡¨ï¼Œå®ƒå°è£…äº†æ‰€æœ‰å¯¹å…³ç³»çš„æ“ä½œï¼Œæ˜¯ä¸€ä¸ªå·¥å…·ç±» Scopeå»ºæ¨¡äº†æ›´åŠ å¤æ‚çš„æƒ…å†µï¼Œå¯¹äºå…³è”ã€å­æŸ¥è¯¢ç­‰ç‰¹æ®Šåœºæ™¯ï¼ˆå¦‚semi-join/lateral joinï¼‰ï¼Œé™¤äº†å½“å‰çš„å…³ç³»ï¼Œè¿˜èƒ½å¼•ç”¨å¤–å±‚çš„å…³ç³»ã€‚é€šè¿‡parentå­—æ®µï¼ˆçˆ¶çº§æ•°æ®åŸŸï¼‰å¯ä»¥å»ºæ¨¡è¿™ç§æƒ…å†µï¼ŒéªŒè¯å­—æ®µåˆæ³•æ€§çš„æ—¶å€™å¯ä»¥æŸ¥æ‰¾çˆ¶çº§æ•°æ®åŸŸæ¥åˆ¤æ–­å­—æ®µæ˜¯å¦å­˜åœ¨ã€‚ --- schema ä¸º tpch.sf1 SELECT (SELECT name FROM region r WHERE regionkey = n.regionkey) AS region name, n.name AS nation_name FROM nation n regionå’Œnationè¿™ä¸¤ä¸ªå…³ç³»åˆ†åˆ«æœ‰è‡ªå·±çš„æ•°æ®åŸŸï¼Œä½†æ˜¯regionæ‰€åœ¨åŸŸçš„parentæŒ‡å‘äº†nationåŸŸï¼Œæ‰€ä»¥åœ¨å­æŸ¥è¯¢å†…ä¸ä»…å¯ä»¥ä½¿ç”¨regionè¡¨çš„åˆ—ï¼Œè¿˜èƒ½ä½¿ç”¨å¤–å±‚nationè¡¨çš„åˆ—ã€‚ å¦å¤–ï¼ŒCTEï¼ˆCommon Table Expressionï¼‰ç‰¹æ€§ä½¿å¾—æŸ¥è¯¢ä¸­å¯ä»¥ä½¿ç”¨ä¸€äº›è‡ªå®šä¹‰çš„è§†å›¾ï¼ˆè¡¨ï¼‰ï¼Œé€šè¿‡namedQuerieså¯ä»¥è®°å½•è¿™äº›ä¿¡æ¯ï¼Œæ³¨æ„å®ƒç”¨æ¥éªŒè¯â€œè¡¨åå­—ç¬¦ä¸²â€æ˜¯å¦åˆæ³•ï¼Œä¸æ˜¯ç”¨æ¥éªŒè¯åˆ—ä¿¡æ¯çš„ã€‚ æ•°æ®åŸŸçš„æ ¸å¿ƒåŠŸèƒ½æ˜¯è¯†åˆ«è¯­æ³•ç»“æ„ä¸­çš„å­—ç¬¦ä¸²ï¼Œåˆ¤æ–­å®ƒæ˜¯å¦æ˜¯ä¸€ä¸ªåˆæ³•çš„åˆ—å¼•ç”¨ï¼ˆColumnReferenceï¼‰ä¿¡æ¯ã€‚è¯¥è¿‡ç¨‹ä¸»è¦å‘ç”Ÿåœ¨è¡¨è¾¾å¼åˆ†æå™¨ExpressionAnalyzerçš„visitIdentifieræ–¹æ³•å’ŒvisitDereferenceExpressionæ–¹æ³•ä¸­ï¼Œä»–ä»¬åˆ†åˆ«ç”¨æ¥åˆ¤æ–­å­—ç¬¦ä¸²å’Œå¸¦\".â€œåˆ†éš”ç¬¦çš„å­—ç¬¦ä¸²æ˜¯å¦åˆæ³•ã€‚è¯†åˆ«åŠ¨ä½œçš„å…¥å£æ˜¯Scope.resolveFieldå‡½æ•°ï¼ŒFieldçš„åç§°åŒ…æ‹¬åˆ—åä»¥åŠåº•å±‚å…³ç³»åæˆ–è€…å…³ç³»åˆ«åã€‚Fieldçš„åç§°åŒ…æ‹¬åˆ—åä»¥åŠåº•å±‚å…³ç³»åæˆ–è€…å…³ç³»åˆ«åã€‚ æ•°æ®æºä¿¡æ¯è¯»å– å¯¹äºæŸ¥è¯¢ç±»çš„SQLè¯­å¥æ¥è¯´ï¼Œå…ƒæ•°æ®è¯»å–ä¸»è¦å‘ç”Ÿåœ¨TableèŠ‚ç‚¹ï¼ŒPrestoå¼•æ“éœ€è·å–ä»¥ä¸‹ä¿¡æ¯ï¼š TableHandleï¼šé€šè¿‡è¿æ¥å™¨è·å–è¡¨çš„æ•°æ®ç»“æ„ï¼Œå®ƒçš„å†…å®¹å’Œåº•å±‚æ•°æ®æºæœ‰å…³ã€‚Prestoä»…å®šä¹‰äº†ä¸€ä¸ªæŠ½è±¡çš„ConnectorTablehandleæ¥å£ï¼Œé‡Œé¢çš„å†…å®¹ä¸ºç©ºã€‚ä½¿ç”¨åœºæ™¯æ¯”å¦‚PageSourceProvder.createPageSourceæ˜¯Prestoå»ºæ¨¡çš„æ•°æ®è¯»å–æ¥å£ï¼Œé‡Œé¢å°±æœ‰TableHandleä½œä¸ºå…¥å‚ï¼Œæ‰€ä»¥å®ƒéœ€è¦åŒ…å«æ•°æ®è¡¨è¯»å–æ‰€éœ€çš„å†…å®¹ã€‚ ColumnHandleï¼šé€šè¿‡è¿æ¥å™¨è·å–å½“å‰è¡¨çš„åˆ—ä¿¡æ¯ï¼Œå’ŒTableHandleä¸€æ ·å®ƒä¹Ÿæ˜¯ä¸€ä¸ªæŠ½è±¡çš„å®šä¹‰ å…ƒæ•°æ®ä¿¡æ¯ï¼šåŒ…æ‹¬ConnectorTableMetadataå’ŒColumnMetadataï¼Œä¸¤è€…åˆ†åˆ«æ˜¯è¡¨å’Œåˆ—çš„å…ƒæ•°æ®ä¿¡æ¯ï¼Œä¸»è¦åŒ…æ‹¬åç§°ã€ç±»å‹ã€æ³¨é‡Šã€å±æ€§ã€‚åˆ—çš„ç±»å‹ä¿¡æ¯ï¼ˆType)æ˜¯æœ€åº•å±‚çš„ç±»å‹ï¼Œå®ƒæ˜¯Prestoå®šä¹‰çš„ç±»å‹ä¿¡æ¯ï¼ŒFieldçš„ç±»å‹ä¿¡æ¯å°±æ˜¯ä»å…ƒæ•°æ®è·å–çš„ã€‚ è¯­ä¹‰ç†è§£ è¯­ä¹‰åˆ†æéœ€è¦æ­£ç¡®åœ°ç†è§£è¡¨è¾¾å¼å«ä¹‰ï¼Œå¤„ç†æœ‰æ­§ä¹‰çš„é€»è¾‘å¹¶æä¾›å½’ä¸€åŒ–çš„æ•°æ®ç»“æ„ï¼Œå°½é‡ä¸ºä¸‹æ¸¸çš„æ‰§è¡Œè®¡åˆ’æ¨¡å—å±è”½è¯­æ³•ç»†èŠ‚ã€‚Prestoçš„åˆ—åå’ŒRowTypeçš„å­å­—æ®µå…¶å®ä»£è¡¨ä¸åŒçš„è¯­ä¹‰ï¼Œä½†æ˜¯å¤ç”¨äº†ç›¸åŒçš„è¯­æ³•ç»“æ„DereferenceExpressionã€‚ DereferenceExpressionèŠ‚ç‚¹çš„è¯­ä¹‰ç†è§£è¿‡ç¨‹å¦‚ä¸‹ï¼ŒPrestoä¸­çš„Dereferenceè¡¨ç¤ºç±»ä¼¼x.y.zè¿™æ ·çš„è¡¨è¾¾å¼ï¼ŒåŒ…å«è‡³å°‘ä¸€ä¸ªâ€.â€œåˆ†éš”ç¬¦ï¼Œå‚è€ƒg4æ–‡ä»¶çš„å®šä¹‰ï¼Œbase=primaryExpressionâ€™.â€˜fieldName=identifierï¼Œå…¶ä¸­suffixæ˜¯æ ‡è¯†ç¬¦ç±»å‹ï¼Œprefixå¯ä»¥æ˜¯ä»»æ„è¡¨è¾¾å¼ç±»å‹ã€‚ å¯¹äºx.y.zè¿™æ ·çš„DereferenceExpressionç»“æ„ï¼Œå¯ä»¥è¡¨ç¤ºä¸ºå¦‚ä¸‹å‡ ç§æƒ…å†µï¼š æ•°æ®åº“ä¸ºxï¼Œè¡¨ä¸ºyï¼Œåˆ—ä¸ºzçš„åˆ—åï¼ˆå¦‚æœä¸€ä¸ªè¯­æ³•ç»“æ„æŒ‡å‘ä¸€ä¸ªåˆ—åï¼Œé‚£ä¹ˆå®ƒæ˜¯ä¸€ä¸ªColumnReferenceï¼‰ xè¡¨yåˆ—çš„zå­—æ®µï¼Œå…¶ä¸­yæ˜¯RowTypeç±»å‹ï¼ˆç±»ä¼¼äºHiveçš„Structç»“æ„ï¼‰ xåˆ—çš„yåˆ—çš„zå­—æ®µï¼Œxåˆ—å’Œyåˆ—éƒ½æ˜¯RowTypeç±»å‹ å› ä¸ºå­˜åœ¨ä¸Šè¿°å¤šç§æƒ…å†µï¼Œæ‰€ä»¥ç¬¬ä¸€æ­¥å°è¯•è·å–è¯¥ç»“æ„çš„QualifiedNameï¼Œæ­¤æ—¶DeferenceExpressionéœ€è¦æ»¡è¶³æ¯ä¸€æ®µéƒ½æ˜¯æ ‡è¯†ç¬¦ï¼Œè¿™æ ·æ‰å¯èƒ½æ˜¯ä¸€ä¸ªåˆ—å¼•ç”¨ã€‚é’ˆå¯¹QualifiedNameéç©ºçš„æƒ…å†µä½¿ç”¨æ•°æ®åŸŸçš„tryRosolveFieldæ–¹æ³•è¯†åˆ«å½“å‰å­—æ®µï¼Œæœ‰å¦‚ä¸‹3ç§å¯èƒ½ï¼š åˆ—å¼•ç”¨è¯†åˆ«æˆåŠŸï¼Œå‘½ä¸­åˆšåˆšæåˆ°çš„æƒ…å†µ1 æœªèƒ½è¯†åˆ«åˆ°åˆ—å¼•ç”¨ï¼Œæ­¤æ—¶è°ƒç”¨æ•°æ®åŸŸçš„isColumnReferenceåˆ¤æ–­è¡¨è¾¾å¼çš„æ‰€æœ‰Prefixå‰ç¼€å­é›†æ˜¯å¦èƒ½è¯†åˆ«ä¸ºåˆ—å¼•ç”¨ï¼Œå³æƒ…å†µ2ã€3ï¼Œè¯†åˆ«æˆåŠŸï¼Œåˆ™è¿›å…¥ä¸‹é¢ä¸»é€»è¾‘ï¼Œè°ƒç”¨processé€’å½’å¤„ç†baseéƒ¨åˆ†çš„è¯­æ³•ç»“æ„ã€‚ è¯†åˆ«å¤±è´¥ï¼Œè¯´æ˜å½“å‰DereferenceExpressionéæ³•ï¼ŒæŠ›å‡ºå¼‚å¸¸ @Override protected Type visitDereferenceExpression(DereferenceExpression node, StackableAstVisitorContext\u003cContext\u003e context) { // å°è¯•è½¬æ¢æˆQualifiedNameï¼Œè¿™ç§æƒ…å†µä¸‹è¦æ±‚æ¯ä¸ªéƒ¨åˆ†éƒ½æ˜¯è¡¨ç¤ºç¬¦ QualifiedName qualifiedName = DereferenceExpression.getQualifiedName(node); // å¦‚æœè¿™ä¸ªDereferenceç»“æ„çœ‹èµ·æ¥åƒæ˜¯å¸¦æœ‰åº“è¡¨é™å®šçš„åˆ—åï¼Œå…ˆå°è¯•åŒ¹é…åˆ°åˆ— if (qualifiedName != null) { Scope scope = context.getContext().getScope(); // å°è¯•è¯†åˆ«è¯¥å­—æ®µ Optional\u003cResolvedField\u003e resolvedField = scope.tryResolveField(node, qualifiedName); if (resolvedField.isPresent()) { return handleResolvedField(node, resolvedField.get(), context); } // æ˜¯ä¸€ä¸ªéæ³•çš„ç»“æ„ if (!scope.isColumnReference(qualifiedName)) { throw missingAttributeException(node, qualifiedName); } } // å‰ç¼€å­˜åœ¨åˆ—å¼•ç”¨ï¼Œè¿›å…¥é€’å½’è§£æï¼Œé€’å½’åˆ†æbaseç»“æ„ Type baseType = process(node.getBase(), context); if (!(baseType instanceof RowType)) { throw ","date":"2025-07-09","objectID":"/posts/presto_query_submission/:2:2","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢æäº¤æµç¨‹","uri":"/posts/presto_query_submission/"},{"categories":["Presto"],"content":"ç”Ÿæˆåˆå§‹é€»è¾‘æ‰§è¡Œè®¡åˆ’ è¯­æ³•åˆ†æç”Ÿæˆäº†æŠ½è±¡è¯­æ³•æ ‘ï¼Œè¯­ä¹‰åˆ†æé€šè¿‡æŸ¥è¯¢å…ƒæ•°æ®å’Œè¿›ä¸€æ­¥è§£æç”Ÿæˆäº†ä¸€ä¸ªAnalysiså…ƒæ•°æ®å¯¹è±¡ã€‚è¯­ä¹‰åˆ†æã€æ‰§è¡Œè®¡åˆ’æ˜¯ä¸¤å¥—ä¸åŒçš„ä½“ç³»ï¼Œç†è®ºä¸Šæ˜¯ç›¸äº’ç‹¬ç«‹ã€å„è‡ªæ¼”è¿›ã€‚æ‰§è¡Œè®¡åˆ’æœ¬è´¨ä¸Šæ˜¯ä¸€ç§ä¸­é—´è¡¨è¾¾å¼ï¼ˆIntermidiate Representation, IR)ï¼Œä¸€ç§ååº•å±‚çš„æè¿°è¯­è¨€ï¼Œè¿˜æ˜¯ä¸€ç§æ ‘çŠ¶ç»“æ„ã€‚IRä½¿å¾—ä¼˜åŒ–å™¨èƒ½å¤Ÿå¯¹SQLæ›´å¥½åœ°è¿›è¡Œä¼˜åŒ–ã€‚PlanNodeç±»æ˜¯æ‰€æœ‰æ‰§è¡Œè®¡åˆ’èŠ‚ç‚¹çš„çˆ¶ç±»ï¼Œå®ƒå®šä¹‰äº†IRç»“æ„çš„é€šç”¨æ–¹æ³•ã€‚ç”Ÿæˆåˆå§‹é€»è¾‘æ‰§è¡Œè®¡åˆ’å°±æ˜¯éå†ASTï¼ˆæŠ½è±¡è¯­æ³•æ ‘ï¼‰ï¼Œç»“åˆAnalysiså…ƒæ•°æ®ç”ŸæˆIRçš„è¿‡ç¨‹ã€‚ä¼˜åŒ–å™¨åˆ™æ˜¯æŒ‰ç…§æŸäº›ä¼˜åŒ–è§„åˆ™ä¿®æ”¹æ‰§è¡Œè®¡åˆ’æ ‘çš„è¿‡ç¨‹ã€‚ åˆå§‹é€»è¾‘æ‰§è¡Œè®¡åˆ’æ•´ä½“ä¸Šåœ¨åšå¦‚ä¸‹ä¸¤ä»¶äº‹æƒ…ã€‚ Statementç»“æ„è½¬æ¢ï¼šæœ¬è´¨ä¸Šæ˜¯ASTåˆ°IRç»“æ„çš„è½¬å˜ï¼Œç»“åˆè¯­ä¹‰åˆ†æé˜¶æ®µçš„Analysisç»“æ„ï¼ŒPrestoé€šè¿‡è®¿é—®è€…æ¨¡å¼è‡ªåº•å‘ä¸ŠæŠŠASTç»“æ„è½¬æ¢æˆä¸åŒPlanNodeç»“æ„ã€‚ Expressionæ”¹å†™ï¼šå¯¹Expressionç»“æ„è¿›è¡Œé‡å†™ï¼Œå°†å˜é‡åæ›¿æ¢æˆSymbolå”¯ä¸€æ ‡è¯†ç¬¦ï¼ŒåŒæ—¶ä¿ç•™å…ƒæ•°æ®ä¿¡æ¯ï¼Œå°†ResolvedFunctionä¿¡æ¯ç¼–ç åˆ°å‡½æ•°åç§°ä¸­ã€‚ è¿™ä¸æ˜¯ç‹¬ç«‹å‘ç”Ÿçš„ä¸¤ä»¶äº‹æƒ…ï¼ŒStatementç»“æ„çš„è½¬æ¢ä¼šä¼´éšç€Expressionçš„æ”¹å†™ï¼Œåªä¸è¿‡Expressionçš„å¤„ç†æ–¹å¼æ¯”è¾ƒç‰¹åˆ«ã€‚ Statementç»“æ„è½¬æ¢ ASTçš„æ ¸å¿ƒéª¨æ¶æ˜¯Statementç»“æ„ï¼ŒSQLè¯­å¥æœ€å¤–å±‚æ˜¯Queryï¼Œé€šè¿‡è¿½è¸ªLogicalPlannerçš„å®ç°å¯ä»¥å®šä½åˆ°createRelationPlanå‡½æ•°ï¼Œå®ƒä¹Ÿæ˜¯åŸºäºè®¿é—®è€…æ¨¡å¼æ¥ç”Ÿæˆåˆå§‹æ‰§è¡Œè®¡åˆ’çš„ï¼Œè¿™é‡Œä¾èµ–ä¸¤ä¸ªæ ¸å¿ƒç±»æ¥å®Œæˆæ‰§è¡Œè®¡åˆ’çš„ç”Ÿæˆã€‚ RelationPlanner: ä¸€ä¸ªè®¿é—®è€…ï¼Œå®ƒä¸“é—¨å¤„ç†å…³ç³»ç»“æ„ä¹‹é—´çš„è¿ç®—ï¼Œæ¯”å¦‚å…³è”JOINã€é›†åˆUnionã€åº•å±‚è¡¨ã€è¡¨æŠ½æ ·ç­‰å…³ç³»ç±»å‹ QueryPlannerï¼šå¯¹å•ä¸ªå…³ç³»è¿›è¡Œçš„å˜æ¢æ“ä½œï¼Œå®ƒåŒ…æ‹¬è¿‡æ»¤ã€æŠ•å½±å˜æ¢ã€èšåˆã€çª—å£å‡½æ•°ã€æ’åºã€åˆ†é¡µç­‰æ“ä½œï¼Œæ ¸å¿ƒå°±æ˜¯æŠŠQuerySpecificationç»“æ„åŒ…å«çš„æ“ä½œè½¬æ¢æˆæ‰§è¡Œè®¡åˆ’èŠ‚ç‚¹ RelationPlannerå’ŒQueryPlannerä¹‹é—´æ˜¯ç›¸äº’åµŒå¥—çš„ï¼Œé‡åˆ°Relationå°±æ˜¯è°ƒç”¨RelationPlannerï¼Œé‡åˆ°Queryå°±ä¼šä½¿ç”¨QueryPlannerã€‚ Expressionæ”¹å†™ Prestoå¼•æ“çš„æ ¸å¿ƒæ¨¡å—ä¹‹é—´å­˜åœ¨è€¦åˆçš„æƒ…å†µï¼Œè™½ç„¶ASTå’ŒIRæœ‰ç€æœ¬è´¨çš„åŒºåˆ«ï¼Œä½†æ˜¯å…·ä½“åˆ°Expressionç»“æ„ï¼ˆExpressionçš„å­ç±»ï¼‰ï¼Œç”±äºå®ƒè¡¨è¾¾çš„æ˜¯ä¸€ç§é€»è¾‘è¿ç®—ï¼Œæ‰€ä»¥è¡¨è¾¾å¼åœ¨è¯­æ³•æ ‘å’Œæ‰§è¡Œè®¡åˆ’ä¹‹é—´æ˜¯å¤ç”¨çš„ï¼Œè¯­æ³•åˆ†æç”Ÿæˆçš„Expressionç»“æ„ä¸€ç›´è¢«æ²¿ç”¨åˆ°æ‰§è¡Œé˜¶æ®µã€‚ IRä½¿ç”¨Symbolä½œä¸ºå…¨å±€å”¯ä¸€æ ‡è¯†ç¬¦æ¥ä»£æ›¿ASTé‡Œé¢çš„å˜é‡åã€‚ASTä¸­çš„å˜é‡åæœ‰å¾ˆå¤šè¡¨ç¤ºæ–¹æ³•ï¼Œå¯èƒ½ä¼šå‡ºç°é‡å¤ï¼Œæ¯”å¦‚a + 1 as aï¼Œè™½ç„¶ASTæŠŠSQLç»“æ„åŒ–æˆè¯­æ³•åˆ†ææ ‘ï¼Œä½†ASTçš„è¡¨è¾¾å¼åªæœ‰äººèƒ½å¤Ÿç†è§£ï¼ŒOLAPå¼•æ“è¿˜ä¸èƒ½ç†è§£è¡¨è¾¾å¼ä¹‹é—´å˜é‡çš„å¼•ç”¨å…³ç³»ï¼Œå¦‚æœt.col_aæˆ–è€…col_aæŒ‡ä»£çš„æ˜¯å“ªä¸€åˆ—ï¼ŒOLAPå¼•æ“æ— æ³•è§£å†³ã€‚ ä¸ºäº†å¤ç”¨ASTçš„Expresssionç»“æ„ï¼ŒPrestoä¸ºSymbolå¼•å…¥äº†ä¸€ä¸ªSymbolReferenceè¡¨è¾¾å¼ï¼Œå®ƒè¡¨ç¤ºçš„æ˜¯ä¸€ä¸ªå†…éƒ¨ç¬¦å·ï¼ˆSymbolï¼‰ï¼Œä»¥å–ä»£ASTé‡Œé¢çš„Identifierç­‰å˜é‡åã€‚å†…éƒ¨ç¬¦å·æœ¬è´¨å°±æ˜¯å­—ç¬¦ä¸²ï¼Œå®ƒæ˜¯å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œå…¨å±€å”¯ä¸€ã€‚Symbolçš„ç”Ÿæˆé€»è¾‘ä½äºSymbolAllocatorä¸­ã€‚ public class Symbol implements Comparable\u003cSymbol\u003e { private final String name; public static Symbol from(Expression expression) { // å¿…é¡»æ˜¯SymbolReferenceç±»å‹çš„è¡¨è¾¾å¼ checkArgument(expression instanceof SymbolReference, \"Unexpected expression: %s\", expression); return new Symbol(((SymbolReference) expression).getName()); } @JsonCreator public Symbol(String name) { requireNonNull(name, \"name is null\"); this.name = name; } @JsonValue // ç”ŸæˆSymbolï¼Œnameæ˜¯å”¯ä¸€çš„ public String getName() { return name; } // Symbolè½¬SymbolReference public SymbolReference toSymbolReference() { return new SymbolReference(name); } // Symbolçš„wrapperï¼Œå¤ç”¨Expressionä½“ç³» public class SymbolReference extends Expression { private final String name; public SymbolReference(String name) { super(Optional.empty()); this.name = name; } è¿™é‡Œæ‰§è¡Œè®¡åˆ’ä¼šç”¨åˆ°å¦‚ä¸‹ä¸¤ç§å…ƒæ•°æ®ä¿¡æ¯ï¼š è¡¨è¾¾å¼çš„ç±»å‹ä¿¡æ¯ï¼šå³Prestoçš„Typeï¼Œé€šè¿‡ä¸€ä¸ªTypeAnalyzerå·¥å…·ç±»åœ¨æ‰§è¡Œè®¡åˆ’æœŸé—´é‡æ–°è¿›è¡Œè¯­ä¹‰åˆ†ææ¥è·å–Expressionç±»å‹ä¿¡æ¯ å‡½æ•°å…ƒæ•°æ®ä¿¡æ¯ResolvedFunctionï¼Œåœ¨è¯­ä¹‰åˆ†æé˜¶æ®µé€šè¿‡å‡½æ•°è§£æè·å–ï¼Œå‚è€ƒExpressionAnalyzer.visitFunctionCallæ–¹æ³•ï¼Œåç»­å¸¸é‡æŠ˜å ç­‰ä¼˜åŒ–å™¨éœ€è¦ç”¨åˆ°å®ƒã€‚è¯¥ä¿¡æ¯ç›´æ¥é€šè¿‡åºåˆ—åŒ–+ZSTDï¼ˆä¸€ç§å‹ç¼©ç®—æ³•åç§°ï¼‰å‹ç¼©+BASE32ç¼–ç çš„æ–¹å¼é›†æˆåˆ°å‡½æ•°åç§°çš„å­—ç¬¦ä¸²ä¸­ã€‚ ä½¿ç”¨PlanNodeè¡¨è¾¾é€»è¾‘æ‰§è¡Œè®¡åˆ’ PlanNodeæ˜¯ä¸€ä¸ªæŠ½è±¡ç±»ï¼ŒåŒ…å«ä»¥ä¸‹æ ¸å¿ƒå‡½æ•°ï¼š getSourcesï¼šç”¨äºéå†æ‰§è¡Œè®¡åˆ’ï¼Œå®ƒä¼šè¿”å›å½“å‰èŠ‚ç‚¹çš„å­èŠ‚ç‚¹ï¼ˆä»æ•°æ®æµçš„è§’åº¦çœ‹ï¼Œå­èŠ‚ç‚¹æ˜¯æ•°æ®çš„ä¸Šæ¸¸ï¼‰ replaceChildrenï¼šæ›¿æ¢å­èŠ‚ç‚¹ï¼Œåœ¨è®¿é—®è€…æ¨¡å¼ä¸­é€’å½’ç”Ÿæˆæ–°çš„æ‰§è¡Œè®¡åˆ’ getOutputSymbolsï¼šå‘çˆ¶èŠ‚ç‚¹æä¾›çš„å­—æ®µé›†åˆï¼Œç±»å‹æ˜¯Symbol acceptï¼šè®¿é—®è€…æ¨¡å¼çš„é€šç”¨æ¥å£ï¼Œæ‰§è¡Œè®¡åˆ’èŠ‚ç‚¹çš„è®¿é—®è€…çˆ¶ç±»æ˜¯PlanVisitor public abstract class PlanNode { private final PlanNodeId id; protected PlanNode(PlanNodeId id) { requireNonNull(id, \"id is null\"); this.id = id; } @JsonProperty(\"id\") public PlanNodeId getId() { return id; } public abstract List\u003cPlanNode\u003e getSources(); public abstract List\u003cSymbol\u003e getOutputSymbols(); public abstract PlanNode replaceChildren(List\u003cPlanNode\u003e newChildren); public \u003cR, C\u003e R accept(PlanVisitor\u003cR, C\u003e visitor, C context) { return visitor.visitPlan(this, context); } } TableScanNode æ•°æ®è¯»å–èŠ‚ç‚¹ï¼Œä¸€ä¸ªTableScanNdoeä»£è¡¨ä¸€ä¸ªåº•å±‚æ•°æ®æºçš„è¯»å–æ“ä½œï¼Œæ ¸å¿ƒæ˜¯å¦‚ä¸‹3ä¸ªå˜é‡ï¼š TableHandleï¼šæè¿°äº†å¦‚ä½•è¯»å–ä¸€å¼ è¡¨ï¼Œé‡Œé¢åŒ…å«äº†è¿™ä¸ªæ•°æ®çš„ä½ç½®ä¿¡æ¯ï¼Œä»¥åŠä¸‹æ¨åçš„å…ƒæ•°æ®ä¿¡æ¯ï¼Œæ¯”å¦‚limitä¸‹æ¨ã€èšåˆä¸‹æ¨ç­‰ä¿¡æ¯ outputSymbolsï¼šå½“å‰èŠ‚ç‚¹æä¾›çš„è¾“å‡ºåˆ—ï¼Œçˆ¶ç±»çš„é‡è½½æ–¹æ³•getOutputSymbolsä¼šè¿”å›è¿™ä¸ªå˜é‡ assignmentsï¼šè®°å½•äº†æ•°æ®æºçš„åˆ—ä¿¡æ¯åˆ°Symbolçš„æ˜ å°„å…³ç³»ã€‚ColumnHandleå»ºæ¨¡äº†ä¸€ä¸ªæ•°æ®åˆ—ï¼Œå®ƒå¯èƒ½å­˜å‚¨é¢å¤–çš„ä¿¡æ¯ï¼Œæ¯”å¦‚ProjectionPushDownä¼˜åŒ–ä¼šè£å‰ªå¤åˆåˆ—çš„è¯»å–ï¼Œå®ƒä»…è¯»å–å¤åˆç»“æ„ä¸­æ‰€éœ€çš„å­åˆ—ï¼ŒHiveè¿æ¥å™¨æŠŠè¿™ä¸ªä¿¡æ¯è®°å½•åœ¨HiveColumnHandleä¸­ã€‚æ³¨æ„è¿™é‡Œçš„Symbolå¹¶ä¸å¯¹åº”æŸä¸ªè¡¨è¾¾å¼ç»“æ„ï¼Œå› ä¸ºè¿™é‡Œå·²ç»æ˜¯Symbolè¡€ç¼˜å…³ç³»çš„æœ€ä¸Šæ¸¸ï¼Œå…¶ä»–Symboléƒ½æ˜¯é€šè¿‡é‡å†™è¡¨è¾¾å¼ç”Ÿæˆçš„ã€‚ FilterNode æ•°æ®è¿‡æ»¤èŠ‚ç‚¹ï¼ŒFilterNodeå¯¹åº”äº†WHEREè¯­å¥çš„æ•°æ®è¿‡æ»¤ç»“æ„ï¼Œå®ƒæ˜¯ä¸€ä¸ªè¡¨è¾¾å¼ï¼Œè®°å½•åœ¨predicateå˜é‡ä¸­ã€‚sourceå˜é‡å­˜å‚¨äº†ä¸Šæ¸¸çš„èŠ‚ç‚¹å¼•ç”¨ã€‚ç»“åˆå‰é¢æåˆ°çš„Symbolæ›¿æ¢æ“ä½œå’Œä¸Šæ¸¸èŠ‚ç‚¹çš„getOutputSymbolså‡½æ•°å¯ä»¥çŸ¥é“ï¼Œåœ¨ç”ŸæˆFilterNodeçš„æ—¶å€™ä¼šæŠŠpredicateè¡¨è¾¾å¼ä¸­çš„å˜é‡æ›¿æ¢æˆä¸Šæ¸¸çš„Symbolï¼Œè¿™æ ·è¡¨è¾¾å¼ç»“æ„å°±ä»ASTç»“æ„è½¬æ¢æˆIRç»“æ„äº†ï¼Œå¼•æ“ä¹Ÿå°±çŸ¥é“è¿™ä¸ªSymbolæ˜¯ä»å“ªé‡Œæ¥çš„ï¼Œæ‰€ä»¥Symbolæœ¬è´¨ä¸Šæ˜¯ä¸€ç§è¡€ç¼˜ä¿¡æ¯ã€‚ ProjectNode æŠ•å½±å˜æ¢èŠ‚ç‚¹ï¼ŒæŠ•å½±å˜æ¢èŠ‚ç‚¹æ˜¯å¾ˆå¸¸è§çš„èŠ‚ç‚¹ï¼Œå¯ä»¥å¯¹åº”SELECTè¯­å¥ä¸­çš„å˜æ¢æ“ä½œï¼Œåœ¨å¼•æ“ä¸­ä¹Ÿæ‰¿æ‹…äº†å¾ˆå¤šéç”¨æˆ·æŒ‡å®šçš„è½¬æ¢æ“ä½œï¼Œè¿™äº›è½¬æ¢ä¹Ÿæ˜¯è¡¨è¾¾å¼ç»“æ„ã€‚å®ƒçš„æ ¸å¿ƒç»“æ„æ˜¯Assignmentsï¼Œå®ƒä½¿ç”¨äº†ä¸€ä¸ªå“ˆå¸Œè¡¨æ¥è®°å½•ä¸æŠ•å½±å˜æ¢è¡¨è¾¾å¼å¯¹åº”çš„è¾“å‡ºåˆ—ï¼ˆåŒæ ·æ˜¯Symbolç±»å‹ï¼‰ ExchangeNode æ•°æ®äº¤æ¢èŠ‚ç‚¹ï¼ŒExchangeNodeç”¨æ¥æè¿°æ•°æ®äº¤æ¢çš„ç›¸å…³å‚æ•°ï¼Œå®ƒçš„åœ°ä½å¾ˆç‰¹","date":"2025-07-09","objectID":"/posts/presto_query_submission/:2:3","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢æäº¤æµç¨‹","uri":"/posts/presto_query_submission/"},{"categories":["Presto"],"content":"æ‰§è¡Œè®¡åˆ’ä¼˜åŒ–çš„ç›®çš„ã€åŸºæœ¬åŸç†å’ŒåŸºç¡€ç®—æ³• ","date":"2025-07-09","objectID":"/posts/presto_query_submission/:3:0","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢æäº¤æµç¨‹","uri":"/posts/presto_query_submission/"},{"categories":["Presto"],"content":"æ‰§è¡Œè®¡åˆ’ä¼˜åŒ–çš„åŸºæœ¬åŸç† SQLä½œä¸ºä¸€é¡¹å›¾çµå¥–çº§åˆ«çš„å‘æ˜ï¼Œå…¶é‡è¦æ„ä¹‰ä¸ä»…åœ¨äºå‘æ˜äº†ä¸€ç§å¯ä»¥ç”¨ä½œæ•°æ®æŸ¥è¯¢çš„è¯­è¨€ï¼Œæ›´é‡è¦çš„æ˜¯å‘æ˜äº†å…³ç³»ä»£æ•°ï¼ˆrelation algebra)è¿™ä¸€å·¥å…·ï¼Œä½¿å¾—è®¡ç®—æœºç†è§£å’Œå¤„ç†æŸ¥è¯¢çš„è¯­ä¹‰æ›´åŠ æ–¹ä¾¿ã€‚SQLæŸ¥è¯¢è¯­è¨€çš„ä¼˜åŒ–ä¹Ÿæ˜¯åŸºäºå…³ç³»ä»£æ•°è¿™ä¸€æ¨¡å‹è¿›è¡Œçš„ã€‚ æ‰€è°“å…³ç³»ä»£æ•°ï¼Œæ˜¯SQLä»è¯­å¥åˆ°æ‰§è¡Œè®¡åˆ’çš„ä¸€ç§ä¸­é—´è¡¨ç¤ºï¼Œé¦–å…ˆå®ƒä¸æ˜¯å•çº¯çš„ASTï¼Œè€Œæ˜¯ä¸€ç§ç»è¿‡è¿›ä¸€æ­¥å¤„ç†å¾—åˆ°çš„ä¸­é—´è¡¨ç¤ºï¼Œå¯ä»¥ç±»æ¯”ä¸€èˆ¬ç¼–ç¨‹è¯­è¨€çš„ä¸­é—´è¯­è¨€ï¼ˆIntermidate representationï¼‰ï¼ŒSQLä¼˜åŒ–çš„æœ¬è´¨æ˜¯å¯¹å…³ç³»ä»£æ•°çš„ä¼˜åŒ–ã€‚ æ€»ç»“ä½¿ç”¨å…³ç³»ä»£æ•°è¿›è¡ŒæŸ¥è¯¢ä¼˜åŒ–çš„è¦ç‚¹ï¼š SQLæŸ¥è¯¢å¯ä»¥è¡¨ç¤ºä¸ºå…³ç³»ä»£æ•° å…³ç³»ä»£æ•°ä½œä¸ºä¸€ç§æ ‘å½¢ç»“æ„ï¼Œå®è´¨ä¸Šä¹Ÿå¯ä»¥è¡¨ç¤ºæŸ¥è¯¢çš„ç‰©ç†å®ç°æ–¹æ¡ˆ å…³ç³»ä»£æ•°å¯ä»¥è¿›è¡Œå±€éƒ¨çš„ç­‰ä»·å˜æ¢ï¼Œå˜æ¢å‰åè¿”å›çš„ç»“æœä¸å˜ï¼Œä½†æ‰§è¡Œçš„æˆæœ¬ä¸åŒ é€šè¿‡å¯»æ‰¾æ‰§è¡Œæˆæœ¬æœ€ä½çš„å…³ç³»ä»£æ•°è¡¨ç¤ºï¼Œå°±å¯ä»¥å°†ä¸€ä¸ªSQLæŸ¥è¯¢ä¼˜åŒ–æˆæ›´ä¸ºé«˜æ•ˆçš„æ–¹æ¡ˆ æ­¤å¤–ï¼Œå¾ˆé‡è¦çš„ä¸€ç‚¹æ˜¯ï¼šå®ç°å…³ç³»ä»£æ•°çš„åŒ–ç®€å’Œä¼˜åŒ–ï¼Œè¦ä¾èµ–æ•°æ®ç³»ç»Ÿçš„ç‰©ç†æ€§è´¨ï¼Œå¦‚å­˜å‚¨è®¾å¤‡çš„ç‰¹æ€§ï¼ˆé¡ºåºè¯»æ€§èƒ½ã€éšæœºè¯»æ€§èƒ½ã€ååé‡ï¼‰ã€å­˜å‚¨å†…å®¹çš„æ ¼å¼å’Œæ’åˆ—ï¼ˆåˆ—å¼å­˜å‚¨ã€è¡Œå¼å­˜å‚¨ã€å¯¹æŸåˆ—è¿›è¡Œåˆ†ç‰‡ï¼‰ã€åŒ…å«çš„å…ƒæ•°æ®å’Œé¢„è®¡ç®—ç»“æœï¼ˆæ˜¯å¦å­˜åœ¨ç´¢å¼•æˆ–ç‰©åŒ–è§†å›¾ï¼‰ã€èšåˆå’Œè®¡ç®—å•å…ƒçš„ç‰¹æ€§ï¼ˆå•çº¿ç¨‹ã€å¹¶å‘è®¡ç®—ã€åˆ†å¸ƒå¼è®¡ç®—ã€ç‰¹æ®ŠåŠ é€Ÿç¡¬ä»¶ï¼‰ã€‚ ç»¼ä¸Šæ‰€è¿°ï¼Œå¯¹SQLæŸ¥è¯¢è¿›è¡Œä¼˜åŒ–ï¼Œæ—¢è¦åœ¨åŸå…ˆé€»è¾‘ç®—å­çš„åŸºç¡€ä¸Šè¿›è¡Œå˜æ¢ï¼Œåˆè¦è€ƒè™‘ç‰©ç†å®ç°çš„ç‰¹æ€§ï¼Œè¿™å°±æ˜¯å¾ˆå¤šæŸ¥è¯¢ç³»ç»Ÿå­˜åœ¨é€»è¾‘æ–¹æ¡ˆå’Œç‰©ç†æ–¹æ¡ˆåŒºåˆ«çš„åŸå› ã€‚åœ¨ä¼˜åŒ–æ—¶ï¼Œå¾€å¾€å­˜åœ¨ä¸€ä¸ªä»é€»è¾‘æ–¹æ¡ˆåˆ°ç‰©ç†æ–¹æ¡ˆè¿›è¡Œå˜æ¢çš„é˜¶æ®µã€‚ äº‹å®ä¸Šï¼Œä»é€»è¾‘æ–¹æ¡ˆåˆ°ç‰©ç†æ–¹æ¡ˆçš„å˜æ¢ä¹Ÿå¯ä»¥åˆ’å½’ä¸ºä¸€ç§å…³ç³»ä»£æ•°çš„ä¼˜åŒ–é—®é¢˜ï¼Œå› ä¸ºå…¶æœ¬è´¨ä¸Šä»ç„¶æ˜¯æŒ‰ç…§ä¸€å®šçš„è§„åˆ™å°†åŸæ¨¡å‹ä¸­çš„å±€éƒ¨ç­‰ä»·åœ°å˜æ¢æˆä¸€ç§å¯ä»¥ç‰©ç†æ‰§è¡Œçš„æ¨¡å‹æˆ–ç®—å­ã€‚ ","date":"2025-07-09","objectID":"/posts/presto_query_submission/:3:1","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢æäº¤æµç¨‹","uri":"/posts/presto_query_submission/"},{"categories":["Presto"],"content":"æ‰§è¡Œè®¡åˆ’ä¼˜åŒ–çš„åŸºæœ¬ç®—æ³• ç°è¡Œä¸»æµçš„æ‰§è¡Œè®¡åˆ’ä¼˜åŒ–ç®—æ³•åˆ†ä¸ºä¸¤ç±»ï¼ŒåŸºäºè§„åˆ™çš„ä¼˜åŒ–ï¼ˆRBOï¼‰å’ŒåŸºäºæˆæœ¬çš„ä¼˜åŒ–ï¼ˆRBOï¼‰ã€‚ åŸºäºè§„åˆ™çš„ä¼˜åŒ–çš„æ€è·¯å¾ˆç®€å•ï¼Œæ ¹æ®ç»éªŒäº‹å…ˆå®šä¹‰ä¸€äº›è§„åˆ™ï¼Œæ¯”å¦‚è°“è¯ä¸‹æ¨ã€topNä¼˜åŒ–ç­‰ï¼Œå°†åŸå…ˆçš„æ‰§è¡Œè®¡åˆ’è½¬æ¢æˆæ–°çš„æ‰§è¡Œè®¡åˆ’ã€‚ åŸºäºè§„åˆ™çš„ä¼˜åŒ–ç®—æ³•åœ¨å®é™…ä½¿ç”¨ä¸­ä»ç„¶é¢å¯¹å¾ˆå¤šé—®é¢˜ï¼š å˜æ¢è§„åˆ™çš„é€‰æ‹©é—®é¢˜ï¼šå“ªäº›è§„åˆ™åº”è¯¥è¢«åº”ç”¨ï¼Ÿä»¥ä»€ä¹ˆé¡ºåºè¢«ä½¿ç”¨ï¼Ÿ å˜æ¢æ•ˆæœè¯„ä¼°çš„é—®é¢˜ï¼šç»è¿‡å˜æ¢çš„æŸ¥è¯¢æ€§èƒ½æ˜¯å¦ä¼šå˜å¥½ï¼Ÿå„ç§å¯èƒ½å¾—æ–¹æ¡ˆé‚£ä¸ªæ›´ä¼˜ï¼Ÿ ç°é˜¶æ®µä¸»æµçš„æ–¹æ³•éƒ½æ˜¯åŸºäºæˆæœ¬ä¼°ç®—çš„æ–¹æ³•ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç»™å®šæŸä¸€å…³ç³»ä»£æ•°ä»£è¡¨çš„æ‰§è¡Œæ–¹æ¡ˆï¼Œå¯¹è¿™ä¸€æ–¹æ¡ˆçš„æ‰§è¡Œæˆæœ¬è¿›è¡Œä¼°ç®—ï¼Œæœ€ç»ˆé€‰æ‹©ä¼°ç®—æˆæœ¬æœ€ä½çš„æ–¹æ¡ˆã€‚è™½ç„¶è¢«ç§°ä¸ºåŸºäºæˆæœ¬çš„ä¼°ç®—ï¼Œä½†è¿™ç±»ç®—æ³•ä»ç„¶éœ€è¦ç»“åˆè§„åˆ™è¿›è¡Œæ–¹æ¡ˆæ¢ç´¢ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒåŸºäºæˆæœ¬çš„æ–¹æ³•å…¶å®æ˜¯é€šè¿‡ä¸æ–­çš„åº”ç”¨è§„åˆ™è¿›è¡Œå˜æ¢å¾—åˆ°æ–°çš„æ‰§è¡Œæ–¹æ¡ˆï¼Œç„¶åå¯¹äºæ–¹æ¡ˆçš„æˆæœ¬ä¼˜åŠ£è¿›è¡Œæœ€ç»ˆé€‰æ‹©ã€‚ Volcano Optimizerï¼ˆç«å±±ä¼˜åŒ–å™¨ï¼‰æ˜¯ä¸€ç§åŸºäºæˆæœ¬çš„ä¼˜åŒ–ç®—æ³•ï¼Œå…¶ç›®çš„æ˜¯åŸºäºä¸€äº›å‡è®¾å’Œå·¥ç¨‹ç®—æ³•çš„å®ç°ï¼Œåœ¨è·å¾—æˆæœ¬è¾ƒä¼˜çš„æ‰§è¡Œè®¡åˆ’çš„åŒæ—¶ï¼Œå¯ä»¥é€šè¿‡å‰ªæå’Œç¼“å­˜ä¸­é—´ç»“æœï¼ˆåŠ¨æ€è§„åˆ’ï¼‰çš„æ–¹æ³•é™ä½è®¡ç®—æ¶ˆè€—ã€‚ æˆæœ¬æœ€ä¼˜å‡è®¾ æˆæœ¬æœ€ä¼˜å‡è®¾æ˜¯ç†è§£Volcano Optimizerå®ç°çš„è¦ç‚¹ä¹‹ä¸€ã€‚è¿™ä¸€å‡è®¾è®¤ä¸ºï¼Œåœ¨æœ€ä¼˜çš„æ–¹æ¡ˆå½“ä¸­ï¼Œå–å±€éƒ¨çš„ç»“æœæ¥çœ‹å…¶æ–¹æ¡ˆä¹Ÿæ˜¯æœ€ä¼˜çš„ã€‚æˆæœ¬æœ€ä¼˜å‡è®¾åˆ©ç”¨äº†è´ªå¿ƒç®—æ³•çš„æ€æƒ³ï¼Œåœ¨è®¡ç®—çš„è¿‡ç¨‹ä¸­ï¼Œå¦‚æœä¸€ä¸ªæ–¹æ¡ˆæ˜¯ç”±å‡ ä¸ªå±€éƒ¨åŒºåŸŸç»„åˆè€Œæˆçš„ï¼Œé‚£ä¹ˆåœ¨è®¡ç®—æ€»æˆæœ¬æ—¶ï¼Œæˆ‘ä»¬åªéœ€è¦è€ƒè™‘æ¯ä¸ªå±€éƒ¨ç›®å‰å·²çŸ¥çš„æœ€ä¼˜æ–¹æ¡ˆå’Œæˆæœ¬å³å¯ã€‚å¯¹äºæˆæœ¬æœ€ä¼˜å‡è®¾çš„æ›´ç›´è§‚çš„æè¿°æ˜¯ï¼Œå¦‚æœå…³ç³»ä»£æ•°å±€éƒ¨çš„æŸä¸ªè¾“å…¥çš„è®¡ç®—æˆæœ¬ä¸Šå‡ï¼Œé‚£ä¹ˆè¿™ä¸€å­æ ‘çš„æ•´ä½“æˆæœ¬è¶‹å‘äºä¸Šå‡ï¼Œåä¹‹ä¼šä¸‹é™ã€‚ä¸Šè¿°å‡è®¾å¯¹äºå¤§éƒ¨åˆ†å…³ç³»ä»£æ•°ç®—å­éƒ½æ˜¯æœ‰æ•ˆçš„ï¼Œä½†æ˜¯å¹¶éç™¾åˆ†ä¹‹ç™¾å‡†ç¡®ã€‚ åŠ¨æ€è§„åˆ’ç®—æ³•ä¸ç­‰ä»·é›†åˆ ç”±äºå¼•å…¥äº†æˆæœ¬æœ€ä¼˜å‡è®¾ï¼Œåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å°±å¯ä»¥å¯¹ä»»æ„å­æ ‘ç›®å‰å·²çŸ¥çš„æœ€ä¼˜æ–¹æ¡ˆå’Œæœ€ä¼˜æˆæœ¬è¿›è¡Œç¼“å­˜ã€‚åœ¨æ­¤åçš„è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œå¦‚æœéœ€è¦åˆ©ç”¨è¿™ä¸€å­æ ‘ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ä¹‹å‰ç¼“å­˜çš„ç»“æœï¼Œè¿™é‡Œåº”ç”¨äº†åŠ¨æ€è§„åˆ™ç®—æ³•çš„æ€æƒ³ã€‚è¦å®ç°è¿™ä¸€ç®—æ³•ï¼Œåªéœ€è¦å»ºç«‹ç¼“å­˜ç»“æœåˆ°å­æ ‘åŒå‘æ˜ å°„ï¼ŒæŸä¸€åˆ»å­æ ‘æ‰€æœ‰å¯èƒ½çš„å˜æ¢æ–¹æ¡ˆç»„æˆçš„é›†åˆç§°ä¸ºç­‰ä»·é›†åˆï¼ˆEquivalent Setï¼‰ï¼Œç­‰ä»·é›†åˆå°†ä¼šç»´æŠ¤è‡ªèº«å…ƒç´ å½“ä¸­å…·æœ‰æœ€ä¼˜æˆæœ¬çš„æ–¹æ¡ˆã€‚ è‡ªåº•å‘ä¸Šå’Œè‡ªé¡¶å‘ä¸‹ è‡ªåº•å‘ä¸Šçš„åŠ¨æ€è§„åˆ’ç®—æ³•æœ€ä¸ºç›´è§‚ï¼šå½“æˆ‘ä»¬è¯•å›¾è®¡ç®—èŠ‚ç‚¹Açš„æœ€ä¼˜æ–¹æ¡ˆæ—¶ï¼Œå…¶å­æ ‘ä¸Šçš„æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”çš„ç­‰ä»·ç»“åˆå’Œæœ€ä¼˜æ–¹æ¡ˆéƒ½å·²ç»è®¡ç®—å®Œæˆäº†ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨AèŠ‚ç‚¹ä¸Šä¸æ–­å¯»æ‰¾å¯ä»¥åº”ç”¨çš„è§„åˆ™ï¼Œå¹¶åˆ©ç”¨å·²ç»è®¡ç®—å¥½çš„å­æ ‘æˆæœ¬è®¡ç®—å‡ºæ¯æ ‘çš„æˆæœ¬ï¼Œå°±å¯ä»¥å¾—åˆ°æœ€ä¼˜æ–¹æ¡ˆã€‚ç„¶è€Œè¿™ä¸€æ–¹æ¡ˆå­˜åœ¨ä»¥ä¸‹éš¾ä»¥è§£å†³çš„é—®é¢˜ï¼š ä¸æ–¹ä¾¿åº”ç”¨å‰ªææŠ€å·§ï¼šåœ¨æŸ¥è¯¢ä¸­å¯èƒ½ä¼šé‡åˆ°åœ¨çˆ¶èŠ‚ç‚¹çš„æŸä¸€ç§æ–¹æ¡ˆæˆæœ¬å¾ˆé«˜ï¼Œåç»­å®Œå…¨æ— éœ€è€ƒè™‘çš„æƒ…å†µï¼Œå°½ç®¡å¦‚æœï¼Œéœ€è¦è¢«åˆ©ç”¨çš„å­è®¡ç®—éƒ½å·²ç»å®Œæˆï¼Œè¿™éƒ¨åˆ†è®¡ç®—ä¸å¯é¿å…ã€‚ éš¾ä»¥å®ç°å¯å‘å¼è®¡ç®—å’Œé™åˆ¶è®¡ç®—å±‚æ•°ï¼šç”±äºç¨‹åºè¦ä¸æ–­é€’å½’åˆ°æœ€åæ‰èƒ½å¾—åˆ°æ¯”è¾ƒå¥½çš„æ–¹æ¡ˆï¼Œå› æ­¤å³ä½¿è®¡ç®—é‡æ¯”è¾ƒå¤§ä¹Ÿæ— æ³•æå‰è·å–åˆ°ä¸€ä¸ªå¯è¡Œçš„æ–¹æ¡ˆå¹¶åœæ­¢è¿è¡Œã€‚ å› æ­¤ï¼ŒVolcano Optimizeré‡‡ç”¨äº†è‡ªé¡¶å‘ä¸‹çš„è®¡ç®—æ–¹æ³•ï¼Œåœ¨è®¡ç®—å¼€å§‹ï¼Œæ¯æ£µå­æ ‘å…ˆæŒ‰ç…§åŸå…ˆçš„æ ·å­è®¡ç®—æˆæœ¬å¹¶ä½œä¸ºåˆå§‹ç»“æœã€‚åœ¨ä¸æ–­åº”ç”¨è§„åˆ™çš„è¿‡ç¨‹ä¸­ï¼Œå¦‚æœå‡ºç°ä¸€ç§æ–°çš„ç»“æ„è¢«åŠ å…¥å½“å‰çš„ç­‰ä»·é›†åˆä¸­ï¼Œä¸”è¿™ç§ç­‰ä»·é›†åˆå…·æœ‰æ›´ä¼˜çš„æˆæœ¬ï¼Œè¿™æ—¶éœ€è¦å‘ä¸Šå†’æ³¡åˆ°æ‰€æœ‰ä¾èµ–è¿™ä¸€å­é›†åˆçš„çˆ¶äº²ç­‰ä»·é›†åˆï¼Œæ›´æ–°é›†åˆé‡Œæ¯ä¸ªå…ƒç´ çš„æˆæœ¬å¹¶å¾—åˆ°æ–°çš„æœ€ä¼˜æˆæœ¬å’Œæ–¹æ¡ˆã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨å‘ä¸Šå†’æ³¡çš„è¿‡ç¨‹ä¸­éœ€è¦éå†çˆ¶äº²é›†åˆå†…çš„æ¯ä¸€ä¸ªæ–¹æ¡ˆï¼Œè¿™æ˜¯å› ä¸ºä¸åŒæ–¹æ¡ˆå¯¹äºæŠ•å…¥æˆæœ¬å˜åŒ–çš„æ•æ„Ÿæ€§ä¸åŒï¼Œä¸èƒ½å‡è®¾ä¹‹å‰çš„æœ€ä¼˜æ–¹æ¡ˆä»ç„¶æ˜¯æœ€ä¼˜çš„ã€‚è‡ªé¡¶å‘ä¸Šçš„æ–¹æ¡ˆå°½ç®¡è§£å†³äº†ä¸€äº›é—®é¢˜ï¼Œä½†æ˜¯ä¹Ÿå¸¦æ¥äº†å¯¹å…³ç³»ä»£æ•°èŠ‚ç‚¹æ“ä½œç¹çã€è¦ä¸æ–­ç»´æŠ¤çˆ¶å­ç­‰ä»·é›†åˆçš„å…³ç³»ç­‰é—®é¢˜ï¼Œå®ç°ç›¸å¯¹å¤æ‚ã€‚æ¨èCalciteã€‚ ","date":"2025-07-09","objectID":"/posts/presto_query_submission/:3:2","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢æäº¤æµç¨‹","uri":"/posts/presto_query_submission/"},{"categories":["Presto"],"content":"æ‰§è¡Œè®¡åˆ’ä¼˜åŒ–çš„è®¾è®¡å®ç° Prestoçš„æ‰§è¡Œè®¡åˆ’åŒ…æ‹¬åˆå§‹é€»è¾‘æ‰§è¡Œè®¡åˆ’ï¼ˆLogicalPlanner.planStatementï¼‰ã€ä¼˜åŒ–åçš„é€»è¾‘æ‰§è¡Œè®¡åˆ’ï¼ˆPlanOptimizer.optimizeï¼‰ã€åˆ†å¸ƒå¼æ‰§è¡Œè®¡åˆ’ï¼ˆPlanFragmenter.createSubPlansï¼‰å’Œç‰©ç†æ‰§è¡Œè®¡åˆ’ï¼ˆLocalExecutionPlanner.planï¼‰4ç§ã€‚ é€»è¾‘æ‰§è¡Œè®¡åˆ’ï¼šå¯¹SQLçš„è¯­æ³•åˆ†ææ ‘è¿›è¡Œç¿»è¯‘ï¼Œè½¬æ¢ä¸­é—´è¡¨è¾¾å¼IRï¼Œè¿™ä¸ªæ—¶å€™å°±æ˜¯ä¸€ä¸ªåˆå§‹é€»è¾‘æ‰§è¡Œè®¡åˆ’ï¼Œå®ƒæ¯”å£°æ˜å¼è¯­è¨€æ›´ä¸°å¯Œã€æ›´åº•å±‚ï¼Œæ¯”å¦‚GROUP BYè¯­å¥ä¼šè¢«ç¿»è¯‘ä¸ºï¼ˆProjectNode/GroupIdNode + AggregationNodeï¼‰ï¼Œè¿™ç§ä¸­é—´ç»“æ„ä½¿å¾—ä¼˜åŒ–å™¨èƒ½å¤Ÿå¾ˆå¥½åœ°å¯¹å®ƒè¿›è¡Œè½¬æ¢ã€‚ ä¼˜åŒ–å™¨ï¼šåˆå§‹é€»è¾‘æ‰§è¡Œè®¡åˆ’å¯ä»¥åšè¿›ä¸€æ­¥çš„ä¼˜åŒ–ï¼Œæ¯”å¦‚è°“è¯ä¸‹æ¨ã€TopNè½¬æ¢ç­‰ï¼Œè¿™ä¸ªæ—¶å€™åˆå§‹é€»è¾‘æ‰§è¡Œè®¡åˆ’å˜æˆä¼˜åŒ–åçš„é€»è¾‘æ‰§è¡Œè®¡åˆ’ã€‚Prestoæœ‰è¿­ä»£å¼ä¼˜åŒ–å™¨å’Œéè¿­ä»£å¼ä¼˜åŒ–å™¨ä¸¤ç§ï¼Œä¸»è¦æ˜¯RBOè§„åˆ™ï¼Œå®ƒä»¬æ ¹æ®PlanOptimizersä¸­å®šä¹‰çš„é¡ºåºæ¥æ‰§è¡Œã€‚è¿™é‡Œå…¶å®è¿˜æœ‰ä¸€ä¸ªåŸºäºCBOçš„ReorderJoinä¼˜åŒ–å™¨ï¼Œå®ƒä½¿ç”¨æŸ¥å¹¶é›†ï¼ˆDisjoint Setï¼‰ç»“æ„è®¡ç®—å‡ºç­‰ä»·çš„JOINç»“æ„ï¼Œç„¶åè¿ç”¨åŠ¨æ€è§„åˆ’ç®—æ³•è®¡ç®—å‡ºæˆæœ¬æœ€ä½çš„JOINé¡ºåºï¼ŒPrestoæ²¡æœ‰åœ¨å…¨å±€çš„ç»´åº¦ç»´æŠ¤å¤šä¸ªæ‰§è¡Œè®¡åˆ’ï¼Œè¿™ä¸€ç‚¹å’Œä¸šç•Œçš„CBOç®—æ³•ä¸ä¸€æ ·ã€‚ åˆ†å¸ƒå¼æ‰§è¡Œè®¡åˆ’å’Œç‰©ç†æ‰§è¡Œè®¡åˆ’ï¼šå› ä¸ºé€»è¾‘æ‰§è¡Œè®¡åˆ’æœ€ç»ˆæ˜¯ä¸ºäº†è½¬æˆç‰©ç†æ‰§è¡Œè®¡åˆ’ï¼Œè€Œä¼˜åŒ–å™¨åœ¨ä¼˜åŒ–çš„è¿‡ç¨‹ä¸­ä¹Ÿä¼šå€ŸåŠ©åº•å±‚çš„ç‰¹æ€§ï¼Œæ‰€ä»¥åœ¨ä¼˜åŒ–çš„è¿‡ç¨‹ä¸­ï¼Œä»é€»è¾‘æ‰§è¡Œè®¡åˆ’åˆ°ç‰©ç†æ‰§è¡Œè®¡åˆ’æ˜¯ä¸€ä¸ªæ¸å˜çš„è¿‡ç¨‹ï¼Œä¸¤è€…ä¹‹é—´æ²¡æœ‰æ˜æ˜¾çš„è¾¹ç•Œã€‚ä¼˜åŒ–åçš„é€»è¾‘è®¡åˆ’å…¶å®æ˜¯ä»‹äºç‰©ç†æ‰§è¡Œè®¡åˆ’å’Œé€»è¾‘æ‰§è¡Œè®¡åˆ’ä¹‹é—´çš„ã€‚Fragmenteræœ€åæŠŠä¼˜åŒ–åçš„é€»è¾‘æ‰§è¡Œè®¡åˆ’è¿›è¡Œåˆ†ç‰‡è½¬æ¢æˆåˆ†å¸ƒå¼æ‰§è¡Œè®¡åˆ’ï¼Œæ¯ä¸ªåˆ†ç‰‡å°±æ˜¯ä¸€ä¸ªæŸ¥è¯¢æ‰§è¡Œé˜¶æ®µï¼Œè¿™ä¸ªFragmentç»“æ„åœ¨æŸ¥è¯¢æ‰§è¡ŒèŠ‚ç‚¹ï¼ˆWorkerï¼‰ä¸Šç¨åŠ è½¬æ¢å°±å˜æˆäº†æœ€ç»ˆçš„ç‰©ç†è®¡åˆ’ã€‚ åœ¨Prestoä¸­æ‰€æœ‰ä¼˜åŒ–å™¨éƒ½éœ€è¦å®ç°PlanOptimizeræ¥å£ï¼Œä¼˜åŒ–å™¨ä¸»è¦åˆ†ä¸ºè¿­ä»£å¼ä¼˜åŒ–å™¨å’Œéè¿­ä»£å¼ä¼˜åŒ–å™¨ï¼ˆç›¸å¯¹äºè¿­ä»£ä¼˜åŒ–å™¨è€Œè¨€ï¼Œå®é™…å®ƒä¸æ˜¯æ˜¾å¼åˆ†ç±»ï¼‰ã€‚è¿­ä»£å¼ä¼˜åŒ–å™¨æŒ‡å¤šä¸ªåŠŸèƒ½ç›¸ä¼¼çš„ä¼˜åŒ–è§„åˆ™ï¼ˆRuleï¼‰ç»„æˆä¸€ä¸ªè§„åˆ™ç»„ï¼Œæ¯ä¸ªè§„åˆ™ä¸“æ³¨ä¸€ç§é€»è¾‘ï¼Œæ¯ä¸ªè§„åˆ™çš„é€»è¾‘ç›¸å¯¹ç®€å•ï¼Œè§„åˆ™ç»„å†…çš„æ‰€æœ‰ä¼˜åŒ–è§„åˆ™ä¼šè¢«å°è¯•åº”ç”¨åˆ°å½“å‰çš„æ‰§è¡Œè®¡åˆ’ï¼Œå¦‚æœæ‰§è¡Œè®¡åˆ’æœ‰æ›´æ–°ï¼Œä¼šä¸æ–­å¾ªç¯åº”ç”¨è§„åˆ™ç»„çš„è§„åˆ™ï¼Œç›´åˆ°æ‰§è¡Œè®¡åˆ’ä¸å†æ”¹å˜ã€‚éè¿­ä»£å¼ä¼˜åŒ–å™¨æ˜¯Prestoé¡¹ç›®æ—©æœŸä½¿ç”¨è¾ƒå¤šçš„ä¼˜åŒ–å™¨ï¼Œè¿™ç±»ä¼˜åŒ–å™¨éƒ½éœ€è¦å®ç°ä¸€ä¸ªå®Œæ•´çš„é€»è¾‘æ‰§è¡Œè®¡åˆ’æ ‘è®¿é—®è€…ï¼ˆvisitorï¼‰å¹¶é€šè¿‡å±‚å±‚éå†å®Œæˆä¼˜åŒ–é€»è¾‘ã€‚å®ƒæ“…é•¿é€šè¿‡è‡ªé¡¶å‘ä¸‹æˆ–è‡ªåº•å‘ä¸Šçš„æ–¹å¼æ¥éå†é€»è¾‘æ‰§è¡Œè®¡åˆ’æ ‘ï¼Œä»è€Œå®Œæˆä¸€äº›é€»è¾‘å¤æ‚ï¼Œéœ€è¦çŠ¶æ€çš„ä¼˜åŒ–æ“ä½œã€‚å¾ˆå¤šç»å…¸çš„ä¼˜åŒ–å™¨ï¼ˆæ¯”å¦‚AddExchangesï¼‰å°±æ˜¯åŸºäºè®¿é—®è€…æ¨¡å¼å®ç°äº†æ’å…¥å…¨å±€æ•°æ®äº¤æ¢èŠ‚ç‚¹ï¼ˆExchangeNodeï¼‰åŠŸèƒ½çš„ã€‚ è¿­ä»£å¼ä¼˜åŒ–å™¨ æ¯ä¸ªä¼˜åŒ–è§„åˆ™éƒ½æœ‰ç‰¹å®šçš„ç›®æ ‡èŠ‚ç‚¹ï¼Œå¯èƒ½è¿˜æœ‰ä¸€äº›é™„åŠ çš„è§¦å‘æ¡ä»¶ï¼Œåˆ¤æ–­å½“å‰æ‰§è¡Œè®¡åˆ’èŠ‚ç‚¹æ˜¯å¦èƒ½åº”ç”¨æŸä¸ªä¼˜åŒ–è§„åˆ™ï¼Œè¿™ç±»éœ€æ±‚ç§°ä¸ºæ¨¡å¼åŒ¹é…ï¼Œå®ƒæä¾›ä¸€ä¸‹å‡ ä¸ªåŠŸèƒ½ï¼š æŒ‡å®šä½œç”¨çš„ç›®æ ‡èŠ‚ç‚¹ æŒ‡å®šèŠ‚ç‚¹éœ€è¦æ»¡è¶³çš„æ¡ä»¶ï¼Œè¿™æ˜¯ä¸€ä¸ªé€»è¾‘è¡¨è¾¾å¼ï¼Œå¯ä»¥ç”±å¤šä¸ªè¯­å¥ç»„æˆä¸€ä¸ªé€»è¾‘ä¸ç»“æ„ æ•è·åŒ¹é…åˆ°çš„èŠ‚ç‚¹ï¼Œæ–¹ä¾¿ä¼˜åŒ–å™¨å¼•ç”¨å®ƒä»¬ public class PushPartialAggregationThroughExchange implements Rule\u003cAggregationNode\u003e { private final Metadata metadata; public PushPartialAggregationThroughExchange(Metadata metadata) { this.metadata = requireNonNull(metadata, \"metadata is null\"); } private static final Capture\u003cExchangeNode\u003e EXCHANGE_NODE = Capture.newCapture(); // patternï¼Œç›®æ ‡èŠ‚ç‚¹ä¸ºAggregationNodeï¼Œä¸”å­èŠ‚ç‚¹ä¸ºExchangeNodeï¼Œä¸”æ•°æ®äº¤æ¢èŠ‚ç‚¹ä¸èƒ½æœ‰æ’åºè¦æ±‚ // å› ä¸ºåœ¨æ’åºæƒ…å†µä¸‹èšåˆå‡½æ•°æ— æ³•è¿›è¡Œé¢„èšåˆä¼˜åŒ–ï¼Œåªèƒ½åœ¨å•ä¸ªä»»åŠ¡ä¸­å®Œæˆèšåˆï¼ŒcaptureAsæ•è·å‘½ä¸­çš„exchangeèŠ‚ç‚¹ private static final Pattern\u003cAggregationNode\u003e PATTERN = aggregation() .with(source().matching( exchange() .matching(node -\u003e node.getOrderingScheme().isEmpty()) .capturedAs(EXCHANGE_NODE))); @Override public Pattern\u003cAggregationNode\u003e getPattern() { return PATTERN; } @Override public Result apply(AggregationNode aggregationNode, Captures captures, Context context) { ExchangeNode exchangeNode = captures.get(EXCHANGE_NODE); boolean decomposable = aggregationNode.isDecomposable(metadata); MemoåŸæœ¬åœ¨Cascacdä¼˜åŒ–ç®—æ³•ä¸­æ˜¯ç”¨æ¥å­˜å‚¨æ¯ä¸ªGroupèŠ‚ç‚¹ä¸‹é¢çš„ç­‰ä»·æ‰§è¡Œè®¡åˆ’ç‰‡æ®µï¼Œå› ä¸ºPrestoå§‹ç»ˆåªæœ‰ä¸€ä¸ªæ‰§è¡Œè®¡åˆ’ï¼Œæ‰€æœ‰Memoçš„ä½œç”¨å˜æˆç»´æŠ¤ä¸€ä¸ªå¯å˜æ‰§è¡Œè®¡åˆ’ï¼ŒåŒæ—¶Memoå¯ä»¥è‡ªåŠ¨è¯†åˆ«éœ€è¦åˆ é™¤çš„èŠ‚ç‚¹ï¼Œå› ä¸ºæ‰§è¡Œè®¡åˆ’åªæœ‰æ›¿æ¢å’Œæ’å…¥æ“ä½œï¼Œä¸ä¼šè¿›è¡Œæ˜¾å¼åˆ é™¤ï¼Œæ‰€ä»¥éœ€è¦ä¸€ç§è¯†åˆ«æœºåˆ¶æ¥åˆ é™¤å·²åºŸå¼ƒçš„èŠ‚ç‚¹ã€‚é€šè¿‡è°ƒç”¨Memo.insertRecursiveè¿›è¡Œåˆå§‹åŒ–ï¼Œæ‰§è¡Œè®¡åˆ’çš„æ¯ä¸ªèŠ‚ç‚¹éƒ½è¢«æ˜ å°„æˆä¸€ä¸ªåˆ†ç»„ï¼Œä¹Ÿå°±æ˜¯Memo.Groupç»“æ„ï¼ŒåŸæœ¬PlanNode.getSourcesè¿”å›çš„ä¸‹æ¸¸èŠ‚ç‚¹ï¼Œåœ¨è¿™é‡Œå…¨éƒ¨æ›¿æ¢æˆè™šæ‹Ÿçš„GroupReferenceèŠ‚ç‚¹ï¼Œé€šè¿‡å®ƒçš„idå¯ä»¥å®šä½åˆ°ä¸€ä¸ªåˆ†ç»„ï¼Œåˆ†ç»„é‡Œå­˜å‚¨ç€å®é™…çš„æ‰§è¡Œè®¡åˆ’èŠ‚ç‚¹ï¼Œé€šè¿‡è¿™ç§æ–¹å¼ï¼ŒèŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»å°±è§£è€¦äº†ï¼Œæ‰€ä»¥Memoçš„æœ¬è´¨æ˜¯ï¼š æŠŠä¸€ä¸ªæ‰§è¡Œè®¡åˆ’å˜æˆä¸€ä¸ªMapç»“æ„ï¼Œé”®æ˜¯åˆ†ç»„çš„idï¼Œå€¼æ˜¯å¯¹åº”çš„GroupèŠ‚ç‚¹ï¼Œé‡Œé¢å­˜å‚¨äº†å®é™…çš„æ‰§è¡Œè®¡åˆ’èŠ‚ç‚¹ æŠŠæ‰§è¡Œè®¡åˆ’èŠ‚ç‚¹çš„ä¸‹æ¸¸èŠ‚ç‚¹æ›¿æ¢æˆGroupReferenceï¼Œè¿™ä¸ªåˆ†ç»„æ˜¯ä¸å˜çš„ï¼Œä½†æ˜¯åˆ†ç»„é‡Œé¢çš„è®¡åˆ’èŠ‚ç‚¹å¯ä»¥å‘ç”Ÿå˜åŒ– IterativeOptimizeråˆå§‹åŒ–æ—¶æ¥æ”¶å¤šä¸ªåŠŸèƒ½ç›¸ä¼¼çš„ä¼˜åŒ–è§„åˆ™ä½œä¸ºå…¥å‚å¹¶æ„å»ºä¸€ä¸ªRuleIndexç»“æ„ï¼Œå®ƒçš„keyæ˜¯ä¼˜åŒ–è§„åˆ™ä¸­PatternåŒ¹é…çš„ç›®æ ‡èŠ‚ç‚¹ï¼Œè¿™æ ·åœ¨éå†æ‰§è¡Œè®¡åˆ’çš„æ—¶å€™å¯ä»¥æ›´é«˜æ•ˆåœ°è¿‡æ»¤æ— å…³è§„åˆ™ã€‚optimizeé€»è¾‘å°†å¾…ä¼˜åŒ–çš„æ‰§è¡Œè®¡åˆ’è½¬æ¢æˆä¸€ä¸ªMemoç»“æ„ï¼Œç„¶åå¯¹æ‰§è¡Œè®¡åˆ’è¿›è¡Œè‡ªé¡¶å‘ä¸‹çš„éå†ï¼Œç”±ä»¥ä¸‹3ä¸­exploreå‡½æ•°è´Ÿè´£ç‰¹å®šèŒƒå›´çš„éå†é€»è¾‘ã€‚ exploreGroupï¼šå®Œæˆå½“å‰èŠ‚ç‚¹æ‰€åœ¨å­æ ‘çš„éå†ï¼Œå®ƒç”±exploreNodeå’ŒexploreChildrenç»„æˆï¼Œå¤–åŠ ä¸€äº›æ¡ä»¶åˆ¤æ–­è¯­å¥ exploreNodeï¼šå¯¹å½“å‰èŠ‚ç‚¹åº”ç”¨è§„åˆ™ç»„çš„æ‰€æœ‰ä¼˜åŒ–è§„åˆ™ exploreChildrenï¼šå¯¹å½“å‰èŠ‚ç‚¹çš„æ‰€æœ‰å­èŠ‚ç‚¹è¿›è¡Œéå† optimizeé‡è½½äº†çˆ¶ç±»çš„æ–¹æ³•ï¼Œnew Memoå‘½ä»¤æŠŠæ‰§è¡Œè®¡åˆ’è½¬æ¢æˆMemoç»“æ„ï¼Œè°ƒç”¨getRootGroupè·å–æ ¹èŠ‚ç‚¹è¿›è¡ŒexploreGroupéå†æ“ä½œã€‚æ³¨æ„æ‰€æœ‰exploreXXXå‡½æ•°éƒ½ä¼šè¿”å›å¸ƒå°”å€¼å˜é‡æ¥æ ‡è¯†æ˜¯å¦æ›´æ–°äº†æ‰§è¡Œè®¡åˆ’ã€‚å¦‚æœæœ‰è¿›å±•ï¼ŒexploreGroupå†…éƒ¨çš„exploreNode+exploreChildrenä¼šé‡æ–°æ‰§è¡Œä¸€æ¬¡ã€‚æ‰§è¡Œè®¡åˆ’æ ‘çš„PlanNodeæœ¬èº«æ˜¯ä¸å¯å˜çš„ï¼Œå¦‚æœå½“å‰èŠ‚ç‚¹æœ‰å˜æ›´ï¼Œéœ€è¦é€’å½’æ›´æ–°æ‰€æœ‰çˆ¶èŠ‚ç‚¹ï¼Œè€Œä¼˜åŒ–å™¨ä¼šé¢‘ç¹å˜æ›´æ‰§è¡Œè®¡åˆ’æ ‘ï¼Œæ‰€ä»¥é€šè¿‡Memoæ¥æ”¯æŒå¯å˜æ‰§è¡Œè®¡åˆ’ã€‚ public class IterativeOptimizer implements PlanOptimizer { private final RuleStatsRecorder stats; private final StatsCalculator statsCalculator; private final CostCalculator costCalculator; private final List\u003cPlanOptimizer\u003e legacyRules; private final RuleIndex ruleIndex; private final Predicate\u003cSession\u003e useLegacyRules; public IterativeOptimizer(RuleStatsRecorder stats, StatsCalculator statsCalculator, CostCalculator costCalculator, Set\u003cRule\u003c?\u003e\u003e rules) { this(stats, statsCalculator, costCalculator, session -\u003e false, ImmutableList.of(), rules); } public IterativeOptimizer(RuleStatsRecorder stats, StatsCalculator statsCalculator, CostCalculator costCalculator, Predicate\u003cSession\u003e useLegacyRules, L","date":"2025-07-09","objectID":"/posts/presto_query_submission/:3:3","tags":["Presto","Trino"],"title":"PrestoæŸ¥è¯¢æäº¤æµç¨‹","uri":"/posts/presto_query_submission/"},{"categories":null,"content":"åœ¨ehcache3ä¸­çœ‹åˆ°ehcache3é€šè¿‡æ–‡ä»¶é”æ¥ä¿è¯å¯¹ç›®å½•çš„å”¯ä¸€æ‹¥æœ‰ public class DefaultLocalPersistenceService implements LocalPersistenceService { private static final Logger LOGGER = LoggerFactory.getLogger(DefaultLocalPersistenceService.class); private final File rootDirectory; private final File lockFile; private FileLock lock; private RandomAccessFile rw; private boolean started; /** * Creates a new service instance using the provided configuration. * * @param persistenceConfiguration the configuration to use */ public DefaultLocalPersistenceService(final DefaultPersistenceConfiguration persistenceConfiguration) { if(persistenceConfiguration != null) { rootDirectory = persistenceConfiguration.getRootDirectory(); } else { throw new NullPointerException(\"DefaultPersistenceConfiguration cannot be null\"); } lockFile = new File(rootDirectory, \".lock\"); } private void internalStart() { if (!started) { createLocationIfRequiredAndVerify(rootDirectory); try { rw = new RandomAccessFile(lockFile, \"rw\"); } catch (FileNotFoundException e) { // should not happen normally since we checked that everything is fine right above throw new RuntimeException(e); } try { lock = rw.getChannel().tryLock(); } catch (OverlappingFileLockException e) { throw new RuntimeException(\"Persistence directory already locked by this process: \" + rootDirectory.getAbsolutePath(), e); } catch (Exception e) { try { rw.close(); } catch (IOException e1) { // ignore silently } throw new RuntimeException(\"Persistence directory couldn't be locked: \" + rootDirectory.getAbsolutePath(), e); } if (lock == null) { throw new RuntimeException(\"Persistence directory already locked by another process: \" + rootDirectory.getAbsolutePath()); } started = true; LOGGER.debug(\"RootDirectory Locked\"); } } /** * {@inheritDoc} */ @Override public synchronized void stop() { if (started) { try { lock.release(); // Closing RandomAccessFile so that files gets deleted on windows and // org.ehcache.internal.persistence.DefaultLocalPersistenceServiceTest.testLocksDirectoryAndUnlocks() // passes on windows rw.close(); try { Files.delete(lockFile.toPath()); } catch (IOException e) { LOGGER.debug(\"Lock file was not deleted {}.\", lockFile.getPath()); } } catch (IOException e) { throw new RuntimeException(\"Couldn't unlock rootDir: \" + rootDirectory.getAbsolutePath(), e); } started = false; LOGGER.debug(\"RootDirectory Unlocked\"); } } ","date":"2025-06-24","objectID":"/posts/file_lock/:0:0","tags":null,"title":"æ–‡ä»¶é”","uri":"/posts/file_lock/"},{"categories":["Java"],"content":"ehcache3æä¾›äº†é™åˆ¶ç¼“å­˜å®¹é‡çš„é€‰æ‹©ï¼Œå¦‚æœå †ä¸Šå­˜å‚¨çš„å®¹é‡è¶…è¿‡äº†æŒ‡å®šçš„å¤§å°ï¼Œåˆ™ä¼šé©±é€ç¼“å­˜ä¸­çš„å…ƒç´ ï¼Œç›´åˆ°æœ‰ç©ºé—´å¯ä»¥å®¹çº³ä¸ºæ­¢ã€‚ è‡ªå·±å®ç°çš„sizeOfEngineåº”è¯¥å®ç°ehcache3æä¾›çš„å¼•æ“æ¥å£ã€‚ public interface SizeOfEngine { /** * Size of the objects on heap including the overhead * * @param key key to be sized * @param holder value holder to be sized * @return size of the objects on heap including the overhead * @throws LimitExceededException if a configured limit is breached */ \u003cK, V\u003e long sizeof(K key, Store.ValueHolder\u003cV\u003e holder) throws LimitExceededException; } ValueHolderåŒ…å«valueä»¥åŠæœ‰å…³çš„å…ƒæ•°æ®ï¼Œæ¯”å¦‚åˆ›å»ºæ—¶é—´ã€è¿‡æœŸæ—¶é—´ç­‰ã€‚ public class DefaultSizeOfEngine implements org.ehcache.core.spi.store.heap.SizeOfEngine { private final long maxObjectGraphSize; private final long maxObjectSize; private final SizeOf sizeOf; private final long chmTreeBinOffset; private final long onHeapKeyOffset; public DefaultSizeOfEngine(long maxObjectGraphSize, long maxObjectSize) { this.maxObjectGraphSize = maxObjectGraphSize; this.maxObjectSize = maxObjectSize; this.sizeOf = SizeOf.newInstance(new SizeOfFilterSource(true).getFilters()); this.onHeapKeyOffset = sizeOf.deepSizeOf(new CopiedOnHeapKey\u003c\u003e(new Object(), new IdentityCopier\u003c\u003e())); this.chmTreeBinOffset = sizeOf.deepSizeOf(ConcurrentHashMap.FAKE_TREE_BIN); } @Override public \u003cK, V\u003e long sizeof(K key, Store.ValueHolder\u003cV\u003e holder) throws org.ehcache.core.spi.store.heap.LimitExceededException { try { return sizeOf.deepSizeOf(new EhcacheVisitorListener(maxObjectGraphSize, maxObjectSize), key, holder) + this.chmTreeBinOffset + this.onHeapKeyOffset; } catch (VisitorListenerException e) { throw new org.ehcache.core.spi.store.heap.LimitExceededException(e.getMessage()); } } } æˆ‘ä»¬åœ¨è®¡ç®—å¯¹è±¡å ç”¨æ—¶ï¼Œç»å¸¸éœ€è¦å¿½ç•¥ä¸€äº›å¯¹è±¡ï¼Œehcache3æä¾›äº†Filteræ¥å£æ¥æ’é™¤å¯¹è±¡ã€‚ /** * Filters all the sizing operation performed by a SizeOfEngine instance * * @author Alex Snaps */ public interface Filter { /** * Adds the class to the ignore list. Can be strict, or include subtypes * * @param clazz the class to ignore * @param strict true if to be ignored strictly, or false to include sub-classes */ // å¿½ç•¥ç±» void ignoreInstancesOf(final Class clazz, final boolean strict); /** * Adds a field to the ignore list. When that field is walked to by the SizeOfEngine, it won't navigate the graph further * * @param field the field to stop navigating the graph at */ // å¿½ç•¥å­—æ®µ void ignoreField(final Field field); } /** * Filter to filter types or fields of object graphs passed to a SizeOf engine * * @author Chris Dennis * @see org.ehcache.sizeof.SizeOf */ public interface SizeOfFilter { /** * Returns the fields to walk and measure for a type * * @param klazz the type * @param fields the fields already \"qualified\" * @return the filtered Set */ Collection\u003cField\u003e filterFields(Class\u003c?\u003e klazz, Collection\u003cField\u003e fields); /** * Checks whether the type needs to be filtered * * @param klazz the type * @return true, if to be filtered out */ boolean filterClass(Class\u003c?\u003e klazz); } /** * Will Cache already visited types */ private class CachingSizeOfVisitor implements ObjectGraphWalker.Visitor { private final WeakIdentityConcurrentMap\u003cClass\u003c?\u003e, Long\u003e cache = new WeakIdentityConcurrentMap\u003c\u003e(); public long visit(final Object object) { Class\u003c?\u003e klazz = object.getClass(); Long cachedSize = cache.get(klazz); if (cachedSize == null) { if (klazz.isArray()) { return sizeOf(object); } else { long size = sizeOf(object); cache.put(klazz, size); return size; } } else { return cachedSize; } } } å®ç°é‡Œæœ‰ä¸€ä¸ªéå¸¸å¸¸ç”¨çš„æ•°æ®ç»“æ„ IdentityHashMap ","date":"2025-06-22","objectID":"/posts/size_of_engine/:0:0","tags":["Java"],"title":"java Size Of Engine","uri":"/posts/size_of_engine/"},{"categories":null,"content":"æé†’ï¼› æœ¬æ–‡æ˜¯å¯¹æ·±å…¥ç†è§£ç¼“å­˜åŸç†ä¸å®æˆ˜è®¾è®¡çš„å­¦ä¹  ","date":"2025-06-21","objectID":"/posts/cache-in-practice/:0:0","tags":null,"title":"ç¼“å­˜å®è·µ","uri":"/posts/cache-in-practice/"},{"categories":null,"content":"Sparké›†ç¾¤å¯ä»¥ä½¿ç”¨ä¸åŒçš„æ–¹å¼è¿›è¡Œéƒ¨ç½²ï¼Œæ¯”å¦‚Standaloneã€Mesos, YARNå’ŒKubernetesï¼Œè¿™å‡ ä¸ªç‰ˆæœ¬çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼šStandaloneç‰ˆæœ¬çš„èµ„æºç®¡ç†å’Œä»»åŠ¡è°ƒåº¦å™¨ç”±Sparkç³»ç»Ÿæœ¬èº«æä¾›ï¼Œå…¶ä»–ç‰ˆæœ¬çš„èµ„æºç®¡ç†å’Œä»»åŠ¡è°ƒåº¦å™¨ä¾èµ–äºç¬¬ä¸‰æ–¹æ¡†æ¶ï¼Œå¦‚YARNå¯ä»¥åŒæ—¶ç®¡ç†Sparkä»»åŠ¡å’ŒHadoop MapReduceä»»åŠ¡ã€‚ Sparké‡‡ç”¨Master-Workerç»“æ„ï¼ŒMasterè´Ÿè´£ç®¡ç†åº”ç”¨å’Œä»»åŠ¡ï¼ŒWorkerèŠ‚ç‚¹è´Ÿè´£æ‰§è¡Œä»»åŠ¡ã€‚ MasterèŠ‚ç‚¹ä¸Šå¸¸é©»Masterè¿›ç¨‹ã€‚è¯¥è¿›ç¨‹è´Ÿè´£ç®¡ç†å…¨éƒ¨çš„WorkerèŠ‚ç‚¹ï¼Œå¦‚å°†Sparkä»»åŠ¡åˆ†é…ç»™WorkerèŠ‚ç‚¹ï¼Œæ”¶é›†WorkerèŠ‚ç‚¹ä¸Šä»»åŠ¡çš„è¿è¡Œä¿¡æ¯ï¼Œç›‘æ§WorkerèŠ‚ç‚¹çš„å­˜æ´»çŠ¶æ€ç­‰ã€‚ WorkerèŠ‚ç‚¹ä¸Šå¸¸é©»Workerè¿›ç¨‹ï¼Œè¯¥è¿›ç¨‹é™¤äº†ä¸MasterèŠ‚ç‚¹é€šä¿¡ï¼Œè¿˜è´Ÿè´£ç®¡ç†Sparkä»»åŠ¡çš„æ‰§è¡Œï¼Œå¦‚å¯åŠ¨Executoræ¥æ‰§è¡Œå…·ä½“çš„Sparkä»»åŠ¡ï¼Œç›‘æ§ä»»åŠ¡è¿è¡ŒçŠ¶æ€ç­‰ã€‚ MasterèŠ‚ç‚¹æ¥æ”¶åˆ°åº”ç”¨åé¦–å…ˆä¼šé€šçŸ¥WorkerèŠ‚ç‚¹å¯åŠ¨Executorï¼Œç„¶ååˆ†é…Sparkè®¡ç®—ä»»åŠ¡ï¼ˆtaskï¼‰åˆ°Executorä¸Šæ‰§è¡Œï¼ŒExecutoræ¥æ”¶åˆ°taskåï¼Œä¸ºæ¯ä¸ªtaskå¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ¥æ‰§è¡Œã€‚ Spark applicationï¼Œå³Sparkåº”ç”¨ï¼ŒæŒ‡çš„æ˜¯ä¸€ä¸ªå¯ä»¥è¿è¡Œçš„Sparkç¨‹åºï¼Œå¦‚WordCount.scalaï¼Œè¯¥ç¨‹åºåŒ…å«mainå‡½æ•°ï¼Œå…¶æ•°æ®å¤„ç†æµç¨‹ä¸€èˆ¬å…ˆä»æ•°æ®æºè¯»å–æ•°æ®ï¼Œå†å¤„ç†æ•°æ®ï¼Œæœ€åè¾“å‡ºç»“æœã€‚åŒæ—¶ï¼Œåº”ç”¨ç¨‹åºä¹ŸåŒ…å«äº†ä¸€äº›é…ç½®å‚æ•°ï¼Œå¦‚éœ€è¦å ç”¨çš„CPUä¸ªæ•°ï¼ŒExecutorå†…å­˜å¤§å°ç­‰ã€‚ Spark Driverï¼Œä¹Ÿå°±æ˜¯Sparké©±åŠ¨ç¨‹åºï¼ŒæŒ‡å®é™…åœ¨è¿è¡ŒSparkåº”ç”¨ä¸­mainå‡½æ•°çš„è¿›ç¨‹ã€‚ Executorï¼Œä¹Ÿç§°ä¸ºSparkæ‰§è¡Œå™¨ï¼Œæ˜¯Sparkè®¡ç®—èµ„æºçš„ä¸€ä¸ªå•ä½ã€‚Sparkå…ˆä»¥Executorä¸ºå•ä½å ç”¨é›†ç¾¤èµ„æºï¼Œç„¶åå¯ä»¥å°†å…·ä½“çš„è®¡ç®—ä»»åŠ¡åˆ†é…ç»™Executoræ‰§è¡Œã€‚ç”±äºSparkæ˜¯ç”±Scalaè¯­è¨€ç¼–å†™çš„ï¼ŒExecutoråœ¨ç‰©ç†ä¸Šæ˜¯ä¸€ä¸ªJVMè¿›ç¨‹ï¼Œå¯ä»¥è¿è¡Œå¤šä¸ªçº¿ç¨‹ï¼ˆè®¡ç®—ä»»åŠ¡ï¼‰ã€‚åœ¨Standaloneç‰ˆæœ¬ä¸­ï¼Œå¯åŠ¨Executorå®é™…ä¸Šæ˜¯å¯åŠ¨äº†ä¸€ä¸ªåå«CoarseGrainedExecutorBackEndçš„JVMè¿›ç¨‹ã€‚ä¹‹æ‰€ä»¥èµ·è¿™ä¹ˆé•¿çš„åå­—ï¼Œæ˜¯ä¸ºäº†ä¸ä¸å…¶ä»–ç‰ˆæœ¬ä¸­çš„Executorè¿›ç¨‹åå†²çªï¼Œå¦‚Mesosã€YARNç­‰ç‰ˆæœ¬ä¼šæœ‰ä¸åŒçš„Executorè¿›ç¨‹åã€‚Workerè¿›ç¨‹å®é™…åªè´Ÿè´£å¯åœå’Œè§‚å¯ŸExecutorçš„æ‰§è¡Œæƒ…å†µã€‚ Taskï¼Œå³Sparkåº”ç”¨çš„è®¡ç®—ä»»åŠ¡ï¼ŒDriveråœ¨è¿è¡ŒSparkåº”ç”¨çš„mainå‡½æ•°æ˜¯ï¼Œä¼šå°†åº”ç”¨æ‹†åˆ†ä¸ºå¤šä¸ªè®¡ç®—ä»»åŠ¡ï¼Œç„¶ååˆ†é…ç»™å¤šä¸ªExecutoræ‰§è¡Œã€‚taskæ˜¯Sparkä¸­æœ€å°çš„è®¡ç®—å•ä½ï¼Œä¸èƒ½å†æ‹†åˆ†ã€‚taskä»¥çº¿ç¨‹æ–¹å¼è¿è¡Œåœ¨Executorè¿›ç¨‹ä¸­ï¼Œæ‰§è¡Œå…·ä½“çš„è®¡ç®—ä»»åŠ¡ï¼Œå¦‚mapç®—å­ã€reduceç®—å­ç­‰ã€‚ç”±äºExecutorå¯ä»¥é…ç½®å¤šä¸ªCPUï¼Œè€Œä¸€ä¸ªtaskä¸€èˆ¬ä½¿ç”¨ä¸€ä¸ªCPUï¼Œå› æ­¤å½“Executorå…·æœ‰å¤šä¸ªCPUæ—¶ï¼Œå¯ä»¥è¿è¡Œå¤šä¸ªtaskã€‚æ¯”å¦‚ä¸€ä¸ªWorkerèŠ‚ç‚¹æœ‰8ä¸ªCPUï¼Œå¯åŠ¨äº†2ä¸ªExecutorï¼Œæ¯ä¸ªExecutorå¯ä»¥å¹¶è¡Œè¿è¡Œ4ä¸ªtaskã€‚Executorçš„æ€»å†…å­˜å¤§å°ç”±ç”¨æˆ·é…ç½®ï¼Œè€Œä¸”Executorçš„å†…å­˜ç©ºé—´ç”±å¤šä¸ªtaskå…±äº«ã€‚ æ¯ä¸ªWorkerè¿›ç¨‹ä¸Šå­˜åœ¨ä¸€ä¸ªæˆ–è€…å¤šä¸ªExecutorRunnerå¯¹è±¡ï¼Œæ¯ä¸ªExecutorRunnerå¯¹è±¡ç®¡ç†ä¸€ä¸ªExecutorã€‚ExecutoræŒæœ‰ä¸€ä¸ªçº¿ç¨‹æ± ï¼Œæ¯ä¸ªçº¿ç¨‹æ‰§è¡Œä¸€ä¸ªtaskã€‚Workerè¿›ç¨‹é€šè¿‡æŒæœ‰çš„ExecutorRunnerå¯¹è±¡æ¥æ§åˆ¶CoarseGrainedExecutorBackendè¿›ç¨‹çš„å¯åœã€‚æ¯ä¸ªSparkåº”ç”¨å¯åŠ¨ä¸€ä¸ªDriverå’Œå¤šä¸ªExecutorï¼Œæ¯ä¸ªExecutoré‡Œé¢è¿è¡Œçš„taskéƒ½å±äºåŒä¸€ä¸ªSparkåº”ç”¨ã€‚ ","date":"2025-06-08","objectID":"/posts/spark_standalone/:0:0","tags":null,"title":"Spark Standalone","uri":"/posts/spark_standalone/"},{"categories":["Spark"],"content":"Sparkçš„é”™è¯¯å®¹å¿æœºåˆ¶çš„æ ¸å¿ƒæ–¹æ³•ä¸»è¦æœ‰ä¸¤ç§ï¼š é€šè¿‡é‡æ–°æ‰§è¡Œè®¡ç®—ä»»åŠ¡æ¥å®¹å¿é”™è¯¯ï¼Œå½“jobæŠ›å‡ºå¼‚å¸¸ä¸èƒ½ç»§ç»­æ‰§è¡Œæ—¶ï¼Œé‡æ–°å¯åŠ¨è®¡ç®—ä»»åŠ¡ï¼Œå†æ¬¡æ‰§è¡Œ é€šè¿‡é‡‡ç”¨checkpointæœºåˆ¶ï¼Œå¯¹ä¸€äº›é‡è¦çš„è¾“å…¥ã€è¾“å‡ºã€ä¸­é—´æ•°æ®è¿›è¡ŒæŒä¹…åŒ–ï¼Œè¿™å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šè§£å†³æ•°æ®ä¸¢å¤±é—®é¢˜ï¼Œè€Œä¸”èƒ½å¤Ÿæé«˜ä»»åŠ¡é‡æ–°è®¡ç®—æ—¶çš„æ•ˆç‡ã€‚ Sparké‡‡ç”¨äº†å»¶è¿Ÿåˆ é™¤ç­–ç•¥ï¼Œå°†ä¸Šæ¸¸stageçš„Shuffle Writeçš„ç»“æœå†™å…¥æœ¬åœ°ç£ç›˜ï¼Œåªæœ‰åœ¨å½“å‰jobå®Œæˆåï¼Œæ‰åˆ é™¤Shuffle Writreå†™å…¥ç£ç›˜çš„æ•°æ®ã€‚è¿™æ ·ï¼Œå³ä½¿stage2ä¸­æŸä¸ªtaskæ‰§è¡Œå¤±è´¥ï¼Œä½†ç”±äºä¸Šæ¸¸çš„stage0å’Œstage1çš„è¾“å‡ºæ•°æ®è¿˜åœ¨ç£ç›˜ä¸Šï¼Œä¹Ÿå¯ä»¥å†æ¬¡é€šè¿‡Shuffle Readè¯»å–å¾—åˆ°ç›¸åŒçš„æ•°æ®ï¼Œé¿å…å†æ¬¡æ‰§è¡Œä¸Šæ¸¸stageä¸­çš„taskï¼Œæ‰€ä»¥ï¼ŒSparkæ ¹æ®ShuffleDependencyåˆ‡åˆ†å‡ºçš„stageæ—¢ä¿è¯äº†taskçš„ç‹¬ç«‹æ€§ï¼Œä¹Ÿæ–¹ä¾¿äº†é”™è¯¯å®¹å¿çš„é‡æ–°è®¡ç®—ã€‚ Sparké‡‡ç”¨äº†lineageæ¥ç»Ÿä¸€å¯¹RDDçš„æ•°æ®å’Œè®¡ç®—ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œä½¿ç”¨å›æº¯æ–¹æ³•è§£å†³ä»å“ªé‡Œå¼€å§‹è®¡ç®—ï¼Œä»¥åŠè®¡ç®—ä»€ä¹ˆçš„é—®é¢˜ã€‚ ä¸ºäº†æé«˜é‡æ–°è®¡ç®—çš„æ•ˆç‡ï¼Œä¹Ÿä¸ºäº†æ›´å¥½çš„è§£å†³æ•°æ®ä¸¢å¤±é—®é¢˜ï¼ŒSparké‡‡ç”¨äº†æ£€æŸ¥ç‚¹ï¼ˆcheckpointï¼‰æœºåˆ¶ã€‚è¯¥æœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†è®¡ç®—è¿‡ç¨‹æ±‡æ€»æŸäº›é‡è¦æ•°æ®è¿›è¡ŒæŒä¹…åŒ–ï¼Œè¿™æ ·åœ¨å†æ¬¡ä¹‹å¿ƒæ—¶å¯ä»¥ä»æ£€æŸ¥ç‚¹æ‰§è¡Œï¼Œä»è€Œå‡å°‘é‡æ–°è®¡ç®—çš„å¼€é”€ã€‚ éœ€è¦è¢«checkpointçš„RDDæ»¡è¶³çš„ç‰¹å¾æ˜¯ï¼ŒRDDçš„æ•°æ®ä¾èµ–å…³ç³»æ¯”è¾ƒå¤æ‚ä¸”é‡æ–°è®¡ç®—ä»£ä»·è¾ƒé«˜ï¼Œå¦‚å…³è”çš„æ•°æ®è¿‡å¤šï¼Œè®¡ç®—é“¾è¿‡é•¿ï¼Œè¢«å¤šæ¬¡é‡å¤ä½¿ç”¨ã€‚ checkpointçš„ç›®çš„æ˜¯å¯¹é‡è¦æ•°æ®è¿›è¡ŒæŒä¹…åŒ–ï¼Œåœ¨èŠ‚ç‚¹å®•æœºæ—¶ä¹Ÿèƒ½å¤Ÿæ¢å¤ï¼Œå› æ­¤éœ€è¦å¯é å­˜å‚¨ï¼Œå¦å¤–ï¼Œcheckpointçš„æ•°æ®é‡å¯èƒ½å¾ˆå¤§ï¼Œå› æ­¤éœ€è¦è¾ƒå¤§çš„å­˜å‚¨ç©ºé—´ï¼Œæ‰€ä»¥ä¸€èˆ¬ä½¿ç”¨åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿæ¥å­˜å‚¨ï¼Œæ¯”å¦‚HDFSæˆ–è€…Alluxioã€‚ åœ¨Sparkä¸­ï¼Œæä¾›äº†spackContext.setCheckpointDir(directory)æ¥å£æ¥è®¾ç½®checkpointçš„å­˜å‚¨è·¯å¾„ï¼ŒåŒæ—¶ï¼Œæä¾›äº†rdd.checkpointæ¥å®ç°checkpointã€‚ ç”¨æˆ·è®¾ç½®rdd.checkpointååªæ ‡è®°æŸä¸ªRDDéœ€è¦æŒä¹…åŒ–ï¼Œè®¡ç®—è¿‡ç¨‹ä¹Ÿåƒæ­£å¸¸ä¸€æ ·è®¡ç®—ï¼Œç­‰åˆ°å½“å‰jobè®¡ç®—ç»“æŸæ—¶å†é‡æ–°å¯åŠ¨è¯¥jobè®¡ç®—ä¸€éï¼Œå¯¹å…¶ä¸­éœ€è¦checkpointçš„RDDè¿›è¡ŒæŒä¹…åŒ–ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“å‰jobç»“æŸåä¼šå¦å¤–å¯åŠ¨ä¸“é—¨çš„jobå»å®Œæˆcheckpointï¼Œéœ€è¦checkpointçš„RDDä¼šè¢«è®¡ç®—ä¸¤æ¬¡ã€‚æ˜¾ç„¶ï¼Œcheckpointå¯åŠ¨é¢å¤–jobæ¥è¿›è¡ŒæŒä¹…åŒ–ä¼šå¢åŠ è®¡ç®—å¼€é”€ï¼Œä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒSparkæ¨èç”¨æˆ·å°†éœ€è¦è¢«checkpointçš„æ•°æ®å…ˆè¿›è¡Œç¼“å­˜ï¼Œè¿™æ ·é¢å¤–å¯åŠ¨çš„ä»»åŠ¡åªéœ€è¦å°†ç¼“å­˜æ•°æ®è¿›è¡Œcheckpointå³å¯ã€‚ RDDéœ€è¦ç»è¿‡Initialized -\u003e checkpointingInProgress -\u003e Checkpointedè¿™ä¸‰ä¸ªé˜¶æ®µæ‰èƒ½çœŸæ­£çš„è¢«checkpoint Initialized å½“åº”ç”¨ç¨‹åºä½¿ç”¨rdd.checkpointè®¾å®šæŸä¸ªRDDéœ€è¦è¢«checkpointæ˜¯ï¼ŒSparkä¸ºè¯¥RDDæ·»åŠ ä¸€ä¸ªcheckpointDataå±æ€§ï¼Œç”¨æ¥ç®¡ç†è¯¥RDDç›¸å…³çš„checkpointä¿¡æ¯ CheckpointingInProgress å½“å‰jobç»“æŸåï¼Œä¼šè°ƒç”¨è¯¥jobæœ€åä¸€ä¸ªRDDçš„doCheckpointæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ ¹æ®finalRDDçš„computing chainå›æº¯æ‰«æï¼Œé‡åˆ°éœ€è¦è¢«checkpointçš„RDDå°±å°†å…¶æ ‡è®°ä¸ºCheckpointingInProgressã€‚ä¹‹åSparkä¼šè°ƒç”¨runJobå†æ¬¡æäº¤ä¸€ä¸ªjobå®Œæˆcheckpoint Checkpointed å†æ¬¡æäº¤çš„jobå¯¹RDDå®Œæˆcheckpointåï¼Œsparkä¼šå»ºç«‹ä¸€ä¸ªæ–°çš„newRDDï¼Œç±»å‹ä¸ºReliableCheckpointRDDï¼Œç”¨äºè¡¨ç¤ºè¢«checkpointåˆ°ç£ç›˜ä¸Šçš„RDDï¼Œå’ŒåŸå…ˆçš„RDDå…³è”ï¼Œå¹¶ä¸”åˆ‡æ–­RDDçš„lineageï¼Œæ•°æ®å·²ç»è¿›è¡Œäº†æŒä¹…åŒ–ï¼Œä¸å†éœ€è¦lineageã€‚ å½“å¯¹æŸä¸ªRDDåŒæ—¶è¿›è¡Œç¼“å­˜å’Œcheckpointæ—¶ï¼Œä¼šå¯¹å…¶å…ˆè¿›è¡Œç¼“å­˜ï¼Œç„¶åå†æ¬¡å¯åŠ¨jobå¯¹å…¶è¿›è¡Œcheckpointã€‚å¦‚æœå•çº¯æ˜¯ä¸ºäº†é™ä½job lineageçš„å¤æ‚ç¨‹åº¦è€Œä¸æ˜¯ä¸ºäº†æŒä¹…åŒ–ï¼ŒSparkæä¾›äº†localCheckpoointæ“ä½œï¼ŒåŠŸèƒ½ä¸Šç­‰ä»·äºæ•°æ®ç¼“å­˜åŠ ä¸Šcheckpointåˆ‡æ–­lineageçš„åŠŸèƒ½ã€‚ checkpintå’Œæ•°æ®ç¼“å­˜çš„åŒºåˆ« ç›®çš„ä¸åŒï¼Œæ•°æ®ç¼“å­˜çš„ç›®çš„æ˜¯åŠ é€Ÿè®¡ç®—ï¼Œå³åŠ é€Ÿåç»­è¿è¡Œçš„jobã€‚è€Œcheckpintçš„ç›®çš„æ˜¯åœ¨jobè¿è¡Œå¤±è´¥åèƒ½å¤Ÿå¿«é€Ÿå›å¤ï¼Œä¹Ÿå°±æ˜¯åŠ é€Ÿå½“å‰éœ€è¦é‡æ–°è¿è¡Œçš„job å­˜å‚¨æ€§è´¨å’Œä½ç½®ä¸åŒã€‚æ•°æ®ç¼“å­˜æ˜¯ä¸ºäº†è¯»å†™é€Ÿåº¦å¿«ï¼Œå› æ­¤ä¸»è¦ä½¿ç”¨å†…å­˜ï¼Œå¶å°”ä½¿ç”¨ç£ç›˜ä½œä¸ºå­˜å‚¨ç©ºé—´ã€‚è€Œcheckpointæ˜¯ä¸ºäº†èƒ½å¤Ÿå¯é è¯»å†™ï¼Œå› æ­¤ä¸»è¦ä½¿ç”¨åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿä½œä¸ºå­˜å‚¨ç©ºé—´ å†™å…¥é€Ÿåº¦å’Œè§„åˆ™ä¸åŒã€‚æ•°æ®ç¼“å­˜é€Ÿåº¦è¾ƒå¿«ï¼Œå¯¹jobçš„æ‰§è¡Œæ—¶é—´å½±å“è¾ƒå°ï¼Œå› æ­¤å¯ä»¥åœ¨jobè¿è¡Œæ—¶è¿›è¡Œç¼“å­˜ï¼Œè€Œcheckpointå†™å…¥é€Ÿåº¦æ…¢ï¼Œä¸ºäº†å‡å°‘å¯¹å½“å‰jobçš„æ—¶å»¶å½±å“ï¼Œä¼šé¢å¤–å¯åŠ¨ä¸“é—¨çš„jobè¿›è¡ŒæŒä¹…åŒ– å¯¹lineageçš„å½±å“ä¸åŒï¼Œå¯¹æŸä¸ªRDDè¿›è¡Œç¼“å­˜åï¼Œå¯¹è¯¥RDDçš„lineageæ²¡æœ‰å½±å“ï¼Œè¿™æ ·å¦‚æœç¼“å­˜åçš„RDDä¸¢å¤±è¿˜å¯ä»¥é‡æ–°è®¡ç®—å¾—åˆ°ï¼Œè€Œå¯¹æŸä¸ªRDDè¿›è¡Œcheckpointä»¥åï¼Œä¼šåˆ‡æ–­è¯¥RDDçš„lineageï¼Œå› ä¸ºè¯¥RDDå·²ç»è¢«å¯é å­˜å‚¨ï¼Œæ‰€ä»¥ä¸éœ€è¦å†ä¿ç•™è¯¥RDDæ˜¯å¦‚ä½•è®¡ç®—å¾—åˆ°çš„ã€‚ åº”ç”¨åœºæ™¯ä¸åŒã€‚æ•°æ®ç¼“å­˜é€‚ç”¨äºä¼šè¢«å¤šæ¬¡è¯»å–ï¼Œå ç”¨ç©ºé—´ä¸æ˜¯éå¸¸å¤§çš„RDDï¼Œè€Œcheckpointé€‚ç”¨äºæ•°æ®ä¾èµ–å…³ç³»æ¯”è¾ƒå¤æ‚ï¼Œé‡æ–°è®¡ç®—ä»£ä»·è¾ƒé«˜çš„RDDï¼Œå¦‚å…³è”çš„æ•°æ®è¿‡å¤šã€è®¡ç®—é“¾è¿‡é•¿ã€è¢«å¤šæ¬¡é‡å¤ä½¿ç”¨ç­‰ã€‚ ","date":"2025-06-07","objectID":"/posts/spark_fault_tolerance/:0:0","tags":["Spark"],"title":"Sparké”™è¯¯å®¹å¿æœºåˆ¶","uri":"/posts/spark_fault_tolerance/"},{"categories":null,"content":"ç¼“å­˜æœºåˆ¶å®é™…ä¸Šæ˜¯ä¸€ç§ç©ºé—´æ¢æ—¶é—´çš„æ–¹æ³•ï¼Œé›†ä½“çš„ï¼Œå¦‚æœæ•°æ®æ»¡è¶³ä¸€ä¸‹3æ¡ï¼Œå°±å¯ä»¥è¿›è¡Œç¼“å­˜. ä¼šè¢«é‡å¤ä½¿ç”¨çš„æ•°æ®ã€‚æ›´ç¡®åˆ‡åœ°ï¼Œä¼šè¢«å¤šä¸ªjobå…±äº«ä½¿ç”¨çš„æ•°æ®ã€‚è¢«å…±äº«ä½¿ç”¨çš„æ¬¡æ•°è¶Šå¤šï¼Œé‚£ä¹ˆç¼“å­˜è¯¥æ•°æ®çš„æ€§ä»·æ¯”è¶Šé«˜ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¯¹äºè¿­ä»£å‹å’Œäº¤äº’å‹åº”ç”¨éå¸¸é€‚åˆã€‚ æ•°æ®ä¸å®œè¿‡å¤§ã€‚è¿‡å¤§ä¼šå ç”¨å¤§é‡çš„å­˜å‚¨ç©ºé—´ï¼Œå¯¼è‡´å†…å­˜ä¸è¶³ï¼Œä¹Ÿä¼šé™ä½æ•°æ®è®¡ç®—æ—¶å¯ä½¿ç”¨çš„ç©ºé—´ã€‚è™½ç„¶ç¼“å­˜æ•°æ®è¿‡å¤§æ—¶ä¹Ÿå¯ä»¥å­˜æ”¾åˆ°ç£ç›˜ï¼Œä½†ç£ç›˜çš„I/Oä»£ä»·æ¯”è¾ƒé«˜ï¼Œæœ‰æ—¶ç”šè‡³ä¸å¦‚é‡æ–°è®¡ç®—å—ã€‚ éé‡å¤ç¼“å­˜çš„æ•°æ®ã€‚é‡å¤ç¼“å­˜çš„æ„æ€æ˜¯å¦‚æœç¼“å­˜äº†æŸä¸ªRDDï¼Œé‚£ä¹ˆè¯¥RDDé€šè¿‡OneToOneDependencyè¿æ¥çš„parent RDDå°±ä¸éœ€è¦è¢«ç¼“å­˜äº†ï¼Œé™¤éæœ‰jobä¸ä½¿ç”¨ç¼“å­˜çš„RDDï¼Œè€Œç›´æ¥ä½¿ç”¨parent RDDã€‚ åŒ…å«æ•°æ®ç¼“å­˜æ“ä½œçš„åº”ç”¨æ‰§è¡Œæµç¨‹ç”Ÿæˆçš„è§„åˆ™ï¼šSparké¦–å…ˆå‡è®¾åº”ç”¨æ²¡æœ‰æ•°æ®ç¼“å­˜ï¼Œæ­£å¸¸ç”Ÿæˆé€»è¾‘å¤„ç†æµç¨‹ï¼ˆRDDä¹‹é—´çš„æ•°æ®ä¾èµ–å…³ç³»ï¼‰ï¼Œç„¶åä»ç¬¬2ä¸ªjobå¼€å§‹ï¼Œå°†cached RDD ä¹‹å‰çš„RDDéƒ½å»æ‰ï¼Œå¾—åˆ°å‰Šå‡åçš„é€»è¾‘å¤„ç†æµç¨‹ã€‚æœ€åï¼Œå°†é€»è¾‘å¤„ç†æµç¨‹è½¬åŒ–ä¸ºç‰©ç†æ‰§è¡Œè®¡åˆ’ã€‚ Sparkä»ä¸‰ä¸ªæ–¹é¢è€ƒè™‘äº†ç¼“å­˜çº§åˆ«ï¼ˆStorage Levelï¼‰ï¼Œåˆ†åˆ«æ˜¯å­˜å‚¨å¤–ç½®ã€æ˜¯å¦åºåˆ—åŒ–å­˜å‚¨ã€æ˜¯å¦å°†ç¼“å­˜æ•°æ®è¿›è¡Œå¤‡ä»½ã€‚ç¼“å­˜çº§åˆ«é’ˆå¯¹çš„æ˜¯RDDä¸­çš„æ‰€æœ‰åˆ†åŒºï¼Œå³å¯¹RDDä¸­æ¯ä¸ªåˆ†åŒºä¸­çš„æ•°æ®éƒ½è¿›è¡Œç¼“å­˜ã€‚å¯¹äºMEMORY_ONLYçº§åˆ«æ¥è¯´ï¼Œåªä½¿ç”¨å†…å­˜è¿›è¡Œç¼“å­˜ï¼Œå¦‚æœæŸä¸ªåˆ†åŒºåœ¨å†…å­˜ä¸­å­˜æ”¾ä¸ä¸‹ï¼Œå°±ä¸å¯¹è¯¥åˆ†åŒºè¿›è¡Œç¼“å­˜ã€‚å½“åç»­jobä¸­çš„taskè®¡ç®—éœ€è¦è¿™ä¸ªåˆ†åŒºä¸­çš„æ•°æ®æ—¶ï¼Œéœ€è¦é‡æ–°è®¡ç®—å¾—åˆ°è¯¥åˆ†åŒºã€‚ rdd.cacheåªæ˜¯å¯¹RDDè¿›è¡Œç¼“å­˜æ ‡è®°ï¼Œä¸æ˜¯ç«‹å³æ‰§è¡Œçš„ï¼Œå®é™…åœ¨actionæ“ä½œçš„jobè®¡ç®—è¿‡ç¨‹ä¸­è¿›è¡Œç¼“å­˜ï¼Œå½“éœ€è¦ç¼“å­˜çš„RDDä¸­çš„recordè¢«è®¡ç®—å‡ºæ¥æ—¶ï¼ŒåŠæ—¶è¿›è¡Œç¼“å­˜ï¼Œå†è¿›è¡Œä¸‹ä¸€æ­¥æ“ä½œã€‚ åœ¨å®ç°ä¸­ï¼ŒSparkåœ¨æ¯ä¸ªexecutorè¿›è¡Œä¸­åˆ†é…ä¸€ä¸ªåŒºåŸŸï¼Œä»¥è¿›è¡Œæ•°æ®ç¼“å­˜ï¼Œè¯¥åŒºåŸŸç”±BlockManageræ¥ç®¡ç†ã€‚å‡è®¾æœ‰ä¸¤ä¸ªtaskï¼Œ task0å’Œtask1è¿è¡Œåœ¨åŒä¸€ä¸ªexecutorè¿›ç¨‹ä¸­ï¼Œå¯¹äºtask0ï¼Œå½“è®¡ç®—å‡ºpartition0åï¼Œå°†partition0å­˜æ”¾åˆ°BlockManagerä¸­çš„memoryStoreå†…ã€‚memoryStoreåŒ…å«äº†ä¸€ä¸ªLinkedHashMapï¼Œç”¨äºå­˜å‚¨RDDçš„åˆ†åŒºã€‚è¯¥LinkedHashMapä¸­çš„Keyæ˜¯blockIdï¼Œå³rddId + partitionIdï¼ŒValueæ˜¯åˆ†åŒºä¸­çš„æ•°æ®ï¼ŒLinkedHashMapåŸºäºåŒå‘é“¾è¡¨ã€‚ å¦‚æœéœ€è¦è®¿é—®ç¼“å­˜çš„åˆ†åŒºï¼Œå¦‚æœåˆ†åŒºåœ¨æœ¬åœ°ï¼Œç›´æ¥è¯»å–å³å¯ï¼Œå¦åˆ™éœ€è¦é€šè¿‡è¿œç¨‹è®¿é—®ï¼Œä¹Ÿå°±æ˜¯é€šè¿‡getRemoteè¯»å–ï¼Œè¿œç¨‹è®¿é—®éœ€è¦å¯¹æ•°æ®è¿›è¡Œåºåˆ—åŒ–å’Œååºåˆ—åŒ–ï¼Œè¿œç¨‹è¯»å–æ—¶æ˜¯ä¸€æ¡æ¡recordè¯»å–ï¼Œå¹¶å¾—åˆ°åŠæ—¶å¤„ç†çš„ã€‚ Sparkæä¾›äº†é€šç”¨çš„ç¼“å­˜æ“ä½œrdd.persistå’Œrdd.unpersistç”¨æ¥ç¼“å­˜å’Œå›æ”¶ç¼“å­˜æ•°æ®ï¼Œä¸ç®¡persistå’Œunpersistéƒ½åªèƒ½é’ˆå¯¹ç”¨æˆ·å¯è§çš„RDDè¿›è¡Œæ“ä½œï¼ŒSparké¢å¤–ç”Ÿæˆçš„rddä¸èƒ½è¢«ç”¨æˆ·æ“ä½œã€‚ Sparké‡‡ç”¨LRUæ›¿æ¢ç®—æ³•ï¼Œç”±äºSparkæ¯è®¡ç®—ä¸€ä¸ªrecordå°±è¿›è¡Œå­˜å‚¨ï¼Œå› æ­¤åœ¨ç¼“å­˜ç»“æŸå‰ï¼ŒSparkä¸èƒ½é¢„çŸ¥è¯¥RDDéœ€è¦çš„å­˜å‚¨ç©ºé—´ï¼Œæ‰€ä»¥Sparké‡‡ç”¨åŠ¨æ€æ›¿æ¢ç­–ç•¥ï¼Œåœ¨å½“å‰å¯ç”¨å†…å­˜ç©ºé—´ä¸è¶³æ—¶ï¼Œæ¯æ¬¡é€šè¿‡LRUæ›¿æ¢ä¸€ä¸ªæˆ–å¤šä¸ªRDDï¼ˆå…·ä½“æ•°ç›®ä¸ä¸€ä¸ªåŠ¨æ€çš„é˜ˆå€¼æœ‰å…³ï¼‰ï¼Œå¦‚æœæ›¿æ¢æ‰æ‰€æœ‰æ—§çš„RDDéƒ½å­˜ä¸ä¸‹æ–°çš„RDDï¼Œé‚£ä¹ˆéœ€è¦åˆ†ä¸¤ç§æƒ…å†µå¤„ç†ï¼Œå¦‚æœæ–°çš„RDDçš„å­˜å‚¨çº§åˆ«åŒ…å«ç£ç›˜ï¼Œé‚£ä¹ˆå¯ä»¥å°†æ–°çš„RDDå­˜æ”¾åˆ°ç£ç›˜ï¼Œå¦‚æœæ–°çš„RDDçš„å­˜å‚¨çº§åˆ«åªæ˜¯å†…å­˜ï¼Œé‚£ä¹ˆå°±ä¸å­˜å‚¨è¯¥RDDï¼ŒSparkç›´æ¥åˆ©ç”¨LinkedHashMapè‡ªå¸¦çš„LRUåŠŸèƒ½å®ç°ç¼“å­˜æ›¿æ¢ã€‚æ­¤å¤–ï¼Œåœ¨è¿›è¡Œç¼“å­˜æ›¿æ¢æ—¶ï¼ŒRDDçš„åˆ†åŒºæ•°æ®ä¸èƒ½è¢«è¯¥RDDçš„å…¶ä»–åˆ†åŒºæ•°æ®æ›¿æ¢ã€‚ åœ¨Sparkä¸­å¯ä»¥é€šè¿‡unpersistä¸»åŠ¨å›æ”¶ç¼“å­˜æ•°æ®ï¼Œä¸åŒäºpersitçš„å»¶è¿Ÿç”Ÿæ•ˆï¼Œunpersistæ“ä½œæ˜¯ç«‹å³ç”Ÿæ•ˆçš„ï¼Œç”¨æˆ·è¿˜å¯ä»¥è®¾å®šunpersistæ˜¯åŒæ­¥é˜»å¡è¿˜æ˜¯å¼‚æ­¥æ‰§è¡Œï¼Œå¦‚unpersist(blocking=true)è¡¨ç¤ºåŒæ­¥é˜»å¡ï¼Œå³ç¨‹åºéœ€è¦ç­‰å¾…unpersitç»“æŸåå†è¿›è¡Œä¸‹ä¸€æ­¥æ“ä½œï¼Œè¿™ä¹Ÿæ˜¯Sparkçš„é»˜è®¤è®¾å®šï¼Œè€Œunpersist(blocking=false)è¡¨ç¤ºå¼‚æ­¥æ‰§è¡Œï¼Œå³è¾¹æ‰§è¡Œunpersistè¾¹è¿›è¡Œä¸‹ä¸€æ­¥æ“ä½œã€‚ å½“å‰çš„ç¼“å­˜æœºåˆ¶åªèƒ½ç”¨åœ¨æ¯ä¸ªSparkåº”ç”¨å†…éƒ¨ï¼Œå³ç¼“å­˜æ•°æ®åªèƒ½åœ¨jobä¹‹é—´å…±äº«ï¼Œä¸èƒ½åœ¨åº”ç”¨ä¹‹é—´å…±äº«ï¼ŒSparkç ”ç©¶è€…åç»­å¼€å‘äº†åˆ†å¸ƒå¼å†…å­˜æ–‡ä»¶ç³»ç»ŸAlluxioç”¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ ","date":"2025-06-07","objectID":"/posts/spark_data_cache/:0:0","tags":null,"title":"Sparkæ•°æ®ç¼“å­˜","uri":"/posts/spark_data_cache/"},{"categories":["Spark"],"content":" // CoarseGrainedExecutorBackend case LaunchTask(data) =\u003e if (executor == null) { exitExecutor(1, \"Received LaunchTask command but executor was null\") } else { val taskDesc = TaskDescription.decode(data.value) logInfo(log\"Got assigned task ${MDC(LogKeys.TASK_ID, taskDesc.taskId)}\") executor.launchTask(this, taskDesc) } æ¥æ”¶åˆ°Driveç«¯ä¼ æ¥çš„taskï¼Œååºåˆ—åŒ–åï¼Œå¯åŠ¨task // Executor def launchTask(context: ExecutorBackend, taskDescription: TaskDescription): Unit = { val taskId = taskDescription.taskId val tr = createTaskRunner(context, taskDescription) runningTasks.put(taskId, tr) val killMark = killMarks.get(taskId) if (killMark != null) { tr.kill(killMark._1, killMark._2) killMarks.remove(taskId) } threadPool.execute(tr) if (decommissioned) { log.error(s\"Launching a task while in decommissioned state.\") } } åˆ›å»ºTaskRunnerï¼Œåœ¨çº¿ç¨‹æ± ä¸­å¼€å§‹æ‰§è¡Œ private[executor] val threadPool = { val threadFactory = new ThreadFactoryBuilder() .setDaemon(true) .setNameFormat(\"Executor task launch worker-%d\") .setThreadFactory((r: Runnable) =\u003e new UninterruptibleThread(r, \"unused\")) .build() Executors.newCachedThreadPool(threadFactory).asInstanceOf[ThreadPoolExecutor] } åº•å±‚çš„çº¿ç¨‹æ± æ˜¯ä¸€ä¸ªcache thread poolï¼Œæ²¡æœ‰é™åˆ¶æ•°é‡ æœ€åè°ƒç”¨Task.runTaskå®é™…æ‰§è¡Œtaskï¼ŒShuffleMapTaskå’ŒResultTaskéƒ½é‡å†™äº†è¿™ä¸ªæ–¹æ³•ã€‚ // ResultTask override def runTask(context: TaskContext): U = { // Deserialize the RDD and the func using the broadcast variables. val threadMXBean = ManagementFactory.getThreadMXBean val deserializeStartTimeNs = System.nanoTime() val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) { threadMXBean.getCurrentThreadCpuTime } else 0L val ser = SparkEnv.get.closureSerializer.newInstance() // ååºåˆ—åŒ–è·å¾—RDDå’Œfunc val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) =\u003e U)]( ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader) _executorDeserializeTimeNs = System.nanoTime() - deserializeStartTimeNs _executorDeserializeCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) { threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime } else 0L // è°ƒç”¨funcè®¡ç®—ç»“æœ func(context, rdd.iterator(partition, context)) } ResultTask.runTaskååºåˆ—åŒ–è·å¾—RDDå’Œfuncåï¼Œè°ƒç”¨funcå‡½æ•°åˆ©ç”¨RDDå¯¹åº”åˆ†åŒºçš„æ•°æ®è®¡ç®—taskçš„ç»“æœã€‚ final def iterator(split: Partition, context: TaskContext): Iterator[T] = { if (storageLevel != StorageLevel.NONE) { getOrCompute(split, context) } else { computeOrReadCheckpoint(split, context) } } RDD.iteratoræ–¹æ³•ä¼šæ£€æŸ¥åˆ†åŒºæ˜¯å¦è¢«ç¼“å­˜æˆ–è€…checkpointï¼Œå¦‚æœæœ‰ï¼Œåˆ™ä¸éœ€è¦é‡æ–°è®¡ç®—ï¼Œå¦åˆ™éœ€è¦é‡æ–°è®¡ç®—ã€‚ class StorageLevel private( private var _useDisk: Boolean, private var _useMemory: Boolean, private var _useOffHeap: Boolean, private var _deserialized: Boolean, private var _replication: Int = 1) extends Externalizable { object StorageLevel { val NONE = new StorageLevel(false, false, false, false) val DISK_ONLY = new StorageLevel(true, false, false, false) val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2) val DISK_ONLY_3 = new StorageLevel(true, false, false, false, 3) val MEMORY_ONLY = new StorageLevel(false, true, false, true) val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2) val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false) val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2) val MEMORY_AND_DISK = new StorageLevel(true, true, false, true) val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2) val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false) val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) val OFF_HEAP = new StorageLevel(true, true, true, false, 1) StorageLevelæ§åˆ¶äº†RDDçš„å­˜å‚¨ï¼Œæ¯ç§StorageLevelè®°å½•äº†æ˜¯å¦ä½¿ç”¨å†…å­˜ã€æ˜¯å¦ä½¿ç”¨ç£ç›˜ã€æ˜¯å¦ä½¿ç”¨å †å¤–ã€æ˜¯å¦åºåˆ—åŒ–å¯¹è±¡ã€æ˜¯å¦åœ¨ä¸åŒèŠ‚ç‚¹é—´å¤åˆ¶RDDã€‚ private def persist(newLevel: StorageLevel, allowOverride: Boolean): this.type = { // TODO: Handle changes of StorageLevel if (storageLevel != StorageLevel.NONE \u0026\u0026 newLevel != storageLevel \u0026\u0026 !allowOverride) { throw SparkCoreErrors.cannotChangeStorageLevelError() } // If this is the first time this RDD is marked for persisting, register it // with the SparkContext for cleanups and accountin","date":"2025-06-07","objectID":"/posts/spark_job_execution/:0:0","tags":["Spark"],"title":"Spark Jobæ‰§è¡Œæµç¨‹","uri":"/posts/spark_job_execution/"},{"categories":["Spark"],"content":"è¿è¡Œåœ¨ä¸åŒstageã€ä¸åŒèŠ‚ç‚¹ä¸Šçš„taskè§é€šè¿‡shuffleæœºåˆ¶ä¼ é€’æ•°æ®ï¼Œshuffleè§£å†³çš„é—®é¢˜æ˜¯å¦‚ä½•å°†æ•°æ®è¿›è¡Œé‡æ–°ç»„ç»‡ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ä¸Šæ¸¸å’Œä¸‹æ¸¸taskä¹‹é—´è¿›è¡Œä¼ é€’å’Œè®¡ç®—ã€‚å¦‚æœåªæ˜¯å•çº¯çš„æ•°æ®ä¼ é€’ï¼Œåˆ™åªéœ€è¦å°†æ•°æ®è¿›è¡Œåˆ†åŒºã€é€šè¿‡ç½‘ç»œä¼ è¾“å³å¯ï¼Œæ²¡æœ‰å¤ªå¤§çš„éš¾åº¦ï¼Œä½†shuffleæœºåˆ¶è¿˜éœ€è¦è¿›è¡Œå„ç§ç±»å‹çš„è®¡ç®—ï¼ˆå¦‚èšåˆã€æ’åºï¼‰ï¼Œè€Œä¸”æ•°æ®é‡ä¸€èˆ¬ä¼šå¾ˆå¤§ï¼Œå¦‚æœæ”¯æŒè¿™äº›ä¸åŒç±»å‹çš„è®¡ç®—ï¼Œå¦‚æœæé«˜shuffleçš„æ€§èƒ½éƒ½æ˜¯shuffleæœºåˆ¶è®¾è®¡çš„éš¾ç‚¹ã€‚ shuffleæœºåˆ¶åˆ†ä¸ºshuffle writeå’Œshuffle readä¸¤ä¸ªé˜¶æ®µï¼Œå‰è€…ä¸»è¦è§£å†³ä¸Šæ¸¸stageè¾“å‡ºæ•°æ®çš„åˆ†åŒºé—®é¢˜ï¼Œåè€…ä¸»è¦è§£å†³ä¸‹æ¸¸stageä»ä¸Šæ¸¸stageè·å–æ•°æ®ã€é‡æ–°ç»„ç»‡ã€å¹¶ä¸ºåç»­æ“ä½œæä¾›æ•°æ®çš„é—®é¢˜ã€‚ åœ¨shuffleè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä¸Šæ¸¸stageç§°ä¸ºmap stageï¼Œä¸‹æ¸¸stageç§°ä¸ºreduce stageï¼Œç›¸åº”åœ°ï¼Œmap stageåŒ…å«å¤šä¸ªmap taskï¼Œreduce stageåŒ…å«å¤šä¸ªreduce taskã€‚ åˆ†åŒºä¸ªæ•°å’Œä¸‹æ¸¸stageçš„taskä¸ªæ•°ä¸€è‡´ï¼Œåˆ†åŒºä¸ªæ•°å¯ä»¥é€šè¿‡ç”¨æˆ·è‡ªå®šä¹‰ï¼Œå¦‚groupByKey(numPartitions)ä¸­çš„numPartitionsä¸€èˆ¬è¢«å®šä¹‰ä¸ºé›†ç¾¤ä¸­å¯ç”¨cpuä¸ªæ•°çš„1~2å€ï¼Œå³å°†æ¯ä¸ªmap taskçš„è¾“å‡ºæ•°æ®åˆ’åˆ†ä¸ºnumPartitionsä»½ï¼Œç›¸åº”åœ°ï¼Œåœ¨reduce stageä¸­å¯åŠ¨numPartitionä¸ªtaskæ¥è·å–å¹¶å¤„ç†è¿™äº›æ•°æ®ã€‚å¦‚æœç”¨æˆ·æ²¡æœ‰è‡ªå®šä¹‰ï¼Œåˆ™é»˜è®¤åˆ†åŒºä¸ªæ•°æ˜¯parent RDDçš„åˆ†åŒºä¸ªæ•°çš„æœ€å¤§å€¼ã€‚ å¯¹map taskè¾“å‡ºçš„æ¯ä¸€ä¸ª\u003cK, V\u003e recodï¼Œæ ¹æ®Keyè®¡ç®—å…¶partitionIdï¼Œå…·æœ‰ä¸åŒpartitionIdçš„recordè¢«è¾“å‡ºåˆ°ä¸åŒçš„åˆ†åŒºï¼ˆæ–‡ä»¶ï¼‰ä¸­ã€‚ æ•°æ®èšåˆçš„æœ¬è´¨æ˜¯å°†ç›¸åŒkeyçš„recordæ”¾åœ¨ä¸€èµ·ï¼Œå¹¶è¿›è¡Œå¿…è¦çš„è®¡ç®—ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯ä»¥åˆ©ç”¨HashMapå®ç°ã€‚æ–¹æ³•æ˜¯ä½¿ç”¨ä¸¤æ­¥èšåˆï¼ˆtwo-phase aggregationï¼‰ï¼Œå…ˆå°†ä¸åŒtasksè·å–åˆ°çš„\u003cK, V\u003e recordå­˜æ”¾åˆ°HashMapä¸­ï¼ŒHashMapä¸­çš„Keyæ˜¯K, Valueæ˜¯list(V)ã€‚ç„¶åï¼Œå¯¹äºHashMapä¸­æ¯ä¸€ä¸ª\u003cK, list(V)\u003e recordï¼Œä½¿ç”¨funcè®¡ç®—å¾—åˆ°\u003cK, func(list(v))\u003e recordã€‚ä¸¤æ­¥èšåˆæ–¹æ¡ˆçš„ä¼˜ç‚¹æ˜¯å¯ä»¥è§£å†³æ•°æ®èšåˆé—®é¢˜ï¼Œé€»è¾‘æ¸…æ™°ã€å®¹æ˜“å®ç°ï¼Œç¼ºç‚¹æ˜¯æ‰€æœ‰shuffleçš„recordéƒ½ä¼šå…ˆè¢«å­˜æ”¾åˆ°HashMapä¸­ï¼Œå ç”¨å†…å­˜ç©ºé—´è¾ƒå¤§ã€‚å¦å¤–ï¼Œå¯¹äºåŒ…å«èšåˆå‡½æ•°çš„æ“ä½œï¼Œå¦‚reduceByKey(func)ï¼Œéœ€è¦å…ˆå°†æ•°æ®èšåˆåˆ°HashMapä¸­ä»¥åå†æ‰§è¡Œfunc()èšåˆå‡½æ•°ï¼Œæ•ˆç‡è¾ƒä½ã€‚ å¯¹äºreduceByKey(func)ç­‰åŒ…å«èšåˆå‡½æ•°çš„æ“ä½œæ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨ä¸€ç§åœ¨çº¿èšåˆï¼ˆOnline aggregationï¼‰çš„æ–¹æ³•æ¥å‡å°‘å†…å­˜ç©ºé—´å ç”¨ã€‚è¯¥æ–¹æ³•åœ¨æ¯ä¸ªrecordåŠ å…¥HashMapæ—¶ï¼ŒåŒæ—¶è¿›è¡Œfunc()èšåˆæ“ä½œï¼Œå¹¶æ›´æ–°ç›¸åº”çš„èšåˆç»“æœã€‚å…·ä½“åœ°ï¼Œå¯¹äºæ¯ä¸€ä¸ªæ–°æ¥çš„\u003cK, V\u003e recordï¼Œé¦–å…ˆä»HashMapä¸­getå‡ºå·²ç»å­˜åœ¨çš„ç»“æœVâ€™ = HashMap.get(K)ï¼Œç„¶åæ‰§è¡Œèšåˆå‡½æ•°å¾—åˆ°æ–°çš„ä¸­é—´ç»“æœVâ€™â€™ = func(V, Vâ€™)ï¼Œæœ€åå°†Vâ€™â€˜å†™å…¥HashMapä¸­ï¼Œå³HashMap.put(K, Vâ€™â€™)ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œèšåˆå‡½æ•°çš„æ‰§è¡Œç»“æœä¼šå°äºåŸå§‹æ•°æ®è§„æ¨¡ï¼Œå³size(func(list(V))) \u003c Size(list(V))ï¼Œå¦‚sum(), max()ç­‰ï¼Œæ‰€ä»¥åœ¨çº¿èšåˆå¯ä»¥å‡å°‘å†…å­˜æ¶ˆè€—ã€‚åœ¨çº¿èšåˆå°†Shuffle Readå’Œèšåˆå‡½æ•°è®¡ç®—è€¦åˆåœ¨ä¸€èµ·ï¼Œå¯ä»¥åŠ é€Ÿè®¡ç®—ã€‚ä½†æ˜¯ï¼Œå¯¹äºä¸åŒ…å«èšåˆå‡½æ•°çš„æ“ä½œï¼Œå¦‚groupByKey()ç­‰ï¼Œåœ¨çº¿èšåˆå’Œä¸¤æ­¥èšåˆæ²¡æœ‰å·®åˆ«ï¼Œå› ä¸ºè¿™äº›æ“ä½œä¸åŒ…å«èšåˆå‡½æ•°ï¼Œæ— æ³•å‡å°‘ä¸­é—´æ•°æ®è§„æ¨¡ã€‚ Shuffle Writerç«¯çš„combineæ“ä½œçš„ç›®çš„æ˜¯å‡å°‘Shuffleçš„æ•°æ®é‡ï¼Œåªæœ‰åŒ…å«èšåˆå‡½æ•°çš„æ•°æ®æ“ä½œéœ€è¦è¿›è¡Œmapæ®µçš„combineï¼Œå¯¹äºä¸åŒ…å«èšåˆå‡½æ•°çš„æ“ä½œï¼Œå¦‚groupByKeyï¼Œæˆ‘ä»¬å³ä½¿è¿›è¡Œäº†combineæ“ä½œï¼Œä¹Ÿä¸èƒ½å‡å°‘ä¸­é—´æ•°æ®çš„è§„æ¨¡ã€‚ä»æœ¬è´¨ä¸Šå°†ï¼Œcombineå’ŒShuffle Readç«¯çš„èšåˆè¿‡ç¨‹æ²¡æœ‰åŒºåˆ«ï¼Œéƒ½æ˜¯å°†\u003cK, V\u003e recordèšåˆæˆ\u003cK, func(list(V))\u003eï¼Œä¸åŒçš„æ˜¯ï¼ŒShuffle Readç«¯èšåˆçš„æ˜¯æ¥è‡ªæ‰€æœ‰map taskè¾“å‡ºçš„æ•°æ®ï¼Œè€Œcombineèšåˆçš„æ˜¯æ¥è‡ªå•ä¸€taskè¾“å‡ºçš„æ•°æ®ã€‚å› æ­¤ä»ç„¶å¯ä»¥é‡‡ç”¨Shuffle Readç«¯åŸºäºHashMapçš„è§£å†³æ–¹æ¡ˆã€‚å…·ä½“åœ°ï¼Œé¦–å…ˆåˆ©ç”¨HashMapè¿›è¡Œcombineï¼Œç„¶åå¯¹HashMapä¸­æ¯ä¸€ä¸ªrecordè¿›è¡Œåˆ†åŒºï¼Œè¾“å‡ºåˆ°å¯¹åº”çš„åˆ†åŒºæ–‡ä»¶ä¸­ã€‚ å¦‚æœéœ€è¦æ’åºï¼Œåœ¨Shuffle Readç«¯å¿…é¡»å¿…é¡»æ‰§è¡Œsortï¼Œå› ä¸ºä»æ¯ä¸ªtaskè·å–çš„æ•°æ®ç»„åˆèµ·æ¥ä»¥åä¸æ˜¯å…¨å±€æŒ‰Keyè¿›è¡Œæ’åºçš„ã€‚å…¶æ¬¡ï¼Œç†è®ºä¸Šï¼Œåœ¨Shuffle Writeç«¯ä¸éœ€è¦è¿›è¡Œæ’åºï¼Œä½†å¦‚æœè¿›è¡Œäº†æ’åºï¼Œé‚£ä¹ˆShuffle Readè·å–åˆ°ï¼ˆæ¥è‡ªä¸åŒtaskï¼‰çš„æ•°æ®æ˜¯å·²ç»éƒ¨åˆ†æœ‰åºçš„æ•°æ®ï¼Œå¯ä»¥å‡å°‘Shuffle Readç«¯æ’åºçš„å¤æ‚åº¦ã€‚ æ ¹æ®æ’åºå’Œèšåˆçš„é¡ºåºï¼Œæœ‰ä¸‰ç§æ–¹æ¡ˆå¯ä¾›é€‰æ‹©ï¼š ç¬¬ä¸€ç§æ–¹æ¡ˆæ˜¯å…ˆæ’åºåèšåˆï¼Œè¿™ç§æ–¹æ¡ˆéœ€è¦å…ˆä½¿ç”¨çº¿æ€§æ•°æ®ç»“æœå¦‚Arrayï¼Œå­˜åœ¨Shuffle Readçš„\u003cK, V\u003e recordï¼Œç„¶åå¯¹Keyè¿›è¡Œæ’åºï¼Œæ’åºåçš„æ•°æ®å¯ä»¥ç›´æ¥ä»å‰åˆ°åè¿›è¡Œæ‰«æèšåˆï¼Œä¸éœ€è¦å†ä½¿ç”¨HashMapè¿›è¡Œhash-basedèšåˆã€‚è¿™ç§æ–¹æ¡ˆä¹Ÿæ˜¯Hadoop MapReduceé‡‡ç”¨çš„æ–¹æ¡ˆï¼Œæ–¹æ¡ˆä¼˜ç‚¹æ˜¯æ—¢å¯ä»¥æ»¡è¶³æ’åºè¦æ±‚åˆå¯ä»¥æ»¡è¶³èšåˆè¦æ±‚ï¼Œç¼ºç‚¹æ˜¯éœ€è¦è¾ƒå¤§å†…å­˜æ¥å­˜å‚¨çº¿æ€§æ•°æ®ç»“æ„ï¼ŒåŒæ—¶æ’åºå’Œèšåˆè¿‡ç¨‹ä¸èƒ½åŒæ—¶è¿›è¡Œï¼Œå³ä¸èƒ½ä½¿ç”¨åœ¨çº¿èšåˆï¼Œæ•ˆç‡è¾ƒä½ã€‚ ç¬¬äºŒç§æ–¹æ¡ˆæ˜¯æ’åºå’ŒèšåˆåŒæ—¶è¿›è¡Œï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¸¦æœ‰æ’åºåŠŸèƒ½çš„Mapï¼Œå¦‚TreeMapæ¥å¯¹ä¸­é—´æ•°æ®è¿›è¡Œèšåˆï¼Œæ¯æ¬¡Shuffle Readè·å–åˆ°ä¸€ä¸ªrecordï¼Œå°±å°†å…¶æ”¾å…¥TreeMapä¸­ä¸ç°æœ‰çš„recordè¿›è¡Œèšåˆï¼Œè¿‡ç¨‹ä¸HashMapç±»ä¼¼ï¼Œåªæœ‰TreeMapè‡ªå¸¦æ’åºåŠŸèƒ½ã€‚è¿™ç§æ–¹æ¡ˆçš„ä¼˜ç‚¹æ˜¯æ’åºå’Œèšåˆå¯ä»¥åŒæ—¶è¿›è¡Œï¼Œç¼ºç‚¹æ˜¯ç›¸æ¯”HashMapï¼ŒTreeMapçš„æ’åºå¤æ‚åº¦è¾ƒé«˜ï¼ŒTreeMapçš„æ’å…¥æ—¶é—´å¤æ‚åº¦ä¸ºO(nlogn)ï¼Œè€Œä¸”éœ€è¦ä¸æ–­è°ƒæ•´æ ‘çš„ç»“æœï¼Œä¸é€‚åˆæ•°æ®è§„æ¨¡éå¸¸å¤§çš„æƒ…å†µã€‚ ç¬¬ä¸‰ç§æ–¹æ¡ˆæ˜¯å…ˆèšåˆå†æ’åºï¼Œå³ç»´æŒç°æœ‰åŸºäºHashMapçš„èšåˆæ–¹æ¡ˆä¸å˜ï¼Œå°†HashMapä¸­çš„recordæˆ–recordçš„å¼•ç”¨æ”¾å…¥çº¿æ€§æ•°æ®ç»“æ„ä¸­å°±è¡Œæ’åºã€‚è¿™ç§æ–¹æ¡ˆçš„ä¼˜ç‚¹æ˜¯èšåˆå’Œæ’åºè¿‡ç¨‹ç‹¬ç«‹ï¼Œçµæ´»æ€§è¾ƒé«˜ï¼Œè€Œä¸”ä¹‹é—´çš„åœ¨çº¿èšåˆæ–¹æ¡ˆä¸éœ€è¦æ”¹åŠ¨ï¼Œç¼ºç‚¹æ˜¯éœ€è¦å¤åˆ¶ï¼ˆcopyï¼‰æ•°æ®æˆ–è€…å¼•ç”¨ï¼Œç©ºé—´å ç”¨è¾ƒå¤§ï¼ŒSparké€‰æ‹©çš„æ˜¯ç¬¬ä¸‰ç§æ–¹æ¡ˆï¼Œè®¾è®¡äº†ç‰¹æ®Šçš„HashMapæ¥é«˜æ•ˆå®Œæˆå…ˆèšåˆå†æ’åºçš„ä»»åŠ¡ã€‚ ç”±äºæˆ‘ä»¬ä½¿ç”¨HashMapå¯¹æ•°æ®è¿›è¡Œcombineå’Œèšåˆï¼Œåœ¨æ•°æ®é‡å¤§çš„æ—¶å€™ï¼Œä¼šå‡ºç°å†…å­˜æº¢å‡ºï¼Œè¿™ä¸ªé—®é¢˜æ—¢å¯èƒ½å‡ºç°åœ¨Shuffle Writeé˜¶æ®µï¼Œä¹Ÿå¯èƒ½å‡ºç°åœ¨Shuffle Readé˜¶æ®µã€‚é€šè¿‡ä½¿ç”¨å†…å­˜+ç£ç›˜æ··åˆå­˜å‚¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼ˆåç£ç›˜ï¼‰ï¼Œå…ˆåœ¨å†…å­˜ï¼ˆå¦‚HashMapï¼‰ä¸­è¿›è¡Œæ•°æ®èšåˆï¼Œå¦‚æœå†…å­˜ç©ºé—´ä¸è¶³ï¼Œåˆ™å°†å†…å­˜ä¸­çš„æ•°æ®spillåˆ°ç£ç›˜ä¸Šï¼Œæ­¤æ—¶ç©ºé—²å‡ºæ¥çš„å†…å­˜å¯ä»¥ç»§ç»­å¤„ç†æ–°çš„æ•°æ®ã€‚æ­¤è¿‡ç¨‹å¯ä»¥ä¸æ–­é‡å¤ï¼Œç›´åˆ°æ•°æ®å¤„ç†å®Œæˆã€‚ç„¶è€Œï¼Œé—®é¢˜æ˜¯spillåˆ°ç£ç›˜ä¸Šçš„æ•°æ®å®é™…ä¸Šæ˜¯éƒ¨åˆ†èšåˆçš„ç»“æœï¼Œå¹¶æ²¡æœ‰å’Œåç»­çš„æ•°æ®è¿›è¡Œè¿‡èšåˆã€‚å› æ­¤ï¼Œä¸ºäº†å¾—åˆ°å®Œæˆçš„èšåˆç»“æœï¼Œæˆ‘ä»¬éœ€è¦å†è¿›è¡Œä¸‹ä¸€æ­¥æ•°æ®æ“ä½œä¹‹å‰å¯¹ç£ç›˜ä¸Šå’Œå†…å­˜ä¸­çš„æ•°æ®è¿›è¡Œå†æ¬¡èšåˆï¼Œè¿™ä¸ªè¿‡ç¨‹æˆ‘ä»¬ç§°ä¸ºå…¨å±€èšåˆï¼Œä¸ºäº†åŠ é€Ÿå…¨å±€èšåˆï¼Œæˆ‘ä»¬éœ€è¦å°†æ•°æ®spillåˆ°ç£ç›˜ä¸Šæ—¶è¿›è¡Œæ’åºï¼Œè¿™æ ·å…¨å±€èšåˆæ‰èƒ½å¤ŸæŒ‰ç…§é¡ºåºè¯»å–spillåˆ°ç£ç›˜ä¸Šçš„æ•°æ®ï¼Œå¹¶å‡å°‘ç£ç›˜I/Oã€‚ ","date":"2025-06-01","objectID":"/posts/spark_shuffle/:0:0","tags":["Spark"],"title":"Spark Shuffleæœºåˆ¶","uri":"/posts/spark_shuffle/"},{"categories":["Spark"],"content":"Sparkä¸­Shuffleæ¡†æ¶çš„è®¾è®¡ ","date":"2025-06-01","objectID":"/posts/spark_shuffle/:1:0","tags":["Spark"],"title":"Spark Shuffleæœºåˆ¶","uri":"/posts/spark_shuffle/"},{"categories":["Spark"],"content":"Shuffle Writeæ¡†æ¶è®¾è®¡å’Œå®ç° åœ¨Shuffle Writeé˜¶æ®µï¼Œæ•°æ®æ“ä½œéœ€è¦åˆ†åŒºã€èšåˆå’Œæ’åº3ä¸ªåŠŸèƒ½ï¼ŒSparkä¸ºäº†æ”¯æŒæ‰€æœ‰å¯èƒ½çš„æƒ…å†µï¼Œè®¾è®¡äº†ä¸€ä¸ªé€šç”¨çš„Shuffle Writeæ¡†æ¶ï¼Œæ¡†æ¶çš„è®¡ç®—é¡ºåºä¸ºmap()è¾“å‡º --\u003e æ•°æ®èšåˆ --\u003e æ’åº --\u003e åˆ†åŒºè¾“å‡ºã€‚map taskæ¯è®¡ç®—å‡ºä¸€ä¸ªrecordåŠå…¶partitionIdï¼Œå°±å°†recordæ”¾å…¥ç±»ä¼¼HashMapçš„æ•°æ®ç»“æ„ä¸­è¿›è¡Œèšåˆï¼Œèšåˆå®Œæˆåï¼Œå†å°†HashMapä¸­çš„æ•°æ®æ”¾å…¥ç±»ä¼¼Arrayçš„æ•°æ®ç»“æ„ä¸­è¿›è¡Œæ’åºï¼Œå³å¯æŒ‰ç…§partitionIdï¼Œä¹Ÿå¯ä»¥æŒ‰ç…§partitionId+Keyè¿›è¡Œæ’åºï¼Œæœ€åæ ¹æ®partitionIdå°†æ•°æ®å†™å…¥ä¸åŒçš„æ•°æ®åˆ†åŒºä¸­ï¼Œå­˜æ”¾åˆ°æœ¬åœ°ç£ç›˜ä¸Šã€‚å…¶ä¸­èšåˆå’Œæ’åºè¿‡ç¨‹æ˜¯å¯é€‰çš„ï¼Œå¦‚æœæ•°æ®æ“ä½œä¸éœ€è¦èšåˆæˆ–è€…æ’åºï¼Œé‚£ä¹ˆå¯ä»¥å»æ‰ç›¸åº”çš„èšåˆæˆ–æ’åºè¿‡ç¨‹ã€‚ ä¸éœ€è¦mapç«¯èšåˆå’Œæ’åº mapä¾æ¬¡è¾“å‡º\u003cK, V\u003e recordå¹¶è®¡ç®—å…¶partititionId, Sparkæ ¹æ®partitionIdï¼Œå°†recordä¾æ¬¡è¾“å‡ºåˆ°ä¸åŒçš„bufferä¸­ï¼Œæ¯å½“bufferå¡«æ»¡å°±å°†recordæº¢å†™åˆ°ç£ç›˜ä¸­çš„åˆ†åŒºæ–‡ä»¶ä¸­ã€‚åˆ†é…bufferçš„åŸå› æ˜¯mapè¾“å‡ºrecordçš„é€Ÿåº¦å¾ˆå¿«ï¼Œéœ€è¦è¿›è¡Œç¼“å†²æ¥å‡å°‘ç£ç›˜I/Oã€‚åœ¨å®ç°ä»£ç ä¸­ï¼ŒSparkå°†è¿™ç§Shuffle Writeçš„æ–¹å¼ç§°ä¸ºBypassMergeSortShuffleWriterï¼Œå³ä¸éœ€è¦è¿›è¡Œæ’åºçš„Shuffle Writeæ–¹å¼ã€‚ è¯¥æ¨¡å¼çš„ä¼˜ç¼ºç‚¹ï¼šä¼˜ç‚¹æ˜¯é€Ÿåº¦å¿«ï¼Œç›´æ¥å°†recordè¾“å‡ºåˆ°ä¸åŒçš„åˆ†åŒºæ–‡ä»¶ä¸­ã€‚ç¼ºç‚¹æ˜¯èµ„æºæ¶ˆè€—è¿‡é«˜ï¼Œæ¯ä¸ªåˆ†åŒºéƒ½éœ€è¦ä¸€ä¸ªbufferï¼ˆå¤§å°ç”±spark.Shuffle.file.bufferæ§åˆ¶ï¼Œé»˜è®¤ä¸º32KBï¼‰ï¼Œä¸”åŒæ—¶éœ€è¦å»ºç«‹å¤šä¸ªåˆ†åŒºæ–‡ä»¶è¿›è¡Œæº¢å†™ã€‚å½“åˆ†åŒºä¸ªæ•°å¤ªå¤§ï¼Œå¦‚10000ï¼Œæ¯ä¸ªmap taskéœ€è¦æœˆ320MBçš„å†…å­˜ï¼Œä¼šé€ æˆå†…å­˜æ¶ˆè€—è¿‡å¤§ï¼Œè€Œä¸”æ¯ä¸ªtaskéœ€è¦åŒæ—¶å»ºç«‹å’Œæ‰“å¼€10000ä¸ªæ–‡ä»¶ï¼Œé€ æˆèµ„æºä¸è¶³ï¼Œå› æ­¤ï¼Œè¯¥shuffleæ–¹æ¡ˆé€‚åˆåˆ†åŒºä¸ªæ•°è¾ƒå°‘çš„æƒ…å†µï¼ˆ\u003c 200ï¼‰ã€‚ è¯¥æ¨¡å¼é€‚ç”¨çš„æ“ä½œç±»å‹ï¼šmapç«¯ä¸éœ€è¦èšåˆï¼Œkeyä¸éœ€è¦æ’åºä¸”åˆ†åŒºä¸ªæ•°è¾ƒå°‘ï¼ˆ\u003c=spark.Shuffle.sort.bypassMergeThresholdï¼Œé»˜è®¤å€¼ä¸º200ï¼‰ï¼Œä¾‹å¦‚ groupByKey(100)ã€partitionBy(100)ã€sortByKey(100)ç­‰ï¼Œæ³¨æ„sortByKeyæ˜¯åœ¨Shuffle Rreadç«¯è¿›è¡Œæ’åºã€‚ ä¸éœ€è¦mapç«¯èšåˆï¼Œä½†éœ€è¦æ’åº åœ¨è¿™ç§æƒ…å†µä¸‹éœ€è¦æŒ‰ç…§partitionId+Keyè¿›è¡Œæ’åºã€‚Sparké‡‡ç”¨çš„å®ç°æ–¹æ³•æ˜¯å»ºç«‹ä¸€ä¸ªArrayæ¥å­˜æ”¾mapè¾“å‡ºçš„recordï¼Œå¹¶å¯¹Arrayä¸­å…ƒç´ çš„Keyè¿›è¡Œç²¾å¿ƒè®¾è®¡ï¼Œå°†æ¯ä¸ª\u003cK, V\u003e recordè½¬åŒ–ä¸º\u003c(PID, K), V\u003e recordå­˜å‚¨ç„¶åæŒ‰ç…§partitionId + Keyå¯¹recordè¿›è¡Œæ’åºï¼Œæœ€åå°†æ‰€æœ‰recordå†™å…¥ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œé€šè¿‡å»ºç«‹ç´¢å¼•æ¥æ ‡ç¤ºæ¯ä¸ªåˆ†åŒºã€‚ å¦‚æœArrayå­˜æ”¾ä¸ä¸‹ï¼Œåˆ™ä¼šå…ˆæ‰©å®¹ï¼Œå¦‚æœè¿˜å­˜æ”¾ä¸ä¸‹ï¼Œå°±å°†Arrayä¸­çš„recordæ’åºåspillåˆ°ç£ç›˜ä¸Šï¼Œç­‰å¾…mapè¾“å‡ºå®Œä»¥åï¼Œå†å°†Arrayä¸­çš„recordä¸ç£ç›˜ä¸Šå·²æ’åºçš„recordè¿›è¡Œå…¨å±€æ’åºï¼Œå¾—åˆ°æœ€ç»ˆæœ‰åºçš„recordï¼Œå¹¶å†™å…¥æ–‡ä»¶ä¸­ã€‚ è¯¥Shuffleæ¨¡å¼è¢«å‘½åä¸ºSortShuffleWriter(KeyOrdering=true)ï¼Œä½¿ç”¨çš„Arrayè¢«å‘½åä¸ºPartitionedPairBufferã€‚ è¯¥Shuffleæ¨¡å¼çš„ä¼˜ç¼ºç‚¹ï¼šä¼˜ç‚¹æ˜¯åªéœ€è¦ä¸€ä¸ªArrayç»“æ„å°±å¯ä»¥æ”¯æŒæŒ‰ç…§partitionId+Keyè¿›è¡Œæ’åºï¼ŒArrayå¤§å°å¯æ§ï¼Œè€Œä¸”å…·æœ‰æ‰©å®¹å’Œspillåˆ°ç£ç›˜çš„åŠŸèƒ½ï¼Œæ”¯æŒä»å°è§„æ¨¡åˆ°å¤§è§„æ¨¡æ•°æ®çš„æ’åºã€‚åŒæ—¶ï¼Œè¾“å‡ºçš„æ•°æ®å·²ç»æŒ‰ç…§partitionIdè¿›è¡Œæ’åºï¼Œå› æ­¤åªéœ€è¦ä¸€ä¸ªåˆ†åŒºæ–‡ä»¶å­˜å‚¨ï¼Œå³å¯æ ‡ç¤ºä¸åŒçš„åˆ†åŒºæ•°æ®ï¼Œå…‹æœäº†ByPassMergeSortShuffleWriterä¸­å»ºç«‹æ–‡ä»¶æ•°è¿‡å¤šçš„é—®é¢˜ï¼Œé€‚ç”¨äºåˆ†åŒºä¸ªæ•°å¾ˆå¤§çš„æƒ…å†µï¼Œç¼ºç‚¹æ˜¯æ’åºå¢åŠ è®¡ç®—æ—¶å»¶ã€‚ è¯¥Shuffleæ¨¡å¼é€‚ç”¨çš„æ“ä½œï¼šmapç«¯ä¸éœ€è¦èšåˆã€Keyéœ€è¦æ’åºã€åˆ†åŒºä¸ªæ•°æ— é™åˆ¶ã€‚ç›®å‰ï¼ŒSparkæœ¬èº«æ²¡æœ‰æä¾›è¿™ç§æ’åºç±»å‹çš„æ•°æ®æ“ä½œï¼Œä½†ä¸æ’é™¤ç”¨æˆ·ä¼šè‡ªå®šä¹‰ï¼Œæˆ–è€…ç³»ç»Ÿæœªæ¥ä¼šæä¾›è¿™ç§ç±»å‹çš„æ“ä½œã€‚sortByKeyæ“ä½œè™½ç„¶éœ€è¦æŒ‰Keyè¿›è¡Œæ’åºï¼Œä½†è¿™ä¸ªæ’åºè¿‡ç¨‹åœ¨Shuffle Readç«¯å®Œæˆå³å¯ï¼Œä¸éœ€è¦åœ¨Shuffle Writeç«¯è¿›è¡Œæ’åºã€‚ SortShuffleWriterå¯ä»¥è§£å†³BypassMergeSortShuffleWriteræ¨¡å¼çš„ç¼ºç‚¹ï¼Œè€ŒBypassMergeSortShuffleWriteré¢å‘çš„æ“ä½œä¸éœ€è¦æŒ‰ç…§Keyè¿›è¡Œæ’åºã€‚å› æ­¤ï¼Œæˆ‘ä»¬åªéœ€è¦å°†â€œæŒ‰PartitionId+keyâ€æ’åºæ”¹æˆâ€œåªæŒ‰PartitionIdæ’åºâ€ï¼Œå°±å¯ä»¥æ”¯æŒä¸éœ€è¦mapç«¯combineã€ä¸éœ€è¦æŒ‰ç…§keyè¿›è¡Œæ’åºã€åˆ†åŒºä¸ªæ•°è¿‡å¤§çš„æ“ä½œï¼Œä¾‹å¦‚ï¼ŒgroupByKey(300), partitionBy(300), sortByKey(300)ã€‚ éœ€è¦mapæ®µèšåˆï¼Œéœ€è¦æˆ–è€…ä¸éœ€è¦æŒ‰ç…§keyè¿›è¡Œæ’åº Sparké‡‡ç”¨çš„å®ç°æ–¹æ³•æ˜¯å»ºç«‹ä¸€ä¸ªç±»ä¼¼HashMapçš„æ•°æ®ç»“æ„å¯¹mapè¾“å‡ºçš„recordè¿›è¡Œèšåˆã€‚HashMapä¸­çš„Keyæ˜¯partitionId+Keyï¼ŒHashMapä¸­çš„Valueæ˜¯ç»è¿‡combineçš„èšåˆç»“æœã€‚èšåˆå®Œæˆåï¼ŒSparkå¯¹HashMapä¸­çš„recordè¿›è¡Œæ’åºï¼Œæœ€åå°†æ’åºåçš„recordå†™å…¥ä¸€ä¸ªåˆ†åŒºæ–‡ä»¶ä¸­ã€‚ è¯¥Shuffleæ¨¡å¼çš„ä¼˜ç¼ºç‚¹ï¼šä¼˜ç‚¹æ˜¯åªéœ€è¦ä¸€ä¸ªHashMapç»“æ„å°±å¯ä»¥æ”¯æŒmapç«¯çš„combineåŠŸèƒ½ï¼ŒHashMapå…·æœ‰æ‰©å®¹å’Œspillåˆ°ç£ç›˜çš„åŠŸèƒ½ï¼Œæ”¯æŒå°è§„æ¨¡åˆ°å¤§è§„æ¨¡æ•°æ®çš„èšåˆï¼Œä¹Ÿé€‚ç”¨äºåˆ†åŒºä¸ªæ•°å¾ˆå¤§çš„æƒ…å†µã€‚åœ¨èšåˆåä½¿ç”¨Arrayæ’åºï¼Œå¯ä»¥çµæ´»æ”¯æŒä¸åŒçš„æ’åºéœ€æ±‚ã€‚ç¼ºç‚¹æ˜¯åœ¨å†…å­˜ä¸­è¿›è¡Œèšåˆï¼Œå†…å­˜æ¶ˆè€—è¾ƒå¤§ï¼Œéœ€è¦é¢å¤–çš„æ•°ç»„è¿›è¡Œæ’åºï¼Œè€Œä¸”å¦‚æœæœ‰æ•°æ®spillåˆ°ç£ç›˜ä¸Šï¼Œè¿˜éœ€è¦å†æ¬¡è¿›è¡Œèšåˆã€‚åœ¨å®ç°ä¸­ï¼ŒSparkåœ¨Shuffle Writeç«¯ä½¿ç”¨ä¸€ä¸ªç»è¿‡ç‰¹æ®Šè®¾è®¡å’Œä¼˜åŒ–çš„HashMapï¼Œå‘½åä¸ºPartitionedAppendOnlylMapï¼Œå¯ä»¥åŒæ—¶æ”¯æŒèšåˆå’Œæ’åºæ“ä½œã€‚ç›¸å½“äºHashMapå’ŒArrayçš„åˆä½“ã€‚ è¯¥Shuffleæ¨¡å¼é€‚ç”¨çš„æ“ä½œï¼šé€‚åˆmapç«¯èšåˆã€éœ€è¦æˆ–è€…ä¸éœ€è¦æŒ‰ç…§Keyè¿›è¡Œæ’åºï¼Œåˆ†åŒºä¸ªæ•°æ— é™åˆ¶çš„åº”ç”¨ï¼Œå¦‚reduceByKeyã€aggregateByKeyç­‰ã€‚ ","date":"2025-06-01","objectID":"/posts/spark_shuffle/:1:1","tags":["Spark"],"title":"Spark Shuffleæœºåˆ¶","uri":"/posts/spark_shuffle/"},{"categories":["Spark"],"content":"Shuffle Readæ¡†æ¶è®¾è®¡å’Œå®ç° åœ¨Shuffle Readé˜¶æ®µï¼Œæ•°æ®æ“ä½œéœ€è¦3ä¸ªåŠŸèƒ½ï¼šè·¨èŠ‚ç‚¹æ•°æ®è·å–ã€èšåˆå’Œæ’åºã€‚Sparkä¸ºäº†æ”¯æŒæ‰€æœ‰çš„æƒ…å†µï¼Œè®¾è®¡äº†ä¸€ä¸ªé€šç”¨çš„Shuffle Readæ¡†æ¶ï¼Œæ¡†æ¶çš„è®¡ç®—é¡ºåºä¸ºæ•°æ®è·å–â€“\u003e èšåˆ â€”\u003e æ’åºè¾“å‡ºã€‚ ä¸éœ€è¦èšåˆï¼Œä¸éœ€è¦æŒ‰ç…§Keyè¿›è¡Œæ’åº è¿™ç§æƒ…å†µæœ€ç®€å•ï¼Œåªéœ€è¦å®ç°æ•°æ®è·å–åŠŸèƒ½å³å¯ã€‚ç­‰å¾…æ‰€æœ‰çš„map taskç»“æŸå, reduce taskå¼€å§‹ä¸æ–­ä»å„ä¸ªmap taskè·å–\u003cK, V\u003e recordï¼Œå¹¶å°†recordè¾“å‡ºåˆ°ä¸€ä¸ªbufferä¸­ï¼ˆå¤§å°ä¸ºspark.reducer.maxSizeInFlight=48MBï¼‰ï¼Œä¸‹ä¸€ä¸ªæ“ä½œç›´æ¥ä»bufferè·å–å³å¯ã€‚ è¯¥Shuffleæ¨¡å¼çš„ä¼˜ç¼ºç‚¹ï¼šä¼˜ç‚¹æ˜¯é€»è¾‘å’Œå®ç°ç®€å•ï¼Œå†…å­˜æ¶ˆè€—å¾ˆå°ã€‚ç¼ºç‚¹æ˜¯ä¸æ”¯æŒèšåˆã€æ’åºç­‰å¤æ‚åŠŸèƒ½ã€‚ è¯¥Shuffleæ¨¡å¼é€‚ç”¨çš„æ“ä½œï¼šé€‚åˆæ—¢ä¸éœ€è¦èšåˆä¹Ÿä¸éœ€è¦æ’åºçš„åº”ç”¨ï¼Œå¦‚partitionByç­‰ã€‚ ä¸éœ€è¦èšåˆï¼Œéœ€è¦æŒ‰Keyè¿›è¡Œæ’åº è·å–æ•°æ®åï¼Œå°†bufferä¸­çš„recordä¾æ¬¡è¾“å‡ºåˆ°ä¸€ä¸ªArrayç»“æ„ï¼ˆPartitionedPairBufferï¼‰ä¸­ã€‚ç”±äºè¿™é‡Œé‡‡ç”¨äº†æœ¬æ¥ç”¨äºShuffle Writeç«¯çš„PartitionedPairBufferç»“æ„ï¼Œæ‰€ä»¥è¿˜ä¿ç•™äº†æ¯ä¸ªrecordçš„partitionIdã€‚ç„¶åï¼Œå¯¹Arrayä¸­çš„recordæŒ‰ç…§Keyè¿›è¡Œæ’åºï¼Œå¹¶å°†æ’åºç»“æœè¾“å‡ºæˆ–è€…ä¼ é€’ç»™ä¸‹ä¸€æ­¥æ“ä½œã€‚å½“å†…å­˜æ— æ³•å­˜åœ¨æ‰€æœ‰çš„recordæ—¶ï¼ŒPartitionedPairBufferå°†recordæ’åºåspillåˆ°ç£ç›˜ä¸Šï¼Œæœ€åå°†å†…å­˜ä¸­å’Œç£ç›˜ä¸Šçš„recordè¿›è¡Œå…¨å±€æ’åºï¼Œå¾—åˆ°æœ€ç»ˆæ’åºåçš„recordã€‚ è¯¥Shuffleæ¨¡å¼çš„ä¼˜ç¼ºç‚¹ï¼šä¼˜ç‚¹æ˜¯åªéœ€è¦ä¸€ä¸ªArrayç»“æ„å°±å¯ä»¥æ”¯æŒæŒ‰ç…§Keyè¿›è¡Œæ’åºï¼ŒArrayå¤§å°å¯æ§ï¼Œè€Œä¸”å…·æœ‰æ‰©å®¹å’Œspillåˆ°ç£ç›˜çš„åŠŸèƒ½ï¼Œä¸å—æ•°æ®è§„æ¨¡é™åˆ¶ã€‚ç¼ºç‚¹æ˜¯æ’åºå¢åŠ è®¡ç®—æ—¶å»¶ã€‚ è¯¥Shuffleæ¨¡å¼é€‚ç”¨çš„æ“ä½œï¼šé€‚åˆreduceç«¯ä¸éœ€è¦èšåˆï¼Œä½†éœ€è¦æŒ‰ç…§Keyè¿›è¡Œæ’åºçš„æ“ä½œï¼Œå¦‚sortByKeyï¼ŒsortByç­‰ã€‚ éœ€è¦èšåˆï¼Œä¸éœ€è¦æˆ–è€…éœ€è¦æŒ‰Keyè¿›è¡Œæ’åº è·å–recordåï¼ŒSparkå»ºç«‹ä¸€ä¸ªç±»ä¼¼HashMapçš„æ•°æ®ç»“æ„ï¼ˆExternalAppendOnlyMapï¼‰å¯¹bufferä¸­çš„recordè¿›è¡Œèšåˆï¼ŒHashMapä¸­çš„Keyæ˜¯recordä¸­çš„Keyï¼ŒHashMapä¸­çš„Valueæ˜¯ç»è¿‡èšåˆå‡½æ•°è®¡ç®—åçš„ç»“æœã€‚ä¹‹åå¦‚æœéœ€è¦æŒ‰ç…§Keyè¿›è¡Œæ’åºï¼Œåˆ™å»ºç«‹ä¸€ä¸ªArrayç»“æ„ï¼Œè¯»å–HashMapä¸­çš„recordï¼Œå¹¶å¯¹recordæŒ‰Keyè¿›è¡Œæ’åºï¼Œæ’åºå®Œæˆåï¼Œå°†ç»“æœè¾“å‡ºæˆ–è€…ä¼ é€’ç»™ä¸‹ä¸€æ­¥æ“ä½œã€‚ è¯¥Shuffleæ¨¡å¼çš„ä¼˜ç¼ºç‚¹ï¼šä¼˜ç‚¹æ˜¯åªéœ€è¦ä¸€ä¸ªHashMapå’Œä¸€ä¸ªArrayç»“æ„å°±å¯ä»¥æ”¯æŒreduceç«¯çš„èšåˆå’Œæ’åºåŠŸèƒ½ï¼ŒHashMapå…·æœ‰æ‰©å®¹å’Œspillåˆ°ç£ç›˜ä¸Šçš„åŠŸèƒ½ï¼Œæ”¯æŒå°è§„æ¨¡åˆ°å¤§è§„æ¨¡æ•°æ®çš„èšåˆï¼Œè¾¹è·å–æ•°æ®è¾¹èšåˆï¼Œæ•ˆç‡è¾ƒé«˜ã€‚ç¼ºç‚¹æ˜¯éœ€è¦åœ¨å†…å­˜ä¸­è¿›è¡Œèšåˆï¼Œå†…å­˜æ¶ˆè´¹è¾ƒå¤§ï¼Œå¦‚æœæœ‰æ•°æ®spillåˆ°ç£ç›˜ä¸Šï¼Œè¿˜éœ€è¦è¿›è¡Œå†æ¬¡èšåˆã€‚å¦å¤–ï¼Œç»è¿‡HashMapèšåˆåçš„æ•°æ®ä»ç„¶éœ€è¦æ‹·è´åˆ°Arrayä¸­è¿›è¡Œæ’åºï¼Œå†…å­˜æ¶ˆè€—è¾ƒå¤§ã€‚åœ¨å®ç°ä¸­ï¼ŒSparkä½¿ç”¨çš„HashMapæ˜¯ä¸€ä¸ªç»è¿‡ç‰¹æ®Šä¼˜åŒ–çš„HashMapï¼Œå‘½åä¸ºExternalAppendOnlyMapï¼Œå¯ä»¥åŒæ—¶æ”¯æŒèšåˆå’Œæ’åºæ“ä½œï¼Œç›¸å½“äºHashMapå’ŒArrayçš„åˆä½“ã€‚ è¯¥Shuffleæ¨¡å¼é€‚ç”¨çš„æ“ä½œï¼šé€‚åˆreduceç«¯éœ€è¦èšåˆã€ä¸éœ€è¦æˆ–éœ€è¦æŒ‰Keyè¿›è¡Œæ’åºçš„æ“ä½œï¼Œå¦‚reduceByKeyã€aggregateByKeyç­‰ã€‚ ","date":"2025-06-01","objectID":"/posts/spark_shuffle/:1:2","tags":["Spark"],"title":"Spark Shuffleæœºåˆ¶","uri":"/posts/spark_shuffle/"},{"categories":["Spark"],"content":"æ”¯æŒé«˜æ•ˆèšåˆå’Œæ’åºçš„æ•°æ®ç»“æ„ ä»”ç»†è§‚å¯ŸShuffle Write/Readè¿‡ç¨‹ï¼Œæˆ‘ä»¬ä¼šå‘ç°Shuffleæœºåˆ¶ä¸­ä½¿ç”¨çš„æ•°æ®ç»“æ„çš„ä¸¤ä¸ªç‰¹å¾ï¼š ä¸€æ˜¯åªéœ€è¦æ”¯æŒrecordçš„æ’å…¥å’Œæ›´æ–°æ“ä½œï¼Œä¸éœ€è¦æ”¯æŒåˆ é™¤æ“ä½œï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥å¯¹æ•°æ®ç»“æ„è¿›è¡Œä¼˜åŒ–ï¼Œå‡å°‘å†…å­˜æ¶ˆè€— äºŒæ˜¯åªæœ‰å†…å­˜æ”¾ä¸ä¸‹æ—¶æ‰éœ€è¦spillåˆ°ç£ç›˜ä¸Šï¼Œå› æ­¤æ•°æ®ç»“æ„è®¾è®¡ä»¥å†…å­˜ä¸ºä¸»ï¼Œç£ç›˜ä¸ºè¾… ","date":"2025-06-01","objectID":"/posts/spark_shuffle/:2:0","tags":["Spark"],"title":"Spark Shuffleæœºåˆ¶","uri":"/posts/spark_shuffle/"},{"categories":["Spark"],"content":"AppendOnlyMap AppendOnlyMapå®é™…ä¸Šæ˜¯ä¸€ä¸ªåªæ”¯æŒrecordæ·»åŠ å’Œå¯¹Valueè¿›è¡Œæ›´æ–°çš„HashMapã€‚äºJava HashMapé‡‡ç”¨æ•°ç»„ + é“¾è¡¨çš„å®ç°ä¸åŒï¼ŒAppendOnlyMapåªä½¿ç”¨æ•°ç»„æ¥å­˜å‚¨å…ƒç´ ï¼Œæ ¹æ®å…ƒç´ çš„Hashå€¼ç¡®å®šå­˜å‚¨ä½ç½®ï¼Œå¦‚æœå­˜å‚¨å…ƒç´ æ—¶å‘ç”ŸHashå€¼å†²çªï¼Œåˆ™ä½¿ç”¨äºŒæ¬¡åœ°å€æ¢æµ‹æ³•ï¼ˆQuadratic probingï¼‰æ¥è§£å†³Hashå€¼çš„å†²çªã€‚ AppendOnlyMapå°†K, Vç›¸é‚»æ”¾åœ¨æ•°ç»„ä¸­ï¼Œå¯¹äºæ¯ä¸ªæ–°æ¥çš„\u003cK, V\u003e recordï¼Œå…ˆä½¿ç”¨Hash(K)è®¡ç®—å…¶å­˜æ”¾ä½ç½®ï¼Œå¦‚æœå­˜æ”¾ä½ç½®ä¸ºç©ºï¼Œå°±æŠŠrecordå­˜æ”¾åˆ°è¯¥ä½ç½®ï¼Œå¦‚æœè¯¥ä½ç½®å·²ç»è¢«å ç”¨ï¼Œåˆ™æ ¹æ®äºŒæ¬¡æ¢æµ‹æ³•å‘åæŒ‡æ•°é€’å¢ä½ç½®ï¼Œç›´åˆ°å‘ç°ç©ºä½ã€‚æŸ¥æ‰¾å’Œæ›´æ–°æ“ä½œä¹Ÿéœ€è¦æ ¹æ®ä¸Šé¢çš„æµç¨‹è¿›è¡Œå¯»å€ã€‚ æ‰©å®¹ï¼šAppendOnlyMapä½¿ç”¨æ•°ç»„æ¥å®ç°çš„é—®é¢˜æ˜¯ï¼Œå¦‚æœæ’å…¥çš„recordå¤ªå¤šï¼Œåˆ™å¾ˆå¿«å°±è¢«å¡«æ»¡ï¼ŒSparkçš„è§£å†³æ–¹æ¡ˆæ˜¯ï¼Œå¦‚æœAppendOnlyMapçš„åˆ©ç”¨ç‡è¾¾åˆ°70%ï¼Œé‚£ä¹ˆå°±æ‰©å¼ ä¸€å€ï¼Œæ‰©å¼ æ„å‘³ç€åŸæ¥çš„Hashå¤±æ•ˆï¼Œå› æ­¤å¯¹æ‰€æœ‰Keyè¿›è¡Œrehashï¼Œé‡æ–°æ’åˆ—æ¯ä¸ªKeyçš„ä½ç½®ã€‚ æ’åºï¼šç”±äºAppendOnlyMapé‡‡ç”¨äº†æ•°ç»„ä½œä¸ºåº•å±‚å­˜å‚¨ç»“æ„ï¼Œå¯ä»¥æ”¯æŒå¿«é€Ÿæ’åºç­‰æ’åºç®—æ³•ã€‚å®ç°å±‚é¢ï¼Œå…ˆå°†æ•°ç»„ä¸­æ‰€æœ‰çš„\u003cK, V\u003e recordè½¬ç§»åˆ°æ•°ç»„çš„å‰ç«¯ï¼Œç”¨beginå’Œendæ¥è¡¨ç¤ºèµ·å§‹ç»“æŸä½ç½®ï¼Œç„¶åè°ƒç”¨æ’åºç®—æ³•å¯¹[begin, end]ä¸­çš„recordè¿›è¡Œæ’åºã€‚å¯¹äºéœ€è¦æŒ‰Keyè¿›è¡Œæ’åºçš„æ“ä½œï¼Œå¦‚sortByKeyï¼Œå¯ä»¥æŒ‰ç…§Keyè¿›è¡Œæ’åºï¼Œå¯¹äºå…¶ä»–æ“ä½œï¼ŒåªæŒ‰ç…§Keyçš„Hashå€¼è¿›è¡Œæ’åºå³å¯ã€‚ è¾“å‡ºï¼šè¿­ä»£AppendOnlyMapæ•°ç»„ä¸­çš„recordï¼Œä»å‰å¾€åæ‰«æè¾“å‡ºå³å¯ã€‚ æºç åˆ†æ å¼€æ”¾åœ°å€ï¼ˆOpen addressingï¼‰æ˜¯è§£å†³hashè¡¨ä¸­hashå†²çªçš„ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ¢æµ‹æˆ–è€…æœç´¢æ•°ç»„ä¸­çš„å¯é€‰ä½ç½®ï¼ˆè¢«ç§°ä¸ºæ¢æµ‹åºåˆ—ï¼‰ï¼Œç›´åˆ°ç›®æ ‡è®°å½•è¢«æ‰¾åˆ°æˆ–è€…ä¸€ä¸ªæœªä½¿ç”¨çš„æ•°ç»„æ§½è¢«å‘ç°ã€‚ç›®å‰ä¸»è¦æœ‰ä»¥ä¸‹ä¸‰ç§æ¢æµ‹æ–¹æ³•ï¼š çº¿æ€§æ¢æµ‹ï¼ˆLinear probingï¼‰æ¢æµ‹åºåˆ—ä¹‹é—´çš„é—´éš”å›ºå®šï¼Œé€šå¸¸è®¾ç½®ä¸º1 äºŒæ¬¡æ¢æµ‹ï¼ˆQuadratic probingï¼‰æ¢æµ‹åºåˆ—ä¹‹é—´çš„é—´éš”çº¿æ€§å¢é•¿ï¼Œå› æ­¤ä¸‹æ ‡å¯ä»¥é€šè¿‡äºŒæ¬¡å‡½æ•°æ¥è¡¨ç¤º åŒæ•£åˆ—ï¼ˆDouble hashï¼‰å¯¹äºæ¯æ¡è®°å½•æ¢æµ‹åºåˆ—ä¹‹é—´çš„é—´éš”å›ºå®šï¼Œé€šè¿‡å¦ä¸€ä¸ªhashå‡½æ•°ç¡®å®š Sparkä¸­ä½¿ç”¨çš„æ˜¯äºŒæ¬¡æ¢æµ‹æ³•ï¼Œå¦‚æœæ•°ç»„é•¿åº¦ä¸º2çš„å¹‚æ¬¡æ–¹ï¼Œå‡å®šä¸ºmï¼Œåˆ™å¯ä»¥é€‰æ‹©æ¢æµ‹åºåˆ— h, h + 1, h + 3 , h+ 6ï¼Œé—´éš”åˆ†åˆ«ä¸º1ï¼Œ 2ï¼Œ 3â€¦ï¼Œä»è€Œä¿è¯mé•¿åº¦çš„æ¢æµ‹åºåˆ—ä¸€å®šæ˜¯æ‰€æœ‰æ•°ç»„ä¸‹æ ‡çš„ä¸€ç§æ’åˆ—ï¼Œæ‰€ä»¥åªè¦æ•°ç»„ä¸­æœ‰ç©ºé—´ï¼Œä¸€å®šå¯ä»¥å°†å…ƒç´ é¡ºåˆ©æ’å…¥ã€‚ å®ç°ä¸Šå’Œä¸€èˆ¬çš„HashMapå·®åˆ«ä¸å¤§ï¼Œè¿™é‡Œä¸»è¦è¯´ä¸€äº›ä¸å¤ªä¸€æ ·çš„åœ°æ–¹ AppendOnlyMapæ”¯æŒKeyä¸ºnullï¼Œå¯¹Keyä¸ºnullçš„æƒ…å†µç‰¹æ®Šå¤„ç†ï¼Œä½¿ç”¨å­—æ®µhashNullValueå’ŒnullValueè®°å½•Keyä¸ºnullçš„recordã€‚ /** Get the value for a given key */ def apply(key: K): V = { assert(!destroyed, destructionMessage) val k = key.asInstanceOf[AnyRef] if (k.eq(null)) { return nullValue } var pos = rehash(k.hashCode) \u0026 mask var i = 1 while (true) { val curKey = data(2 * pos) if (k.eq(curKey) || k.equals(curKey)) { return data(2 * pos + 1).asInstanceOf[V] } else if (curKey.eq(null)) { return null.asInstanceOf[V] } else { val delta = i pos = (pos + delta) \u0026 mask i += 1 } } null.asInstanceOf[V] } applyæ–¹æ³•ç”¨äºè·å–æŒ‡å®šKeyå¯¹åº”çš„Valueï¼Œå¯¹AppendOnlyMapè¿›è¡ŒåŸåœ°æ’åºåï¼ŒåŸå…ˆçš„HashMapçš„æ€§è´¨å·²ç»ä¸§å¤±ï¼Œä½¿ç”¨destoryedå­—æ®µè¡¨ç¤ºè¿™ç§æƒ…å†µï¼Œé™¤äº†å¯¹Keyä¸ºnullçš„ç‰¹æ®Šå¤„ç†å¤–ï¼Œé€šè¿‡äºŒæ¬¡æ¢æµ‹æ³•å¯»æ‰¾å¯¹åº”è®°å½•ã€‚ /** Set the value for a key */ def update(key: K, value: V): Unit = { assert(!destroyed, destructionMessage) val k = key.asInstanceOf[AnyRef] if (k.eq(null)) { if (!haveNullValue) { incrementSize() } nullValue = value haveNullValue = true return } var pos = rehash(key.hashCode) \u0026 mask var i = 1 while (true) { val curKey = data(2 * pos) if (curKey.eq(null)) { data(2 * pos) = k data(2 * pos + 1) = value.asInstanceOf[AnyRef] incrementSize() // Since we added a new key return } else if (k.eq(curKey) || k.equals(curKey)) { data(2 * pos + 1) = value.asInstanceOf[AnyRef] return } else { val delta = i pos = (pos + delta) \u0026 mask i += 1 } } } updateæ›´æ–°æ“ä½œä¹Ÿæ˜¯é€šè¿‡äºŒæ¬¡æ¢æµ‹æ³•å¯»æ‰¾å¯¹åº”ä½ç½®å¹¶æ’å…¥æˆ–è€…æ›´æ–°ã€‚ /** * Return an iterator of the map in sorted order. This provides a way to sort the map without * using additional memory, at the expense of destroying the validity of the map. */ def destructiveSortedIterator(keyComparator: Comparator[K]): Iterator[(K, V)] = { destroyed = true // Pack KV pairs into the front of the underlying array var keyIndex, newIndex = 0 while (keyIndex \u003c capacity) { if (data(2 * keyIndex) != null) { data(2 * newIndex) = data(2 * keyIndex) data(2 * newIndex + 1) = data(2 * keyIndex + 1) newIndex += 1 } keyIndex += 1 } assert(curSize == newIndex + (if (haveNullValue) 1 else 0)) new Sorter(new KVArraySortDataFormat[K, AnyRef]).sort(data, 0, newIndex, keyComparator) new Iterator[(K, V)] { var i = 0 var nullValueReady = haveNullValue def hasNext: Boolean = (i \u003c newIndex || nullValueReady) def next(): (K, V) = { if (nullValueReady) { nullValueReady = false (null.asInstanceOf[K], nullValue) } else { val item = (data(2 * i).asInstanceOf[K], data(2 * i + 1).asInstanceOf[V]) i += 1 item } } } } destructiveSortedIteratorå°†HashMapä¸­çš„æ‰€æœ‰è®°å½•æ•´ç†åˆ°æ•°ç»„çš„å¼€å¤´ï¼Œç„¶åè°ƒç”¨tim sortè¿›è¡ŒåŸåœ°æ’åºï¼Œtim sortç»“åˆäº†å½’å¹¶æ’åºå’Œæ’å…¥æ’åºï¼Œæœ€åè¿”å›è®°å½•çš„è¿­ä»£å™¨ã€‚ ","date":"2025-06-01","objectID":"/posts/spark_shuffle/:2:1","tags":["Spark"],"title":"Spark Shuffleæœºåˆ¶","uri":"/posts/spark_shuffle/"},{"categories":["Spark"],"content":"ExternalAppendOnlyMap AppendOnlyMapçš„ä¼˜ç‚¹æ˜¯èƒ½å¤Ÿå°†èšåˆå’Œæ’åºåŠŸèƒ½å¾ˆå¥½åœ°ç»“åˆåœ¨ä¸€èµ·ï¼Œç¼ºç‚¹æ˜¯åªèƒ½ä½¿ç”¨å†…å­˜ï¼Œéš¾ä»¥é€‚ç”¨äºå†…å­˜ç©ºé—´ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒSparkåŸºäºAppendOnlyMapè®¾è®¡å®ç°äº†åŸºäºå†…å­˜+ç£ç›˜çš„ExternalAppendOnlyMapï¼Œç”¨äºShuffle Readç«¯å¤§è§„æ¨¡æ•°æ®èšåˆã€‚åŒæ—¶ï¼Œç”±äºShuffle Writeç«¯èšåˆéœ€è¦è€ƒè™‘partitionIdï¼ŒSparkä¹Ÿè®¾è®¡äº†å¸¦æœ‰partitionIdçš„ExternalAppendOnlyMapï¼Œåä¸ºPartitionedAppendOnlyMapã€‚ ExternalAppendOnlyMapçš„å·¥ä½œåŸç†æ˜¯ï¼Œå…ˆæŒæœ‰ä¸€ä¸ªAppendOnlyMapæ¥ä¸æ–­æ¥æ”¶å’Œèšåˆæ–°æ¥çš„recordï¼ŒAppendOnlyMapå¿«è¢«è£…æ»¡æ—¶æ£€æŸ¥ä¸€ä¸‹å†…å­˜å‰©ä½™ç©ºé—´æ˜¯å¦å¯ä»¥æ‰©å±•ï¼Œå¯ä»¥çš„è¯ç›´æ¥åœ¨å†…å­˜ä¸­æ‰©å±•ï¼Œå¦åˆ™å¯¹AppendOnlyMapä¸­çš„recordè¿›è¡Œæ’åºï¼Œç„¶åå°†recordéƒ½spillåˆ°ç£ç›˜ä¸Šã€‚å› ä¸ºrecordä¸æ–­åˆ°æ¥ï¼Œå¯èƒ½ä¼šå¤šæ¬¡å¡«æ»¡AppendOnlyMapï¼Œæ‰€ä»¥è¿™ä¸ªspillè¿‡ç¨‹å¯ä»¥å‡ºç°å¤šæ¬¡å½¢æˆå¤šä¸ªspillæ–‡ä»¶ã€‚ç­‰recordéƒ½å¤„ç†å®Œï¼Œæ­¤æ—¶AppendOnlyMapä¸­å¯èƒ½è¿˜ç•™å­˜ä¸€äº›èšåˆåçš„recordï¼Œç£ç›˜ä¸Šä¹Ÿæœ‰å¤šä¸ªspillæ–‡ä»¶ã€‚å› ä¸ºè¿™äº›æ•°æ®éƒ½ç»è¿‡äº†éƒ¨åˆ†èšåˆï¼Œè¿˜éœ€è¦è¿›è¡Œå…¨å±€èšåˆï¼ˆmergeï¼‰ã€‚å› æ­¤ExternalAppendOnlyMapçš„æœ€åä¸€æ­¥æ˜¯å°†å†…å­˜ä¸­çš„AppendOnlyMapçš„æ•°æ®å’Œç£ç›˜ä¸Šspillæ–‡ä»¶ä¸­çš„æ•°æ®è¿›è¡Œå…¨å±€èšåˆï¼Œå¾—åˆ°æœ€ç»ˆç»“æœã€‚ AppendOnlyMapçš„å¤§å°ä¼°è®¡ è™½ç„¶æˆ‘ä»¬çŸ¥é“AppendOnlyMapä¸­æŒæœ‰çš„æ•°ç»„çš„é•¿åº¦å’Œå¤§å°ï¼Œä½†æ•°ç»„é‡Œé¢å­˜æ”¾çš„æ˜¯Keyå’ŒValueçš„å¼•ç”¨ï¼Œå¹¶ä¸æ˜¯å®ƒä»¬çš„å®é™…å¯¹è±¡å¤§å°ï¼Œè€Œä¸”Valueä¼šä¸æ–­è¢«æ›´æ–°ï¼Œå®é™…å¤§å°ä¸æ–­å˜åŒ–ã€‚æƒ³è¦å‡†å¤‡å¾—åˆ°AppendOnlyMapçš„å¤§å°æ¯”è¾ƒå›°éš¾ã€‚ä¸€ç§ç®€å•çš„è§£å†³æ–¹æ³•æ˜¯åœ¨æ¯æ¬¡æ’å…¥recordæˆ–å¯¹ç°æœ‰recordçš„Valueè¿›è¡Œæ›´æ–°åï¼Œéƒ½æ‰«æä¸€ä¸‹AppendOnlyMapä¸­å­˜æ”¾çš„recordï¼Œè®¡ç®—æ¯ä¸ªrecordçš„å®é™…å¯¹è±¡å¤§å°å¹¶ç›¸åŠ ï¼Œä½†è¿™æ ·ä¼šéå¸¸è€—æ—¶ã€‚ Sparkè®¾è®¡äº†ä¸€ä¸ªå¢é‡å¼çš„é«˜æ•ˆä¼°ç®—ç®—æ³•ï¼Œåœ¨æ¯ä¸ªrecordæ’å…¥æˆ–æ›´æ–°æ—¶æ ¹æ®å†å²ç»Ÿè®¡å€¼å’Œå½“å‰å˜åŒ–é‡ç›´æ¥ä¼°ç®—å½“å‰AppendOnlyMapçš„å¤§å°ï¼Œç®—æ³•çš„å¤æ‚åº¦ä¸ºO(1)ï¼Œå¼€é”€å¾ˆå°ï¼Œåœ¨recordæ’å…¥å’Œèšåˆè¿‡ç¨‹ä¸­ä¼šå®šæœŸå¯¹å½“å‰AppendOnlyMapä¸­çš„recordè¿›è¡ŒæŠ½æ ·ï¼Œç„¶åç²¾ç¡®è®¡ç®—è¿™äº›recordçš„æ€»å¤§å°ã€æ€»ä¸ªæ•°ã€æ›´æ–°ä¸ªæ•°åŠå¹³å‡å€¼ç­‰ï¼Œå¹¶ä½œä¸ºå†å²ç»Ÿè®¡å€¼ã€‚è¿›è¡ŒæŠ½æ ·æ˜¯å› ä¸ºAppendOnlyMapä¸­çš„recordå¯èƒ½æœ‰ä¸Šä¸‡ä¸ªï¼Œéš¾ä»¥å¯¹æ¯ä¸ªéƒ½ç²¾ç¡®è®¡ç®—ã€‚ä¹‹åï¼Œæ¯å½“æœ‰recordæ’å…¥æˆ–æ›´æ–°æ—¶ï¼Œä¼šæ ¹æ®å†å²ç»Ÿè®¡å€¼å’Œå†å²å¹³å‡çš„å˜åŒ–å€¼ï¼Œå¢é‡ä¼°ç®—AppendOnlyMapçš„æ€»å¤§å°ï¼Œè¯¦è§SizeTracker.estimateSizeæ–¹æ³•ã€‚æŠ½æ ·ä¹Ÿä¼šå®šæœŸè¿›è¡Œï¼Œæ›´æ–°ç»Ÿè®¡å€¼ä»¥è·å–æ›´é«˜çš„ç²¾åº¦ã€‚ Spillè¿‡ç¨‹ä¸æ’åº å½“AppendOnlyMapè¾¾åˆ°å†…å­˜é™åˆ¶æ—¶ï¼Œä¼šå°†recordæ’åºåå†™å…¥ç£ç›˜ä¸­ã€‚æ’åºæ˜¯ä¸ºäº†æ–¹ä¾¿ä¸‹ä¸€æ­¥å…¨å±€èšåˆï¼ˆèšåˆå†…å­˜å’Œç£ç›˜ä¸Šçš„recordï¼‰æ—¶å¯ä»¥é‡‡ç”¨æ›´é«˜æ•ˆçš„merge-sortï¼ˆå¤–éƒ¨æ’åº+èšåˆï¼‰ã€‚é‚£ä¹ˆï¼Œé—®é¢˜æ˜¯ä¾æ®ä»€ä¹ˆå¯¹recordè¿›è¡Œæ’åºï¼Ÿè‡ªç„¶æƒ³è¦å¯ä»¥æ ¹æ®recordçš„Keyè¿›è¡Œæ’åºï¼Œä½†æ˜¯è¿™å°±è¦æ±‚æ“ä½œå®šä¹‰Keyçš„æ’åºæ–¹æ³•ï¼Œå¦‚sortByKeyç­‰æ“ä½œå®šä¹‰äº†æŒ‰ç…§Keyè¿›è¡Œçš„æ’åºã€‚å¤§éƒ¨åˆ†æ“ä½œï¼Œå¦‚groupByKeyï¼Œå¹¶æ²¡æœ‰å®šä¹‰Keyçš„æ’åºæ–¹æ³•ï¼Œä¹Ÿä¸éœ€è¦è¾“å‡ºç»“æœæŒ‰ç…§Keyè¿›è¡Œæ’åºã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒSparké‡‡ç”¨æŒ‰ç…§Keyçš„Hashå€¼è¿›è¡Œæ’åºçš„æ–¹æ³•ï¼Œè¿™æ ·æ—¢å¯ä»¥è¿›è¡Œmerge-sortï¼Œåˆä¸è¦æ±‚æ“ä½œå®šä¹‰Keyæ’åºçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•çš„é—®é¢˜æ˜¯ä¼šå‡ºç°Hashå€¼å†²çªï¼Œä¹Ÿå°±æ˜¯ä¸åŒçš„Keyå…·æœ‰ç›¸åŒçš„Hashå€¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒSparkåœ¨merge-sortçš„åŒæ—¶ä¼šæ¯”è¾ƒKeyçš„Hashå€¼æ˜¯å¦ç›¸ç­‰ï¼Œä»¥åŠKeyçš„å®é™…å€¼æ˜¯å¦ç›¸ç­‰ã€‚ å…¨å±€èšåˆ ç”±äºæœ€ç»ˆçš„spillæ–‡ä»¶å’Œå†…å­˜ä¸­çš„AppendOnlyMapéƒ½æ˜¯ç»è¿‡éƒ¨åˆ†èšåˆåçš„ç»“æœï¼Œå…¶ä¸­å¯èƒ½å­˜åœ¨ç›¸åŒKeyçš„recordï¼Œå› æ­¤è¿˜éœ€è¦ä¸€ä¸ªå…¨å±€èšåˆé˜¶æ®µå°†AppendOnlyMapä¸­çš„recordä¸spillæ–‡ä»¶ä¸­çš„recordè¿›è¡Œèšåˆï¼Œå¾—åˆ°æœ€ç»ˆèšåˆåçš„ç»“æœã€‚ å…¨å±€èšåˆçš„æ–¹æ³•æ˜¯å»ºç«‹ä¸€ä¸ªæœ€å°å †æˆ–è€…æœ€å¤§å †ï¼Œæ¯æ¬¡ä»å„ä¸ªspillæ–‡ä»¶ä¸­è¯»å–å‰å‡ ä¸ªå…·æœ‰ç›¸åŒKeyï¼ˆæˆ–è€…ç›¸åŒKeyçš„hashå€¼ï¼‰çš„recordï¼Œç„¶åä¸AppendOnlyMapä¸­çš„recordè¿›è¡Œèšåˆï¼Œå¹¶è¾“å‡ºèšåˆåçš„ç»“æœã€‚ æºç åˆ†æ SizeTracker /** * A general interface for collections to keep track of their estimated sizes in bytes. * We sample with a slow exponential back-off using the SizeEstimator to amortize the time, * as each call to SizeEstimator is somewhat expensive (order of a few milliseconds). */ private[spark] trait SizeTracker { import SizeTracker._ /** * Controls the base of the exponential which governs the rate of sampling. * E.g., a value of 2 would mean we sample at 1, 2, 4, 8, ... elements. */ private val SAMPLE_GROWTH_RATE = 1.1 /** Samples taken since last resetSamples(). Only the last two are kept for extrapolation. */ private val samples = new mutable.Queue[Sample] /** The average number of bytes per update between our last two samples. */ private var bytesPerUpdate: Double = _ /** Total number of insertions and updates into the map since the last resetSamples(). */ private var numUpdates: Long = _ /** The value of 'numUpdates' at which we will take our next sample. */ private var nextSampleNum: Long = _ resetSamples() /** * Reset samples collected so far. * This should be called after the collection undergoes a dramatic change in size. */ protected def resetSamples(): Unit = { numUpdates = 1 nextSampleNum = 1 samples.clear() takeSample() } /** * Callback to be invoked after every update. */ protected def afterUpdate(): Unit = { numUpdates += 1 if (nextSampleNum == numUpdates) { takeSample() } } /** * Take a new sample of the current collection's size. */ private def takeSample(): Unit = { samples.enqueue(Sample(SizeEstimator.estimate(this), numUpdates)) // Only use the last two samples to extrapolate if (samples.size \u003e 2) { samples.dequeue() } val bytesDelta = samples.toList.reverse match { case latest :: previous :: tail =\u003e (latest.size - previous.size).toDouble / (latest.numUpdates - previous.numUpdates) // If fewer than 2 samples, assume no change case _ =\u003e 0 } bytesPerUpdate = math.max(0, bytesDelta) nextSampleNum = math.ceil(numUpdates * SAMPLE_GROWTH_RATE).toLong } /** * Estimate the current size of the collection in bytes. O(1) time. */ d","date":"2025-06-01","objectID":"/posts/spark_shuffle/:2:2","tags":["Spark"],"title":"Spark Shuffleæœºåˆ¶","uri":"/posts/spark_shuffle/"},{"categories":["Spark"],"content":"PartitionedAppendOnlyMap /** * Implementation of WritablePartitionedPairCollection that wraps a map in which the keys are tuples * of (partition ID, K) */ private[spark] class PartitionedAppendOnlyMap[K, V] extends SizeTrackingAppendOnlyMap[(Int, K), V] with WritablePartitionedPairCollection[K, V] { def partitionedDestructiveSortedIterator(keyComparator: Option[Comparator[K]]) : Iterator[((Int, K), V)] = { val comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator) destructiveSortedIterator(comparator) } def insert(partition: Int, key: K, value: V): Unit = { update((partition, key), value) } } PartitionedAppendOnlyMapç”¨äºåœ¨Shuffle Writeç«¯å¯¹recordè¿›è¡Œèšåˆï¼ˆcombineï¼‰ã€‚PartitionedAppendOnlyMapçš„åŠŸèƒ½å’Œå®ç°ä¸ExternalAppendOnlyMapçš„åŠŸèƒ½å’Œå®ç°åŸºæœ¬ä¸€æ ·ï¼Œå”¯ä¸€åŒºåˆ«æ˜¯PartitionedAppendOnlyMapä¸­çš„Keyæ˜¯PartitionId + Keyï¼Œè¿™æ ·æ—¢å¯ä»¥æ ¹æ®partitionIdè¿›è¡Œæ’åºï¼ˆé¢å‘ä¸éœ€è¦æŒ‰keyè¿›è¡Œæ’åºçš„æ“ä½œï¼‰ï¼Œä¹Ÿå¯ä»¥æ ¹æ®partitionId + Keyè¿›è¡Œæ’åºï¼ˆé¢å‘éœ€è¦æŒ‰Keyè¿›è¡Œæ’åºçš„æ“ä½œï¼‰ï¼Œä»è€Œåœ¨Shuffle Writeé˜¶æ®µå¯ä»¥è¿›è¡Œèšåˆã€æ’åºå’Œåˆ†åŒºã€‚ ","date":"2025-06-01","objectID":"/posts/spark_shuffle/:2:3","tags":["Spark"],"title":"Spark Shuffleæœºåˆ¶","uri":"/posts/spark_shuffle/"},{"categories":["Spark"],"content":"PartitionedPairBuffer private[spark] class PartitionedPairBuffer[K, V](initialCapacity: Int = 64) extends WritablePartitionedPairCollection[K, V] with SizeTracker { import PartitionedPairBuffer._ require(initialCapacity \u003c= MAXIMUM_CAPACITY, s\"Can't make capacity bigger than ${MAXIMUM_CAPACITY} elements\") require(initialCapacity \u003e= 1, \"Invalid initial capacity\") // Basic growable array data structure. We use a single array of AnyRef to hold both the keys // and the values, so that we can sort them efficiently with KVArraySortDataFormat. private var capacity = initialCapacity private var curSize = 0 private var data = new Array[AnyRef](2 * initialCapacity) /** Add an element into the buffer */ def insert(partition: Int, key: K, value: V): Unit = { if (curSize == capacity) { growArray() } data(2 * curSize) = (partition, key.asInstanceOf[AnyRef]) data(2 * curSize + 1) = value.asInstanceOf[AnyRef] curSize += 1 afterUpdate() } /** Double the size of the array because we've reached capacity */ private def growArray(): Unit = { if (capacity \u003e= MAXIMUM_CAPACITY) { throw new IllegalStateException(s\"Can't insert more than ${MAXIMUM_CAPACITY} elements\") } val newCapacity = if (capacity * 2 \u003e MAXIMUM_CAPACITY) { // Overflow MAXIMUM_CAPACITY } else { capacity * 2 } val newArray = new Array[AnyRef](2 * newCapacity) System.arraycopy(data, 0, newArray, 0, 2 * capacity) data = newArray capacity = newCapacity resetSamples() } /** Iterate through the data in a given order. For this class this is not really destructive. */ override def partitionedDestructiveSortedIterator(keyComparator: Option[Comparator[K]]) : Iterator[((Int, K), V)] = { val comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator) new Sorter(new KVArraySortDataFormat[(Int, K), AnyRef]).sort(data, 0, curSize, comparator) iterator() } private def iterator(): Iterator[((Int, K), V)] = new Iterator[((Int, K), V)] { var pos = 0 override def hasNext: Boolean = pos \u003c curSize override def next(): ((Int, K), V) = { if (!hasNext) { throw new NoSuchElementException } val pair = (data(2 * pos).asInstanceOf[(Int, K)], data(2 * pos + 1).asInstanceOf[V]) pos += 1 pair } } } PartitionedPariBufferæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŸºäºå†…å­˜+ç£ç›˜çš„Arrayï¼Œéšç€æ•°æ®æ·»åŠ ï¼Œä¸æ–­çš„æ‰©å®¹ï¼Œä½†åˆ°è¾¾å†…å­˜é™åˆ¶æ—¶ï¼Œå°±å°†Arrayä¸­çš„æ•°æ®æŒ‰ç…§partitionIdæˆ–è€…partitionId+Keyè¿›è¡Œæ’åºï¼Œç„¶åspillåˆ°ç£ç›˜ä¸Šï¼Œè¯¥è¿‡ç¨‹å¯ä»¥è¿›è¡Œå¤šæ¬¡ï¼Œæœ€åå¯¹å†…å­˜ä¸­å’Œç£ç›˜ä¸Šçš„æ•°æ®è¿›è¡Œå…¨å±€æ’åºï¼Œè¾“å‡ºæˆ–è€…æä¾›ç»™ä¸‹ä¸€ä¸ªæ“ä½œã€‚ ","date":"2025-06-01","objectID":"/posts/spark_shuffle/:2:4","tags":["Spark"],"title":"Spark Shuffleæœºåˆ¶","uri":"/posts/spark_shuffle/"},{"categories":["Spark"],"content":"ExternalSorter private[spark] class ExternalSorter[K, V, C]( context: TaskContext, aggregator: Option[Aggregator[K, V, C]] = None, partitioner: Option[Partitioner] = None, ordering: Option[Ordering[K]] = None, serializer: Serializer = SparkEnv.get.serializer) extends Spillable[WritablePartitionedPairCollection[K, C]](context.taskMemoryManager()) with Logging with ShuffleChecksumSupport { å­—æ®µï¼š aggregator: Option[Aggregator[K, V, C]] å¯é€‰çš„èšåˆå‡½æ•° partitioner: åˆ†åŒº orderingï¼š Option[Ordering[K]] å¯é€‰çš„æ’åº å¦‚æœéœ€è¦èšåˆï¼Œä½¿ç”¨PartitionedAppendOnlyMapä½œä¸ºå†…å­˜ä¸­çš„æ•°æ®ç»“æ„ï¼ˆç±»ä¼¼äºHashMapï¼‰ï¼Œå¦åˆ™ä½¿ç”¨PartitionedPairBufferä½œä¸ºå†…å­˜æ•°æ®ç»“æ„ï¼ˆç±»ä¼¼äºåŠ¨æ€æ•°ç»„ï¼‰ã€‚ ç”±äºéœ€è¦åˆ†åŒºï¼Œæ‰€ä»¥ä¸ç®¡Keyéœ€ä¸éœ€è¦æ’åºï¼Œéƒ½éœ€è¦æŒ‰ç…§partitionIdè¿›è¡Œæ’åºï¼Œç„¶åæ‰èƒ½å†™å…¥map out writerã€‚ ","date":"2025-06-01","objectID":"/posts/spark_shuffle/:2:5","tags":["Spark"],"title":"Spark Shuffleæœºåˆ¶","uri":"/posts/spark_shuffle/"},{"categories":null,"content":"è¿™ç¯‡æ˜¯å¯¹javadoopå¯¹concurrentHashMapéå¸¸æ£’çš„æºç è§£æçš„å­¦ä¹ ã€‚ ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:0:0","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"Java7 HashMap HashMapæ˜¯ä¸€ä¸ªéå¹¶å‘å®‰å…¨çš„hashmapï¼Œä½¿ç”¨é“¾è¡¨æ•°ç»„å®ç°ï¼Œé€»è¾‘æ¯”è¾ƒç®€å•ã€‚ è¦æ±‚å®¹é‡å§‹ç»ˆä¸º$2^n$ï¼Œè¿™æ ·å¯ä»¥åˆ©ç”¨ä½è¿ç®—è®¡ç®—ä¸‹æ ‡ï¼Œindex = hash \u0026 (length - 1) æ¯æ¬¡æ‰©å®¹ä¸ºåŸå…ˆçš„2å€ï¼Œè¿™æ ·è¿ç§»æ—§æ•°æ®æ—¶ï¼Œä¼šå°†ä½ç½®table[i]ä¸­çš„é“¾è¡¨çš„æ‰€æœ‰èŠ‚ç‚¹ï¼Œåˆ†æ‹†åˆ°æ–°çš„æ•°ç»„ä¸­çš„newTable[i]å’ŒnewTable[i + oldLenght]ä½ç½®ã€‚ ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:1:0","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"Java7 ConcurrentHashMap ConcurrentHashMapç”±ä¸€ä¸ªSegmentæ•°ç»„å®ç°ï¼ŒSegmenté€šè¿‡ç»§æ‰¿ReentrantLockæ¥è¿›è¡ŒåŠ é”ï¼Œæ‰€ä»¥æ¯æ¬¡éœ€è¦åŠ é”çš„æ“ä½œé”ä½çš„å°±æ˜¯ä¸€ä¸ªSegmentï¼Œæœ‰äº›åœ°æ–¹å°†Segmentç§°ä¸ºåˆ†æ®µé”ï¼Œè¿™æ ·åªè¦ä¿è¯æ¯ä¸ªSegmentéƒ½æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œå°±å®ç°äº†å…¨å±€çš„çº¿ç¨‹å®‰å…¨ã€‚ concurrencyLevel: é»˜è®¤ä¸º16ï¼Œä¹Ÿå°±æ˜¯è¯´ConcurrentHashMapæœ‰16ä¸ªSegmentï¼Œæ‰€ä»¥ç†è®ºä¸Šï¼Œæœ€å¤šå¯ä»¥åŒæ—¶æ”¯æŒ16ä¸ªçº¿ç¨‹å¹¶å‘å†™ï¼Œåªè¦å®ƒä»¬çš„æ“ä½œåˆ†åˆ«åˆ†å¸ƒåœ¨ä¸åŒçš„Segmentä¸Šï¼Œè¿™ä¸ªå€¼å¯ä»¥åœ¨åˆå§‹åŒ–çš„æ—¶å€™è®¾ç½®ä¸ºå…¶ä»–å€¼ï¼Œä½†æ˜¯ä¸€æ—¦åˆå§‹åŒ–åï¼Œå®ƒæ˜¯ä¸å¯ä»¥æ‰©å®¹çš„ã€‚ å‡è®¾concurrentcyLevelä¸º16ï¼Œé‚£ä¹ˆhashå€¼çš„é«˜4ä½è¢«ç”¨äºæ‰¾åˆ°å¯¹åº”çš„Segmentã€‚ Segmentå†…éƒ¨æ˜¯æœ‰æ•°ç»„+é“¾è¡¨ç»„æˆçš„ putæ“ä½œéœ€è¦å¯¹SegmentåŠ ç‹¬å é”ï¼Œå†…éƒ¨æ“ä½œç±»ä¼¼äºHashMap getæ“ä½œå®Œå…¨æ²¡æœ‰åŠ é”ï¼Œå®Œå…¨ç”±ä»£ç å®ç°ä¿è¯ä¸ä¼šå‘ç”Ÿé—®é¢˜ã€‚ ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:2:0","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"Java8 HashMap Java8å¯¹HashMapè¿›è¡Œäº†ä¸€äº›ä¿®æ”¹ï¼Œå¼•å…¥äº†çº¢é»‘æ ‘ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡hashå¿«é€Ÿå®šä½åˆ°æ•°ç»„ä¸­çš„å…·ä½“ä¸‹è¡¨ï¼Œä½†ä¹‹åéœ€è¦éå†æ•´ä¸ªé“¾è¡¨å¯»æ‰¾æˆ‘ä»¬éœ€è¦çš„é”®å€¼å¯¹ï¼Œæ—¶é—´å¤æ‚åº¦å–å†³äºé“¾è¡¨çš„é•¿åº¦ï¼Œä¸ºO(n)ã€‚ä¸ºäº†é™ä½è¿™éƒ¨åˆ†çš„å¼€é”€ï¼Œåœ¨java8ä¸­ï¼Œå½“é“¾è¡¨ä¸­çš„å…ƒç´ è¾¾åˆ°äº†8ä¸ªæ—¶ï¼Œä¼šå°†é“¾è¡¨è½¬æ¢ä¸ºçº¢é»‘æ ‘ï¼Œæ­¤æ—¶æŸ¥æ‰¾çš„æ—¶é—´å¤æ‚åº¦ä¸ºÂ O(logn)ã€‚ Java7ä¸­ä½¿ç”¨Entryæ¥ä»£è¡¨æ¯ä¸ªHashMapçš„æ•°æ®èŠ‚ç‚¹ï¼ŒJava8ä¸­ä½¿ç”¨Nodeï¼ŒåŸºæœ¬æ²¡æœ‰åŒºåˆ«ï¼Œéƒ½æ˜¯key, value, hashå’Œnextè¿™å››ä¸ªå±æ€§ï¼Œä¸è¿‡ï¼ŒNodeåªèƒ½ç”¨äºé“¾è¡¨çš„æƒ…å†µï¼Œçº¢é»‘æ ‘çš„æƒ…å†µéœ€è¦ä½¿ç”¨TreeNodeã€‚ ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:3:0","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"Java8 ConcurrentHashMap /** * The array of bins. Lazily initialized upon first insertion. * Size is always a power of two. Accessed directly by iterators. */ transient volatile Node\u003cK,V\u003e[] table; /** * The next table to use; non-null only while resizing. */ // è¿ç§»æ—¶ä½¿ç”¨çš„ä¸´æ—¶æ•°ç»„ private transient volatile Node\u003cK,V\u003e[] nextTable; ConcurrentHashMapåº•å±‚ä¹Ÿæ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ è¦ä¹ˆæ˜¯é“¾è¡¨ï¼Œè¦ä¹ˆæ˜¯çº¢é»‘æ ‘ã€‚ ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:0","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"spreadå‡½æ•° static final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash /** * Spreads (XORs) higher bits of hash to lower and also forces top * bit to 0. Because the table uses power-of-two masking, sets of * hashes that vary only in bits above the current mask will * always collide. (Among known examples are sets of Float keys * holding consecutive whole numbers in small tables.) So we * apply a transform that spreads the impact of higher bits * downward. There is a tradeoff between speed, utility, and * quality of bit-spreading. Because many common sets of hashes * are already reasonably distributed (so don't benefit from * spreading), and because we use trees to handle large sets of * collisions in bins, we just XOR some shifted bits in the * cheapest possible way to reduce systematic lossage, as well as * to incorporate impact of the highest bits that would otherwise * never be used in index calculations because of table bounds. */ static final int spread(int h) { return (h ^ (h \u003e\u003e\u003e 16)) \u0026 HASH_BITS; } spreadå‡½æ•°å°†åŸæ¥çš„hashå€¼è¿›è¡Œå¤„ç†ï¼Œè·å–æ–°çš„hashå€¼ï¼Œå°½é‡é¿å…hashç¢°æ’ã€‚ ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:1","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"putè¿‡ç¨‹åˆ†æ public V put(K key, V value) { return putVal(key, value, false); } final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); // å¾—åˆ°hashå€¼ int hash = spread(key.hashCode()); // è®°å½•ç›¸åº”é“¾è¡¨çš„é•¿åº¦ int binCount = 0; for (Node\u003cK,V\u003e[] tab = table;;) { Node\u003cK,V\u003e f; int n, i, fh; // å¦‚æœæ•°ç»„ä¸ºç©ºï¼Œè¿›è¡Œæ•°ç»„åˆå§‹åŒ– if (tab == null || (n = tab.length) == 0) tab = initTable(); // æŸ¥æ‰¾è¯¥hashå¯¹åº”çš„æ•°ç»„ä½ç½®å¤„çš„å…ƒç´  else if ((f = tabAt(tab, i = (n - 1) \u0026 hash)) == null) { // å¦‚æœæ•°ç»„è¯¥ä½ç½®ä¸ºç©ºï¼Œç”¨ä¸€æ¬¡casæ“ä½œå°†æ–°å€¼æ”¾å…¥å…¶ä¸­ï¼Œå¦‚æœcaså¤±è´¥ï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªå¾ªç¯ if (casTabAt(tab, i, null, new Node\u003cK,V\u003e(hash, key, value, null))) break; // no lock when adding to empty bin } // å½“å‰ä½ç½®å·²ç»æ‰©å®¹å®Œæˆï¼ŒMOVEDç”¨äºæ ‡è®°æ‰©å®¹ // helpTransferä¹‹åä¼šè¿›å…¥ä¸‹ä¸€è½®å¾ªç¯ // è¿™é‡Œä¹Ÿèƒ½çœ‹å‡ºï¼Œputæ“ä½œå¦‚æœé‡åˆ°å¯¹åº”çš„hashæ¡¶å·²ç»è¢«è¿ç§»ï¼Œé‚£ä¹ˆä¸å¾—å·²ï¼Œå½“å‰çº¿ç¨‹éœ€è¦ååŠ©transferï¼Œç›´åˆ°æ•´ä¸ªtableè¿ç§»å®Œæˆ else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { // fæ˜¯è¯¥ä½ç½®çš„å¤´èŠ‚ç‚¹ï¼Œè€Œä¸”ä¸ä¸ºç©º V oldVal = null; // è·å–æ•°ç»„è¯¥ä½ç½®å¤´ç»“ç‚¹çš„ç›‘è§†å™¨é” synchronized (f) { // è·å–é”ä¹‹åé‡æ–°åˆ¤æ–­ä¸€ä¸‹å½“å‰ä½ç½®çš„èŠ‚ç‚¹æ˜¯å¦å·²ç»æ”¹å˜ï¼Œå¦‚æœå·²ç»æ”¹å˜ï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªå¾ªç¯ // å½“è¿ç§»å®Œæˆæ—¶ï¼Œå¤´èŠ‚ç‚¹æ”¹æˆForwardingNodeï¼Œåˆ¤æ–­å¤±è´¥ï¼Œä¼šè¿›å…¥ä¸‹ä¸€è½®å¾ªç¯ï¼Œèµ°MOVEDåˆ†æ”¯ if (tabAt(tab, i) == f) { // å¤´ç»“ç‚¹çš„hashå€¼å¤§äº0ï¼Œè¯´æ˜æ˜¯é“¾è¡¨ if (fh \u003e= 0) { // ç”¨äºç´¯åŠ ï¼Œè®°å½•é“¾è¡¨çš„é•¿åº¦ binCount = 1; for (Node\u003cK,V\u003e e = f;; ++binCount) { K ek; // å¦‚æœå‘ç°äº†ç›¸ç­‰çš„keyï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œå€¼è¦†ç›–ï¼Œæœ€åè·³å‡ºå¾ªç¯ if (e.hash == hash \u0026\u0026 ((ek = e.key) == key || (ek != null \u0026\u0026 key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } // åˆ°äº†é“¾è¡¨çš„æœ€æœ«ç«¯ï¼Œå°†è¿™ä¸ªæ–°å€¼æ”¾åˆ°é“¾è¡¨çš„æœ€åé¢ Node\u003cK,V\u003e pred = e; if ((e = e.next) == null) { pred.next = new Node\u003cK,V\u003e(hash, key, value, null); break; } } } else if (f instanceof TreeBin) { // çº¢é»‘æ ‘ Node\u003cK,V\u003e p; binCount = 2; if ((p = ((TreeBin\u003cK,V\u003e)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) { // åˆ¤æ–­æ˜¯å¦è¦å°†é“¾è¡¨è½¬æ¢ä¸ºçº¢é»‘æ ‘ï¼Œä¸´ç•Œå€¼å’ŒHashMapä¸€æ ·ï¼Œä¹Ÿæ˜¯8 if (binCount \u003e= TREEIFY_THRESHOLD) // è¿™ä¸ªæ–¹æ³•å’ŒHashMapä¸­ç¨å¾®æœ‰ä¸€ç‚¹ç‚¹ä¸åŒï¼Œé‚£å°±æ˜¯å®ƒä¸æ˜¯ä¸€å®šä¼šè¿›è¡Œçº¢é»‘æ ‘è½¬æ¢ // å¦‚æœå½“å‰æ•°æ®çš„é•¿åº¦å°äº64ï¼Œé‚£ä¹ˆä¼šé€‰æ‹©è¿›è¡Œæ•°ç»„æ‰©å®¹ï¼Œè€Œä¸æ˜¯è½¬æ¢ä¸ºçº¢é»‘æ ‘ treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } addCount(1L, binCount); return null; } å¯ä»¥çœ‹åˆ°ï¼ŒConcurrentHashMapåœ¨è¿›è¡Œé‡è¦æ“ä½œæ—¶ï¼Œä¼šå¯¹æ•°ç»„ä¸­çš„å…ƒç´ è¿›è¡ŒåŠ é”ï¼Œè¿™æ ·ä¿è¯äº†åŠ é”çš„ç²’åº¦é€‚åˆï¼Œé¿å…è¿‡ç²—å¯¼è‡´å¹¶å‘æ€§èƒ½ä¸‹é™ã€‚ ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:2","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"åˆå§‹åŒ–æ•°ç»„ initTable /** * Initializes table, using the size recorded in sizeCtl. */ private final Node\u003cK,V\u003e[] initTable() { Node\u003cK,V\u003e[] tab; int sc; while ((tab = table) == null || tab.length == 0) { // å…¶ä»–çº¿ç¨‹å·²ç»åœ¨åˆå§‹åŒ–æ•°ç»„äº†ï¼Œspinç­‰å¾… if ((sc = sizeCtl) \u003c 0) Thread.yield(); // lost initialization race; just spin // CASä¸€ä¸‹ï¼Œå°†sizeCtlè®¾ç½®ä¸º-1ï¼Œè¡¨ç¤ºæŠ¢åˆ°äº†é” else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if ((tab = table) == null || tab.length == 0) { // DEFAULT_CAPACITYé»˜è®¤åˆå§‹å®¹é‡ä¸º16 int n = (sc \u003e 0) ? sc : DEFAULT_CAPACITY; // åˆå§‹åŒ–æ•°ç»„ï¼Œé•¿åº¦ä¸º16æˆ–è€…åˆå§‹åŒ–æ—¶æä¾›çš„é•¿åº¦ @SuppressWarnings(\"unchecked\") Node\u003cK,V\u003e[] nt = (Node\u003cK,V\u003e[])new Node\u003c?,?\u003e[n]; // å°†æ–°çš„æ•°ç»„èµ‹å€¼ç»™tableï¼Œtableæ˜¯volatile table = tab = nt; sc = n - (n \u003e\u003e\u003e 2); } } finally { // è®¾ç½®sizeCtlä¸ºsc sizeCtl = sc; } break; } } return tab; } åˆå§‹åŒ–ä¸€ä¸ªåˆé€‚å¤§å°çš„æ•°ç»„ï¼Œç„¶åä¼šè®¾ç½®sizeCtlã€‚ åˆå§‹åŒ–æ–¹æ³•ä¸­çš„å¹¶å‘é—®é¢˜æ˜¯é€šè¿‡å¯¹sizeCtlè¿›è¡Œä¸€ä¸ªCASæ“ä½œæ¥æ§åˆ¶çš„ã€‚ ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:3","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"helpTransfer /** * The maximum number of threads that can help resize. * Must fit in 32 - RESIZE_STAMP_BITS bits. */ private static final int MAX_RESIZERS = (1 \u003c\u003c (32 - RESIZE_STAMP_BITS)) - 1; /** * Helps transfer if a resize is in progress. */ final Node\u003cK,V\u003e[] helpTransfer(Node\u003cK,V\u003e[] tab, Node\u003cK,V\u003e f) { Node\u003cK,V\u003e[] nextTab; int sc; if (tab != null \u0026\u0026 (f instanceof ForwardingNode) \u0026\u0026 (nextTab = ((ForwardingNode\u003cK,V\u003e)f).nextTable) != null) { int rs = resizeStamp(tab.length) \u003c\u003c RESIZE_STAMP_SHIFT; while (nextTab == nextTable \u0026\u0026 table == tab \u0026\u0026 (sc = sizeCtl) \u003c 0) { // è¿™é‡Œæœ‰ä¸‰ç§æƒ…å†µï¼Œä¸å¸®å¿™transfer // 1. å¸®åŠ©è¿ç§»çš„çº¿ç¨‹æ•°å·²ç»è¾¾åˆ°ä¸Šé™ // 2. // 3. transferIndexå°äº0ï¼Œè¡¨ç¤ºæ•°ç»„å·²ç»è¿ç§»å®Œæˆ if (sc == rs + MAX_RESIZERS || sc == rs + 1 || transferIndex \u003c= 0) break; // CASæ“ä½œå°†sizeCtrlåŠ ä¸€ï¼ŒæˆåŠŸåè¿›è¡Œtransferå¹¶è·³å‡ºå¾ªç¯ if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) { transfer(tab, nextTab); break; } } return nextTab; } return table; } ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:4","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"é“¾è¡¨è½¬çº¢é»‘æ ‘ï¼š treeifyBin /** * Replaces all linked nodes in bin at given index unless table is * too small, in which case resizes instead. */ private final void treeifyBin(Node\u003cK,V\u003e[] tab, int index) { Node\u003cK,V\u003e b; int n, sc; if (tab != null) { // MIN_TREEIFY_CAPACITYä¸º64 // æ‰€ä»¥ï¼Œå¦‚æœæ•°ç»„é•¿åº¦å°äº64çš„æ—¶å€™ï¼Œå…¶å®ä¹Ÿå°±æ˜¯32æˆ–è€…16æˆ–è€…æ›´å°çš„æ—¶å€™ï¼Œä¼šè¿›è¡Œæ•°ç»„æ‰©å®¹ if ((n = tab.length) \u003c MIN_TREEIFY_CAPACITY) // è§¦å‘æ‰©å®¹ tryPresize(n \u003c\u003c 1); // å½“å‰ä½ç½®æ˜¯é“¾è¡¨ else if ((b = tabAt(tab, index)) != null \u0026\u0026 b.hash \u003e= 0) { // åŠ é” synchronized (b) { if (tabAt(tab, index) == b) { // éå†é“¾è¡¨ï¼Œå»ºç«‹ä¸€é¢—çº¢é»‘æ ‘ TreeNode\u003cK,V\u003e hd = null, tl = null; for (Node\u003cK,V\u003e e = b; e != null; e = e.next) { TreeNode\u003cK,V\u003e p = new TreeNode\u003cK,V\u003e(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; } // å°†çº¢é»‘æ ‘è®¾ç½®åˆ°æ•°ç»„ç›¸åº”ä½ç½® setTabAt(tab, index, new TreeBin\u003cK,V\u003e(hd)); } } } } } ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:5","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"æ‰©å®¹ï¼š tryPreSize /** * Tries to presize table to accommodate the given number of elements. * * @param size number of elements (doesn't need to be perfectly accurate) */ private final void tryPresize(int size) { // c: sizeçš„1.5å€ï¼Œå†åŠ 1ï¼Œå†å¾€ä¸Šå»æœ€è¿‘çš„2çš„næ¬¡æ–¹ int c = (size \u003e= (MAXIMUM_CAPACITY \u003e\u003e\u003e 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size \u003e\u003e\u003e 1) + 1); int sc; // resizeå¼€å§‹å, sizeCtlä¸ºè´Ÿæ•°ï¼Œæ‰€ä»¥å¦‚æœå·²ç»å¼€å§‹resizeï¼Œè¿™æ®µé€»è¾‘ä¼šè¢«è·³è¿‡ while ((sc = sizeCtl) \u003e= 0) { Node\u003cK,V\u003e[] tab = table; int n; // åˆå§‹åŒ–æ•°ç»„ï¼Œå’Œä¹‹å‰ç±»ä¼¼ if (tab == null || (n = tab.length) == 0) { n = (sc \u003e c) ? sc : c; if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if (table == tab) { @SuppressWarnings(\"unchecked\") Node\u003cK,V\u003e[] nt = (Node\u003cK,V\u003e[])new Node\u003c?,?\u003e[n]; table = nt; sc = n - (n \u003e\u003e\u003e 2); } } finally { sizeCtl = sc; } } } // å®¹é‡è¶³å¤Ÿï¼Œè·³å‡ºå¾ªç¯ else if (c \u003c= sc || n \u003e= MAXIMUM_CAPACITY) break; else if (tab == table) { int rs = resizeStamp(n); // https://bugs.java.com/bugdatabase/view_bug.do?bug_id=8215409 // sc \u003c 0æ°¸è¿œä¸ä¼šæˆç«‹ï¼Œæ‰€ä»¥è¿™æ®µä»£ç ä¸èµ·ä½œç”¨ï¼Œåœ¨jdk11ä¸­å·²ç»å»æ‰ // å¤åˆ¶ç²˜è´´æ˜¯æ¯ä½ç¨‹åºå‘˜çš„å¿…å¤‡æŠ€èƒ½ if (sc \u003c 0) { Node\u003cK,V\u003e[] nt; if ((sc \u003e\u003e\u003e RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex \u003c= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); } // è¿™é‡Œä¸ºä»€ä¹ˆè¦åŠ 2ï¼Œæ²¡æœ‰çœ‹æ‡‚ else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs \u003c\u003c RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); } } } /** * The maximum number of threads that can help resize. * Must fit in 32 - RESIZE_STAMP_BITS bits. */ // é«˜16ä½ä¸º0ï¼Œä½16ä½ä¸º1 private static final int MAX_RESIZERS = (1 \u003c\u003c (32 - RESIZE_STAMP_BITS)) - 1; /** * The number of bits used for generation stamp in sizeCtl. * Must be at least 6 for 32bit arrays. */ private static int RESIZE_STAMP_BITS = 16; /** * Returns the stamp bits for resizing a table of size n. * Must be negative when shifted left by RESIZE_STAMP_SHIFT. */ // nä¸ºæ•°ç»„é•¿åº¦ï¼Œæ‰€ä»¥ä¸€å®šæ˜¯2^nï¼Œnçš„å‰å¯¼é›¶ä¸ªæ•°å®é™…ä¸Šç”¨æ›´å°‘çš„ä½æ•°ç¼–ç äº†n // ä»ä½ä½èµ·ç¬¬16ä½ä¸º1ï¼ˆä»1å¼€å§‹è®¡æ•°ï¼‰ï¼Œè¿™æ ·å·¦ç§»16ä½åä¸€å®šæ˜¯ä¸€ä¸ªè´Ÿæ•° // æ‰€ä»¥ resizeStamp(int n)çš„æ•ˆæœæ˜¯å°†nè¿›è¡Œäº†é‡æ–°ç¼–ç ï¼Œå¹¶ä¸”æ·»åŠ äº†resizeæˆ³è®° static final int resizeStamp(int n) { return Integer.numberOfLeadingZeros(n) | (1 \u003c\u003c (RESIZE_STAMP_BITS - 1)); } ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:6","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"æ•°æ®è¿ç§»ï¼š transfer å°†åŸæ¥çš„tabæ•°ç»„ä¸­çš„å…ƒç´ è¿ç§»åˆ°æ–°çš„nextTabæ•°ç»„ä¸­ã€‚ ä¹‹å‰æåˆ°çš„tryPresizeæ–¹æ³•ä¸­è°ƒç”¨transferä¸æ¶‰åŠå¤šçº¿ç¨‹ï¼Œä½†transferæ–¹æ³•å¯ä»¥åœ¨å…¶ä»–åœ°æ–¹è¢«è°ƒç”¨ã€‚å…¸å‹åœ°ï¼Œæˆ‘ä»¬ä¹‹å‰åœ¨è¯´putæ–¹æ³•çš„æ—¶å€™å·²ç»è¯´è¿‡äº†ï¼ŒhelpTransferæ–¹æ³•ä¸­ä¼šè°ƒç”¨transferæ–¹æ³•ã€‚ æ­¤æ–¹æ³•æ”¯æŒå¤šçº¿ç¨‹æ‰§è¡Œï¼Œå¤–å›´è°ƒç”¨æ­¤æ–¹æ³•æ—¶ï¼Œä¼šä¿è¯ç¬¬ä¸€ä¸ªå‘èµ·æ•°æ®è¿ç§»çš„çº¿ç¨‹ï¼ŒnextTabä¸ºnullï¼Œä¹‹åå†è°ƒç”¨æ­¤æ–¹æ³•çš„æ—¶å€™ï¼ŒnextTabä¸ä¼šä¸ºnullã€‚ åŸæ•°ç»„é•¿åº¦ä¸ºnï¼Œæ‰€æœ‰æˆ‘ä»¬æœ‰nä¸ªè¿ç§»ä»»åŠ¡ï¼Œè®©æ¯ä¸ªçº¿ç¨‹æ¯æ¬¡è´Ÿè´£ä¸€ä¸ªå°ä»»åŠ¡æ˜¯æœ€ç®€å•çš„ï¼Œæ¯åšå®Œä¸€ä¸ªä»»åŠ¡å†æ£€æµ‹æ˜¯å¦æœ‰å…¶ä»–æ²¡åšå®Œçš„ä»»åŠ¡ï¼Œå¸®åŠ©è¿ç§»æ—§å¯ä»¥äº†ï¼Œè€ŒDoug Leaä½¿ç”¨äº†ä¸€ä¸ªstrideï¼Œç®€å•ç†è§£å°±æ˜¯æ­¥é•¿ï¼Œæ¯ä¸ªçº¿ç¨‹æ¯æ¬¡è´Ÿè´£è¿ç§»å…¶ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œå¦‚æ¯æ¬¡è¿ç§»16ä¸ªå°ä»»åŠ¡ã€‚æ‰€ä»¥æˆ‘ä»¬å°±éœ€è¦ä¸€ä¸ªå…¨å±€çš„è°ƒåº¦è€…æ¥å®‰æ’å“ªä¸ªçº¿ç¨‹æ‰§è¡Œå“ªå‡ ä¸ªä»»åŠ¡ï¼Œè¿™ä¸ªå°±æ˜¯å±æ€§transferIndexçš„ä½œç”¨ã€‚ ç¬¬ä¸€ä¸ªå‘èµ·æ•°æ®è¿ç§»çš„çº¿ç¨‹ä¼šå°†transferIndexæŒ‡å‘åŸæ•°ç»„æœ€åçš„ä½ç½®ï¼Œç„¶åä»åå¾€å‰çš„strideä¸ªä»»åŠ¡å±äºç¬¬ä¸€ä¸ªçº¿ç¨‹ï¼Œç„¶åå°†transferIndexæŒ‡å‘æ–°çš„ä½ç½®ï¼Œå†å¾€å‰çš„strideä¸ªä»»åŠ¡å±äºç¬¬äºŒä¸ªçº¿ç¨‹ï¼Œä»¥æ­¤ç±»æ¨ï¼Œå½“ç„¶ï¼Œè¿™é‡Œè¯´çš„ç¬¬äºŒä¸ªçº¿ç¨‹ä¸æ˜¯çœŸçš„ä¸€å®šæŒ‡ä»£äº†ç¬¬äºŒä¸ªçº¿ç¨‹ï¼Œä¹Ÿå¯ä»¥æ˜¯åŒä¸€ä¸ªçº¿ç¨‹ï¼Œå…¶å®å°±æ˜¯å°†ä¸€ä¸ªå¤§çš„è¿ç§»ä»»åŠ¡åˆ†ä¸ºä¸€ä¸ªä¸ªä»»åŠ¡åŒ…ã€‚ ä¹‹å‰æåˆ°ï¼ŒåŸæ•°ç»„iä½ç½®çš„é”®å€¼å¯¹ä¼šè¢«åˆ†é…åˆ°æ–°æ•°ç»„iä½ç½®å’Œæ–°æ•°ç»„i + oldLengthä½ç½®ï¼Œè¿™æ ·æ¯ä¸ªè¿ç§»å°ä»»åŠ¡ç›¸äº’ä¹‹å‰ä¸å­˜åœ¨èµ„æºç«äº‰ã€‚ /** * The next table index (plus one) to split while resizing. */ private transient volatile int transferIndex; /** * Moves and/or copies the nodes in each bin to new table. See * above for explanation. */ private final void transfer(Node\u003cK,V\u003e[] tab, Node\u003cK,V\u003e[] nextTab) { int n = tab.length, stride; // stideåœ¨å•æ ¸ä¸‹ç›´æ¥ç­‰äºnï¼Œå¤šæ ¸æ¨¡å¼ä¸‹ä¸º n \u003e\u003e\u003e 3 / NCPUï¼Œæœ€å°å€¼ä¸º16 if ((stride = (NCPU \u003e 1) ? (n \u003e\u003e\u003e 3) / NCPU : n) \u003c MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range // å¦‚æœnextTabä¸ºnullï¼Œå…ˆè¿›è¡Œä¸€æ¬¡åˆå§‹åŒ– // ä¹‹å‰æè¿‡ï¼Œå¤–å›´ä¼šä¿è¯ç¬¬ä¸€ä¸ªå‘èµ·è¿ç§»çš„çº¿ç¨‹è°ƒç”¨æ­¤æ–¹æ³•æ—¶ï¼Œå‚æ•°nextTabä¸ºnull // ä¹‹åå‚ä¸è¿ç§»çš„çº¿ç¨‹è°ƒç”¨æ­¤æ–¹æ³•æ—¶ï¼ŒnextTabä¸ä¸ºnull if (nextTab == null) { // initiating try { @SuppressWarnings(\"unchecked\") // å®¹é‡ç¿»å€ Node\u003cK,V\u003e[] nt = (Node\u003cK,V\u003e[])new Node\u003c?,?\u003e[n \u003c\u003c 1]; nextTab = nt; } catch (Throwable ex) { // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; } // èµ‹å€¼ç»™nextTableå±æ€§ nextTable = nextTab; // transferå±æ€§ç”¨äºæ§åˆ¶è¿ç§»çš„ä½ç½®ï¼Œåˆå§‹ä¸ºåŸå…ˆæ•°ç»„çš„é•¿åº¦ transferIndex = n; } int nextn = nextTab.length; // ForwardingNodeç¿»è¯‘è¿‡æ¥å°±æ˜¯æ­£åœ¨è¿ç§»çš„Node // è¿™ä¸ªæ„é€ æ–¹æ³•ä¼šç”Ÿæˆä¸€ä¸ªNode, key, valueå’Œnextéƒ½æ˜¯nullï¼Œå…³é”®æ˜¯hashä¸ºMOVED // åé¢æˆ‘ä»¬ä¼šçœ‹åˆ°ï¼ŒåŸæ•°ç»„ä¸­ä½ç½®iå¤„çš„èŠ‚ç‚¹å®Œæˆè¿ç§»å·¥ä½œåï¼Œå°†ä¼šå°†ä½ç½®iå¤„è®¾ç½®ä¸ºè¿™ä¸ªForwardingNodeï¼Œç”¨æ¥å‘Šè¯‰å…¶ä»–çº¿ç¨‹è¯¥ä½ç½®å·²ç»å¤„ç†è¿‡äº† // æ‰€ä»¥å®ƒå…¶å®ç›¸å½“äºä¸€ä¸ªæ ‡å¿— ForwardingNode\u003cK,V\u003e fwd = new ForwardingNode\u003cK,V\u003e(nextTab); // advanceæŒ‡çš„æ˜¯åšå®Œäº†ä¸€ä¸ªä½ç½®çš„è¿ç§»å·¥ä½œï¼Œå¯ä»¥å‡†å¤‡åšä¸‹ä¸€ä¸ªä½ç½®çš„äº† boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab // iæ˜¯ä½ç½®ç´¢å¼•ï¼Œboundæ˜¯è¾¹ç•Œï¼Œæ³¨æ„æ˜¯ä»åå¾€å‰ for (int i = 0, bound = 0;;) { Node\u003cK,V\u003e f; int fh; // advanceä¸ºtrueè¡¨ç¤ºå¯ä»¥è¿›è¡Œä¸‹ä¸€ä¸ªä½ç½®çš„è¿ç§»äº† // ç®€å•ç†è§£ç»“å±€ï¼šiæŒ‡å‘äº†transferIndex, boundæŒ‡å‘äº†transferIndex - stride while (advance) { int nextIndex, nextBound; if (--i \u003e= bound || finishing) advance = false; // å°†transferIndexèµ‹å€¼ç»™nextIndex // è¿™é‡ŒtransferIndexä¸€æ—¦å°äºç­‰äº0ï¼Œè¯´æ˜åŸæ•°ç»„çš„æ‰€æœ‰ä½ç½®éƒ½æœ‰ç›¸åº”çš„çº¿ç¨‹å»å¤„ç†äº† else if ((nextIndex = transferIndex) \u003c= 0) { i = -1; advance = false; } else if (U.compareAndSetInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex \u003e stride ? nextIndex - stride : 0))) { // nextBoundæ˜¯è¿™æ¬¡è¿ç§»ä»»åŠ¡çš„è¾¹ç•Œï¼Œæ³¨æ„æ˜¯ä»åå¾€å‰ bound = nextBound; i = nextIndex - 1; advance = false; } } if (i \u003c 0 || i \u003e= n || i + n \u003e= nextn) { int sc; if (finishing) { // æ‰€æœ‰è¿ç§»æ“ä½œéƒ½å·²ç»å®Œæˆ nextTable = null; // å°†æ–°çš„nextTabèµ‹å€¼ç»™tableå±æ€§ï¼Œå®Œæˆè¿ç§» table = nextTab; // é‡æ–°è®¡ç®—sizeCtrl: nä¸ºåŸæ•°ç»„é•¿åº¦ï¼Œæ‰€ä»¥sizeCtrlå¾—å‡ºçš„å€¼å°†æ˜¯æ–°æ•°ç»„é•¿åº¦çš„0.75å€ sizeCtl = (n \u003c\u003c 1) - (n \u003e\u003e\u003e 1); return; } // ä¹‹å‰æˆ‘ä»¬è¯´è¿‡ï¼ŒsizeCtrlåœ¨è¿ç§»å‰ä¼šè®¾ç½®ä¸º (rs \u003c\u003c RESIZE_STAMP_SHIFT) + 2 // ç„¶åï¼Œæ¯æœ‰ä¸€ä¸ªçº¿ç¨‹å‚ä¸è¿ç§»å°±ä¼šå°†sizeCtrl + 1 // è¿™é‡Œä½¿ç”¨CASæ“ä½œå¯¹sizeCtrlè¿›è¡Œå‡ä¸€ï¼Œè¡¨ç¤ºåšå®Œäº†å±äºè‡ªå·±çš„ä»»åŠ¡ if (U.compareAndSetInt(this, SIZECTL, sc = sizeCtl, sc - 1)) { // ä»»åŠ¡ç»“æŸï¼Œæ–¹æ³•é€€å‡º if ((sc - 2) != resizeStamp(n) \u003c\u003c RESIZE_STAMP_SHIFT) return; // æ‰€æœ‰çš„è¿ç§»ä»»åŠ¡éƒ½å·²ç»åšå®Œ finishing = advance = true; i = n; // recheck before commit } } // å¦‚æœä½ç½®iå¤„æ˜¯ç©ºçš„ï¼Œæ²¡æœ‰ä»»ä½•èŠ‚ç‚¹ï¼Œé‚£ä¹ˆæ”¾å…¥åˆšæ‰åˆå§‹åŒ–çš„ForwardingNodeèŠ‚ç‚¹ else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); // è¯¥ä½ç½®å¤„æ˜¯ä¸€ä¸ªForwardingNodeï¼Œä»£è¡¨è¯¥ä½ç½®å·²ç»è¿ç§»è¿‡äº† else if ((fh = f.hash) == MOVED) advance = true; // already processed else { // å¯¹æ•°ç»„è¯¥ä½ç½®å¤„çš„èŠ‚ç‚¹åŠ é”ï¼Œå¼€å§‹å¤„ç†æ•°ç»„è¯¥ä½ç½®å¤„çš„è¿ç§»å·¥ä½œ synchronized (f) { if (tabAt(tab, i) == f) { Node\u003cK,V\u003e ln, hn; // å¤´èŠ‚ç‚¹çš„hashå¤§äº0ï¼Œè¯´æ˜æ˜¯é“¾è¡¨çš„NodeèŠ‚ç‚¹ if (fh \u003e= 0) { // è¿™é‡Œå±•å¼€è§£é‡Šä¸€ä¸‹runBits // å‡è®¾ä¸€ä¸ªkeyçš„hashå€¼ä¸º 0010 // tableæ•°ç»„çš„é•¿åº¦æ€»æ˜¯2^nï¼Œè¿™é‡Œå‡å®šä¸º4,ä¹Ÿå°±æ˜¯ 0100 // æ‰€ä»¥è¿™ä¸ªå¯¹è±¡ä¼šè¢«æ”¾å…¥ i = hash \u0026 (length - 1)ä½ç½®ï¼ˆæ•ˆæœç­‰ä»·äº hash % length)ï¼Œå½“ç„¶è¿™æ˜¯å› ä¸ºlengthä¸€å®šä¸º2^n // è¿˜æœ‰å“ªäº›å¯¹è±¡å¯èƒ½è¢«æ”¾å…¥è¿™ä¸ªæ•°ç»„ä½ç½®å‘¢ï¼Œéœ€è¦æ»¡è¶³ hash \u0026 0011 = 0010 // æ˜¾ç„¶ä½ä¸¤ä½è¦ä¿æŒä¸€è‡´ï¼Œä¸èƒ½æ”¹å˜ï¼Œå…¶ä½™é«˜ä½å¯ä»¥ä»»æ„ // ç°åœ¨å‘ç”Ÿäº†ä¸¤å€æ‰©å®¹ï¼Œiä½ç½®éƒ½éœ€è¦è¿ç§»åˆ°æ–°çš„ä½ç½® æ–°çš„ä½ç½®é€šè¿‡ hash \u0026 0111å†³å®š // åŸå…ˆæ•°ç»„ä½ç½®ä¸­å…ƒç´ çš„hashå€¼å€’æ•°ç¬¬ä¸‰ä½å¯èƒ½ä¸º0ï¼Œä¹Ÿå¯èƒ½ä¸º1ï¼Œä¹Ÿå°±æ˜¯è¿™é‡Œçš„runBitsï¼Œæ‰€ä»¥å…ƒç´ ä¼šè¢«åˆ†å¸ƒåˆ°ä¸åŒçš„ä½ç½® // å¦‚æœä¸º0ï¼Œå’ŒåŸå…ˆä¸€æ ·ï¼Œåˆ†å¸ƒåˆ°iä¸ºæ­¢ // å¦‚æœæ˜¯1ï¼Œåˆ†å¸ƒåˆ° i + oldLengthä½ç½®ï¼ŒoldLengthè¡¨ç¤ºåŸå…ˆçš„æ•°ç»„é•¿åº¦ int runBit = fh \u0026 n; Node\u003cK,V\u003e lastR","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:7","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"getè¿‡ç¨‹åˆ†æ public V get(Object key) { Node\u003cK,V\u003e[] tab; Node\u003cK,V\u003e e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null \u0026\u0026 (n = tab.length) \u003e 0 \u0026\u0026 (e = tabAt(tab, (n - 1) \u0026 h)) != null) { // åˆ¤æ–­å¤´èŠ‚ç‚¹æ˜¯å¦å°±æ˜¯æŸ¥æ‰¾çš„èŠ‚ç‚¹ if ((eh = e.hash) == h) { if ((ek = e.key) == key || (ek != null \u0026\u0026 key.equals(ek))) return e.val; } // å¦‚æœå¤´èŠ‚ç‚¹çš„hashå°äº0ï¼Œè¯´æ˜æ­£åœ¨æ‰©å®¹ï¼Œæˆ–è€…è¯¥ä½ç½®æ˜¯çº¢é»‘æ ‘ else if (eh \u003c 0) // å‚è€ƒ ForwardingNode.find(int h, Object k) å’Œ TreeBin.find(int h, Object k) return (p = e.find(h, key)) != null ? p.val : null; // éå†é“¾è¡¨ while ((e = e.next) != null) { if (e.hash == h \u0026\u0026 ((ek = e.key) == key || (ek != null \u0026\u0026 key.equals(ek)))) return e.val; } } return null; } å¯ä»¥çœ‹åˆ°getæ“ä½œçš„å®ç°åŸºæœ¬ä¸Šæ˜¯æ— é”çš„ã€‚ /** * A node inserted at head of bins during transfer operations. */ static final class ForwardingNode\u003cK,V\u003e extends Node\u003cK,V\u003e { final Node\u003cK,V\u003e[] nextTable; ForwardingNode(Node\u003cK,V\u003e[] tab) { super(MOVED, null, null, null); this.nextTable = tab; } Node\u003cK,V\u003e find(int h, Object k) { // loop to avoid arbitrarily deep recursion on forwarding nodes // é€šè¿‡å¾ªç¯æ¥é¿å…å¯¹ForwardingNodeçš„é€’å½’ outer: for (Node\u003cK,V\u003e[] tab = nextTable;;) { Node\u003cK,V\u003e e; int n; // æ²¡æœ‰æ‰¾åˆ°å…ƒç´ ï¼Œè¿”å›null if (k == null || tab == null || (n = tab.length) == 0 || (e = tabAt(tab, (n - 1) \u0026 h)) == null) return null; for (;;) { int eh; K ek; // å¤´èŠ‚ç‚¹å°±æ˜¯æ‰€éœ€è¦çš„èŠ‚ç‚¹ï¼Œç›´æ¥è¿”å› if ((eh = e.hash) == h \u0026\u0026 ((ek = e.key) == k || (ek != null \u0026\u0026 k.equals(ek)))) return e; if (eh \u003c 0) { if (e instanceof ForwardingNode) { tab = ((ForwardingNode\u003cK,V\u003e)e).nextTable; continue outer; } else return e.find(h, k); } // æœç´¢åˆ°äº†é“¾è¡¨æœ«å°¾ï¼Œè¿”å›null if ((e = e.next) == null) return null; } } } } ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:8","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"clearè¿‡ç¨‹åˆ†æ /** * Removes all of the mappings from this map. */ public void clear() { long delta = 0L; // negative number of deletions int i = 0; Node\u003cK,V\u003e[] tab = table; while (tab != null \u0026\u0026 i \u003c tab.length) { int fh; // éå†æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´  Node\u003cK,V\u003e f = tabAt(tab, i); // å…ƒç´ ä¸ºnullï¼Œç»§ç»­ä¸‹ä¸€ä¸ª if (f == null) ++i; // å¦‚æœå½“å‰ä½ç½®çš„å…ƒç´ å·²ç»è¢«ç§»åŠ¨åˆ°æ–°çš„æ•°ç»„ä¸­ï¼Œå¸®åŠ©transferï¼Œç„¶årestart // restart è·³åˆ°æ–°çš„tableä¸Šï¼Œé‡æ–°å¼€å§‹ else if ((fh = f.hash) == MOVED) { tab = helpTransfer(tab, f); i = 0; // restart } else { // è·å–å½“å‰ä½ç½®çš„å¤´èŠ‚ç‚¹çš„ç›‘è§†å™¨ synchronized (f) { if (tabAt(tab, i) == f) { Node\u003cK,V\u003e p = (fh \u003e= 0 ? f : (f instanceof TreeBin) ? ((TreeBin\u003cK,V\u003e)f).first : null); // éå†é“¾è¡¨æˆ–è€…çº¢é»‘æ ‘ï¼Œè·å–åˆ é™¤çš„èŠ‚ç‚¹ä¸ªæ•° while (p != null) { --delta; p = p.next; } // æ¸…ç©ºå½“å‰ä½ç½® setTabAt(tab, i++, null); } } } } if (delta != 0L) addCount(delta, -1); } ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:9","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"removeæ“ä½œ /** * Removes the key (and its corresponding value) from this map. * This method does nothing if the key is not in the map. * * @param key the key that needs to be removed * @return the previous value associated with {@code key}, or * {@code null} if there was no mapping for {@code key} * @throws NullPointerException if the specified key is null */ public V remove(Object key) { return replaceNode(key, null, null); } /** * Implementation for the four public remove/replace methods: * Replaces node value with v, conditional upon match of cv if * non-null. If resulting value is null, delete. */ // cvæ˜¯compareValueçš„ç¼©å†™ï¼Œè¡¨ç¤ºæœŸæœ›å€¼ // å¦‚æœcvä¸ä¸ºnullå¹¶ä¸”åŒ¹é…å½“å‰å€¼ï¼Œå°†èŠ‚ç‚¹çš„å€¼æ›¿æ¢ä¸ºvï¼Œå¦‚æœæ›¿æ¢åçš„å€¼ä¸ºnullï¼Œåˆ™åˆ é™¤è¯¥èŠ‚ç‚¹ final V replaceNode(Object key, V value, Object cv) { int hash = spread(key.hashCode()); for (Node\u003cK,V\u003e[] tab = table;;) { Node\u003cK,V\u003e f; int n, i, fh; // é€šè¿‡hashå€¼åˆ¤æ–­keyä¸å­˜åœ¨ï¼Œç›´æ¥è¿”å› if (tab == null || (n = tab.length) == 0 || (f = tabAt(tab, i = (n - 1) \u0026 hash)) == null) break; // tableæ­£åœ¨æ‰©å®¹ï¼Œå¸®åŠ©æ‰©å®¹ï¼Œç„¶åè¿›å…¥ä¸‹ä¸€è½®å¾ªç¯ else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { V oldVal = null; boolean validated = false; // å¯¹æ•°ç»„å½“å‰ä½ç½®çš„å¤´èŠ‚ç‚¹åŠ ç›‘è§†å™¨é” synchronized (f) { if (tabAt(tab, i) == f) { if (fh \u003e= 0) { validated = true; // éå†é“¾è¡¨ for (Node\u003cK,V\u003e e = f, pred = null;;) { K ek; if (e.hash == hash \u0026\u0026 ((ek = e.key) == key || (ek != null \u0026\u0026 key.equals(ek)))) { V ev = e.val; // å¦‚æœcvä¸ºnullæˆ–è€…cvå’Œå½“å‰èŠ‚ç‚¹çš„valueç›¸ç­‰ if (cv == null || cv == ev || (ev != null \u0026\u0026 cv.equals(ev))) { oldVal = ev; // valueä¸ä¸ºnullï¼Œæ›¿æ¢å½“å‰å€¼ if (value != null) e.val = value; // åˆ é™¤å½“å‰èŠ‚ç‚¹ï¼Œå½“å‰èŠ‚ç‚¹ä¸æ˜¯å¤´èŠ‚ç‚¹ else if (pred != null) pred.next = e.next; // åˆ é™¤å½“å‰èŠ‚ç‚¹ï¼Œå½“å‰èŠ‚ç‚¹æ˜¯å¤´èŠ‚ç‚¹ else setTabAt(tab, i, e.next); } break; } pred = e; if ((e = e.next) == null) break; } } // çº¢é»‘æ ‘é€»è¾‘ else if (f instanceof TreeBin) { validated = true; TreeBin\u003cK,V\u003e t = (TreeBin\u003cK,V\u003e)f; TreeNode\u003cK,V\u003e r, p; if ((r = t.root) != null \u0026\u0026 (p = r.findTreeNode(hash, key, null)) != null) { V pv = p.val; if (cv == null || cv == pv || (pv != null \u0026\u0026 cv.equals(pv))) { oldVal = pv; if (value != null) p.val = value; else if (t.removeTreeNode(p)) setTabAt(tab, i, untreeify(t.first)); } } } else if (f instanceof ReservationNode) throw new IllegalStateException(\"Recursive update\"); } } if (validated) { if (oldVal != null) { if (value == null) addCount(-1L, -1); return oldVal; } break; } } } return null; } ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:10","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"sizeæ“ä½œ å¯ä»¥çœ‹åˆ°ï¼Œsizeé‡‡ç”¨ç±»ä¼¼äºLongAdderçš„æ–¹å¼ï¼Œå°†ç»Ÿè®¡mappingæ•°é‡çš„å·¥ä½œåˆ†æ•£åˆ°å¤šä¸ªå˜é‡ä¸Šï¼Œé¿å…å½±å“æ€§èƒ½ã€‚ /** * A padded cell for distributing counts. Adapted from LongAdder * and Striped64. See their internal docs for explanation. */ @jdk.internal.vm.annotation.Contended static final class CounterCell { volatile long value; CounterCell(long x) { value = x; } } /** * Base counter value, used mainly when there is no contention, * but also as a fallback during table initialization * races. Updated via CAS. */ private transient volatile long baseCount; /** * Table of counter cells. When non-null, size is a power of 2. */ private transient volatile CounterCell[] counterCells; /** * {@inheritDoc} */ public int size() { long n = sumCount(); return ((n \u003c 0L) ? 0 : (n \u003e (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); } final long sumCount() { CounterCell[] cs = counterCells; long sum = baseCount; if (cs != null) { for (CounterCell c : cs) if (c != null) sum += c.value; } return sum; } /** * Adds to count, and if table is too small and not already * resizing, initiates transfer. If already resizing, helps * perform transfer if work is available. Rechecks occupancy * after a transfer to see if another resize is already needed * because resizings are lagging additions. * * @param x the count to add * @param check if \u003c0, don't check resize, if \u003c= 1 only check if uncontended */ private final void addCount(long x, int check) { CounterCell[] cs; long b, s; if ((cs = counterCells) != null || !U.compareAndSetLong(this, BASECOUNT, b = baseCount, s = b + x)) { CounterCell c; long v; int m; boolean uncontended = true; if (cs == null || (m = cs.length - 1) \u003c 0 || (c = cs[ThreadLocalRandom.getProbe() \u0026 m]) == null || !(uncontended = U.compareAndSetLong(c, CELLVALUE, v = c.value, v + x))) { fullAddCount(x, uncontended); return; } if (check \u003c= 1) return; s = sumCount(); } if (check \u003e= 0) { Node\u003cK,V\u003e[] tab, nt; int n, sc; while (s \u003e= (long)(sc = sizeCtl) \u0026\u0026 (tab = table) != null \u0026\u0026 (n = tab.length) \u003c MAXIMUM_CAPACITY) { int rs = resizeStamp(n) \u003c\u003c RESIZE_STAMP_SHIFT; if (sc \u003c 0) { if (sc == rs + MAX_RESIZERS || sc == rs + 1 || (nt = nextTable) == null || transferIndex \u003c= 0) break; if (U.compareAndSetInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); } else if (U.compareAndSetInt(this, SIZECTL, sc, rs + 2)) transfer(tab, null); s = sumCount(); } } } TODOï¼šåé¢å†çœ‹ ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:11","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":["Spark"],"content":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’ç”Ÿæˆæ–¹æ³• Sparkå…·ä½“é‡‡ç”¨3ä¸ªæ­¥éª¤æ¥ç”Ÿæˆç‰©ç†æ‰§è¡Œè®¡åˆ’ï¼Œé¦–å…ˆæ ¹æ®actionæ“ä½œé¡ºåºå°†åº”ç”¨åˆ’åˆ†ä¸ºä½œä¸šï¼ˆjobï¼‰ï¼Œç„¶åæ ¹æ®æ¯ä¸ªjobçš„é€»è¾‘å¤„ç†æµç¨‹ä¸­çš„ShuffleDependencyä¾èµ–å…³ç³»ï¼Œå°†jobåˆ’åˆ†ä¸ºæ‰§è¡Œé˜¶æ®µï¼ˆstageï¼‰ã€‚æœ€ååœ¨æ¯ä¸ªstageä¸­ï¼Œæ ¹æ®æœ€åç”Ÿæˆçš„RDDçš„åˆ†åŒºä¸ªæ•°ç”Ÿæˆå¤šä¸ªè®¡ç®—ä»»åŠ¡ï¼ˆtaskï¼‰ã€‚ ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:1:0","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"æ ¹æ®actionæ“ä½œå°†åº”ç”¨åˆ’åˆ†ä¸ºä½œä¸šï¼ˆjobï¼‰ å½“åº”ç”¨ç¨‹åºå‡ºç°actionæ“ä½œæ—¶ï¼Œå¦‚resultRDD.action()ï¼Œè¡¨ç¤ºåº”ç”¨ä¼šç”Ÿæˆä¸€ä¸ªjobï¼Œè¯¥jobçš„é€»è¾‘å¤„ç†æµç¨‹ä¸ºä»è¾“å‡ºæ•°æ®åˆ°resultRDDçš„é€»è¾‘å¤„ç†æµç¨‹ã€‚ ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:1:1","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"æ ¹æ®ShuffleDependencyä¾èµ–å…³ç³»å°†jobåˆ’åˆ†æˆæ‰§è¡Œé˜¶æ®µï¼ˆstageï¼‰ å¯¹äºæ¯ä¸ªjobï¼Œä»å…¶æœ€åçš„RDDå¾€å‰å›æº¯æ•´ä¸ªé€»è¾‘å¤„ç†æµç¨‹ï¼Œå¦‚æœé‡åˆ°NarrowDependencyï¼Œåˆ™å°†å½“å‰RDDçš„parent RDDçº³å…¥ï¼Œå¹¶ç»§ç»­å‘å‰è¿½æº¯ï¼Œå½“é‡åˆ°ShuffleDependencyæ—¶ï¼Œåœæ­¢å›æº¯ï¼Œå°†å½“å‰å·²ç»çº³å…¥çš„æ‰€æœ‰RDDæŒ‰ç…§å…¶ä¾èµ–å…³ç³»å»ºç«‹ä¸€ä¸ªæ‰§è¡Œé˜¶æ®µï¼Œå‘½åä¸ºstage iã€‚ å¦‚æœå°†å­˜åœ¨ShuffleDependencyä¾èµ–çš„RDDä¹Ÿçº³å…¥åŒä¸€ä¸ªstageï¼Œè®¡ç®—æ¯ä¸ªåˆ†åŒºæ—¶éƒ½éœ€è¦é‡å¤è®¡ç®—ShuffleDependencyä¸Šæ¸¸çš„RDDï¼Œè¿™æ˜¾ç„¶æ²¡æœ‰å¿…è¦ã€‚ ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:1:2","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"æ ¹æ®åˆ†åŒºè®¡ç®—å°†å„ä¸ªstageåˆ’åˆ†æˆè®¡ç®—ä»»åŠ¡ï¼ˆtaskï¼‰ æ¯ä¸ªåˆ†åŒºä¸Šçš„è®¡ç®—é€»è¾‘ç›¸åŒï¼Œè€Œä¸”æ˜¯ç‹¬ç«‹çš„ï¼Œå› æ­¤æ¯ä¸ªåˆ†åŒºä¸Šçš„è®¡ç®—å¯ä»¥ç‹¬ç«‹æˆä¸ºä¸€ä¸ªtaskã€‚åŒä¸€ä¸ªstageä¸­çš„taskå¯ä»¥åŒæ—¶åˆ†å‘åˆ°ä¸åŒçš„æœºå™¨å¹¶è¡Œæ‰§è¡Œã€‚ ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:1:3","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"jobã€stageå’Œtaskçš„è®¡ç®—é¡ºåº jobçš„æäº¤æ—¶é—´å’Œactionè¢«è°ƒç”¨çš„æ—¶é—´æœ‰å…³ï¼Œå½“åº”ç”¨ç¨‹åºæ‰§è¡Œåˆ°rdd.action()æ—¶ï¼Œå°±ä¼šç«‹å³å°†rdd.actioin()å½¢æˆçš„jobæäº¤ç»™sparkã€‚jobçš„é€»è¾‘å¤„ç†æµç¨‹å®é™…ä¸Šæ˜¯ä¸€ä¸ªDAGå›¾ï¼Œç»è¿‡stageåˆ’åˆ†åï¼Œä»ç„¶æ˜¯DAGå›¾å½¢çŠ¶ã€‚æ¯ä¸ªstageçš„è¾“å‡ºæ•°æ®è¦ä¸æ˜¯jobçš„è¾“å…¥æ•°æ®ï¼Œè¦ä¸æ˜¯ä¸Šæ¸¸stageçš„è¾“å‡ºç»“æœã€‚å› æ­¤ï¼Œè®¡ç®—é¡ºåºä»åŒ…å«è¾“å…¥æ•°æ®çš„stageå¼€å§‹ï¼Œä»å‰å¾€åä¾æ¬¡æ‰§è¡Œï¼Œä»…å½“ä¸Šæ¸¸stageéƒ½æ‰§è¡Œå®Œæˆåï¼Œå†æ‰§è¡Œä¸‹æ¸¸çš„stageã€‚stageä¸­çš„æ¯ä¸ªtaskå› ä¸ºæ˜¯ç‹¬ç«‹è€Œä¸”åŒæ„çš„ï¼Œå¯ä»¥å¹¶è¡Œæ‰§è¡Œæ²¡æœ‰å…ˆåä¹‹åˆ†ã€‚ ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:1:4","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"taskå†…éƒ¨æ•°æ®çš„å­˜å‚¨çš„è®¡ç®—é—®é¢˜ï¼ˆæµæ°´çº¿è®¡ç®—ï¼‰ å‡è®¾ä¸€ä¸ªåˆ†åŒºä¸­æœ‰ä¸‰æ¡è®°å½•ï¼Œåˆ†åˆ«ä¸ºrecord1, record2, record3ï¼Œéœ€è¦å¯¹åˆ†åŒºå…ˆæ‰§è¡Œf()æ“ä½œï¼Œå†æ‰§è¡Œg()æ“ä½œï¼Œå‡è®¾f()æ“ä½œå’Œg()æ“ä½œéƒ½åªä¾èµ–äºä¸Šæ¸¸åˆ†åŒºä¸­çš„å•æ¡è®°å½•ï¼Œåˆ™å¯ä»¥é‡‡ç”¨æµæ°´çº¿è®¡ç®—ã€‚ç±»ä¼¼äºrecord1 -\u003e f(record1) -\u003e record1' -\u003e g(record') -\u003e record''ï¼Œåœ¨taskè®¡ç®—æ—¶åªéœ€è¦å†å†…å­˜ä¸­ä¿ç•™å½“å‰è¢«å¤„ç†çš„å•ä¸ªrecordå³å¯ï¼Œæ²¡æœ‰å¿…è¦åœ¨æ‰§è¡Œf(record1)ä¹‹å‰å°†record2å’Œrecord3æå‰è®¡ç®—å‡ºæ¥æ”¾å…¥å†…å­˜ä¸­ã€‚å½“ç„¶ï¼Œå¦‚æœf()æ“ä½œå’Œg()æ“ä½œéƒ½ä¾èµ–äºä¸Šæ¸¸åˆ†åŒºä¸­çš„å¤šæ¡è®°å½•ï¼Œåˆ™æµæ°´çº¿è®¡ç®—é€€åŒ–åˆ°è®¡ç®—-å›æ”¶æ¨¡å¼ï¼Œéœ€è¦ä¸€æ¬¡è¯»å–ä¸Šæ¸¸åˆ†åŒºä¸­çš„æ‰€æœ‰æ•°æ®ï¼Œæ¯æ‰§è¡Œå®Œä¸€ä¸ªæ“ä½œï¼Œå›æ”¶ä¹‹å‰çš„ä¸­é—´è®¡ç®—ç»“æœã€‚ Sparké‡‡ç”¨æµæ°´çº¿å¼è®¡ç®—æ¥æé«˜taskçš„æ‰§è¡Œæ•ˆç‡ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨é‡ã€‚è¿™ä¹Ÿæ˜¯Sparkå¯ä»¥åœ¨æœ‰é™å†…å­˜ä¸­å¤„ç†å¤§é‡å¤§è§„æ¨¡æ•°æ®çš„åŸå› ã€‚ç„¶è€Œå¯¹äºæŸäº›éœ€è¦èšåˆä¸­é—´è®¡ç®—ç»“æœçš„æ“ä½œï¼Œè¿˜æ˜¯éœ€è¦å ç”¨ä¸€å®šçš„å†…å­˜ç©ºé—´ï¼Œä¹Ÿä¼šåœ¨ä¸€å®šç¨‹åº¦ä¸Šå½±å“æµæ°´çº¿è®¡ç®—çš„æ•ˆç‡ã€‚ ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:1:5","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"taské—´çš„æ•°æ®ä¼ é€’å’Œè®¡ç®—é—®é¢˜ stageä¹‹é—´å­˜åœ¨çš„ä¾èµ–å…³ç³»æ˜¯ShuffleDependencyï¼Œè€ŒShuffleDependencyæ˜¯éƒ¨åˆ†ä¾èµ–çš„ï¼Œä¹Ÿå°±æ˜¯ä¸‹æ¸¸stageä¸­çš„æ¯ä¸ªtaskéœ€è¦ä»parent RDDçš„æ¯ä¸ªåˆ†åŒºä¸­è·å–éƒ¨åˆ†æ•°æ®ã€‚ShuffleDependencyçš„æ•°æ®åˆ’åˆ†æ–¹å¼åŒ…æ‹¬Hashåˆ’åˆ†ã€Rangeåˆ’åˆ†ç­‰ï¼Œä¹Ÿå°±æ˜¯è¦æ±‚ä¸Šæ¸¸stageé¢„å…ˆå°†è¾“å‡ºæ•°æ®è¿›è¡Œåˆ’åˆ†ï¼ŒæŒ‰ç…§åˆ†åŒºå­˜åœ¨ï¼Œåˆ†åŒºä¸ªæ•°å’Œä¸‹æ¸¸taskçš„ä¸ªæ•°ä¸€è‡´ï¼Œè¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸ºShuffle Writeã€‚æŒ‰ç…§åˆ†åŒºå­˜æ”¾å®Œæˆåï¼Œä¸‹æ¸¸çš„taskå°†å±äºè‡ªå·±åˆ†åŒºçš„æ•°æ®é€šè¿‡ç½‘ç»œä¼ è¾“è·å–ï¼Œç„¶åå°†æ¥è‡ªä¸Šæ¸¸ä¸åŒåˆ†åŒºçš„æ•°æ®èšåˆåœ¨ä¸€èµ·è¿›è¡Œå¤„ç†ï¼Œè¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸ºShuffle Readã€‚ ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:1:6","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"stageå’Œtaskçš„å‘½åæ–¹å¼ åœ¨Sparkä¸­ï¼Œstageä¹Ÿå¯ä»¥æœ‰å¤šä¸ªï¼Œæœ‰äº›stageæ—¢åŒ…å«ç±»ä¼¼reduceçš„èšåˆæ“ä½œæœ‰åŒ…å«mapæ“ä½œï¼Œæ‰€ä»¥ä¸€èˆ¬ä¸åŒºåˆ†æ˜¯map stageè¿˜æ˜¯reduce stageï¼Œè€Œç›´æ¥ä½¿ç”¨stage iæ¥å‘½åã€‚ å¦‚æœtaskçš„è¾“å‡ºç»“æœéœ€è¦è¿›è¡ŒShuffleWriteï¼Œä»¥ä¾¿ä¼ è¾“ç»™ä¸‹ä¸€ä¸ªstageï¼Œé‚£ä¹ˆè¿™äº›taskè¢«ç§°ä¸ºShuffleMapTasksï¼Œè€Œå¦‚æœtaskçš„è¾“å‡ºç»“æœä¼šæ±‡æ€»åˆ°Driverç«¯æˆ–è€…ç›´æ¥å†™å…¥åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼Œé‚£ä¹ˆè¿™äº›taskè¢«ç§°ä¸ºResultTasksã€‚ ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:1:7","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"ç”Ÿæˆç‰©ç†æ‰§è¡Œè®¡åˆ’çš„æºç åˆ†æ ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:0","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"demoç¨‹åº // scalastyle:off println package org.apache.spark.examples import java.util.concurrent.TimeUnit import scala.collection.compat.immutable.ArraySeq import org.apache.spark.{HashPartitioner, SparkContext} import org.apache.spark.sql.SparkSession object FilterDemo { def main(args: Array[String]): Unit = { val spark = SparkSession .builder() .appName(\"MapDemo\") .master(\"local\") .getOrCreate() val sc = spark.sparkContext.asInstanceOf[SparkContext] val data1 = Array[(Int, Char)]((1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'e'), (3, 'f'), (2, 'g'), (1, 'h')) val rdd1 = sc.parallelize(ArraySeq.unsafeWrapArray(data1), 3) val partitionedRDD = rdd1.partitionBy(new HashPartitioner(3)) val data2 = Array[(Int, String)]((1, \"A\"), (2, \"B\"), (3, \"C\"), (4, \"D\")) val rdd2 = sc.parallelize(ArraySeq.unsafeWrapArray(data2), 2) .map(x =\u003e (x._1, x._2 + \"\" + x._2)) val data3 = Array[(Int, String)]((3, \"X\"), (5, \"Y\"), (3, \"Z\"), (4, \"Y\")) val rdd3 = sc.parallelize(ArraySeq.unsafeWrapArray(data3), 2) val unionedRDD = rdd2.union(rdd3) val resultRDD = partitionedRDD.join(unionedRDD) resultRDD.count() spark.stop() } } æ¶‰åŠåˆ°ä»¥ä¸‹RDDç±»åˆ« ParallelCollectionRDD ShuffledRDD CoGroupedRDD MapPartitionsRDD UnionRDD ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:1","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"runJob def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum def runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] =\u003e U): Array[U] = { runJob(rdd, func, rdd.partitions.indices) } def runJob[T, U: ClassTag]( rdd: RDD[T], func: Iterator[T] =\u003e U, partitions: Seq[Int]): Array[U] = { val cleanedFunc = clean(func) runJob(rdd, (ctx: TaskContext, it: Iterator[T]) =\u003e cleanedFunc(it), partitions) } def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =\u003e U, partitions: Seq[Int]): Array[U] = { val results = new Array[U](partitions.size) runJob[T, U](rdd, func, partitions, (index, res) =\u003e results(index) = res) results } def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =\u003e U, partitions: Seq[Int], resultHandler: (Int, U) =\u003e Unit): Unit = { if (stopped.get()) { throw new IllegalStateException(\"SparkContext has been shutdown\") } val callSite = getCallSite() val cleanedFunc = clean(func) logInfo(log\"Starting job: ${MDC(LogKeys.CALL_SITE_SHORT_FORM, callSite.shortForm)}\") if (conf.getBoolean(\"spark.logLineage\", false)) { logInfo(log\"RDD's recursive dependencies:\\n\" + log\"${MDC(LogKeys.RDD_DEBUG_STRING, rdd.toDebugString)}\") } dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get) progressBar.foreach(_.finishAll()) rdd.doCheckpoint() } countæ“ä½œä¼šè°ƒç”¨runJobåˆ›å»ºå¹¶æ‰§è¡Œæ–°çš„jobï¼ŒgetCallSiteé€šè¿‡å †æ ˆæ‰¾åˆ°ç”¨æˆ·è°ƒç”¨ä»£ç çš„ä½ç½®ä»¥åŠè°ƒç”¨çš„sparkæ–¹æ³•ï¼Œç±»ä¼¼äºunion at FilterDemo.scala:35ã€‚ def runJob[T, U]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =\u003e U, partitions: Seq[Int], callSite: CallSite, resultHandler: (Int, U) =\u003e Unit, properties: Properties): Unit = { val start = System.nanoTime val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties) ThreadUtils.awaitReady(waiter.completionFuture, Duration.Inf) waiter.completionFuture.value.get match { case scala.util.Success(_) =\u003e logInfo(log\"Job ${MDC(LogKeys.JOB_ID, waiter.jobId)} finished: \" + log\"${MDC(LogKeys.CALL_SITE_SHORT_FORM, callSite.shortForm)}, took \" + log\"${MDC(LogKeys.TIME, (System.nanoTime - start) / 1e6)} ms\") case scala.util.Failure(exception) =\u003e logInfo(log\"Job ${MDC(LogKeys.JOB_ID, waiter.jobId)} failed: \" + log\"${MDC(LogKeys.CALL_SITE_SHORT_FORM, callSite.shortForm)}, took \" + log\"${MDC(LogKeys.TIME, (System.nanoTime - start) / 1e6)} ms\") // SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler. val callerStackTrace = Thread.currentThread().getStackTrace.tail exception.setStackTrace(exception.getStackTrace ++ callerStackTrace) throw exception } } runJobä¼šè°ƒç”¨submitJob æäº¤ä»»åŠ¡ï¼Œè·å¾—JobWaiterå¥æŸ„ï¼Œå¹¶ç­‰å¾…ä»»åŠ¡ç»“æŸã€‚ def submitJob[T, U]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =\u003e U, partitions: Seq[Int], callSite: CallSite, resultHandler: (Int, U) =\u003e Unit, properties: Properties): JobWaiter[U] = { // Check to make sure we are not launching a task on a partition that does not exist. val maxPartitions = rdd.partitions.length partitions.find(p =\u003e p \u003e= maxPartitions || p \u003c 0).foreach { p =\u003e throw new IllegalArgumentException( \"Attempting to access a non-existent partition: \" + p + \". \" + \"Total number of partitions: \" + maxPartitions) } // SPARK-23626: `RDD.getPartitions()` can be slow, so we eagerly compute // `.partitions` on every RDD in the DAG to ensure that `getPartitions()` // is evaluated outside of the DAGScheduler's single-threaded event loop: eagerlyComputePartitionsForRddAndAncestors(rdd) val jobId = nextJobId.getAndIncrement() if (partitions.isEmpty) { val clonedProperties = Utils.cloneProperties(properties) if (sc.getLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION) == null) { clonedProperties.setProperty(SparkContext.SPARK_JOB_DESCRIPTION, callSite.shortForm) } val time = clock.getTimeMillis() listenerBus.post( SparkListenerJobStart(jobId, time, Seq.empty, clonedProperties)) listenerBus.post( SparkListenerJobEnd(jobId, time, JobSucceeded)) // Return immediately if the job is running 0 tasks return new JobWaiter[U](this, jobId, 0, resultHandler) } assert(partitions.nonEmpty) val func2 = func.as","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:2","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"createResultStage private def createResultStage( rdd: RDD[_], func: (TaskContext, Iterator[_]) =\u003e _, partitions: Array[Int], jobId: Int, callSite: CallSite): ResultStage = { // è·å–å½“å‰RDDç›´æ¥ä¾èµ–çš„shuffleDependencies val (shuffleDeps, resourceProfiles) = getShuffleDependenciesAndResourceProfiles(rdd) val resourceProfile = mergeResourceProfilesForStage(resourceProfiles) checkBarrierStageWithDynamicAllocation(rdd) checkBarrierStageWithNumSlots(rdd, resourceProfile) checkBarrierStageWithRDDChainPattern(rdd, partitions.toSet.size) // è·å–parent stagesï¼Œè¿™é‡Œå…¶ä»–æ˜¯ä¸€ä¸ªé€’å½’è¿‡ç¨‹ï¼Œå†…éƒ¨ä¼šè°ƒç”¨getShuffleDependenciesAndResourceProfiles val parents = getOrCreateParentStages(shuffleDeps, jobId) // stageIdæ˜¯æ•´ä¸ªSparkContextèŒƒå›´å†…å”¯ä¸€çš„ val id = nextStageId.getAndIncrement() // åˆ›å»ºæ–°çš„ResultStageï¼Œå°†parent stagesä¼ å…¥ä½œä¸ºå‚æ•° val stage = new ResultStage(id, rdd, func, partitions, parents, jobId, callSite, resourceProfile.id) stageIdToStage(id) = stage updateJobIdStageIdMaps(jobId, stage) stage } createResultStageè´Ÿè´£æ„å»ºæ•´ä¸ªJobçš„Stageä¾èµ–å…³ç³»ï¼Œé€šè¿‡é€’å½’åœ°è·å–ShuffleDependencyå°†jobåˆ‡å‰²æˆå¤šä¸ªstageï¼Œå¹¶æœ€ç»ˆè¿”å›ResultStageã€‚ private[scheduler] def getShuffleDependenciesAndResourceProfiles( rdd: RDD[_]): (HashSet[ShuffleDependency[_, _, _]], HashSet[ResourceProfile]) = { val parents = new HashSet[ShuffleDependency[_, _, _]] val resourceProfiles = new HashSet[ResourceProfile] val visited = new HashSet[RDD[_]] val waitingForVisit = new ListBuffer[RDD[_]] waitingForVisit += rdd while (waitingForVisit.nonEmpty) { val toVisit = waitingForVisit.remove(0) if (!visited(toVisit)) { visited += toVisit Option(toVisit.getResourceProfile()).foreach(resourceProfiles += _) toVisit.dependencies.foreach { case shuffleDep: ShuffleDependency[_, _, _] =\u003e parents += shuffleDep case dependency =\u003e waitingForVisit.prepend(dependency.rdd) } } } (parents, resourceProfiles) } getShuffleDependenciesAndResourceProfilesè¿”å›ç»™å®š RDD ç›´æ¥ä¾èµ–çš„ShuffleDependencyï¼Œä»¥åŠè¯¥stageä¸­ä¸è¿™äº› RDD ç›¸å…³è”çš„ResourceProfilesã€‚ éå†å½“å‰RDDçš„æ‰€æœ‰ä¾èµ–ï¼Œå°†RDDçš„ResourceProfileæ·»åŠ åˆ°ç»“æœresourceProfilesï¼Œä¾èµ–å¦‚æœæ˜¯ShuffleDependencyï¼Œåˆ™å°†ShuffleDependencyæ·»åŠ åˆ°ç»“æœé›†ä¸­ï¼Œå¦‚æœé‡åˆ°å…¶ä»–ç±»å‹çš„ä¾èµ–ï¼Œåˆ™å¼€å§‹é€’å½’éå†çˆ¶RDDã€‚å½“ç„¶å®é™…å®ç°äº†ä¸ºäº†é¿å…StackOverFlowErrorï¼Œé‡‡ç”¨äº†æ‰‹åŠ¨ç»´æŠ¤æ ˆçš„æ–¹æ³•ã€‚ private def getOrCreateParentStages(shuffleDeps: HashSet[ShuffleDependency[_, _, _]], firstJobId: Int): List[Stage] = { shuffleDeps.map { shuffleDep =\u003e getOrCreateShuffleMapStage(shuffleDep, firstJobId) }.toList } private def getOrCreateShuffleMapStage( shuffleDep: ShuffleDependency[_, _, _], firstJobId: Int): ShuffleMapStage = { shuffleIdToMapStage.get(shuffleDep.shuffleId) match { case Some(stage) =\u003e stage case None =\u003e // Create stages for all missing ancestor shuffle dependencies. getMissingAncestorShuffleDependencies(shuffleDep.rdd).foreach { dep =\u003e // Even though getMissingAncestorShuffleDependencies only returns shuffle dependencies // that were not already in shuffleIdToMapStage, it's possible that by the time we // get to a particular dependency in the foreach loop, it's been added to // shuffleIdToMapStage by the stage creation process for an earlier dependency. See // SPARK-13902 for more information. if (!shuffleIdToMapStage.contains(dep.shuffleId)) { createShuffleMapStage(dep, firstJobId) } } // Finally, create a stage for the given shuffle dependency. createShuffleMapStage(shuffleDep, firstJobId) } } å¯¹äºæ¯ä¸ªShuffleDependencyï¼Œè·å–å¯¹åº”çš„ShuffleMapStageã€‚ é€šè¿‡shuffleIdæŸ¥è¯¢ShuffleMapStageï¼Œå¦‚æœå­˜åœ¨ï¼Œç›´æ¥è¿”å›ã€‚ å¦‚æœä¸å­˜åœ¨ï¼Œè·å–å½“å‰ShuffleDependencyç›´æ¥æˆ–é—´æ¥ä¾èµ–çš„æ‰€æœ‰ä¸Šæ¸¸ç¼ºå¤±çš„ShuffleDependencyï¼Œå†æ¬¡æ£€æŸ¥ShuffleDependencyæ˜¯å¦å·²ç»åˆ›å»ºShuffleMapStageï¼Œå¦‚æœæ²¡æœ‰åˆ›å»ºï¼Œåˆ™è°ƒç”¨createShuffleMapStageåˆ›å»ºï¼Œæœ€åæ‰€æœ‰ä¸Šæ¸¸çš„ShuffleMapStageå·²ç»åˆ›å»ºå®Œæ¯•ï¼Œåˆ›å»ºå½“å‰ShuffleDependencyçš„ShuffleMapStageã€‚ /** Find ancestor shuffle dependencies that are not registered in shuffleToMapStage yet */ private def getMissingAncestorShuffleDependencies( rdd: RDD[_]): ListBuffer[ShuffleDependency[_, _, _]] = { val ancestors = new ListBuffer[ShuffleDependency[_, _, _]] val visited = new HashSet[RDD[_]] // We are manually maintaining a stack here to prevent StackOverflowError // caused by recursively visiting val waitingForVisit = new ListBuffer[RDD[_]] waitingForVisit += rdd while (waitingForVisit.nonEmpty) { val toVisit = wa","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:3","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"submitStage /** Submits stage, but first recursively submits any missing parents. */ private def submitStage(stage: Stage): Unit = { val jobId = activeJobForStage(stage) if (jobId.isDefined) { logDebug(s\"submitStage($stage (name=${stage.name};\" + s\"jobs=${stage.jobIds.toSeq.sorted.mkString(\",\")}))\") // waitingStages æ­£åœ¨ç­‰å¾…çš„stageé›†åˆ // runningStages æ­£åœ¨æ‰§è¡Œçš„stageé›†åˆ // failedStages å¤±è´¥ç­‰å¾…æ‰‹åŠ¨æäº¤é‡è¯•çš„é›†åˆ if (!waitingStages(stage) \u0026\u0026 !runningStages(stage) \u0026\u0026 !failedStages(stage)) { // stageå°è¯•æ¬¡æ•°è¶…è¿‡æœ€å¤§é™åˆ¶ï¼Œabort stage if (stage.getNextAttemptId \u003e= maxStageAttempts) { val reason = s\"$stage (name=${stage.name}) has been resubmitted for the maximum \" + s\"allowable number of times: ${maxStageAttempts}, which is the max value of \" + s\"config `${config.STAGE_MAX_ATTEMPTS.key}` and \" + s\"`${config.STAGE_MAX_CONSECUTIVE_ATTEMPTS.key}`.\" abortStage(stage, reason, None) } else { // æ‰¾åˆ°stageç›´æ¥ä¾èµ–çš„ç¼ºå¤±çš„stage val missing = getMissingParentStages(stage).sortBy(_.id) logDebug(\"missing: \" + missing) if (missing.isEmpty) { logInfo(log\"Submitting ${MDC(STAGE, stage)} (${MDC(RDD_ID, stage.rdd)}), \" + log\"which has no missing parents\") // ä¾èµ–çš„stageéƒ½å·²ç»å°±ç»ªï¼Œç›´æ¥æäº¤å½“å‰stageçš„task submitMissingTasks(stage, jobId.get) } else { // å¦åˆ™å°è¯•æäº¤ä¾èµ–çš„stageï¼Œè¿›å…¥é€’å½’æµç¨‹ for (parent \u003c- missing) { submitStage(parent) } // å½“å‰stageåŠ å…¥ç­‰å¾…é›†åˆ waitingStages += stage } } } } else { abortStage(stage, \"No active job for stage \" + stage.id, None) } } submitStageé¦–å…ˆéœ€è¦æŸ¥æ‰¾å¹¶æäº¤ä»»ä½•ç¼ºå¤±çš„çˆ¶stageï¼Œå¦‚æœå­˜åœ¨è¿™æ ·çš„çˆ¶stageï¼Œä¼šé€’å½’æäº¤çˆ¶stageï¼Œå¹¶å°†è‡ªèº«åŠ å…¥ç­‰å¾…é›†åˆä¸­ï¼Œå¦åˆ™ï¼Œç›´æ¥æäº¤å½“å‰stageçš„ç¼ºå¤±taskã€‚ ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:4","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"getMissingParentStages private def getMissingParentStages(stage: Stage): List[Stage] = { val missing = new HashSet[Stage] val visited = new HashSet[RDD[_]] // We are manually maintaining a stack here to prevent StackOverflowError // caused by recursively visiting val waitingForVisit = new ListBuffer[RDD[_]] waitingForVisit += stage.rdd def visit(rdd: RDD[_]): Unit = { if (!visited(rdd)) { visited += rdd // stageä¾èµ–çš„rddæ˜¯å¦å·²ç»è®¡ç®—è¿‡å¹¶ä¸”ç¼“å­˜ val rddHasUncachedPartitions = getCacheLocs(rdd).contains(Nil) if (rddHasUncachedPartitions) { // å¦‚æœrddéœ€è¦é‡æ–°è®¡ç®—ï¼Œéå†rddçš„ä¾èµ–å…³ç³» for (dep \u003c- rdd.dependencies) { dep match { // è·å–ShuffleDependencyå¯¹åº”çš„ShuffleMapStageï¼Œå¦‚æœmapStageçš„ç»“æœä¸å¯å¾—ï¼Œæ·»åŠ åˆ°ç»“æœé›†ä¸­ case shufDep: ShuffleDependency[_, _, _] =\u003e val mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId) // Mark mapStage as available with shuffle outputs only after shuffle merge is // finalized with push based shuffle. If not, subsequent ShuffleMapStage won't // read from merged output as the MergeStatuses are not available. if (!mapStage.isAvailable || !mapStage.shuffleDep.shuffleMergeFinalized) { missing += mapStage } else { // Forward the nextAttemptId if skipped and get visited for the first time. // Otherwise, once it gets retried, // 1) the stuffs in stage info become distorting, e.g. task num, input byte, e.t.c // 2) the first attempt starts from 0-idx, it will not be marked as a retry mapStage.increaseAttemptIdOnFirstSkip() } // å¦‚æœæ˜¯çª„ä¾èµ–ï¼Œåˆ™ç»§ç»­å›æº¯ case narrowDep: NarrowDependency[_] =\u003e waitingForVisit.prepend(narrowDep.rdd) } } } } } while (waitingForVisit.nonEmpty) { visit(waitingForVisit.remove(0)) } missing.toList } getMissingParentStagesæ‰¾åˆ°å½“å‰stageç›´æ¥ä¾èµ–çš„ç¼ºå¤±çš„stageã€‚ ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:5","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"submitMissingTasks é€šè¿‡findMissingPartitionsæ‰¾åˆ°stageå¯¹åº”çš„æ‰€æœ‰éœ€è¦è®¡ç®—çš„åˆ†åŒºçš„idï¼Œè°ƒç”¨getPreferredLocså¾—åˆ°æ¯ä¸ªpartitionçš„é¦–é€‰ä½ç½®ã€‚ è°ƒç”¨stage.makeNewStageAttemptåˆ›å»ºæ–°çš„stageå°è¯•ã€‚è®°å½•stageçš„submissionTimeå‘listenerBuså‘å¸ƒSparkListenerStageSubmittedäº‹ä»¶ // TODO: Maybe we can keep the taskBinary in Stage to avoid serializing it multiple times. // Broadcasted binary for the task, used to dispatch tasks to executors. Note that we broadcast // the serialized copy of the RDD and for each task we will deserialize it, which means each // task gets a different copy of the RDD. This provides stronger isolation between tasks that // might modify state of objects referenced in their closures. This is necessary in Hadoop // where the JobConf/Configuration object is not thread-safe. var taskBinary: Broadcast[Array[Byte]] = null var partitions: Array[Partition] = null try { // For ShuffleMapTask, serialize and broadcast (rdd, shuffleDep). // For ResultTask, serialize and broadcast (rdd, func). var taskBinaryBytes: Array[Byte] = null // taskBinaryBytes and partitions are both effected by the checkpoint status. We need // this synchronization in case another concurrent job is checkpointing this RDD, so we get a // consistent view of both variables. RDDCheckpointData.synchronized { taskBinaryBytes = stage match { case stage: ShuffleMapStage =\u003e JavaUtils.bufferToArray( closureSerializer.serialize((stage.rdd, stage.shuffleDep): AnyRef)) case stage: ResultStage =\u003e JavaUtils.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): AnyRef)) } partitions = stage.rdd.partitions } if (taskBinaryBytes.length \u003e TaskSetManager.TASK_SIZE_TO_WARN_KIB * 1024) { logWarning(log\"Broadcasting large task binary with size \" + log\"${MDC(NUM_BYTES, Utils.bytesToString(taskBinaryBytes.length))}\") } taskBinary = sc.broadcast(taskBinaryBytes) } catch { // In the case of a failure during serialization, abort the stage. case e: NotSerializableException =\u003e abortStage(stage, \"Task not serializable: \" + e.toString, Some(e)) runningStages -= stage // Abort execution return case e: Throwable =\u003e abortStage(stage, s\"Task serialization failed: $e\\n${Utils.exceptionString(e)}\", Some(e)) runningStages -= stage // Abort execution return } val artifacts = jobIdToActiveJob(jobId).artifacts val tasks: Seq[Task[_]] = try { val serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array() stage match { case stage: ShuffleMapStage =\u003e stage.pendingPartitions.clear() partitionsToCompute.map { id =\u003e val locs = taskIdToLocations(id) val part = partitions(id) stage.pendingPartitions += id new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber(), taskBinary, part, stage.numPartitions, locs, artifacts, properties, serializedTaskMetrics, Option(jobId), Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier()) } case stage: ResultStage =\u003e partitionsToCompute.map { id =\u003e val p: Int = stage.partitions(id) val part = partitions(p) val locs = taskIdToLocations(id) new ResultTask(stage.id, stage.latestInfo.attemptNumber(), taskBinary, part, stage.numPartitions, locs, id, artifacts, properties, serializedTaskMetrics, Option(jobId), Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier()) } } } catch { case NonFatal(e) =\u003e abortStage(stage, s\"Task creation failed: $e\\n${Utils.exceptionString(e)}\", Some(e)) runningStages -= stage return } if (tasks.nonEmpty) { logInfo(log\"Submitting ${MDC(NUM_TASKS, tasks.size)} missing tasks from \" + log\"${MDC(STAGE, stage)} (${MDC(RDD_ID, stage.rdd)}) (first 15 tasks are \" + log\"for partitions ${MDC(PARTITION_IDS, tasks.take(15).map(_.partitionId))})\") val shuffleId = stage match { case s: ShuffleMapStage =\u003e Some(s.shuffleDep.shuffleId) case _: ResultStage =\u003e None } taskScheduler.submitTasks(new TaskSet( tasks.toArray, stage.id, stage.latestInfo.attemptNumber(), jobId, properties, stage.resourceProfileId, shuffleId)) å¯¹äºShuffleMapStageï¼Œåºåˆ—åŒ–stage.rddå’Œstage.shuffleDepï¼Œå¯¹äºResultStageï¼Œåºåˆ—åŒ–stage.rddå’Œstage.funcã€‚è°ƒç”¨SparkContext.broadcastå°†åºåˆ—åŒ–ç»“æœå¹¿æ’­ã€‚ å¯¹äºéœ€è¦","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:6","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"resoruceOffers TODO ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:7","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"TaskDescription private[spark] class TaskDescription( val taskId: Long, // taskId val attemptNumber: Int, // task attemp numberï¼Œå”¯ä¸€æ ‡è®°æ¯æ¬¡é‡è¯• val executorId: String, // æ‰§è¡Œtaskçš„executorèŠ‚ç‚¹ val name: String, val index: Int, // Index within this task's TaskSet val partitionId: Int, // å®é™…è®¡ç®—çš„åˆ†åŒºid val artifacts: JobArtifactSet, // jaråŒ…å’Œæ–‡ä»¶ç­‰ val properties: Properties, // å±æ€§ val cpus: Int, // éœ€è¦åˆ†é…çš„cpuä¸ªæ•° // resources is the total resources assigned to the task // Eg, Map(\"gpu\" -\u003e Map(\"0\" -\u003e ResourceAmountUtils.toInternalResource(0.7))): // assign 0.7 of the gpu address \"0\" to this task val resources: immutable.Map[String, immutable.Map[String, Long]], // éœ€è¦åˆ†é…çš„å…¶ä»–èµ„æº val serializedTask: ByteBuffer) { // åºåˆ—åŒ–çš„Task assert(cpus \u003e 0, \"CPUs per task should be \u003e 0\") override def toString: String = s\"TaskDescription($name)\" } TaskDescriptionæè¿°ä¸€ä¸ªå°†è¢«ä¼ åˆ°executorä¸Šè¿›è¡Œæ‰§è¡Œçš„taskï¼Œé€šå¸¸ç”±TaskSetManager.resourceOfferåˆ›å»ºï¼ŒTaskDescriptionå’ŒTaskéœ€è¦è¢«åºåˆ—åŒ–ä¼ åˆ°executorä¸Šï¼Œå½“TaskDescriptionè¢«executoræ¥æ”¶åˆ°ï¼Œexecutoré¦–å…ˆéœ€è¦å¾—åˆ°ä¸€ç³»åˆ—çš„jaråŒ…å’Œæ–‡ä»¶ï¼Œå¹¶æ·»åŠ è¿™äº›åˆ°classpathï¼Œç„¶åè®¾ç½®å±æ€§ï¼Œå†ååºåˆ—åŒ–Taskå¯¹è±¡ï¼ˆserializedTask)ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå±æ€§propertiesè¢«åŒ…å«åœ¨TaskDescriptionä¸­ï¼Œå°½ç®¡å®ƒä»¬åŒæ ·åŒ…å«åœ¨serialized taskä¸­ã€‚ å¯ä»¥çœ‹åˆ°ï¼ŒTaskDescriptionå·²ç»ç¡®å®šäº†taskå°†è¢«å‘é€åˆ°çš„executorIdä»¥åŠå¯¹åº”çš„RDDåˆ†åŒºå’Œèµ„æºéœ€æ±‚ã€‚ ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:8","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"launchTasks // Launch tasks returned by a set of resource offers private def launchTasks(tasks: Seq[Seq[TaskDescription]]): Unit = { for (task \u003c- tasks.flatten) { val serializedTask = TaskDescription.encode(task) if (serializedTask.limit() \u003e= maxRpcMessageSize) { Option(scheduler.taskIdToTaskSetManager.get(task.taskId)).foreach { taskSetMgr =\u003e try { var msg = \"Serialized task %s:%d was %d bytes, which exceeds max allowed: \" + s\"${RPC_MESSAGE_MAX_SIZE.key} (%d bytes). Consider increasing \" + s\"${RPC_MESSAGE_MAX_SIZE.key} or using broadcast variables for large values.\" msg = msg.format(task.taskId, task.index, serializedTask.limit(), maxRpcMessageSize) taskSetMgr.abort(msg) } catch { case e: Exception =\u003e logError(\"Exception in error callback\", e) } } } else { val executorData = executorDataMap(task.executorId) // Do resources allocation here. The allocated resources will get released after the task // finishes. executorData.freeCores -= task.cpus task.resources.foreach { case (rName, addressAmounts) =\u003e executorData.resourcesInfo(rName).acquire(addressAmounts) } logDebug(s\"Launching task ${task.taskId} on executor id: ${task.executorId} hostname: \" + s\"${executorData.executorHost}.\") executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask))) } } } launchTasksæ‰¹é‡å¤„ç†TaskDecriptionï¼Œé¦–å…ˆåºåˆ—åŒ–TaskDescriptionï¼Œå¦‚æœåºåˆ—åŒ–åçš„é•¿åº¦é«˜äºé˜ˆå€¼ï¼Œåˆ™æ”¾å¼ƒå½“å‰ä»»åŠ¡ï¼Œå¦åˆ™ï¼Œç”³è¯·å¯¹åº”çš„cpuå’Œå…¶ä»–å„ç±»èµ„æºï¼Œæœ€ç»ˆè°ƒç”¨executorEndpoint.sendå‘é€RPCè¯·æ±‚LaunchTaskã€‚ è¿™æ ·ä»»åŠ¡å°±å¯ä»¥è¢«executoræ¥æ”¶ï¼Œå¹¶ä¸”æ‰§è¡Œäº†ã€‚ ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:9","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"getPreferredLocs def getPreferredLocs(rdd: RDD[_], partition: Int): Seq[TaskLocation] = { getPreferredLocsInternal(rdd, partition, new HashSet) } private def getPreferredLocsInternal( rdd: RDD[_], partition: Int, visited: HashSet[(RDD[_], Int)]): Seq[TaskLocation] = { // If the partition has already been visited, no need to re-visit. // This avoids exponential path exploration. SPARK-695 if (!visited.add((rdd, partition))) { // Nil has already been returned for previously visited partitions. return Nil } // If the partition is cached, return the cache locations val cached = getCacheLocs(rdd)(partition) if (cached.nonEmpty) { return cached } // If the RDD has some placement preferences (as is the case for input RDDs), get those val rddPrefs = rdd.preferredLocations(rdd.partitions(partition)).toList if (rddPrefs.nonEmpty) { return rddPrefs.filter(_ != null).map(TaskLocation(_)) } // If the RDD has narrow dependencies, pick the first partition of the first narrow dependency // that has any placement preferences. Ideally we would choose based on transfer sizes, // but this will do for now. rdd.dependencies.foreach { case n: NarrowDependency[_] =\u003e for (inPart \u003c- n.getParents(partition)) { val locs = getPreferredLocsInternal(n.rdd, inPart, visited) if (locs != Nil) { return locs } } case _ =\u003e } Nil } getPreferredLocsè·å–ä¸ç‰¹å®š RDD çš„æŸä¸ªåˆ†åŒºç›¸å…³è”çš„ä½ç½®ï¼ˆlocalityï¼‰ä¿¡æ¯ã€‚é¦–å…ˆæ£€æŸ¥partitionæ˜¯å¦è¢«cacheï¼Œå¦‚æœè¢«cacheï¼Œç›´æ¥è¿”å›ï¼Œå¦åˆ™å¦‚æœRDDè‡ªèº«æœ‰ä½ç½®ä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨ï¼Œå‡è®¾RDDæ˜¯ä¸€ä¸ªinput RDDçš„åœºæ™¯ï¼Œæœ€åå°è¯•è·å–RDDç¬¬ä¸€ä¸ªçª„ä¾èµ–çš„ç¬¬ä¸€ä¸ªåˆ†åŒºçš„ä½ç½®ä¿¡æ¯ï¼Œè¿™é‡ŒSparkä¹Ÿæåˆ°ï¼Œç†æƒ³æƒ…å†µä¸‹åº”è¯¥åŸºäºtransfer sizeè¿›è¡Œé€‰æ‹©ã€‚ ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:10","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"JobWaiterçš„å®ç° private[spark] trait JobListener { def taskSucceeded(index: Int, result: Any): Unit def jobFailed(exception: Exception): Unit } JobListerneræ¥å£ç”¨äºç›‘å¬taskå®Œæˆæˆ–è€…å¤±è´¥çš„äº‹ä»¶ï¼Œå½“ä¸€ä¸ªtaskå®Œæˆæˆ–è€…æ•´ä¸ªjobå¤±è´¥æ—¶è¢«é€šçŸ¥ã€‚ /** * An object that waits for a DAGScheduler job to complete. As tasks finish, it passes their * results to the given handler function. */ private[spark] class JobWaiter[T]( dagScheduler: DAGScheduler, val jobId: Int, totalTasks: Int, resultHandler: (Int, T) =\u003e Unit) extends JobListener with Logging { private val finishedTasks = new AtomicInteger(0) // If the job is finished, this will be its result. In the case of 0 task jobs (e.g. zero // partition RDDs), we set the jobResult directly to JobSucceeded. private val jobPromise: Promise[Unit] = if (totalTasks == 0) Promise.successful(()) else Promise() def jobFinished: Boolean = jobPromise.isCompleted def completionFuture: Future[Unit] = jobPromise.future /** * Sends a signal to the DAGScheduler to cancel the job with an optional reason. The * cancellation itself is handled asynchronously. After the low level scheduler cancels * all the tasks belonging to this job, it will fail this job with a SparkException. */ def cancel(reason: Option[String]): Unit = { dagScheduler.cancelJob(jobId, reason) } /** * Sends a signal to the DAGScheduler to cancel the job. The cancellation itself is * handled asynchronously. After the low level scheduler cancels all the tasks belonging * to this job, it will fail this job with a SparkException. */ def cancel(): Unit = cancel(None) override def taskSucceeded(index: Int, result: Any): Unit = { // resultHandler call must be synchronized in case resultHandler itself is not thread safe. synchronized { resultHandler(index, result.asInstanceOf[T]) } if (finishedTasks.incrementAndGet() == totalTasks) { jobPromise.success(()) } } override def jobFailed(exception: Exception): Unit = { if (!jobPromise.tryFailure(exception)) { logWarning(\"Ignore failure\", exception) } } } jobPromiseå­—æ®µæ˜¯ä¸€ä¸ªPromiseå¯¹è±¡ï¼ŒPromise æ˜¯ä¸€ä¸ªè¡¨ç¤ºæœªæ¥ç»“æœçš„å¯¹è±¡ï¼Œå®ƒå¯ä»¥è¢«æ‰‹åŠ¨å®Œæˆï¼ˆèµ‹å€¼ï¼‰æˆ–å¤±è´¥ï¼ˆæŠ›å‡ºå¼‚å¸¸ï¼‰ã€‚ ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:11","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"getPartitions final def partitions: Array[Partition] = { checkpointRDD.map(_.partitions).getOrElse { if (partitions_ == null) { stateLock.synchronized { if (partitions_ == null) { partitions_ = getPartitions partitions_.zipWithIndex.foreach { case (partition, index) =\u003e require(partition.index == index, s\"partitions($index).partition == ${partition.index}, but it should equal $index\") } } } } partitions_ } } getPartitionsæ˜¯RDDä¸­çš„è™šæ–¹æ³•ï¼Œç”±RDDå­ç±»è´Ÿè´£å®ç°ã€‚ /** * An identifier for a partition in an RDD. */ trait Partition extends Serializable { /** * Get the partition's index within its parent RDD */ def index: Int // A better default implementation of HashCode override def hashCode(): Int = index override def equals(other: Any): Boolean = super.equals(other) } private[spark] class ParallelCollectionRDD[T: ClassTag]( sc: SparkContext, @transient private val data: Seq[T], numSlices: Int, locationPrefs: Map[Int, Seq[String]]) extends RDD[T](sc, Nil) { // TODO: Right now, each split sends along its full data, even if later down the RDD chain it gets // cached. It might be worthwhile to write the data to a file in the DFS and read it in the split // instead. // UPDATE: A parallel collection can be checkpointed to HDFS, which achieves this goal. override def getPartitions: Array[Partition] = { val slices = ParallelCollectionRDD.slice(data, numSlices).toArray slices.indices.map(i =\u003e new ParallelCollectionPartition(id, i, slices(i))).toArray } ParallelCollectionRDDçš„getPartitionså‡½æ•°é¦–å…ˆå°†è¾“å…¥çš„æ•°æ®åˆ†æˆnumSlicesä»½ï¼Œç„¶åç”Ÿæˆå¯¹åº”çš„åˆ†åŒºã€‚ private[spark] class MapPartitionsRDD[U: ClassTag, T: ClassTag]( var prev: RDD[T], f: (TaskContext, Int, Iterator[T]) =\u003e Iterator[U], // (TaskContext, partition index, iterator) preservesPartitioning: Boolean = false, isFromBarrier: Boolean = false, isOrderSensitive: Boolean = false) extends RDD[U](prev) { override def getPartitions: Array[Partition] = firstParent[T].partitions MapPartitionsRDDç›´æ¥ç»§æ‰¿çˆ¶RDDçš„åˆ†åŒºã€‚ @DeveloperApi class UnionRDD[T: ClassTag]( sc: SparkContext, var rdds: Seq[RDD[T]]) extends RDD[T](sc, Nil) { // Nil since we implement getDependencies // visible for testing private[spark] val isPartitionListingParallel: Boolean = rdds.length \u003e conf.get(RDD_PARALLEL_LISTING_THRESHOLD) override def getPartitions: Array[Partition] = { val parRDDs = if (isPartitionListingParallel) { // scalastyle:off parvector val parArray = new ParVector(rdds.toVector) parArray.tasksupport = UnionRDD.partitionEvalTaskSupport // scalastyle:on parvector parArray } else { rdds } val array = new Array[Partition](parRDDs.iterator.map(_.partitions.length).sum) var pos = 0 for ((rdd, rddIndex) \u003c- rdds.zipWithIndex; split \u003c- rdd.partitions) { array(pos) = new UnionPartition(pos, rdd, rddIndex, split.index) pos += 1 } array } UnionRDDç±»ä¼¼äºMapPartitionsRDDï¼Œåˆ†åŒºä¾æ®çˆ¶RDDçš„åˆ†åŒºç”Ÿæˆã€‚ä¾æ¬¡éå†æ¯ä¸ªçˆ¶RDDçš„æ¯ä¸ªåˆ†åŒºï¼Œç”Ÿæˆå¯¹åº”çš„UnionRDDçš„åˆ†åŒºã€‚ @DeveloperApi class ShuffledRDD[K: ClassTag, V: ClassTag, C: ClassTag]( @transient var prev: RDD[_ \u003c: Product2[K, V]], part: Partitioner) extends RDD[(K, C)](prev.context, Nil) { override def getPartitions: Array[Partition] = { Array.tabulate[Partition](part.numPartitions)(i =\u003e new ShuffledRDDPartition(i)) } private[spark] class ShuffledRDDPartition(val idx: Int) extends Partition { override val index: Int = idx } ShuffledRDDçš„åˆ†åŒºæ€»æ•°é€šè¿‡partitioner.numPartitionså¾—åˆ°ï¼Œç”Ÿæˆçš„åˆ†åŒºä¸ºShuffledRDDPartitionã€‚ @DeveloperApi class CoGroupedRDD[K: ClassTag]( @transient var rdds: Seq[RDD[_ \u003c: Product2[K, _]]], part: Partitioner) extends RDD[(K, Array[Iterable[_]])](rdds.head.context, Nil) { override def getPartitions: Array[Partition] = { val array = new Array[Partition](part.numPartitions) for (i \u003c- array.indices) { // Each CoGroupPartition will have a dependency per contributing RDD array(i) = new CoGroupPartition(i, rdds.zipWithIndex.map { case (rdd, j) =\u003e // Assume each RDD contributed a single dependency, and get it dependencies(j) match { case s: ShuffleDependency[_, _, _] =\u003e None case _ =\u003e Some(new NarrowCoGroupSplitDep(rdd, i, rdd.partitions(i))) } }.toArray) } array } CoGroupedRDDé€šè¿‡numPar","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:12","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"Stage ä¸€ä¸ªstageæ˜¯ä¸€ç»„å¹¶è¡Œçš„taskï¼Œè¿™äº›taskéƒ½æ‰§è¡Œç›¸åŒçš„å‡½æ•°ï¼Œå¹¶ä¸”éœ€è¦ä½œä¸ºä¸€ä¸ªspark jobçš„ä¸€éƒ¨åˆ†æ¥è¿è¡Œï¼Œå…·æœ‰ç›¸åŒçš„shuffleä¾èµ–ã€‚æ¯ä¸€ä¸ªç”±è°ƒåº¦å™¨æ‰§è¡Œçš„ä»»åŠ¡DAGéƒ½ä¼šåœ¨å‘ç”Ÿshuffleçš„è¾¹ç•Œå¤„åˆ†å‰²æˆå¤šä¸ªstageï¼Œç„¶åDAGScheduleræŒ‰ç…§æ‹“æ‰‘é¡ºåºæ¥ä¾æ¬¡è¿è¡Œè¿™äº›stageã€‚ æ¯ä¸ªstageå¯ä»¥æ˜¯shuffle map stageï¼Œæˆ–è€…æ˜¯result stageã€‚å¦‚æœæ˜¯shuffle map stageï¼Œé‚£ä¹ˆä»–çš„taskç»“æœå°†ä½œä¸ºå…¶ä»–stageçš„è¾“å…¥ï¼›å¦‚æœæ˜¯result stageï¼Œé‚£ä¹ˆå®ƒçš„taskä¼šç›´æ¥é€šè¿‡åœ¨ä¸€ä¸ªRDDä¸Šè¿è¡ŒæŸä¸ªå‡½æ•°æ¥æ‰§è¡Œä¸€ä¸ªspark actionã€‚å¯¹äºshuffle map stageï¼ŒSparkè¿˜ä¼šè¿½è¸ªæ¯ä¸ªè¾“å‡ºåˆ†åŒºæ‰€åœ¨çš„èŠ‚ç‚¹ä½ç½®ã€‚ æ¯ä¸ªStageè¿˜æœ‰ä¸€ä¸ªfirstJobIdç”¨äºæ ‡è¯†æœ€ç»ˆæäº¤è¯¥stageçš„jobï¼Œå½“ä½¿ç”¨FIFOè°ƒåº¦ç­–ç•¥æ—¶ï¼Œè¿™ä¸ªå­—æ®µå¯ä»¥è®©è°ƒåº¦å™¨ä¼˜å…ˆè®¡ç®—æ¥è‡ªè¾ƒæ—©jobçš„stagesï¼Œæˆ–è€…åœ¨å¤±è´¥æ—¶æ›´å¿«çš„æ¢å¤è¿™äº›è¾ƒæ—©çš„stagesã€‚ ç”±äºå®¹é”™æ¢å¤ï¼ˆfault recoveryï¼‰çš„éœ€è¦ï¼Œä¸€ä¸ªstageå¯èƒ½ä¼šè¢«å¤šæ¬¡é‡è¯•æ‰§è¡Œã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œstageå¯¹è±¡ä¼šç»´æŠ¤å¤šä¸ªStageInfoå®ä¾‹ï¼Œç”¨äºä¼ é€’ç»™ç›‘å¬å™¨ï¼ˆlistenersï¼‰æˆ–è€…web uiï¼Œæœ€æ–°çš„ä¸€æ¬¡å°è¯•ä¿¡æ¯å¯ä»¥é€šè¿‡latestInfoå­—æ®µè®¿é—®ã€‚ private[scheduler] abstract class Stage( val id: Int, val rdd: RDD[_], val numTasks: Int, val parents: List[Stage], val firstJobId: Int, val callSite: CallSite, val resourceProfileId: Int) extends Logging { val numPartitions = rdd.partitions.length /** Set of jobs that this stage belongs to. */ val jobIds = new HashSet[Int] /** The ID to use for the next new attempt for this stage. */ private var nextAttemptId: Int = 0 private[scheduler] def getNextAttemptId: Int = nextAttemptId val name: String = callSite.shortForm val details: String = callSite.longForm /** * Pointer to the [[StageInfo]] object for the most recent attempt. This needs to be initialized * here, before any attempts have actually been created, because the DAGScheduler uses this * StageInfo to tell SparkListeners when a job starts (which happens before any stage attempts * have been created). */ private var _latestInfo: StageInfo = StageInfo.fromStage(this, nextAttemptId, resourceProfileId = resourceProfileId) /** * Set of stage attempt IDs that have failed. We keep track of these failures in order to avoid * endless retries if a stage keeps failing. * We keep track of each attempt ID that has failed to avoid recording duplicate failures if * multiple tasks from the same stage attempt fail (SPARK-5945). */ val failedAttemptIds = new HashSet[Int] private[scheduler] def clearFailures() : Unit = { failedAttemptIds.clear() } /** Creates a new attempt for this stage by creating a new StageInfo with a new attempt ID. */ def makeNewStageAttempt( numPartitionsToCompute: Int, taskLocalityPreferences: Seq[Seq[TaskLocation]] = Seq.empty): Unit = { val metrics = new TaskMetrics metrics.register(rdd.sparkContext) _latestInfo = StageInfo.fromStage( this, nextAttemptId, Some(numPartitionsToCompute), metrics, taskLocalityPreferences, resourceProfileId = resourceProfileId) nextAttemptId += 1 } /** Forward the nextAttemptId if skipped and get visited for the first time. */ def increaseAttemptIdOnFirstSkip(): Unit = { if (nextAttemptId == 0) { nextAttemptId = 1 } } /** Returns the StageInfo for the most recent attempt for this stage. */ def latestInfo: StageInfo = _latestInfo override final def hashCode(): Int = id override final def equals(other: Any): Boolean = other match { case stage: Stage =\u003e stage != null \u0026\u0026 stage.id == id case _ =\u003e false } /** Returns the sequence of partition ids that are missing (i.e. needs to be computed). */ def findMissingPartitions(): Seq[Int] def isIndeterminate: Boolean = { rdd.outputDeterministicLevel == DeterministicLevel.INDETERMINATE } } æ„é€ å‚æ•°: id å”¯ä¸€çš„stage id rdd è¯¥stageæ‰€è¿è¡Œçš„RDDï¼Œå¦‚æœæ˜¯shuffle map stageï¼Œé‚£ä¹ˆå°±æ˜¯æˆ‘ä»¬è¦åœ¨å…¶ä¸Šè¿è¡Œmapä»»åŠ¡çš„rddï¼Œå¦‚æœæ˜¯result stageï¼Œé‚£ä¹ˆå°±æ˜¯æˆ‘ä»¬æ‰§è¡ŒæŸä¸ªactionæ“ä½œæ‰€é’ˆå¯¹çš„ç›®æ ‡rdd numTasks stageä¸­çš„taskæ€»æ•°ï¼Œç‰¹åˆ«åœ°result stageå¯èƒ½ä¸ä¼šè®¡ç®—rddçš„æ‰€æœ‰åˆ†åŒºï¼Œæ¯”å¦‚first, lookup, takeç­‰æ“ä½œ parents è¿™ä¸ªstageä¾èµ–çš„stageåˆ—è¡¨ï¼ˆé€šè¿‡shuffle dependenyä¾èµ–ï¼‰ firstJobId è¿™ä¸ªstageæ‰€å±çš„é¦–ä¸ªjobï¼Œç”¨äºFIFO è°ƒåº¦ å…¶ä»–å­—æ®µï¼š jobIds è¿™ä¸ªstageæ‰€å±çš„æ‰€æœ‰job nextAttemptId stageæ¯æ¬¡é‡è¯•éƒ½ä¼šè·å¾—æ–°çš„newAttemptIdï¼Œåˆå§‹å€¼ä¸º0 _latestInfo æœ€æ–°ä¸€æ¬¡å°è¯•çš„StageInfo failedAttemptId stageå°è¯•å¤±è´¥çš„é›†åˆ æ–¹æ³•ï¼š makeNewStageAttempt åˆ›å»ºæ–°çš„TaskMetricsï¼Œå¹¶æ³¨å†Œåˆ°SparkContextä¸­ã€‚åˆ›å»ºæ–°çš„StageInfoå¹¶é€’å¢nextAttemptId findMissingPartitions è¿”å›éœ€è¦è®¡ç®—çš„partition id çš„åºåˆ— ResultStage private[spark] class ResultStage( id: Int, rdd: RDD[_], val func: (TaskContext, Iterator[_]) =\u003e _, val partitions: Array[Int], parents: List[Stage], firstJobId: Int, callSite: CallSite, resourceProfileId: Int) extends Stage(id, rdd, partitions.length, parents, firstJobId, callSi","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:13","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"ActiveJob private[spark] class ActiveJob( val jobId: Int, val finalStage: Stage, val callSite: CallSite, val listener: JobListener, val artifacts: JobArtifactSet, val properties: Properties) { /** * Number of partitions we need to compute for this job. Note that result stages may not need * to compute all partitions in their target RDD, for actions like first() and lookup(). */ val numPartitions = finalStage match { case r: ResultStage =\u003e r.partitions.length case m: ShuffleMapStage =\u003e m.numPartitions } /** Which partitions of the stage have finished */ val finished = Array.fill[Boolean](numPartitions)(false) var numFinished = 0 } DAGSchedulerä¸­çš„ä¸€ä¸ªè¿è¡Œä¸­jobï¼Œjobå¯ä»¥æœ‰ä¸¤ä¸ªé€»è¾‘ç±»å‹ï¼Œresult jobé€šè¿‡è®¡ç®—ResultStageæ‰§è¡Œactionï¼Œmap-stage jobåœ¨ä¸‹æ¸¸stageè¢«æäº¤å‰è®¡ç®—ShuffleMapStageçš„mapè¾“å‡ºã€‚åè€…è¢«ç”¨äºè‡ªé€‚åº”æŸ¥è¯¢è®¡åˆ’ï¼Œåœ¨æäº¤åç»­stageä¹‹å‰æŸ¥çœ‹mapè¾“å‡ºçš„ç»Ÿè®¡ä¿¡æ¯ï¼Œæˆ‘ä»¬é€šè¿‡è¯¥ç±»ä¸­çš„finalStageåŒºåˆ†è¿™ä¸¤ç±»jobã€‚ åªæœ‰å®¢æˆ·ç«¯é€šè¿‡DAGSchedulerçš„submitJobæˆ–è€…submitMapStageæ–¹æ³•ç›´æ¥æäº¤çš„å¶å­stageï¼Œæ‰ä¼šè¢«ä½œä¸ºjobè¿›è¡Œè¿½è¸ªï¼Œä½†æ˜¯ï¼Œæ— è®ºæ˜¯é‚£ç§ç±»å‹çš„jobï¼Œéƒ½å¯èƒ½ä¼šè§¦å‘å…¶ä¾èµ–çš„å‰é¢stageçš„æ‰§è¡Œï¼ˆè¿™äº›stageæ˜¯DAGä¸­æ‰€ä»¥æ¥çš„RDDæ‰€å¯¹åº”çš„stageï¼‰ï¼Œå¹¶ä¸”å¤šä¸ªjobå¯èƒ½ä¼šå…±äº«å…¶ä¸­çš„ä¸€äº›å‰ç½®stageã€‚è¿™äº›ä¾èµ–å…³ç³»æœ‰DAGSchedulerå†…éƒ¨è¿›è¡Œç®¡ç†ã€‚ ä¸€ä¸ªjobèµ·å§‹äºä¸€ä¸ªç›®æ ‡RDDï¼Œä½†æœ€ç»ˆå¯èƒ½ä¼šåŒ…å«RDDè¡€ç¼˜å…³ç³»ä¸­æ¶‰åŠåˆ°çš„å…¶ä»–æ‰€æœ‰RDD ActiveJobçš„æ„é€ å‚æ•°åŒ…æ‹¬ï¼š JobId jobçš„å”¯ä¸€id finalStage jobè®¡ç®—çš„stage mapPartitionså­—æ®µè¡¨ç¤ºjobä¸­éœ€è¦è®¡ç®—çš„åˆ†åŒºçš„ä¸ªæ•°ï¼Œæ³¨æ„ï¼ŒResultStageå¯èƒ½ä¸éœ€è¦è®¡ç®—RDDä¸­çš„æ‰€æœ‰åˆ†åŒºï¼Œæ¯”å¦‚å¯¹äºfirstæˆ–è€…lookupæ“ä½œã€‚ finishedå­—æ®µè®°å½•stageä¸­çš„å“ªäº›åˆ†åŒºå·²ç»è®¡ç®—å®Œæˆã€‚ numFinishedå­—æ®µè®°å½•å·²ç»è®¡ç®—å®Œæˆçš„åˆ†åŒºçš„ä¸ªæ•°ã€‚ ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:14","tags":["Spark"],"title":"Sparkç‰©ç†æ‰§è¡Œè®¡åˆ’","uri":"/posts/spark_physical_plan/"},{"categories":["Grpc"],"content":"OpenTelemetry OpenTelemetryæ˜¯CNCFå­µåŒ–çš„å¼€æºè§‚æµ‹æ¡†æ¶ï¼Œç”¨äºåˆ›å»ºå’Œç®¡ç†é¥æµ‹æ•°æ®ï¼ˆtelemetryï¼‰ï¼Œæ¯”å¦‚metrics, tracing, logsã€‚è¿™ä¸€é¡¹ç›®çš„æ ¸å¿ƒç›®æ ‡æ˜¯é¿å…ä¾èµ–äºvendorï¼Œä»è€Œå¯ä»¥æ›´å¥½åœ°é›†æˆå’Œæ‹“å±•ã€‚ ä¸ºä»€ä¹ˆéœ€è¦è§‚æµ‹ï¼Ÿ æ›´å¿«æ’æŸ¥é—®é¢˜ è§‚æµ‹ç„¶åæ”¹è¿›ï¼Œæå‡ç³»ç»Ÿæ€§èƒ½ æŒæœ‰ç›‘æµ‹å’Œé¢„è­¦ ","date":"2025-05-13","objectID":"/posts/grpc-metric/:1:0","tags":["Grpc"],"title":"Grpc Metric","uri":"/posts/grpc-metric/"},{"categories":["Spark"],"content":"Sparkåº”ç”¨ç¨‹åºéœ€è¦å…ˆè½¬åŒ–ä¸ºé€»è¾‘å¤„ç†æµç¨‹ï¼Œé€»è¾‘å¤„ç†æµç¨‹ä¸»è¦åŒ…æ‹¬ï¼š RDDæ•°æ®æ¨¡å‹ æ•°æ®æ“ä½œ æ•°æ®ä¾èµ–å…³ç³» æ•°æ®æ“ä½œåˆ†ä¸ºä¸¤ç§ï¼Œtransformationæ“ä½œå¹¶ä¸ä¼šè§¦å‘jobçš„å®é™…æ‰§è¡Œï¼Œactionæ“ä½œåˆ›å»ºjobå¹¶ç«‹å³æ‰§è¡Œã€‚ç±»ä¼¼äºjavaä¸­çš„streamï¼Œé‡‡ç”¨æ‡’åŠ è½½çš„æ–¹å¼ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:0:0","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"å¸¸ç”¨transformationæ•°æ®æ“ä½œ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:0","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"map // scalastyle:off println package org.apache.spark.examples import scala.collection.compat.immutable.ArraySeq import org.apache.spark.SparkContext import org.apache.spark.sql.SparkSession object MapDemo { def main(args: Array[String]): Unit = { val spark = SparkSession .builder() .appName(\"MapDemo\") .master(\"local\") .getOrCreate() val sc = spark.sparkContext.asInstanceOf[SparkContext] val array = Array[(Int, Char)]( (1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (2, 'e'), (3, 'f'), (2, 'g'), (1, 'h')) val inputRDD = sc.parallelize( ArraySeq.unsafeWrapArray(array) , 3) val resultRDD = inputRDD.map(r =\u003e s\"${r._1}_${r._2}\") resultRDD.foreach(println) spark.stop() } } è¿™é‡Œç»™å‡ºäº†ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œé€šè¿‡mapå‡½æ•°å°†keyå’Œvalueæ‹¼æ¥èµ·æ¥ã€‚ def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]()) } private[spark] class ParallelCollectionRDD[T: ClassTag]( sc: SparkContext, @transient private val data: Seq[T], numSlices: Int, locationPrefs: Map[Int, Seq[String]]) extends RDD[T](sc, Nil) { parallelizeå°†ä¸€ä¸ªå±€åœ°çš„scalaé›†åˆåˆ†å¸ƒå¼åŒ–æˆRDDï¼Œä½†å®é™…ä¸Šä»…ä»…æ˜¯æ„å»ºParallelCollectionRDDè€Œå·²ï¼Œæ²¡æœ‰ä¾èµ–äºå…¶ä»–RDDï¼Œæ‰€ä»¥ä¼ å…¥çš„ä¸ºNilã€‚ def map[U: ClassTag](f: T =\u003e U): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) =\u003e iter.map(cleanF)) } private[spark] class MapPartitionsRDD[U: ClassTag, T: ClassTag]( var prev: RDD[T], f: (TaskContext, Int, Iterator[T]) =\u003e Iterator[U], // (TaskContext, partition index, iterator) preservesPartitioning: Boolean = false, isFromBarrier: Boolean = false, isOrderSensitive: Boolean = false) extends RDD[U](prev) { mapå‡½æ•°å¯¹è¾“å‡ºçš„RDDçš„æ¯æ¡è®°å½•åº”ç”¨ç›®æ ‡å‡½æ•°ï¼Œè·å¾—æ–°çš„MapPartitionRDDï¼Œä¾èµ–äºä¹‹å‰çš„RDD prevã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:1","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"mapValues val resultRDD = inputRDD.mapValues(x =\u003e s\"${x} + 1\") å¯¹ä¹‹å‰çš„mapä¾‹å­ç¨ä½œè°ƒæ•´ï¼Œè°ƒç”¨mapValuesè€Œä¸æ˜¯mapå‡½æ•°ï¼Œä¸æ”¹å˜keyï¼Œåªå¯¹valueè¿›è¡Œè½¬æ¢ã€‚ def mapValues[U](f: V =\u003e U): RDD[(K, U)] = self.withScope { val cleanF = self.context.clean(f) new MapPartitionsRDD[(K, U), (K, V)](self, (context, pid, iter) =\u003e iter.map { case (k, v) =\u003e (k, cleanF(v)) }, preservesPartitioning = true) } å®é™…è°ƒç”¨äº†PairRDDFunctionsä¸­çš„mapValueæ–¹æ³•ï¼Œæœ€ç»ˆç”Ÿæˆçš„ä¾ç„¶æ˜¯MapPartitionsRDDï¼Œä½†æœ‰ä¸¤ç‚¹ä¸åŒï¼Œä¸€æ˜¯ç›®æ ‡å‡½æ•°åªå¯¹valueè¿›è¡Œè½¬æ¢ï¼ŒäºŒæ˜¯preservePartitioningä¸ºtrueã€‚è¿™é‡Œå¾ˆå¥½ç†è§£ï¼Œmapå‡½æ•°ä¼šå¯¹é”®å€¼å¯¹è¿›è¡Œæ“ä½œï¼Œpartitionerå¯èƒ½ä¼šå¤±æ•ˆï¼Œè€ŒmapVlauesåªå¯¹valueè¿›è¡Œæ“ä½œï¼Œä¸å½±å“keyï¼Œæ‰€ä»¥partitionerä¾ç„¶ä¿æŒã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:2","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"filter val resultRDD = inputRDD.filter(r =\u003e r._1 % 2 == 0) filterå¯¹è¾“å…¥RDDä¸­çš„æ¯æ¡è®°å½•è¿›è¡Œfuncæ“ä½œï¼Œå¦‚æœç»“æœä¸ºtrueï¼Œåˆ™ä¿ç•™è¿™æ¡è®°å½•ï¼Œæ‰€æœ‰ä¿ç•™çš„è®°å½•å½¢æˆæ–°çš„RDD def filter(f: T =\u003e Boolean): RDD[T] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[T, T]( this, (_, _, iter) =\u003e iter.filter(cleanF), preservesPartitioning = true) } filteræ“ä½œåç”Ÿæˆçš„RDDä¾ç„¶æ˜¯MapPartitionsRDDï¼Œæ²¡æœ‰ä¿®æ”¹é”®å€¼å¯¹ï¼ŒpreservesPartitioningä¸ºtrueã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:3","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"filterByRange val resultRDD = inputRDD.filterByRange(2, 4) filterByRangeå¯¹è¾“å…¥RDDä¸­çš„æ•°æ®è¿›è¡Œè¿‡æ»¤ï¼Œåªä¿ç•™[lower, upper]ä¹‹é—´çš„è®°å½•ã€‚ def filterByRange(lower: K, upper: K): RDD[P] = self.withScope { def inRange(k: K): Boolean = ordering.gteq(k, lower) \u0026\u0026 ordering.lteq(k, upper) val rddToFilter: RDD[P] = self.partitioner match { case Some(rp: RangePartitioner[_, _]) =\u003e // getPartitionè·å–åˆ†åŒºå·ï¼ŒpartitionIndicesè¡¨ç¤ºå¯èƒ½åŒ…å«ç›®æ ‡è®°å½•çš„åˆ†åŒºid Range val partitionIndices = (rp.getPartition(lower), rp.getPartition(upper)) match { case (l, u) =\u003e Math.min(l, u) to Math.max(l, u) } PartitionPruningRDD.create(self, partitionIndices.contains) case _ =\u003e self } rddToFilter.filter { case (k, v) =\u003e inRange(k) } } filterByRangeæ“ä½œå±äºOrderedRDDFunctionsï¼Œå¦‚æœRDDé€šè¿‡RangePartitioneråˆ†åŒºï¼Œè¿™ä¸ªæ“ä½œå¯ä»¥æ‰§è¡Œçš„æ›´åŠ é«˜æ•ˆï¼Œä»…éœ€è¦å¯¹å¯èƒ½åŒ…å«åŒ¹é…å…ƒç´ çš„åˆ†åŒºè¿›è¡Œæ‰«æï¼Œå¦åˆ™éœ€è¦å¯¹æ‰€æœ‰åˆ†åŒºåº”ç”¨filterã€‚ @DeveloperApi object PartitionPruningRDD { def create[T](rdd: RDD[T], partitionFilterFunc: Int =\u003e Boolean): PartitionPruningRDD[T] = { new PartitionPruningRDD[T](rdd, partitionFilterFunc)(rdd.elementClassTag) } } @DeveloperApi class PartitionPruningRDD[T: ClassTag]( prev: RDD[T], partitionFilterFunc: Int =\u003e Boolean) extends RDD[T](prev.context, List(new PruneDependency(prev, partitionFilterFunc))) { PartitionPruningRDDç”¨äºRDDåˆ†åŒºçš„å‰ªæï¼Œé¿å…å¯¹æ‰€æœ‰åˆ†åŒºè¿›è¡Œæ“ä½œã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:4","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"flatMap val array = Array[String]( \"how do you do\", \"are you ok\", \"thanks\", \"bye bye\", \"I'm ok\" ) val inputRDD = sc.parallelize( ArraySeq.unsafeWrapArray(array) , 3) val resultRDD = inputRDD.flatMap(x =\u003e x.split(\" \")) å¯¹è¾“å…¥RDDä¸­æ¯ä¸ªå…ƒç´ ï¼ˆå¦‚Listï¼‰æ‰§è¡Œfuncæ“ä½œï¼Œå¾—åˆ°æ–°å…ƒç´ ï¼Œç„¶åå°†æ‰€æœ‰æ–°å…ƒç´ ç»„åˆå¾—åˆ°æ–°RDDã€‚ä¾‹å¦‚è¾“å…¥RDDä¸­æŸä¸ªåˆ†åŒºåŒ…å«ä¸¤ä¸ªå…ƒç´ List(1, 2)å’ŒList(3, 4)ï¼Œfuncæ˜¯å¯¹Listä¸­çš„æ¯ä¸ªå…ƒç´ åŠ 1ï¼Œé‚£ä¹ˆæœ€åå¾—åˆ°çš„æ–°RDDä¸­è¯¥åˆ†åŒºçš„å…ƒç´ ä¸º(2, 3, 4, 5)ï¼Œå®ä¾‹ä»£ç ä¼šåšåˆ†è¯æ“ä½œï¼Œç»„æˆæ–°çš„RDDã€‚ def flatMap[U: ClassTag](f: T =\u003e IterableOnce[U]): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) =\u003e iter.flatMap(cleanF)) } flatMapæœ€ç»ˆè¿”å›çš„ä¹Ÿæ˜¯MapPartitionsRDDï¼Œå¯¹æ¯ä¸ªåˆ†åŒºçš„iterè°ƒç”¨flatMapå‡½æ•° ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:5","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"flatMapValues val array = Array[(Int, String)]( (1, \"how do you do\"), (2, \"are you ok\"), (4, \"thanks\"), (5, \"bye bye\"), (2, \"I'm ok\") ) val inputRDD = sc.parallelize( ArraySeq.unsafeWrapArray(array) , 3) val resultRDD = inputRDD.flatMapValues(x =\u003e x.split(\" \")) ä¸flatMapç±»ä¼¼ï¼Œä½†åªå¯¹RDDä¸­\u003cK, V\u003e recordä¸­Valueè¿›è¡Œæ“ä½œã€‚ def flatMapValues[U](f: V =\u003e IterableOnce[U]): RDD[(K, U)] = self.withScope { val cleanF = self.context.clean(f) new MapPartitionsRDD[(K, U), (K, V)](self, (context, pid, iter) =\u003e iter.flatMap { case (k, v) =\u003e cleanF(v).iterator.map(x =\u003e (k, x)) }, preservesPartitioning = true) } flatMapValuesåŒæ ·å±äºPairRDDFunctionç±»ï¼Œé€šè¿‡flatMapæ“ä½œ\u003cK, V\u003e recordä¸­çš„valueï¼Œä½†ä¸æ”¹å˜keyï¼ŒflatMapValuesä¿æŒåŸå…ˆRDDçš„åˆ†åŒºç‰¹æ€§ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:6","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"sample val array = Array[(Int, Char)]( (1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (2, 'e'), (3, 'f'), (2, 'g'), (1, 'h')) val inputRDD = sc.parallelize( ArraySeq.unsafeWrapArray(array) , 3) val sampleRDD = inputRDD.sample(false, 0.5) å¯¹RDDä¸­çš„æ•°æ®è¿›è¡ŒæŠ½æ ·ã€‚ def sample( withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] = { require(fraction \u003e= 0, s\"Fraction must be nonnegative, but got ${fraction}\") withScope { if (withReplacement) { new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed) } else { new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed) } } } sampleå‡½æ•°ç”¨äºå¯¹RDDæŠ½æ ·ï¼ŒwithRepalcementè¡¨ç¤ºæŠ½æ ·æ˜¯å¦æœ‰æ”¾å›ï¼Œfractionè¡¨ç¤ºæŠ½æ ·æ¯”ä¾‹ï¼Œåœ¨æœ‰æ”¾å›æŠ½æ ·ä¸­è¡¨ç¤ºæ¯ä¸ªå…ƒç´ æœŸæœ›è¢«é€‰ä¸­çš„æ¬¡æ•°ï¼Œfraction \u003e= 0ï¼Œä½¿ç”¨æ³Šæ¾æŠ½æ ·ï¼Œåœ¨æ— æ”¾å›æŠ½æ ·ä¸­è¡¨ç¤ºæ¯ä¸ªå…ƒç´ è¢«é€‰ä¸­çš„æ¦‚ç‡ï¼Œfraction åœ¨0åˆ°1ä¹‹é—´ï¼Œä½¿ç”¨ä¼¯åŠªåˆ©æŠ½æ ·ã€‚ä¸èƒ½ä¿è¯æŠ½æ ·æ•°é‡ç²¾ç¡®çš„ç­‰äºç»™å®šRDDè®°å½•æ€»æ•° * fractionã€‚ private[spark] class PartitionwiseSampledRDD[T: ClassTag, U: ClassTag]( prev: RDD[T], sampler: RandomSampler[T, U], preservesPartitioning: Boolean, @transient private val seed: Long = Utils.random.nextLong) extends RDD[U](prev) { PartitionwiseSampledRDDè¡¨ç¤ºä»çˆ¶ RDD çš„å„ä¸ªåˆ†åŒºåˆ†åˆ«è¿›è¡ŒæŠ½æ ·è€Œç”Ÿæˆçš„ RDDï¼Œå¯¹äºçˆ¶RDDçš„æ¯ä¸ªåˆ†åŒºï¼Œä¸€ä¸ªRandomSamplerå®ä¾‹è¢«ç”¨äºè·å¾—è¿™ä¸ªåˆ†åŒºä¸­è®°å½•çš„éšæœºæŠ½æ ·ç»“æœã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:7","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"sampleByKey val array = Array[(Int, Char)]( (1, 'a'), (2, 'b'), (1, 'c'), (2, 'd'), (2, 'e'), (1, 'f'), (2, 'g'), (1, 'h')) val inputRDD = sc.parallelize( ArraySeq.unsafeWrapArray(array) , 3) val map = Map((1 -\u003e 0.8), (2 -\u003e 0.5)) val sampleRDD = inputRDD.sampleByKey(false, map) å¯¹è¾“å…¥RDDä¸­çš„æ•°æ®è¿›è¡ŒæŠ½æ ·ï¼Œä¸ºæ¯ä¸ªKeyè®¾ç½®æŠ½æ ·æ¯”ä¾‹ã€‚ def sampleByKey(withReplacement: Boolean, fractions: Map[K, Double], seed: Long = Utils.random.nextLong): RDD[(K, V)] = self.withScope { require(fractions.values.forall(v =\u003e v \u003e= 0.0), \"Negative sampling rates.\") val samplingFunc = if (withReplacement) { StratifiedSamplingUtils.getPoissonSamplingFunction(self, fractions, false, seed) } else { StratifiedSamplingUtils.getBernoulliSamplingFunction(self, fractions, false, seed) } self.mapPartitionsWithIndex(samplingFunc, preservesPartitioning = true, isOrderSensitive = true) } ä½¿ç”¨ç®€å•éšæœºæŠ½æ ·å¹¶ä»…éå†ä¸€æ¬¡RDDï¼Œæ ¹æ®fractionsä¸ºä¸åŒçš„é”®æŒ‡å®šä¸åŒçš„é‡‡æ ·ç‡ï¼Œä»è¯¥RDDåˆ›å»ºä¸€ä¸ªæ ·æœ¬ï¼Œæ‰€ç”Ÿæˆçš„æ ·æœ¬å¤§å°å¤§è‡´ç­‰äºå¯¹æ‰€æœ‰å‰ªææ‰§è¡Œmath.ceil(numItems * samplingRate)çš„æ€»å’Œã€‚ mapPartitionsWithIndexé€šè¿‡å¯¹RDDçš„æ¯ä¸ªåˆ†åŒºåº”ç”¨ç›®æ ‡å‡½æ•°å¾—åˆ°æ–°çš„RDDï¼ŒåŒæ—¶è·Ÿè¸ªåŸæ¥åˆ†åŒºçš„indexã€‚åº”è¯¥æ˜¯å°†indexä¼ å…¥ï¼Œæ¥ä¿è¯ä¸åŒåˆ†åŒºè·å¾—ä¸åŒçš„éšæœºæ€§ï¼ˆåªæ˜¯çŒœæµ‹ï¼‰ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:8","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"mapPartitions def mapPartitions[U: ClassTag]( f: Iterator[T] =\u003e Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) =\u003e cleanedF(iter), preservesPartitioning) } mapPartitionså¯¹è¾“å…¥RDDä¸­çš„æ¯ä¸ªåˆ†åŒºè¿›è¡Œfuncå¤„ç†ï¼Œè¾“å‡ºæ–°çš„ä¸€ç»„æ•°æ®ï¼Œç›¸è¾ƒäºmapæ“ä½œï¼Œå…·æœ‰æ›´å¤§çš„è‡ªç”±åº¦ï¼Œå¯ä»¥ä»¥ä»»æ„æ–¹å¼å¤„ç†æ•´ä¸ªåˆ†åŒºçš„æ•°æ®ï¼Œè€Œä¸æ˜¯åªèƒ½é€æ¡éå†åˆ†åŒºä¸­çš„è®°å½•ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:9","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"mapPartitionsWithIndex private[spark] def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) =\u003e Iterator[U], preservesPartitioning: Boolean, isOrderSensitive: Boolean): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) =\u003e cleanedF(index, iter), preservesPartitioning, isOrderSensitive = isOrderSensitive) } mapPartitionsWithIndexå’ŒmapPartitionsè¯­ä¹‰ç±»ä¼¼ï¼Œåªæ˜¯å¤šä¼ å…¥äº†partition Idï¼Œåˆ©ç”¨è¿™ä¸ªidï¼Œå¯ä»¥å®ç°å¯¹ä¸åŒåˆ†åŒºåˆ†åˆ«å¤„ç†ï¼Œæ¯”å¦‚ä¹‹å‰sampleByKeyæ“ä½œå°±åˆ©ç”¨äº†partition Idã€‚ val list = List(1, 2, 3, 4, 5, 6, 7, 8, 9) val inputRDD = sc.parallelize( list, 3 ) val resultRDD = inputRDD.mapPartitionsWithIndex((pid, iter) =\u003e { iter.map(Value =\u003e s\"Pid: ${pid}, Value: ${Value}\") }) resultRDD.foreach(println) æ¯”å¦‚å¯ä»¥åˆ©ç”¨è¿™ä¸ªå‡½æ•°æ‰“å°RDDçš„å†…å®¹ï¼Œäº†è§£æ¯ä¸ªåˆ†åŒºä¸­æœ‰å“ªäº›æ•°æ®ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:10","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"partitionBy val array = Array[(Int, Char)]( (1, 'a'), (2, 'b'), (1, 'c'), (2, 'd'), (2, 'e'), (1, 'f'), (2, 'g'), (1, 'h')) val inputRDD = sc.parallelize( ArraySeq.unsafeWrapArray(array) , 3) val resultRDD = inputRDD.partitionBy(new HashPartitioner(2)) val resultRDD2 = inputRDD.partitionBy(new RangePartitioner(2, inputRDD)) partitionByä½¿ç”¨æ–°çš„partitionerå¯¹RDDè¿›è¡Œåˆ†åŒºï¼Œè¦æ±‚RDDæ˜¯\u003cK, V\u003eç±»å‹ã€‚ def partitionBy(partitioner: Partitioner): RDD[(K, V)] = self.withScope { if (keyClass.isArray \u0026\u0026 partitioner.isInstanceOf[HashPartitioner]) { throw SparkCoreErrors.hashPartitionerCannotPartitionArrayKeyError() } if (self.partitioner == Some(partitioner)) { self } else { new ShuffledRDD[K, V, V](self, partitioner) } } partitionByå¦‚æœæä¾›çš„partitionerå’ŒRDDåŸå…ˆçš„partitionerç›¸åŒï¼Œåˆ™è¿”å›åŸæ¥çš„RDDï¼Œå¦åˆ™è¿”å›ShuffledRDDã€‚ @DeveloperApi class ShuffledRDD[K: ClassTag, V: ClassTag, C: ClassTag]( @transient var prev: RDD[_ \u003c: Product2[K, V]], part: Partitioner) extends RDD[(K, C)](prev.context, Nil) { ShuffledRDDè¡¨ç¤ºshuffleåçš„RDDï¼Œå³é‡æ–°åˆ†åŒºåçš„æ•°æ®ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:11","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"groupByKey def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])] = self.withScope { groupByKey(new HashPartitioner(numPartitions)) } def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])] = self.withScope { // groupByKey shouldn't use map side combine because map side combine does not // reduce the amount of data shuffled and requires all map side data be inserted // into a hash table, leading to more objects in the old gen. val createCombiner = (v: V) =\u003e CompactBuffer(v) val mergeValue = (buf: CompactBuffer[V], v: V) =\u003e buf += v val mergeCombiners = (c1: CompactBuffer[V], c2: CompactBuffer[V]) =\u003e c1 ++= c2 val bufs = combineByKeyWithClassTag[CompactBuffer[V]]( createCombiner, mergeValue, mergeCombiners, partitioner, mapSideCombine = false) bufs.asInstanceOf[RDD[(K, Iterable[V])]] } å°†RDD1ä¸­çš„\u003cK, V\u003e recordæŒ‰ç…§keyèšåˆåœ¨ä¸€èµ·ï¼Œå½¢æˆK, List\u003cV\u003eï¼ŒnumPartitionsè¡¨ç¤ºç”Ÿæˆçš„rdd2çš„åˆ†åŒºä¸ªæ•°ã€‚groupByKeyçš„è¡Œä¸ºå’Œçˆ¶RDDçš„partitioneræœ‰å…³ï¼Œå¦‚æœçˆ¶RDDå’Œç”Ÿæˆçš„å­RDDçš„partitioinerç›¸åŒï¼Œåˆ™ä¸éœ€è¦shuffleï¼Œå¦åˆ™éœ€è¦è¿›è¡Œshuffleã€‚å‡å¦‚åœ¨è¿™é‡ŒæŒ‡å®šåˆ†åŒºæ•°ä¸º3ï¼Œå­RDDçš„paritionerä¸ºHashPartitioner(3)ï¼Œå¦‚æœçˆ¶RDDçš„partitionerç›¸åŒï¼Œæ˜¾ç„¶æ²¡æœ‰å¿…è¦å†è¿›è¡Œä¸€æ¬¡shuffleã€‚ def combineByKeyWithClassTag[C]( createCombiner: V =\u003e C, mergeValue: (C, V) =\u003e C, mergeCombiners: (C, C) =\u003e C, partitioner: Partitioner, mapSideCombine: Boolean = true, serializer: Serializer = null)(implicit ct: ClassTag[C]): RDD[(K, C)] = self.withScope { require(mergeCombiners != null, \"mergeCombiners must be defined\") // required as of Spark 0.9.0 // å¦‚æœkeyçš„ç±»å‹ä¸ºæ•°ç»„ï¼Œåˆ™ä¸æ”¯æŒmapç«¯èšåˆä»¥åŠhashåˆ†åŒº if (keyClass.isArray) { if (mapSideCombine) { throw SparkCoreErrors.cannotUseMapSideCombiningWithArrayKeyError() } if (partitioner.isInstanceOf[HashPartitioner]) { throw SparkCoreErrors.hashPartitionerCannotPartitionArrayKeyError() } } val aggregator = new Aggregator[K, V, C]( self.context.clean(createCombiner), self.context.clean(mergeValue), self.context.clean(mergeCombiners)) // å¦‚æœpartitionerç›¸åŒ if (self.partitioner == Some(partitioner)) { self.mapPartitions(iter =\u003e { // è®¿é—®ThreadLocalå˜é‡ï¼Œè·å–å½“å‰çš„taskContext val context = TaskContext.get() // aggregatoråˆ›å»ºExternalAppendOnlyMapï¼Œç”¨äºå®ç°combiner new InterruptibleIterator(context, aggregator.combineValuesByKey(iter, context)) }, preservesPartitioning = true) } else { // parttionerä¸ç›¸åŒï¼Œè¿›è¡Œä¸€æ¬¡shuffle new ShuffledRDD[K, V, C](self, partitioner) .setSerializer(serializer) .setAggregator(aggregator) .setMapSideCombine(mapSideCombine) } } åœ¨paritionerç›¸åŒçš„æƒ…å†µä¸‹ï¼Œè°ƒç”¨äº†mapPartitionsæ–¹æ³•ï¼Œå®é™…çš„æ“ä½œç”±aggregator.combineValuesByKeyå®ç°ã€‚ @DeveloperApi case class Aggregator[K, V, C] ( createCombiner: V =\u003e C, mergeValue: (C, V) =\u003e C, mergeCombiners: (C, C) =\u003e C) { def combineValuesByKey( iter: Iterator[_ \u003c: Product2[K, V]], context: TaskContext): Iterator[(K, C)] = { val combiners = new ExternalAppendOnlyMap[K, V, C](createCombiner, mergeValue, mergeCombiners) combiners.insertAll(iter) updateMetrics(context, combiners) combiners.iterator } def combineCombinersByKey( iter: Iterator[_ \u003c: Product2[K, C]], context: TaskContext): Iterator[(K, C)] = { val combiners = new ExternalAppendOnlyMap[K, C, C](identity, mergeCombiners, mergeCombiners) combiners.insertAll(iter) updateMetrics(context, combiners) combiners.iterator } /** Update task metrics after populating the external map. */ private def updateMetrics(context: TaskContext, map: ExternalAppendOnlyMap[_, _, _]): Unit = { Option(context).foreach { c =\u003e c.taskMetrics().incMemoryBytesSpilled(map.memoryBytesSpilled) c.taskMetrics().incDiskBytesSpilled(map.diskBytesSpilled) c.taskMetrics().incPeakExecutionMemory(map.peakMemoryUsedBytes) } } } Aggregatorè¿™ä¸ªç±»æœ‰ä¸‰ä¸ªå‚æ•°ï¼š createCombiner ç”¨äºä»åˆå€¼åˆ›å»ºèšåˆç»“æœï¼Œæ¯”å¦‚ a -\u003e list[a] mergeValue å°†æ–°çš„å€¼åŠ å…¥èšåˆç»“æœï¼Œæ¯”å¦‚ b -\u003e list[a, b] mergeCombiners å°†ä¸¤ä¸ªèšåˆç»“æœå†èšåˆï¼Œæ¯”å¦‚ [c, d] -\u003e list[a, b, c, d] å¯ä»¥çœ‹åˆ°combineValuesByKeyæ“ä½œåˆ›å»ºäº†ExternalAppendOnlyMapï¼ŒåŠŸèƒ½ç±»ä¼¼äºhashmapï¼Œèšåˆæ“ä½œä½¿ç”¨ä¼ å…¥çš„èšåˆå‡½æ•°ï¼Œå°†åˆ†åŒºä¸­çš„æ‰€æœ‰æ•°æ®æ’å…¥mapä¸­èšåˆï¼ŒExternalAppendOnlyMapå®ç°äº†åç£ç›˜ï¼Œåœ¨å®Œæˆæ’å…¥åä¼šæ›´æ–°å†…å­˜çš„ä¿¡æ¯ï¼Œå¹¶è¿”å›mapçš„è¿­ä»£å™¨ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:12","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"reduceByKey def reduceByKey(func: (V, V) =\u003e V, numPartitions: Int): RDD[(K, V)] = self.withScope { reduceByKey(new HashPartitioner(numPartitions), func) } def reduceByKey(partitioner: Partitioner, func: (V, V) =\u003e V): RDD[(K, V)] = self.withScope { combineByKeyWithClassTag[V]((v: V) =\u003e v, func, func, partitioner) } reduceByKeyä½¿ç”¨reduceå‡½æ•°æŒ‰keyèšåˆï¼Œåœ¨mapç«¯å…ˆå±€åœ°combineç„¶åå†åœ¨reduceç«¯èšåˆã€‚ groupByKeyæ²¡æœ‰mapç«¯èšåˆçš„åŸå› æ˜¯å³ä½¿èšåˆä¹Ÿä¸èƒ½å‡å°‘ä¼ è¾“çš„æ•°æ®é‡å’Œå†…å­˜ç”¨é‡ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:13","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"aggregateByKey def aggregateByKey[U: ClassTag](zeroValue: U, numPartitions: Int)(seqOp: (U, V) =\u003e U, combOp: (U, U) =\u003e U): RDD[(K, U)] = self.withScope { aggregateByKey(zeroValue, new HashPartitioner(numPartitions))(seqOp, combOp) } def aggregateByKey[U: ClassTag](zeroValue: U, partitioner: Partitioner)(seqOp: (U, V) =\u003e U, combOp: (U, U) =\u003e U): RDD[(K, U)] = self.withScope { // Serialize the zero value to a byte array so that we can get a new clone of it on each key val zeroBuffer = SparkEnv.get.serializer.newInstance().serialize(zeroValue) val zeroArray = new Array[Byte](zeroBuffer.limit) zeroBuffer.get(zeroArray) lazy val cachedSerializer = SparkEnv.get.serializer.newInstance() val createZero = () =\u003e cachedSerializer.deserialize[U](ByteBuffer.wrap(zeroArray)) // We will clean the combiner closure later in `combineByKey` val cleanedSeqOp = self.context.clean(seqOp) combineByKeyWithClassTag[U]((v: V) =\u003e cleanedSeqOp(createZero(), v), cleanedSeqOp, combOp, partitioner) } aggregateByKeyåº•å±‚ä¹Ÿæ˜¯è°ƒç”¨äº†combineByKeyï¼Œå¯ä»¥çœ‹åšæ˜¯ä¸€ä¸ªæ›´åŠ é€šç”¨çš„reduceByKeyï¼Œæ”¯æŒè¿”å›ç±»å‹å’Œvalueç±»å‹ä¸ä¸€è‡´ï¼Œæ”¯æŒmapç«¯èšåˆå‡½æ•°å’Œreduceèšåˆå‡½æ•°ä¸ç›¸åŒã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:14","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"combineByKey def combineByKeyWithClassTag[C]( createCombiner: V =\u003e C, mergeValue: (C, V) =\u003e C, mergeCombiners: (C, C) =\u003e C, partitioner: Partitioner, mapSideCombine: Boolean = true, serializer: Serializer = null)(implicit ct: ClassTag[C]): RDD[(K, C)] = self.withScope { å‰è¿°çš„èšåˆå‡½æ•°éƒ½æ˜¯åŸºäºcombineByKeyå®ç°çš„ï¼Œæ‰€ä»¥combineByKeyä¹Ÿæä¾›äº†æœ€å¤§çš„çµæ´»æ€§ï¼Œæ¯”å¦‚aggregateByKeyåªèƒ½æŒ‡å®šåˆå§‹å€¼ï¼Œç„¶è€ŒcombineByKeyå¯ä»¥é€šè¿‡å‡½æ•°ä¸ºä¸åŒKeyæŒ‡å®šä¸åŒçš„åˆå§‹å€¼ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:15","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"foldByKey def foldByKey(zeroValue: V, numPartitions: Int)(func: (V, V) =\u003e V): RDD[(K, V)] = self.withScope { foldByKey(zeroValue, new HashPartitioner(numPartitions))(func) } def foldByKey( zeroValue: V, partitioner: Partitioner)(func: (V, V) =\u003e V): RDD[(K, V)] = self.withScope { // Serialize the zero value to a byte array so that we can get a new clone of it on each key val zeroBuffer = SparkEnv.get.serializer.newInstance().serialize(zeroValue) val zeroArray = new Array[Byte](zeroBuffer.limit) zeroBuffer.get(zeroArray) // When deserializing, use a lazy val to create just one instance of the serializer per task lazy val cachedSerializer = SparkEnv.get.serializer.newInstance() val createZero = () =\u003e cachedSerializer.deserialize[V](ByteBuffer.wrap(zeroArray)) val cleanedFunc = self.context.clean(func) combineByKeyWithClassTag[V]((v: V) =\u003e cleanedFunc(createZero(), v), cleanedFunc, cleanedFunc, partitioner) } foldByKeyæ˜¯ä¸€ä¸ªç®€åŒ–çš„aggregateByKeyï¼ŒseqOpå’ŒcombineOpå…±ç”¨ä¸€ä¸ªfuncã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:16","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"cogroup/groupWith def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))] = self.withScope { cogroup(other, defaultPartitioner(self, other)) } def cogroup[W](other: RDD[(K, W)], partitioner: Partitioner) : RDD[(K, (Iterable[V], Iterable[W]))] = self.withScope { if (partitioner.isInstanceOf[HashPartitioner] \u0026\u0026 keyClass.isArray) { throw SparkCoreErrors.hashPartitionerCannotPartitionArrayKeyError() } val cg = new CoGroupedRDD[K](Seq(self, other), partitioner) cg.mapValues { case Array(vs, w1s) =\u003e (vs.asInstanceOf[Iterable[V]], w1s.asInstanceOf[Iterable[W]]) } } cogroupä¸­æ–‡ç¿»è¯‘æˆè”åˆåˆ†ç»„ï¼Œå°†å¤šä¸ªRDDä¸­å…·æœ‰ç›¸åŒKeyçš„Valueèšåˆåœ¨ä¸€èµ·ï¼Œå‡è®¾rdd1åŒ…å«\u003cK, V\u003e recordï¼Œrdd2åŒ…å«\u003cK, W\u003e recordï¼Œåˆ™ä¸¤è€…èšåˆç»“æœä¸º\u003cK, (List\u003cV\u003e, List\u003cW\u003e)ã€‚è¿™ä¸ªæ“ä½œè¿˜æœ‰å¦ä¸€ä¸ªåå­—groupwithã€‚ cogroupæ“ä½œå®é™…ç”Ÿæˆäº†ä¸¤ä¸ªRDDï¼ŒCoGroupedRDDå°†æ•°æ®èšåˆåœ¨ä¸€èµ·ï¼ŒMapPartitionsRDDä»…å¯¹ç»“æœçš„æ•°æ®ç±»å‹è¿›è¡Œè½¬æ¢ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:17","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"join def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))] = self.withScope { join(other, defaultPartitioner(self, other)) } def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))] = self.withScope { this.cogroup(other, partitioner).flatMapValues( pair =\u003e for (v \u003c- pair._1.iterator; w \u003c- pair._2.iterator) yield (v, w) ) } joinå’ŒSQLä¸­çš„joinç±»ä¼¼ï¼Œå°†ä¸¤ä¸ªRDDä¸­ç›¸åŒkeyçš„valueè”æ¥èµ·æ¥ï¼Œå‡è®¾rdd1ä¸­çš„æ•°æ®ä¸º\u003cK, V\u003eï¼Œrdd2ä¸­çš„æ•°æ®ä¸º\u003cK, W\u003eï¼Œé‚£ä¹ˆjoinä¹‹åçš„ç»“æœä¸º\u003cK, (V, W)\u003eã€‚åœ¨å®ç°ä¸­ï¼Œjoiné¦–å…ˆè°ƒç”¨äº†cogroupç”ŸæˆCoGroupedRDDå’ŒMapPartitionedRDDï¼Œç„¶åä½¿ç”¨flatMapValuesè®¡ç®—ç›¸åŒkeyä¸‹valueçš„ç¬›å¡å°”ç§¯ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:18","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"cartesian def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { new CartesianRDD(sc, this, other) } cartesianæ“ä½œç”Ÿæˆä¸¤ä¸ªRDDçš„ç¬›å¡å°”ç§¯ï¼Œå‡è®¾RDD1ä¸­çš„åˆ†åŒºä¸ªæ•°ä¸ºmï¼Œrdd2ä¸­çš„åˆ†åŒºä¸ªæ•°ä¸ºnï¼Œcartesianæ“ä½œä¼šç”Ÿæˆm * nä¸ªåˆ†åŒºï¼Œrdd1å’Œrdd2ä¸­çš„åˆ†åŒºä¸¤ä¸¤ç»„åˆï¼Œç»„åˆåå½¢æˆCartesianRDDä¸­çš„ä¸€ä¸ªåˆ†åŒºï¼Œè¯¥åˆ†åŒºä¸­çš„æ•°æ®æ˜¯rdd1å’Œrdd2ç›¸åº”çš„ä¸¤ä¸ªåˆ†åŒºä¸­æ•°æ®çš„ç¬›å¡å°”ç§¯ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:19","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"sortByKey def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length) : RDD[(K, V)] = self.withScope { val part = new RangePartitioner(numPartitions, self, ascending) new ShuffledRDD[K, V, V](self, part) .setKeyOrdering(if (ascending) ordering else ordering.reverse) } sortByKeyå¯¹rdd1ä¸­\u003cK, V\u003e recordè¿›è¡Œæ’åºï¼Œæ³¨æ„åªå¯¹keyè¿›è¡Œæ’åºï¼Œåœ¨ç›¸åŒKeyçš„æƒ…å†µç›¸çˆ±ï¼Œå¹¶ä¸å¯¹valueè¿›è¡Œæ’åºã€‚sortByKeyé¦–å…ˆé€šè¿‡rangeåˆ’åˆ†å°†æ•°æ®åˆ†å¸ƒåˆ°shuffledRDDçš„ä¸åŒåˆ†åŒºä¸­ï¼Œå¯ä»¥ä¿è¯åœ¨ç”Ÿæˆçš„RDDä¸­ï¼Œpartition1ä¸­çš„æ‰€æœ‰recordçš„keyå°äºï¼ˆæˆ–å¤§äºï¼‰partition2ä¸­æ‰€æœ‰recordçš„keyã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:20","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"coalesce def coalesce(numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null) : RDD[T] = withScope { require(numPartitions \u003e 0, s\"Number of partitions ($numPartitions) must be positive.\") if (shuffle) { /** Distributes elements evenly across output partitions, starting from a random partition. */ val distributePartition = (index: Int, items: Iterator[T]) =\u003e { var position = new XORShiftRandom(index).nextInt(numPartitions) items.map { t =\u003e // Note that the hash code of the key will just be the key itself. The HashPartitioner // will mod it with the number of total partitions. position = position + 1 (position, t) } } : Iterator[(Int, T)] // include a shuffle step so that our upstream tasks are still distributed new CoalescedRDD( new ShuffledRDD[Int, T, T]( mapPartitionsWithIndexInternal(distributePartition, isOrderSensitive = true), new HashPartitioner(numPartitions)), numPartitions, partitionCoalescer).values } else { new CoalescedRDD(this, numPartitions, partitionCoalescer) } } private[spark] def mapPartitionsWithIndexInternal[U: ClassTag]( f: (Int, Iterator[T]) =\u003e Iterator[U], preservesPartitioning: Boolean = false, isOrderSensitive: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) =\u003e f(index, iter), preservesPartitioning = preservesPartitioning, isOrderSensitive = isOrderSensitive) } coalesceç”¨äºå°†rddçš„åˆ†åŒºä¸ªæ•°é™ä½æˆ–è€…å‡é«˜ï¼Œåœ¨ä¸ä½¿ç”¨shuffleçš„æƒ…å†µä¸‹ï¼Œä¼šç›´æ¥ç”ŸæˆCoalescedRDDï¼Œç›´æ¥å°†ç›¸é‚»çš„åˆ†åŒºåˆå¹¶ï¼Œåˆ†åŒºä¸ªæ•°åªèƒ½é™ä½ä¸èƒ½å‡é«˜ï¼Œå½“rddä¸­ä¸åŒåˆ†åŒºä¸­çš„æ•°æ®é‡å·®åˆ«è¾ƒå¤§æ—¶ï¼Œç›´æ¥åˆå¹¶å®¹æ˜“é€ æˆæ•°æ®å€¾æ–œï¼ˆå…ƒç´ é›†ä¸­äºå°‘æ•°åˆ†åŒºä¸­ï¼‰ã€‚ä½¿ç”¨shffuleç›´æ¥è§£å†³æ•°æ®å€¾æ–œé—®é¢˜ï¼Œé€šè¿‡mapPartitionsWithIndexå¯¹è¾“å‡ºRDDçš„æ¯ä¸ªåˆ†åŒºè¿›è¡Œæ“ä½œï¼Œä¸ºåŸæ¥çš„è®°å½•å¢åŠ Keyï¼ŒKeyæ˜¯ä¸€ä¸ªIntï¼Œå¯¹æ¯ä¸ªåˆ†åŒºå¾—åˆ°ä¸€ä¸ªéšæœºçš„èµ·å§‹ä½ç½®ï¼Œåç»­è®°å½•çš„Keyæ˜¯å‰ä¸€æ¡è®°å½•çš„Key + 1ï¼Œæœ€åä½¿ç”¨hashåˆ†ç»„æ—¶ç›¸é‚»çš„è®°å½•ä¼šè¢«åˆ†åˆ°ä¸åŒçš„ç»„ã€‚æœ€ç»ˆç”ŸæˆCoalescedRDDï¼Œå¹¶ä¸¢å¼ƒæ–°ç”Ÿæˆçš„Keyï¼Œé€šè¿‡mapæ“ä½œè·å–åŸæ¥çš„è®°å½•ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:21","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"repartition def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { coalesce(numPartitions, shuffle = true) } repartitionæ“ä½œåº•å±‚ä½¿ç”¨äº†coalesceçš„shuffleç‰ˆæœ¬ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:22","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"repartitionAndSortWithinPartitions def repartitionAndSortWithinPartitions(partitioner: Partitioner): RDD[(K, V)] = self.withScope { if (self.partitioner == Some(partitioner)) { self.mapPartitions(iter =\u003e { val context = TaskContext.get() val sorter = new ExternalSorter[K, V, V](context, None, None, Some(ordering)) new InterruptibleIterator(context, sorter.insertAllAndUpdateMetrics(iter).asInstanceOf[Iterator[(K, V)]]) }, preservesPartitioning = true) } else { new ShuffledRDD[K, V, V](self, partitioner).setKeyOrdering(ordering) } } repartitionAndSortWithinPartitionså¯ä»¥çµæ´»ä½¿ç”¨å„ç§partitionerå¯¹æ•°æ®è¿›è¡Œåˆ†åŒºï¼Œå¹¶ä¸”å¯ä»¥å¯¹è¾“å‡ºRDDä¸­çš„æ¯ä¸ªåˆ†åŒºä¸­çš„Keyè¿›è¡Œæ’åºã€‚è¿™æ ·ç›¸æ¯”äºè°ƒç”¨repartitionç„¶ååœ¨æ¯ä¸ªåˆ†åŒºå†…æ’åºæ•ˆç‡æ›´é«˜ï¼Œå› ä¸ºrepartitionAndSortWithinPartitionså¯ä»¥å°†æ’åºä¸‹æ¨åˆ°shuffleæœºåˆ¶ä¸­ï¼Œæ³¨æ„ç»“æœåªèƒ½ä¿è¯æ˜¯åˆ†åŒºå†…æœ‰åºï¼Œä¸èƒ½ä¿è¯å…¨å±€æœ‰åºã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:23","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"intersection def intersection(other: RDD[T]): RDD[T] = withScope { this.map(v =\u003e (v, null)).cogroup(other.map(v =\u003e (v, null))) .filter { case (_, (leftGroup, rightGroup)) =\u003e leftGroup.nonEmpty \u0026\u0026 rightGroup.nonEmpty } .keys } intersectionæ±‚rdd1å’Œrdd2çš„äº¤é›†ï¼Œè¾“å‡ºRDDä¸åŒ…å«ä»»ä½•é‡å¤çš„å…ƒç´ ã€‚ä»å®ç°ä¸­å¯ä»¥çœ‹åˆ°ï¼Œé¦–å…ˆé€šè¿‡mapå‡½æ•°å°†recordè½¬åŒ–ä¸º\u003cK, V\u003eç±»å‹ï¼ŒVä¸ºå›ºå®šå€¼nullï¼Œç„¶åé€šè¿‡cogroupå°†rdd1å’Œrdd2ä¸­çš„recordèšåˆåœ¨ä¸€èµ·ï¼Œè¿‡æ»¤æ‰ä¸ºç©ºçš„recordï¼Œæœ€ååªä¿ç•™keyï¼Œå¾—åˆ°äº¤é›†å…ƒç´ ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:24","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"distinct def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { def removeDuplicatesInPartition(partition: Iterator[T]): Iterator[T] = { // Create an instance of external append only map which ignores values. val map = new ExternalAppendOnlyMap[T, Null, Null]( createCombiner = _ =\u003e null, mergeValue = (a, b) =\u003e a, mergeCombiners = (a, b) =\u003e a) map.insertAll(partition.map(_ -\u003e null)) map.iterator.map(_._1) } partitioner match { case Some(_) if numPartitions == partitions.length =\u003e mapPartitions(removeDuplicatesInPartition, preservesPartitioning = true) case _ =\u003e map(x =\u003e (x, null)).reduceByKey((x, _) =\u003e x, numPartitions).map(_._1) } } distinctæ˜¯å»é‡æ“ä½œï¼Œå¯¹rddä¸­çš„æ•°æ®è¿›è¡Œå»é‡ï¼Œå¦‚æœrddå·²ç»æœ‰partitionerå¹¶ä¸”åˆ†åŒºä¸ªæ•°å’Œé¢„æœŸåˆ†åŒºä¸ªæ•°ç›¸åŒï¼Œç›´æ¥èµ°åˆ†åŒºå†…å»é‡çš„é€»è¾‘ï¼Œé€šè¿‡åˆ›å»ºä¸€ä¸ªExternalAppendOnlyMapï¼Œå¾—åˆ°å»é‡åçš„æ•°æ®ã€‚å…¶ä»–æƒ…å†µä¸‹éœ€è¦èµ°shuffleé€»è¾‘ï¼Œé¦–å…ˆå°†recordæ˜ å°„ä¸º\u003cK, V\u003eï¼ŒVä¸ºå›ºå®šå€¼nullï¼Œç„¶åè°ƒç”¨reduceByKeyè¿›è¡Œèšåˆï¼Œæœ€ç»ˆåªä¿ç•™keyã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:25","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"union def union(other: RDD[T]): RDD[T] = withScope { sc.union(this, other) } def union[T: ClassTag](first: RDD[T], rest: RDD[T]*): RDD[T] = withScope { union(Seq(first) ++ rest) } def union[T: ClassTag](rdds: Seq[RDD[T]]): RDD[T] = withScope { // è¿‡æ»¤ç©ºçš„RDD val nonEmptyRdds = rdds.filter(!_.partitions.isEmpty) val partitioners = nonEmptyRdds.flatMap(_.partitioner).toSet if (nonEmptyRdds.forall(_.partitioner.isDefined) \u0026\u0026 partitioners.size == 1) { new PartitionerAwareUnionRDD(this, nonEmptyRdds) } else { new UnionRDD(this, nonEmptyRdds) } } unionè¡¨ç¤ºå°†rdd1å’Œrdd2ä¸­çš„å…ƒç´ åˆå¹¶åˆ°ä¸€èµ·ã€‚å¦‚æœæ‰€æœ‰RDDçš„partitioneréƒ½ç›¸åŒï¼Œåˆ™æ„é€ PartitionerAwareUnionRDDï¼Œåˆ†åŒºä¸ªæ•°ä¸rdd1å’Œrdd2çš„åˆ†åŒºä¸ªæ•°ç›¸åŒï¼Œä¸”è¾“å‡ºRDDä¸­æ¯ä¸ªåˆ†åŒºä¸­çš„æ•°æ®éƒ½æ˜¯rdd1å’Œrdd2å¯¹åº”åˆ†åŒºåˆå¹¶çš„ç»“æœã€‚å¦‚æœrdd1å’Œrdd2çš„partitionerä¸åŒï¼Œåˆå¹¶åçš„RDDä¸ºUnionRDDï¼Œåˆ†åŒºä¸ªæ•°æ˜¯rdd1å’Œrdd2çš„åˆ†åŒºä¸ªæ•°ä¹‹å’Œï¼Œè¾“å‡ºRDDä¸­çš„æ¯ä¸ªåˆ†åŒºä¹Ÿä¸€ä¸€å¯¹åº”rdd1æˆ–è€…rdd2ä¸­çš„ç›¸åº”çš„åˆ†åŒºã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:26","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"zip def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { zipPartitions(other, preservesPartitioning = false) { (thisIter, otherIter) =\u003e new Iterator[(T, U)] { def hasNext: Boolean = (thisIter.hasNext, otherIter.hasNext) match { case (true, true) =\u003e true case (false, false) =\u003e false case _ =\u003e throw SparkCoreErrors.canOnlyZipRDDsWithSamePartitionSizeError() } def next(): (T, U) = (thisIter.next(), otherIter.next()) } } } å°†rdd1å’Œrdd2ä¸­çš„å…ƒç´ æŒ‰ç…§ä¸€ä¸€å¯¹åº”å…³ç³»è¿æ¥åœ¨ä¸€èµ·ï¼Œæ„æˆ\u003cK, V\u003e recordã€‚è¯¥æ“ä½œè¦æ±‚rdd1å’Œrdd2çš„åˆ†åŒºä¸ªæ•°ç›¸åŒï¼Œè€Œä¸”æ¯ä¸ªåˆ†åŒºåŒ…å«çš„å…ƒç´ ä¸ªæ•°ç›¸åŒã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:27","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"zipParitions def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B]) =\u003e Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD2(sc, sc.clean(f), this, rdd2, preservesPartitioning) } zipPartitionså°†rdd1å’Œrdd2ä¸­çš„åˆ†åŒºæŒ‰ç…§ä¸€ä¸€å¯¹åº”å…³ç³»è¿æ¥åœ¨ä¸€èµ·ï¼Œå½¢æˆæ–°çš„rddã€‚æ–°çš„rddä¸­çš„æ¯ä¸ªåˆ†åŒºçš„æ•°æ®éƒ½é€šè¿‡å¯¹rdd1å’Œrdd2ä¸­å¯¹åº”åˆ†åŒºæ‰§è¡Œfuncå‡½æ•°å¾—åˆ°ï¼Œè¯¥æ“ä½œè¦æ±‚rdd1å’Œrdd2çš„åˆ†åŒºä¸ªæ•°ç›¸åŒï¼Œä½†ä¸è¦æ±‚æ¯ä¸ªåˆ†åŒºåŒ…å«ç›¸åŒçš„å…ƒç´ ä¸ªæ•°ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:28","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"zipWithIndex def zipWithIndex(): RDD[(T, Long)] = withScope { new ZippedWithIndexRDD(this) } å¯¹rdd1ä¸­çš„æ•°æ®è¿›è¡Œç¼–å·ï¼Œç¼–å·æ–¹å¼æ˜¯ä»0å¼€å§‹æŒ‰åºé€’å¢ï¼Œç›´æ¥è¿”å›ZippedWithIndexRDD ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:29","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"zipWtihUniqueId def zipWithUniqueId(): RDD[(T, Long)] = withScope { val n = this.partitions.length.toLong this.mapPartitionsWithIndex { case (k, iter) =\u003e Utils.getIteratorZipWithIndex(iter, 0L).map { case (item, i) =\u003e (item, i * n + k) } } } def getIteratorZipWithIndex[T](iter: Iterator[T], startIndex: Long): Iterator[(T, Long)] = { new Iterator[(T, Long)] { require(startIndex \u003e= 0, \"startIndex should be \u003e= 0.\") var index: Long = startIndex - 1L def hasNext: Boolean = iter.hasNext def next(): (T, Long) = { index += 1L (iter.next(), index) } } } å¯¹rdd1ä¸­çš„æ•°æ®è¿›è¡Œç¼–å·ï¼Œç¼–å·æ–¹å¼ä¸ºround-robinï¼Œå°±åƒç»™æ¯ä¸ªäººè½®æµå‘æ‰‘å…‹ç‰Œï¼Œå¦‚æœæŸäº›åˆ†åŒºæ¯”è¾ƒå°ï¼ŒåŸæœ¬åº”è¯¥åˆ†ç»™è¿™ä¸ªåˆ†åŒºçš„ç¼–å·ä¼šè½®ç©ºï¼Œè€Œä¸æ˜¯åˆ†é…ç»™å¦ä¸€ä¸ªåˆ†åŒºã€‚zipWithUniqueIdé€šè¿‡mapPartitionsWithIndexå®ç°ï¼Œè¿”å›MapPartitionsRDD ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:30","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"subtractByKey def subtractByKey[W: ClassTag](other: RDD[(K, W)]): RDD[(K, V)] = self.withScope { subtractByKey(other, self.partitioner.getOrElse(new HashPartitioner(self.partitions.length))) } def subtractByKey[W: ClassTag](other: RDD[(K, W)], p: Partitioner): RDD[(K, V)] = self.withScope { new SubtractedRDD[K, V, W](self, other, p) } subtractByKeyè®¡ç®—å‡ºkeyåœ¨rdd1ä¸­è€Œä¸åœ¨rdd2ä¸­çš„recordï¼Œé€»è¾‘ç±»ä¼¼äºcogroupï¼Œä½†å®ç°æ¯”CoGroupedRDDæ›´åŠ é«˜æ•ˆï¼Œç”ŸæˆSubtractedRDDã€‚ ä½¿ç”¨rdd1çš„paritioneræˆ–è€…åˆ†åŒºä¸ªæ•°ï¼Œå› ä¸ºç»“æœé›†ä¸ä¼šå¤§äºrdd1 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:31","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"subtract def subtract(other: RDD[T]): RDD[T] = withScope { subtract(other, partitioner.getOrElse(new HashPartitioner(partitions.length))) } def subtract( other: RDD[T], p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { if (partitioner == Some(p)) { // Our partitioner knows how to handle T (which, since we have a partitioner, is // really (K, V)) so make a new Partitioner that will de-tuple our fake tuples val p2 = new Partitioner() { override def numPartitions: Int = p.numPartitions override def getPartition(k: Any): Int = p.getPartition(k.asInstanceOf[(Any, _)]._1) } // Unfortunately, since we're making a new p2, we'll get ShuffleDependencies // anyway, and when calling .keys, will not have a partitioner set, even though // the SubtractedRDD will, thanks to p2's de-tupled partitioning, already be // partitioned by the right/real keys (e.g. p). this.map(x =\u003e (x, null)).subtractByKey(other.map((_, null)), p2).keys } else { this.map(x =\u003e (x, null)).subtractByKey(other.map((_, null)), p).keys } } å°†recordæ˜ å°„ä¸º\u003cK, V\u003e recordï¼ŒVä¸ºnullï¼Œæ˜¯ä¸€ä¸ªæ¯”è¾ƒå¸¸è§çš„æ€è·¯ï¼Œè¿™æ ·å¯ä»¥å¤ç”¨ä»£ç ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:32","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"sortBy def sortBy[K]( f: (T) =\u003e K, ascending: Boolean = true, numPartitions: Int = this.partitions.length) (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope { this.keyBy[K](f) .sortByKey(ascending, numPartitions) .values } def keyBy[K](f: T =\u003e K): RDD[(K, T)] = withScope { val cleanedF = sc.clean(f) map(x =\u003e (cleanedF(x), x)) } sortByåŸºäºfuncçš„è®¡ç®—ç»“æœå¯¹rdd1ä¸­çš„recorcè¿›è¡Œæ’åºï¼Œåº•å±‚ä½¿ç”¨sortByKeyå®ç°ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:33","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"glom def glom(): RDD[Array[T]] = withScope { new MapPartitionsRDD[Array[T], T](this, (_, _, iter) =\u003e Iterator(iter.toArray)) } å°†rdd1ä¸­çš„æ¯ä¸ªåˆ†åŒºçš„recordåˆå¹¶åˆ°ä¸€ä¸ªlistä¸­ï¼Œåº•å±‚é€šè¿‡MapPartitionsRDDå®ç°ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:34","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"å¸¸ç”¨actionæ•°æ®æ“ä½œ actionæ•°æ®æ“ä½œæ˜¯ç”¨æ¥å¯¹è®¡ç®—ç»“æœè¿›è¡Œåå¤„ç†ï¼ŒåŒæ—¶æäº¤è®¡ç®—jobã€‚å¯ä»¥é€šè¿‡è¿”å›å€¼åŒºåˆ†ä¸€ä¸ªæ“ä½œæ˜¯actionè¿˜æ˜¯transformationï¼Œtransformationæ“ä½œä¸€èˆ¬è¿”å›RDDç±»å‹ï¼Œè€Œactionæ“ä½œä¸€èˆ¬è¿”å›æ•°å€¼ã€æ•°æ®ç»“æœï¼ˆå¦‚Mapï¼‰æˆ–è€…ä¸è¿”å›ä»»ä½•å€¼ï¼ˆæ¯”å¦‚å†™ç£ç›˜ï¼‰ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:0","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"count def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum def getIteratorSize(iterator: Iterator[_]): Long = { if (iterator.knownSize \u003e= 0) iterator.knownSize.toLong else { var count = 0L while (iterator.hasNext) { count += 1L iterator.next() } count } } countæ“ä½œé¦–å…ˆè®¡ç®—æ¯ä¸ªåˆ†åŒºä¸­recordçš„æ•°ç›®ï¼Œç„¶ååœ¨Driverç«¯è¿›è¡Œç´¯åŠ æ“ä½œï¼Œè¿”å›rddä¸­åŒ…å«çš„recordä¸ªæ•°ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:1","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"countByKey def countByKey(): Map[K, Long] = self.withScope { self.mapValues(_ =\u003e 1L).reduceByKey(_ + _).collect().toMap } countByKeyç»Ÿè®¡rddä¸­æ¯ä¸ªkeyå‡ºç°çš„æ¬¡æ•°ï¼Œè¿”å›ä¸€ä¸ªmapï¼Œè¦æ±‚rddæ˜¯\u003cK, V\u003eç±»å‹ã€‚countByKeyé¦–å…ˆé€šè¿‡mapValueså°†\u003cK, V\u003e recordä¸­çš„Valueè®¾ç½®ä¸º1ï¼Œç„¶ååˆ©ç”¨reduceByKeyç»Ÿè®¡æ¯ä¸ªkeyå‡ºç°çš„æ¬¡æ•°ï¼Œæœ€åä½¿ç”¨collectæ–¹æ³•å°†ç»“æœæ”¶é›†åˆ°Driverç«¯ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:2","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"countByValue def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long] = withScope { map(value =\u003e (value, null)).countByKey() } countByValueå¹¶ä¸æ˜¯ç»Ÿè®¡\u003cK, V\u003e recordä¸­æ¯ä¸ªValueå‡ºç°çš„æ¬¡æ•°ï¼Œè€Œæ˜¯ç»Ÿè®¡æ¯ä¸ªrecordå‡ºç°çš„æ¬¡æ•°ã€‚åº•å±‚é¦–å…ˆé€šè¿‡mapå‡½æ•°å°†recordè½¬æˆ\u003cK, V\u003e recordï¼ŒValueä¸ºnullï¼Œç„¶åè°ƒç”¨countByKeyç»Ÿè®¡Keyçš„æ¬¡æ•°ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:3","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"collect def collect(): Array[T] = withScope { val results = sc.runJob(this, (iter: Iterator[T]) =\u003e iter.toArray) import org.apache.spark.util.ArrayImplicits._ Array.concat(results.toImmutableArraySeq: _*) } collectæ“ä½œå°†rddä¸­çš„recordæ”¶é›†åˆ°Driverç«¯ï¼Œè¿”å›ç±»å‹ä¸ºArray[T] ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:4","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"collectAsMap def collectAsMap(): Map[K, V] = self.withScope { val data = self.collect() val map = new mutable.HashMap[K, V] map.sizeHint(data.length) data.foreach { pair =\u003e map.put(pair._1, pair._2) } map } collectAsMapé€šè¿‡collectè°ƒç”¨å°†\u003cK, V\u003e recordæ”¶é›†åˆ°Driverç«¯ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:5","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"foreach def foreach(f: T =\u003e Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) =\u003e iter.foreach(cleanF)) } å°†rddä¸­çš„æ¯ä¸ªrecordæŒ‰ç…§funcè¿›è¡Œå¤„ç†ï¼Œåº•å±‚è°ƒç”¨runJobã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:6","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"foreachPartitions def foreachPartition(f: Iterator[T] =\u003e Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) =\u003e cleanF(iter)) } å°†rddä¸­çš„æ¯ä¸ªåˆ†åŒºä¸­çš„æ•°æ®æŒ‰ç…§funcè¿›è¡Œå¤„ç†ï¼Œåº•å±‚è°ƒç”¨runJobã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:7","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"fold def fold(zeroValue: T)(op: (T, T) =\u003e T): T = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) val cleanOp = sc.clean(op) val foldPartition = (iter: Iterator[T]) =\u003e iter.fold(zeroValue)(cleanOp) val mergeResult = (_: Int, taskResult: T) =\u003e jobResult = op(jobResult, taskResult) sc.runJob(this, foldPartition, mergeResult) jobResult } foldå°†rddä¸­çš„recordæŒ‰ç…§funcè¿›è¡Œèšåˆï¼Œé¦–å…ˆåœ¨rddçš„æ¯ä¸ªåˆ†åŒºä¸­è®¡ç®—å‡ºå±€éƒ¨ç»“æœå³å‡½æ•°foldPartitionï¼Œç„¶ååœ¨Driveræ®µå°†å±€éƒ¨ç»“æœèšåˆæˆæœ€ç»ˆç»“æœå³å‡½æ•°mergeResultã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œfoldæ¯æ¬¡èšåˆæ˜¯åˆå§‹å€¼zeroValueéƒ½ä¼šå‚ä¸è®¡ç®—ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:8","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"reduce def reduce(f: (T, T) =\u003e T): T = withScope { val cleanF = sc.clean(f) val reducePartition: Iterator[T] =\u003e Option[T] = iter =\u003e { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } var jobResult: Option[T] = None val mergeResult = (_: Int, taskResult: Option[T]) =\u003e { if (taskResult.isDefined) { jobResult = jobResult match { case Some(value) =\u003e Some(f(value, taskResult.get)) case None =\u003e taskResult } } } sc.runJob(this, reducePartition, mergeResult) // Get the final result out of our Option, or throw an exception if the RDD was empty jobResult.getOrElse(throw SparkCoreErrors.emptyCollectionError()) } å°†rddä¸­çš„recordæŒ‰ç…§funcè¿›è¡Œèšåˆï¼Œè¿™é‡Œæ²¡æœ‰æä¾›åˆå§‹å€¼ï¼Œæ‰€ä»¥éœ€è¦å¤„ç†ç©ºå€¼çš„æƒ…å†µã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:9","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"aggregate def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =\u003e U, combOp: (U, U) =\u003e U): U = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance()) val cleanSeqOp = sc.clean(seqOp) val aggregatePartition = (it: Iterator[T]) =\u003e it.foldLeft(zeroValue)(cleanSeqOp) val mergeResult = (_: Int, taskResult: U) =\u003e jobResult = combOp(jobResult, taskResult) sc.runJob(this, aggregatePartition, mergeResult) jobResult } å°†rddä¸­çš„recordæŒ‰ç…§funcè¿›è¡Œèšåˆï¼Œè¿™é‡Œæä¾›äº†åˆå§‹å€¼ï¼Œåˆ†åŒºèšåˆå’ŒDriverç«¯èšåˆéƒ½ä¼šä½¿ç”¨åˆå§‹å€¼ã€‚ ä¸ºä»€ä¹ˆå·²ç»æœ‰äº†reduceByKeyã€aggregateByKeyç­‰æ“ä½œï¼Œè¿˜è¦å®šä¹‰aggreagteå’Œreduceç­‰æ“ä½œå‘¢ï¼Ÿè™½ç„¶reduceByKeyã€aggregateByKeyç­‰æ“ä½œå¯ä»¥å¯¹æ¯ä¸ªåˆ†åŒºä¸­çš„recordï¼Œä»¥åŠè·¨åˆ†åŒºä¸”å…·æœ‰ç›¸åŒKeyçš„recordè¿›è¡Œèšåˆï¼Œä½†è¿™äº›èšåˆéƒ½æ˜¯åœ¨éƒ¨åˆ†æ•°æ®ä¸Šï¼Œç±»ä¼¼äº\u003cK, func(list(V))ï¼Œè€Œä¸æ˜¯é’ˆå¯¹æ‰€æœ‰recordè¿›è¡Œå…¨å±€èšåˆï¼Œå³func(\u003cK, list(V))ã€‚ ç„¶è€Œaggregateã€reduceç­‰æ“ä½œå­˜åœ¨ç›¸åŒçš„é—®é¢˜ï¼Œå½“éœ€è¦mergeçš„éƒ¨åˆ†ç»“æœå¾ˆå¤§æ—¶ï¼Œæ•°æ®ä¼ è¾“é‡å¾ˆå¤§ï¼Œè€Œä¸”Driveræ˜¯å•ç‚¹mergeï¼Œå­˜åœ¨æ•ˆç‡å’Œå†…å­˜ç©ºé—´é™åˆ¶çš„é—®é¢˜ï¼Œä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒSparkå¯¹è¿™äº›èšåˆæ“ä½œè¿›è¡Œäº†ä¼˜åŒ–ï¼Œæå‡ºäº†treeAggregateå’ŒtreeReduceæ“ä½œã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:10","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"treeAggregate def treeAggregate[U: ClassTag](zeroValue: U)( seqOp: (U, T) =\u003e U, combOp: (U, U) =\u003e U, depth: Int = 2): U = withScope { treeAggregate(zeroValue, seqOp, combOp, depth, finalAggregateOnExecutor = false) } def treeAggregate[U: ClassTag]( zeroValue: U, seqOp: (U, T) =\u003e U, combOp: (U, U) =\u003e U, depth: Int, finalAggregateOnExecutor: Boolean): U = withScope { require(depth \u003e= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") if (partitions.length == 0) { Utils.clone(zeroValue, context.env.closureSerializer.newInstance()) } else { val cleanSeqOp = context.clean(seqOp) val cleanCombOp = context.clean(combOp) val aggregatePartition = (it: Iterator[T]) =\u003e it.foldLeft(zeroValue)(cleanSeqOp) var partiallyAggregated: RDD[U] = mapPartitions(it =\u003e Iterator(aggregatePartition(it))) var numPartitions = partiallyAggregated.partitions.length val scale = math.max(math.ceil(math.pow(numPartitions, 1.0 / depth)).toInt, 2) // If creating an extra level doesn't help reduce // the wall-clock time, we stop tree aggregation. // Don't trigger TreeAggregation when it doesn't save wall-clock time while (numPartitions \u003e scale + math.ceil(numPartitions.toDouble / scale)) { numPartitions /= scale val curNumPartitions = numPartitions partiallyAggregated = partiallyAggregated.mapPartitionsWithIndex { (i, iter) =\u003e iter.map((i % curNumPartitions, _)) }.foldByKey(zeroValue, new HashPartitioner(curNumPartitions))(cleanCombOp).values } if (finalAggregateOnExecutor \u0026\u0026 partiallyAggregated.partitions.length \u003e 1) { // map the partially aggregated rdd into a key-value rdd // do the computation in the single executor with one partition // get the new RDD[U] partiallyAggregated = partiallyAggregated .map(v =\u003e (0.toByte, v)) .foldByKey(zeroValue, new ConstantPartitioner)(cleanCombOp) .values } val copiedZeroValue = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) partiallyAggregated.fold(copiedZeroValue)(cleanCombOp) } } treeAggreagteæ˜¯ä¸ºäº†è§£å†³aggregateåœ¨Driverç«¯èšåˆå¯¼è‡´çš„æ•°æ®ä¼ è¾“é‡å¤§ã€å•ç‚¹mergeã€å†…å­˜ç©ºé—´é™åˆ¶ç­‰é—®é¢˜ï¼Œæ€è·¯ç±»ä¼¼äºå½’å¹¶æ’åºçš„å±‚æ¬¡å½’å¹¶ï¼Œæ¯å±‚éƒ½å°†åˆ†åŒºæ•°ç›®é™ä½ä¸ºåŸæ¥çš„1/scaleï¼Œä¹Ÿå°±æ˜¯ä¸€é¢—è¿‘ä¼¼å®Œç¾çš„å¹³è¡¡æ ‘ï¼Œè®©æ¯å±‚æ¯ä¸ªèŠ‚ç‚¹çš„è´Ÿè½½éƒ½ç›¸å¯¹åˆç†ã€‚æˆ‘ä»¬å¯ä»¥åœ¨å‚æ•°ä¸­æŒ‡å®šdepthï¼Œå‡è®¾åˆ†åŒºæ•°é‡ä¸ºNï¼Œåˆ™è¿‘ä¼¼æœ‰N / (scale^depth) = 1ã€‚å½“ç„¶Sparkåœ¨ä½•æ—¶åœæ­¢å±€éƒ¨èšåˆåšäº†ä¼˜åŒ–ï¼Œå¹³è¡¡æ•ˆç‡å’Œå¼€é”€ï¼Œé€‰æ‹©åœ¨numPartitions \u003e scale + math.ceil(numPartitions.toDouble / scaleæ—¶åœæ­¢å±€éƒ¨èšåˆï¼ŒnumPartitionsè¡¨ç¤ºå½“å‰åˆ†åŒºæ•°ï¼ŒnumParttions/scaleè¡¨ç¤ºå¦‚æœç»§ç»­å±€éƒ¨èšåˆä¸‹ä¸€å±‚çš„åˆ†åŒºæ•°ï¼Œä¸ºä»€ä¹ˆä¼šæœ‰ä¸€ä¸ªé¢å¤–çš„scaleï¼Œæˆ‘è®¤ä¸ºåº”è¯¥æ˜¯ä¸ºäº†é¿å…æç«¯æƒ…å†µï¼Œæ¯”å¦‚åˆ†åŒºæ•°ä¸º2ï¼Œscaleä¸º2ï¼Œ é‚£ä¹ˆå¦‚æœæ²¡æœ‰é¢å¤–çš„scaleä½œä¸ºæˆæœ¬ï¼Œè¿™é‡Œä¼šç»§ç»­å±€éƒ¨èšåˆï¼Œç„¶åæœ‰äº†é¢å¤–çš„scaleã€‚ å®ç°ä¸Šå±€éƒ¨èšåˆä½¿ç”¨äº†foldByKeyï¼Œå°½ç®¡å½¢å¼ä¸Šä½¿ç”¨äº†ShuffleDependencyï¼Œä½†æ˜¯ç”±äºæ¯ä¸ªåˆ†åŒºä¸­åªæœ‰ä¸€æ¡è®°å½•ï¼Œå®é™…æ•°æ®ä¼ è¾“æ—¶ç±»ä¼¼äºå¤šå¯¹ä¸€çš„NarrowDependencyã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:11","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"treeReduce treeReduceæ˜¯reduceçš„ä¼˜åŒ–ç‰ˆæœ¬ã€‚åº•å±‚å®é™…ä¸Šè°ƒç”¨äº†treeAggregateã€‚ def treeReduce(f: (T, T) =\u003e T, depth: Int = 2): T = withScope { require(depth \u003e= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") val cleanF = context.clean(f) val reducePartition: Iterator[T] =\u003e Option[T] = iter =\u003e { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } val partiallyReduced = mapPartitions(it =\u003e Iterator(reducePartition(it))) val op: (Option[T], Option[T]) =\u003e Option[T] = (c, x) =\u003e { if (c.isDefined \u0026\u0026 x.isDefined) { Some(cleanF(c.get, x.get)) } else if (c.isDefined) { c } else if (x.isDefined) { x } else { None } } partiallyReduced.treeAggregate(Option.empty[T])(op, op, depth) .getOrElse(throw SparkCoreErrors.emptyCollectionError()) } ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:12","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"reduceByKeyLocally def reduceByKeyLocally(func: (V, V) =\u003e V): Map[K, V] = self.withScope { val cleanedF = self.sparkContext.clean(func) if (keyClass.isArray) { throw SparkCoreErrors.reduceByKeyLocallyNotSupportArrayKeysError() } val reducePartition = (iter: Iterator[(K, V)]) =\u003e { val map = new JHashMap[K, V] iter.foreach { pair =\u003e val old = map.get(pair._1) map.put(pair._1, if (old == null) pair._2 else cleanedF(old, pair._2)) } Iterator(map) } : Iterator[JHashMap[K, V]] val mergeMaps = (m1: JHashMap[K, V], m2: JHashMap[K, V]) =\u003e { m2.asScala.foreach { pair =\u003e val old = m1.get(pair._1) m1.put(pair._1, if (old == null) pair._2 else cleanedF(old, pair._2)) } m1 } : JHashMap[K, V] self.mapPartitions(reducePartition).reduce(mergeMaps).asScala } reduceByKeyLocallyé¦–å…ˆåœ¨rddçš„å„ä¸ªåˆ†åŒºä¸­è¿›è¡Œèšåˆï¼Œå¹¶ä½¿ç”¨HashMapæ¥å­˜å‚¨èšåˆç»“æœï¼Œç„¶åå°†æ•°æ®æ±‡æ€»åˆ°Driverç«¯è¿›è¡Œå…¨å±€èšåˆï¼Œä»ç„¶æ˜¯å°†èšåˆç»“æœå­˜åœ¨åˆ°HashMapã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:13","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"take def take(num: Int): Array[T] = withScope { val scaleUpFactor = Math.max(conf.get(RDD_LIMIT_SCALE_UP_FACTOR), 2) if (num == 0) { new Array[T](0) } else { val buf = new ArrayBuffer[T] val totalParts = this.partitions.length var partsScanned = 0 while (buf.size \u003c num \u0026\u0026 partsScanned \u003c totalParts) { // The number of partitions to try in this iteration. It is ok for this number to be // greater than totalParts because we actually cap it at totalParts in runJob. var numPartsToTry = conf.get(RDD_LIMIT_INITIAL_NUM_PARTITIONS) val left = num - buf.size if (partsScanned \u003e 0) { // If we didn't find any rows after the previous iteration, multiply by // limitScaleUpFactor and retry. Otherwise, interpolate the number of partitions we need // to try, but overestimate it by 50%. We also cap the estimation in the end. if (buf.isEmpty) { numPartsToTry = partsScanned * scaleUpFactor } else { // As left \u003e 0, numPartsToTry is always \u003e= 1 numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor) } } val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts)) val res = sc.runJob(this, (it: Iterator[T]) =\u003e it.take(left).toArray, p) res.foreach(buf ++= _.take(num - buf.size)) partsScanned += p.size } buf.toArray } } takeè¡¨ç¤ºä»rddä¸­å–å‡ºå‰numä¸ªrecordã€‚takeæ“ä½œé¦–å…ˆå–å‡ºrddä¸­ç¬¬ä¸€ä¸ªåˆ†åŒºçš„å‰numä¸ªrecordï¼Œå¦‚æœnumå¤§äºpartition1ä¸­recordçš„æ€»æ•°ï¼Œåˆ™takeä¼šç»§ç»­ä»åç»­çš„åˆ†åŒºä¸­å–å‡ºrecordï¼Œä¸ºäº†æé«˜æ•ˆç‡ï¼Œsparkä¼šæ ¹æ®å‰é¢åˆ†åŒºåˆ†åŒºçš„å¹³å‡å¤§å°ä¼°è®¡åç»­éœ€è¦å–å‡ ä¸ªåˆ†åŒºæ¥æ»¡è¶³takeçš„éœ€æ±‚ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:14","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"first def first(): T = withScope { take(1) match { case Array(t) =\u003e t case _ =\u003e throw SparkCoreErrors.emptyCollectionError() } } åªå–å‡ºrddä¸­çš„ç¬¬ä¸€ä¸ªrecordã€‚åº•å±‚é€šè¿‡take(1)å®ç°ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:15","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"takeOrdered def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { if (num == 0 || this.getNumPartitions == 0) { Array.empty } else { this.mapPartitionsWithIndex { case (pid, iter) =\u003e if (iter.nonEmpty) { // Priority keeps the largest elements, so let's reverse the ordering. Iterator.single(collectionUtils.takeOrdered(iter, num)(ord).toArray) } else if (pid == 0) { // make sure partition 0 always returns an array to avoid reduce on empty RDD Iterator.single(Array.empty[T]) } else { Iterator.empty } }.reduce { (array1, array2) =\u003e val size = math.min(num, array1.length + array2.length) val array = Array.ofDim[T](size) collectionUtils.mergeOrdered[T](Seq(array1, array2))(ord).copyToArray(array, 0, size) array } } } å–å‡ºrddä¸­æœ€å°çš„numä¸ªrecordã€‚é¦–å…ˆä½¿ç”¨mapPartitionsWithIndexåœ¨æ¯ä¸ªåˆ†åŒºä¸­æ‰¾å‡ºæœ€å°çš„numä¸ªrecordï¼Œå› ä¸ºå…¨å±€æœ€å°çš„nä¸ªå…ƒç´ ä¸€å®šæ˜¯æ¯ä¸ªåˆ†åŒºä¸­æœ€å°çš„nä¸ªå…ƒç´ çš„å­é›†ï¼Œç„¶åé€šè¿‡reduceæ“ä½œå°†è¿™äº›recordæ”¶é›†åˆ°Driveræ®µï¼Œè¿›è¡Œæ’åºï¼Œç„¶åå–å‡ºå‰numä¸ªrecordã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:16","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"top def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { takeOrdered(num)(ord.reverse) } å–å‡ºrddä¸­æœ€å¤§çš„numä¸ªrecordã€‚åº•å±‚é€šè¿‡takeOrderedå®ç°ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:17","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"max/min def max()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.max) } def min()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.min) } è¿”å›rddä¸­çš„æœ€å¤§ã€æœ€å°å€¼ã€‚åº•å±‚åŸºäºreduceå®ç°ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:18","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"isEmpty def isEmpty(): Boolean = withScope { partitions.length == 0 || take(1).length == 0 } åˆ¤æ–­rddæ˜¯å¦ä¸ºç©ºï¼Œå¦‚æœrddä¸åŒ…å«ä»»ä½•recordï¼Œåˆ™è¿”å›trueã€‚å¦‚æœåˆ†åŒºæ•°ä¸º0ï¼Œåˆ™rddä¸€å®šä¸ºç©ºï¼Œåˆ†åŒºæ•°å¤§äº0å¹¶ä¸æ„å‘³ç€rddä¸€å®šä¸ä¸ºç©ºï¼Œéœ€è¦é€šè¿‡take(1)åˆ¤æ–­æ˜¯å¦æœ‰æ•°æ®ã€‚å¦‚æœå¯¹rddæ‰§è¡Œä¸€äº›æ•°æ®æ“ä½œï¼Œæ¯”å¦‚è¿‡æ»¤ã€æ±‚äº¤é›†ç­‰ï¼Œrddä¸ºç©ºçš„è¯ï¼Œé‚£ä¹ˆæ‰§è¡Œå…¶ä»–æ“ä½œä¹Ÿä¸€å®šä¸ºç©ºï¼Œå› æ­¤ï¼Œæå‰åˆ¤æ–­rddæ˜¯å¦ä¸ºç©ºï¼Œå¯ä»¥é¿å…æäº¤å†—ä½™çš„jobã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:19","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"lookup def lookup(key: K): Seq[V] = self.withScope { self.partitioner match { case Some(p) =\u003e val index = p.getPartition(key) val process = (it: Iterator[(K, V)]) =\u003e { val buf = new ArrayBuffer[V] for (pair \u003c- it if pair._1 == key) { buf += pair._2 } buf.toSeq } : Seq[V] val res = self.context.runJob(self, process, Array(index).toImmutableArraySeq) res(0) case None =\u003e self.filter(_._1 == key).map(_._2).collect().toImmutableArraySeq } } loopupå‡½æ•°æ‰¾å‡ºrddä¸­åŒ…å«ç‰¹å®škeyçš„valueï¼Œå°†è¿™äº›valueå½¢æˆListã€‚loopupé¦–å…ˆè¿‡æ»¤å‡ºç»™å®škeyçš„recordï¼Œç„¶åä½¿ç”¨mapå¾—åˆ°ç›¸åº”çš„valueï¼Œæœ€åä½¿ç”¨collectå°†è¿™äº›valueæ”¶é›†åˆ°Driverç«¯å½¢æˆlistã€‚å¦‚æœrddçš„partitionerå·²ç»ç¡®å®šï¼Œé‚£ä¹ˆåœ¨è¿‡æ»¤å‰ï¼Œé€šè¿‡getPartitionç¡®å®škeyæ‰€åœ¨çš„åˆ†åŒºï¼Œå‡å°‘æ“ä½œçš„æ•°æ®é‡ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:20","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"saveAsTextFile def saveAsTextFile(path: String): Unit = withScope { saveAsTextFile(path, null) } def saveAsTextFile(path: String, codec: Class[_ \u003c: CompressionCodec]): Unit = withScope { this.mapPartitions { iter =\u003e val text = new Text() iter.map { x =\u003e require(x != null, \"text files do not allow null rows\") text.set(x.toString) (NullWritable.get(), text) } }.saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path, codec) } saveAsTextFileå°†rddä¿å­˜æˆæ–‡æœ¬æ–‡ä»¶ï¼Œé€šè¿‡toStringæ“ä½œè·å–recordçš„å­—ç¬¦ä¸²å½¢å¼ï¼Œç„¶åå°†recordè½¬åŒ–ä¸º\u003cNullWriter, Text\u003eç±»å‹ï¼ŒNullWriterçš„æ„æ€æ˜¯æ§è¡€ï¼Œä¹Ÿå°±æ˜¯æ¯æ¡è¾“å‡ºæ•°æ®åªåŒ…å«ç±»ä¼¼ä¸ºæ–‡æœ¬çš„Valueã€‚åº•å±‚è°ƒç”¨saveAsHadoopFileã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:21","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"saveAsObjectFile def saveAsObjectFile(path: String): Unit = withScope { this.mapPartitions(iter =\u003e iter.grouped(10).map(_.toArray)) .map(x =\u003e (NullWritable.get(), new BytesWritable(Utils.serialize(x)))) .saveAsSequenceFile(path) } saveAsObjectFileå°†rddä¿å­˜ä¸ºåºåˆ—åŒ–å¯¹è±¡å½¢å¼çš„SequenceFileï¼Œé’ˆå¯¹æ™®é€šå¯¹è±¡ç±»å‹ï¼Œå°†recordæƒŠé†’åºåˆ—åŒ–ï¼Œå¹¶ä¸”ä»¥æ¯10ä¸ªrecordä¸º1ç»„è½¬åŒ–ä¸ºSequenceFile\u003cNullableWritable, Array[Object]\u003eï¼Œè°ƒç”¨saveAsSequenceFileå°†æ–‡ä»¶å†™å…¥HDFSä¸­ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:22","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"saveAsSequenceFile def saveAsSequenceFile( path: String, codec: Option[Class[_ \u003c: CompressionCodec]] = None): Unit = self.withScope { def anyToWritable[U: IsWritable](u: U): Writable = u // TODO We cannot force the return type of `anyToWritable` be same as keyWritableClass and // valueWritableClass at the compile time. To implement that, we need to add type parameters to // SequenceFileRDDFunctions. however, SequenceFileRDDFunctions is a public class so it will be a // breaking change. val convertKey = self.keyClass != _keyWritableClass val convertValue = self.valueClass != _valueWritableClass logInfo(log\"Saving as sequence file of type \" + log\"(${MDC(LogKeys.KEY, _keyWritableClass.getSimpleName)},\" + log\"${MDC(LogKeys.VALUE, _valueWritableClass.getSimpleName)})\") val format = classOf[SequenceFileOutputFormat[Writable, Writable]] val jobConf = new JobConf(self.context.hadoopConfiguration) if (!convertKey \u0026\u0026 !convertValue) { self.saveAsHadoopFile(path, _keyWritableClass, _valueWritableClass, format, jobConf, codec) } else if (!convertKey \u0026\u0026 convertValue) { self.map(x =\u003e (x._1, anyToWritable(x._2))).saveAsHadoopFile( path, _keyWritableClass, _valueWritableClass, format, jobConf, codec) } else if (convertKey \u0026\u0026 !convertValue) { self.map(x =\u003e (anyToWritable(x._1), x._2)).saveAsHadoopFile( path, _keyWritableClass, _valueWritableClass, format, jobConf, codec) } else if (convertKey \u0026\u0026 convertValue) { self.map(x =\u003e (anyToWritable(x._1), anyToWritable(x._2))).saveAsHadoopFile( path, _keyWritableClass, _valueWritableClass, format, jobConf, codec) } } saveAsSequenceFileå°†rddä¿å­˜ä¸ºSequenceFileå½¢å¼çš„æ–‡ä»¶ï¼Œé’ˆå¯¹\u003cK, V\u003e ç±»å‹çš„recordï¼Œå°†recordè¿›è¡Œåºåˆ—åŒ–åï¼Œä»¥SequenceFileå½¢å¼å†™å…¥åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿä¸­ï¼Œåº•å±‚è°ƒç”¨saveAsHadoopFileå®ç°ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:23","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"saveAsHadoopFile def saveAsHadoopFile( path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ \u003c: OutputFormat[_, _]], conf: JobConf = new JobConf(self.context.hadoopConfiguration), codec: Option[Class[_ \u003c: CompressionCodec]] = None): Unit = self.withScope { // Rename this as hadoopConf internally to avoid shadowing (see SPARK-2038). val hadoopConf = conf hadoopConf.setOutputKeyClass(keyClass) hadoopConf.setOutputValueClass(valueClass) conf.setOutputFormat(outputFormatClass) for (c \u003c- codec) { hadoopConf.setCompressMapOutput(true) hadoopConf.set(\"mapreduce.output.fileoutputformat.compress\", \"true\") hadoopConf.setMapOutputCompressorClass(c) hadoopConf.set(\"mapreduce.output.fileoutputformat.compress.codec\", c.getCanonicalName) hadoopConf.set(\"mapreduce.output.fileoutputformat.compress.type\", CompressionType.BLOCK.toString) } // Use configured output committer if already set if (conf.getOutputCommitter == null) { hadoopConf.setOutputCommitter(classOf[FileOutputCommitter]) } // When speculation is on and output committer class name contains \"Direct\", we should warn // users that they may loss data if they are using a direct output committer. val speculationEnabled = self.conf.get(SPECULATION_ENABLED) val outputCommitterClass = hadoopConf.get(\"mapred.output.committer.class\", \"\") if (speculationEnabled \u0026\u0026 outputCommitterClass.contains(\"Direct\")) { val warningMessage = log\"${MDC(CLASS_NAME, outputCommitterClass)} \" + log\"may be an output committer that writes data directly to \" + log\"the final location. Because speculation is enabled, this output committer may \" + log\"cause data loss (see the case in SPARK-10063). If possible, please use an output \" + log\"committer that does not have this behavior (e.g. FileOutputCommitter).\" logWarning(warningMessage) } FileOutputFormat.setOutputPath(hadoopConf, SparkHadoopWriterUtils.createPathFromString(path, hadoopConf)) saveAsHadoopDataset(hadoopConf) } def saveAsHadoopDataset(conf: JobConf): Unit = self.withScope { val config = new HadoopMapRedWriteConfigUtil[K, V](new SerializableJobConf(conf)) SparkHadoopWriter.write( rdd = self, config = config) } saveAsHadoopFileå°†rddä¿å­˜ä¸ºHaddop HDFSæ–‡ä»¶ç³»ç»Ÿæ”¯æŒçš„æ–‡ä»¶ï¼Œè¿›è¡Œå¿…è¦çš„åˆå§‹åŒ–å’Œé…ç½®åï¼Œé€šè¿‡SparkHadoopWriterå°†rddå†™å…¥hadoopä¸­ã€‚ ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:24","tags":["Spark"],"title":"Sparké€»è¾‘å¤„ç†æµç¨‹","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"RDDæ•°æ®æ¨¡å‹ RDD ï¼ˆResilient Distributed DataSet)æ˜¯sparkå¯¹è®¡ç®—è¿‡ç¨‹ä¸­è¾“å…¥è¾“å‡ºæ•°æ®ä»¥åŠä¸­é—´æ•°æ®çš„æŠ½è±¡ï¼Œè¡¨ç¤ºä¸å¯å˜ã€åˆ†åŒºçš„é›†åˆæ•°æ®ï¼Œå¯ä»¥è¢«å¹¶è¡Œå¤„ç†ã€‚ abstract class RDD[T: ClassTag]( @transient private var _sc: SparkContext, @transient private var deps: Seq[Dependency[_]] ) extends Serializable with Logging { RDDç±»åŒ…å«ä¸€äº›åŸºç¡€æ“ä½œï¼Œæ¯”å¦‚mapã€filterå’Œpersistï¼Œå¦å¤– PairRDDFunctionsåŒ…å«ä¸“é—¨å¤„ç†é”®å€¼å¯¹RDDçš„æ“ä½œï¼Œæ¯”å¦‚groupByKeyå’Œjoin DoubleRDDFunctionsåŒ…å«æ•°æ®ä¸ºDoubleç±»å‹çš„RDDçš„æ“ä½œ SequenceFileRDDFunctionåŒ…å«å¯ä»¥è¢«ä¿å­˜ä¸ºSequenceFielsçš„RDDçš„æ“ä½œ OrderedRDDFunctionsé”®å€¼å¯¹RDDï¼Œkeyé€šè¿‡éšå¼è½¬æ¢åæ”¯æŒæ’åº RDDä¸»è¦æœ‰5ç§å±æ€§ï¼š åˆ†åŒºåˆ—è¡¨ è®¡ç®—æ¯ä¸ªåˆ†åŒºçš„å‡½æ•° å¯¹å…¶ä»–RDDçš„ä¾èµ–ç»„æˆçš„ä¾èµ–é“¾è¡¨ å¯é€‰ï¼Œé”®å€¼å¯¹RDDè¿›è¡Œåˆ†åŒºçš„Partitioner æ¯”å¦‚æŸä¸ªRDDæ˜¯hashåˆ†åŒºçš„ å¯é€‰ï¼Œè®¡ç®—æ¯ä¸ªåˆ†åŒºçš„æœ¬åœ°åŒ–åå¥½åˆ—è¡¨ï¼Œæ¯”å¦‚ä¾æ®hdfsæ–‡ä»¶çš„blockä½ç½®ç»™å®šåå¥½ï¼Œé™ä½ç½‘ç»œä¼ è¾“å¼€é”€ ","date":"2025-05-10","objectID":"/posts/spark-base/:1:0","tags":["Spark"],"title":"SparkåŸºç¡€çŸ¥è¯†","uri":"/posts/spark-base/"},{"categories":["Spark"],"content":"RDDå¸¸ç”¨å±æ€§ SparkContext RDDæ‰€å±çš„ä¸Šä¸‹æ–‡ Seq[Dependency[_]] å½“å‰RDDä¾èµ–çš„RDDåˆ—è¡¨ Option[Partitioner] partitionerï¼Œå¯ä»¥è¢«å­ç±»é‡å†™ï¼Œè¡¨ç¤ºRDDæ˜¯å¦‚ä½•åˆ†åŒºçš„ Array[Partition] RDDæ‹¥æœ‰çš„æ‰€æœ‰åˆ†åŒº ","date":"2025-05-10","objectID":"/posts/spark-base/:1:1","tags":["Spark"],"title":"SparkåŸºç¡€çŸ¥è¯†","uri":"/posts/spark-base/"},{"categories":["Spark"],"content":"Partition /** * An identifier for a partition in an RDD. */ trait Partition extends Serializable { /** * Get the partition's index within its parent RDD */ def index: Int // A better default implementation of HashCode override def hashCode(): Int = index override def equals(other: Any): Boolean = super.equals(other) } Partitionè¡¨ç¤ºRDDä¸­çš„ä¸€ä¸ªåˆ†åŒº private[spark] class PartitionPruningRDDPartition(idx: Int, val parentSplit: Partition) extends Partition { override val index = idx } PartitionPruningRDDPartitionè¡¨ç¤ºçˆ¶RDDè¢«å‰ªæåç”Ÿæˆçš„å­RDDä¸­çš„åˆ†åŒºã€‚idxè¡¨ç¤ºå­RDDä¸­åˆ†åŒºçš„partition Idï¼Œparentsplitè¡¨ç¤ºå¯¹åº”çš„çˆ¶RDDä¸­çš„åˆ†åŒºã€‚ ","date":"2025-05-10","objectID":"/posts/spark-base/:1:2","tags":["Spark"],"title":"SparkåŸºç¡€çŸ¥è¯†","uri":"/posts/spark-base/"},{"categories":["Spark"],"content":"Partitioner abstract class Partitioner extends Serializable { def numPartitions: Int def getPartition(key: Any): Int } Partitionerå®šä¹‰äº†é”®å€¼å¯¹RDDä¸­çš„å…ƒç´ å¦‚ä½•é€šè¿‡keyè¿›è¡Œåˆ†åŒºï¼Œæ˜ å°„æ¯ä¸ªkeyåˆ°ä¸€ä¸ªpartition IDï¼Œä»0åˆ° numPartitions - 1ã€‚æ³¨æ„partitionerå¿…é¡»æ˜¯ç¡®å®šæ€§çš„ï¼Œç»™å®šç›¸åŒçš„partition keyå¿…é¡»è¿”å›ç›¸åŒçš„åˆ†åŒºã€‚ HashPartitioner class HashPartitioner(partitions: Int) extends Partitioner { require(partitions \u003e= 0, s\"Number of partitions ($partitions) cannot be negative.\") def numPartitions: Int = partitions def getPartition(key: Any): Int = key match { case null =\u003e 0 case _ =\u003e Utils.nonNegativeMod(key.hashCode, numPartitions) } override def equals(other: Any): Boolean = other match { case h: HashPartitioner =\u003e h.numPartitions == numPartitions case _ =\u003e false } override def hashCode: Int = numPartitions } HashPartitionerä½¿ç”¨javaçš„Object.hashCodeå®ç°äº†åŸºäºhashçš„åˆ†åŒºï¼Œjavaæ•°æ®çš„hashCodeåŸºäºæ•°æ®çš„identityè€Œä¸æ˜¯ä»–ä»¬çš„å†…å®¹ï¼Œæ‰€ä»¥å°è¯•å¯¹RDD[Array[_]]æˆ–è€…RDD[(Array[_], _)]ä½¿ç”¨HashPartitionerå°†äº§ç”Ÿéé¢„æœŸæ•ˆæœã€‚ RangePartitioner class RangePartitioner[K : Ordering : ClassTag, V]( partitions: Int, rdd: RDD[_ \u003c: Product2[K, V]], private var ascending: Boolean = true, val samplePointsPerPartitionHint: Int = 20) extends Partitioner { RangePartitionerå°†å¯æ’åºçš„å‡ ç‡æŒ‰èŒƒå›´åˆ’åˆ†æˆå¤§è‡´ç›¸ç­‰çš„åŒºé—´ï¼ŒèŒƒå›´æ˜¯é€šè¿‡å¯¹ä¼ å…¥çš„RDDè¿›è¡Œé‡‡æ ·ç¡®å®šçš„ã€‚åˆ†åŒºçš„å®é™…æ•°é‡å¯èƒ½å’Œpartitionså‚æ•°ä¸ä¸€è‡´ï¼Œæ¯”å¦‚å½“é‡‡æ ·çš„è®°å½•å°‘äºpartitionsæ—¶ã€‚ def getPartition(key: Any): Int = { val k = key.asInstanceOf[K] var partition = 0 if (rangeBounds.length \u003c= 128) { // åˆ†åŒºä¸ªæ•°å¾ˆå°‘ï¼Œæ²¡æœ‰å¿…è¦èµ°äºŒåˆ†æŸ¥æ‰¾ while (partition \u003c rangeBounds.length \u0026\u0026 ordering.gt(k, rangeBounds(partition))) { partition += 1 } } else { // Determine which binary search method to use only once. partition = binarySearch(rangeBounds, k) // binarySearch either returns the match location or -[insertion point]-1 if (partition \u003c 0) { partition = -partition-1 } if (partition \u003e rangeBounds.length) { partition = rangeBounds.length } } if (ascending) { partition } else { rangeBounds.length - partition } } partitioneræœ€é‡è¦çš„å‡½æ•°getPartitionï¼Œç”¨äºç¡®å®šæŸä¸ª\u003cK, V\u003e recordåº”è¯¥åˆ†åˆ°å“ªä¸ªpartitionã€‚ // å‰partitions - 1ä¸ªåˆ†åŒºçš„ä¸Šè¾¹ç•Œ private var rangeBounds: Array[K] = { if (partitions \u003c= 1) { Array.empty } else { // ä¸ºäº†ä½¿è¾“å‡ºåˆ†åŒºå¤§è‡´å¹³è¡¡æ‰€éœ€è¦çš„é‡‡æ ·æ•°æ®é‡ï¼Œæœ€å¤§ä¸Šé™ä¸º100ä¸‡ val sampleSize = math.min(samplePointsPerPartitionHint.toDouble * partitions, 1e6) // å‡è®¾è¾“å‡ºçš„åˆ†åŒºå¤§è‡´å¹³è¡¡ï¼Œè¿™é‡Œè¶…é‡‡æ ·ä¸€éƒ¨åˆ† val sampleSizePerPartition = math.ceil(3.0 * sampleSize / rdd.partitions.length).toInt val (numItems, sketched) = RangePartitioner.sketch(rdd.map(_._1), sampleSizePerPartition) if (numItems == 0L) { Array.empty } else { // å¦‚æœæŸä¸ªåˆ†åŒºåŒ…å«çš„å…ƒç´ æ•°é‡è¿œå¤šä½™å¹³å‡å€¼ï¼Œå°†å¯¹è¯¥åˆ†åŒºé‡æ–°é‡‡æ ·ï¼Œä»¥ç¡®ä¿ä»è¯¥åˆ†åŒºä¸­æ”¶é›†åˆ°è¶³å¤Ÿçš„æ ·æœ¬ // fractionè¡¨ç¤ºæ ·æœ¬æ•°é‡å’Œæ•°æ®æ€»é‡çš„æ¯”å€¼ val fraction = math.min(sampleSize / math.max(numItems, 1L), 1.0) val candidates = ArrayBuffer.empty[(K, Float)] val imbalancedPartitions = mutable.Set.empty[Int] sketched.foreach { case (idx, n, sample) =\u003e // æŒ‰ç…§æ¯”ä¾‹å½“å‰åˆ†åŒºåº”è¯¥æŠ½æ ·çš„å¹³å‡æ•°é‡é«˜äºå®é™…é‡‡æ ·æ•°é‡ï¼Œè®¤ä¸ºå½“å‰åˆ†åŒºéœ€è¦é‡é‡‡æ · if (fraction * n \u003e sampleSizePerPartition) { imbalancedPartitions += idx } else { // weightæ˜¯é‡‡æ ·æ¦‚ç‡çš„å€’æ•°ï¼Œä¸¾ä¸ªä¾‹å­ï¼Œå‡è®¾æœ‰ä¸¤ä¸ªåˆ†åŒºï¼Œéƒ½é‡‡æ ·äº†30ä¸ªæ ·æœ¬ // ä½†aåˆ†åŒºå¤§å°ä¸º300ï¼Œbåˆ†åŒºå¤§å°ä¸º60ï¼Œæ˜¾ç„¶aå’Œbåˆ†åŒºé‡‡æ ·çš„æ¯ä¸ªæ ·æœ¬åº”è¯¥å çš„æƒé‡ä¸åŒ // weightçš„ä½œç”¨å°±åœ¨äºæ­¤ val weight = (n.toDouble / sample.length).toFloat for (key \u003c- sample) { candidates += ((key, weight)) } } } if (imbalancedPartitions.nonEmpty) { // ä»…å¯¹éœ€è¦é‡æ–°æŠ½æ ·çš„åˆ†åŒºè¿›è¡Œæ“ä½œ val imbalanced = new PartitionPruningRDD(rdd.map(_._1), imbalancedPartitions.contains) val seed = byteswap32(-rdd.id - 1) // ä½¿ç”¨sampleè¿›è¡ŒæŠ½æ ·, æŠ½æ ·çš„æ¯”ä¾‹ä¸ºfraction // å‡è®¾ç¬¬ä¸€æ¬¡æŠ½æ ·ï¼Œæ€»æ•°ä¸º3000ï¼ŒæŠ½æ ·å¤§å°ä¸º30ï¼Œå¹³å‡æŠ½æ ·æ¯”ä¾‹ä¸º0.1ï¼Œæ‰€ä»¥è¿›è¡Œé‡æŠ½æ ·ï¼Œè¿™æ¬¡æŠ½æ ·å æ¯”ä¸º0.1ï¼Œä¹Ÿå°±æ˜¯300 val reSampled = imbalanced.sample(withReplacement = false, fraction, seed).collect() val weight = (1.0 / fraction).toFloat candidates ++= reSampled.map(x =\u003e (x, weight)) } // å¦‚æœé‡‡æ ·çš„è®°å½•å°‘äºpartitionsï¼Œåˆ™æœ€ç»ˆçš„åˆ†åŒºæ•°é‡ä¹Ÿä¼šå°‘äºpartitions RangePartitioner.determineBounds(candidates, math.min(partitions, candidates.size)) } } } def sketch[K : ClassTag]( rdd: RDD[K], sampleSizePerPartition: Int): (Long, Array[(Int, Long, Array[K])]) = { val shift = rdd.id // val classTagK = classTag[K] // to avoid serializing the entire partitioner object val sketched = rdd.mapPartitionsWithIndex { (idx, iter) =\u003e val seed = byteswap32(idx ^ (shift \u003c\u003c 16)) val (sample, n) = SamplingUtils.reservoirSampleAndCount( iter, sampleSizePerPartition, seed) Iterator((idx, n,","date":"2025-05-10","objectID":"/posts/spark-base/:1:3","tags":["Spark"],"title":"SparkåŸºç¡€çŸ¥è¯†","uri":"/posts/spark-base/"},{"categories":["Spark"],"content":"Dependency @DeveloperApi abstract class Dependency[T] extends Serializable { def rdd: RDD[T] } RDDä¾èµ–çš„åŸºç¡€ç±»ã€‚ NarrowDependency @DeveloperApi abstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] { /** * Get the parent partitions for a child partition. * @param partitionId a partition of the child RDD * @return the partitions of the parent RDD that the child partition depends upon */ def getParents(partitionId: Int): Seq[Int] override def rdd: RDD[T] = _rdd } çª„ä¾èµ–NarrowDependencyï¼Œå­RDDçš„æ¯ä¸ªåˆ†åŒºä¾èµ–äºçˆ¶RDDçš„ä¸€å°éƒ¨åˆ†åˆ†åŒºï¼Œçª„ä¾èµ–å…è®¸æµæ°´çº¿æ‰§è¡Œï¼ŒgetParenetsè¿”å›å­RDDåˆ†åŒºä¾èµ–çš„æ‰€æœ‰çˆ¶RDDåˆ†åŒºã€‚ PruneDependency private[spark] class PruneDependency[T](rdd: RDD[T], partitionFilterFunc: Int =\u003e Boolean) extends NarrowDependency[T](rdd) { @transient val partitions: Array[Partition] = rdd.partitions .filter(s =\u003e partitionFilterFunc(s.index)).zipWithIndex // idxæ˜¯å­RDDçš„partition Idï¼Œä»0å¼€å§‹ // splitæ˜¯å¯¹åº”çš„çˆ¶RDDä¸­çš„åˆ†åŒº .map { case(split, idx) =\u003e new PartitionPruningRDDPartition(idx, split) : Partition } override def getParents(partitionId: Int): List[Int] = { List(partitions(partitionId).asInstanceOf[PartitionPruningRDDPartition].parentSplit.index) } } PruneDependencyæ˜¯çª„ä¾èµ–çš„ä¸€ç§ï¼Œå­RDDä¸­çš„åˆ†åŒºæ˜¯çˆ¶RDDä¸­åˆ†åŒºå‰ªæåçš„å­é›†ï¼Œå­RDDä¸­çš„æ¯ä¸ªåˆ†åŒºå”¯ä¸€ä¾èµ–äºçˆ¶RDDçš„å¯¹åº”åˆ†åŒºã€‚ OneToOneDependency @DeveloperApi class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) { override def getParents(partitionId: Int): List[Int] = List(partitionId) } OneToOneDependencyè¡¨ç¤ºçˆ¶rddå’Œå­rddçš„åˆ†åŒºä¹‹é—´æ˜¯ä¸€ä¸€æ˜ å°„å…³ç³»ã€‚ RangeDependency /** * :: DeveloperApi :: * Represents a one-to-one dependency between ranges of partitions in the parent and child RDDs. * @param rdd the parent RDD * @param inStart the start of the range in the parent RDD * @param outStart the start of the range in the child RDD * @param length the length of the range */ @DeveloperApi class RangeDependency[T](rdd: RDD[T], inStart: Int, outStart: Int, length: Int) extends NarrowDependency[T](rdd) { override def getParents(partitionId: Int): List[Int] = { if (partitionId \u003e= outStart \u0026\u0026 partitionId \u003c outStart + length) { List(partitionId - outStart + inStart) } else { Nil } } } RangeDependencyè¡¨ç¤ºçˆ¶ RDD å’Œå­ RDD ä¸­åˆ†åŒºèŒƒå›´ä¹‹é—´çš„ä¸€å¯¹ä¸€ä¾èµ–å…³ç³»ã€‚ä¾ç„¶æ˜¯ä¸€ä¸€å¯¹åº”å…³ç³»ï¼Œä½†åˆ†åŒºå·å¯èƒ½ä¸ç›¸åŒã€‚ ShuffleDependency å…ˆé€šè¿‡ä¸€ä¸ªä¾‹å­æ¥è¯´æ˜ShuffleDependencyçš„ç”¨é€” def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))] = self.withScope { this.cogroup(other, partitioner).flatMapValues( pair =\u003e for (v \u003c- pair._1.iterator; w \u003c- pair._2.iterator) yield (v, w) ) } def cogroup[W](other: RDD[(K, W)], partitioner: Partitioner) : RDD[(K, (Iterable[V], Iterable[W]))] = self.withScope { if (partitioner.isInstanceOf[HashPartitioner] \u0026\u0026 keyClass.isArray) { throw SparkCoreErrors.hashPartitionerCannotPartitionArrayKeyError() } val cg = new CoGroupedRDD[K](Seq(self, other), partitioner) cg.mapValues { case Array(vs, w1s) =\u003e (vs.asInstanceOf[Iterable[V]], w1s.asInstanceOf[Iterable[W]]) } } // CoGroupedRDD override def getDependencies: Seq[Dependency[_]] = { rdds.map { rdd: RDD[_] =\u003e if (rdd.partitioner == Some(part)) { logDebug(\"Adding one-to-one dependency with \" + rdd) new OneToOneDependency(rdd) } else { logDebug(\"Adding shuffle dependency with \" + rdd) new ShuffleDependency[K, Any, CoGroupCombiner]( rdd.asInstanceOf[RDD[_ \u003c: Product2[K, _]]], part, serializer) } } } å‡è®¾æœ‰ä¸€ä¸ªjoinæ“ä½œï¼ŒæŒ‡å®šäº†ç»“æœRDDçš„Partitionerï¼Œå†…éƒ¨è°ƒç”¨äº†cogroupç”Ÿæˆäº†CoGroupedRDDï¼Œå¹¶ä¸”å°†ä¾èµ–çš„RDDéƒ½ä½œä¸ºå‚æ•°ä¼ å…¥ï¼Œå¦‚æœä¾èµ–çš„RDDå’ŒæŒ‡å®šçš„Partitionerç›¸åŒï¼Œåˆ™æ˜¯çª„ä¾èµ–ï¼Œå¦åˆ™æ˜¯å®½ä¾èµ–ï¼Œç”ŸæˆShufflDependencyã€‚ @DeveloperApi class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag]( @transient private val _rdd: RDD[_ \u003c: Product2[K, V]], val partitioner: Partitioner, val serializer: Serializer = SparkEnv.get.serializer, val keyOrdering: Option[Ordering[K]] = None, val aggregator: Option[Aggregator[K, V, C]] = None, val mapSideCombine: Boolean = false, val shuffleWriterProcessor: ShuffleWriteProcessor = new ShuffleWriteProcessor) extends Dependency[Product2[K, V]] with Logging { if (mapSideCombine) { require(aggregator.isDefined, \"Map-side combine without Aggregator specified!\") } override def rdd: RDD[Product2[K, V]] = _rdd.asInstanceOf[RDD[Product2[K, V]]] private[spar","date":"2025-05-10","objectID":"/posts/spark-base/:1:4","tags":["Spark"],"title":"SparkåŸºç¡€çŸ¥è¯†","uri":"/posts/spark-base/"},{"categories":["grpc"],"content":"gprcæµç¨‹æ¦‚æ‹¬ grpcçš„æµç¨‹å¯ä»¥å¤§è‡´åˆ†æˆä¸¤ä¸ªé˜¶æ®µï¼Œåˆ†åˆ«ä¸ºgrpcè¿æ¥é˜¶æ®µå’Œgrpcäº¤äº’é˜¶æ®µï¼Œå¦‚å›¾æ‰€ç¤ºï¼ˆæ­¤å›¾æ¥è‡ªåé¢çš„å‚è€ƒæ–‡çŒ®ï¼‰ã€‚ åœ¨RPCè¿æ¥é˜¶æ®µï¼Œclientå’Œserverä¹‹é—´å»ºç«‹èµ·TCPè¿æ¥ï¼Œgrpcåº•å±‚ä¾èµ–äºHTTP2ï¼Œå› æ­¤clientå’Œserverè¿˜éœ€è¦åè°ƒframeçš„ç›¸å…³è®¾ç½®ï¼Œä¾‹å¦‚frameçš„å¤§å°ï¼Œæ»‘åŠ¨çª—å£çš„å¤§å°ç­‰ã€‚ åœ¨RPCäº¤äº’é˜¶æ®µï¼Œclientå°†æ•°æ®å‘é€ç»™serverï¼Œå¹¶ç­‰å¾…serveræ‰§è¡Œæ‰§è¡Œmethodä¹‹åè¿”å›ç»“æœã€‚ ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:1:0","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"Clientçš„æµç¨‹ åœ¨RPCè¿æ¥é˜¶æ®µï¼Œclientæ¥æ”¶åˆ°ä¸€ä¸ªç›®æ ‡åœ°å€å’Œä¸€ç³»åˆ—çš„DialOptionsï¼Œç„¶å é…ç½®è¿æ¥å‚æ•°ï¼Œinterceptorç­‰ï¼Œå¯åŠ¨resolver Rosovleræ ¹æ®ç›®æ ‡åœ°å€è·å–serverçš„åœ°å€åˆ—è¡¨ï¼ˆæ¯”å¦‚ä¸€ä¸ªDNS nameå¯èƒ½ä¼šæŒ‡å‘å¤šä¸ªserver ipï¼ŒdnsResolveræ˜¯grpcå†…ç½®çš„resolverä¹‹ä¸€ï¼‰ï¼Œå¯åŠ¨balancer Balanceræ ¹æ®å¹³è¡¡ç­–ç•¥ï¼Œä»è¯¸å¤šserveråœ°å€ä¸­é€‰æ‹©ä¸€ä¸ªæˆ–è€…å¤šä¸ªå»ºç«‹TCPè¿æ¥ clientåœ¨TCPè¿æ¥å»ºç«‹å®Œæˆä¹‹åï¼Œç­‰å¾…serverå‘æ¥çš„HTTP2 Setting frameï¼Œå¹¶è°ƒæ•´è‡ªèº«çš„HTTP2ç›¸å…³é…ç½®ï¼Œéšåå‘serverå‘é€HTTP2 Setting frame åœ¨RPCäº¤äº’é˜¶æ®µï¼ŒæŸä¸ªrpcæ–¹æ³•è¢«è°ƒç”¨å Clientåˆ›å»ºä¸€ä¸ªstreamå¯¹è±¡ç”¨æ¥ç®¡ç†æ•´ä¸ªäº¤äº’æµç¨‹ Clientå°†service name, method nameç­‰ä¿¡æ¯æ”¾åˆ°header frameä¸­å¹¶å‘é€ç»™server clientå°†methodçš„å‚æ•°ä¿¡æ¯æ”¾åˆ°data frameä¸­å¹¶å‘é€ç»™server clientç­‰å¾…serverä¼ å›çš„header frameå’Œdata frameï¼Œä¸€æ¬¡rpc callçš„result statusä¼šè¢«åŒ…å«åœ¨header frameä¸­ï¼Œè€Œmethodçš„è¿”å›å€¼è¢«åŒ…å«åœ¨data frameä¸­ ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:1:1","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"Serveræµç¨‹ åœ¨rpcè¿æ¥é˜¶æ®µï¼Œserveråœ¨å®Œæˆä¸€äº›åˆå§‹åŒ–çš„é…ç½®ä¹‹åï¼Œå¼€å§‹ç›‘å¬æŸä¸ªtcpç«¯å£ï¼Œåœ¨å’ŒæŸä¸ªclientå»ºç«‹äº†tcpè¿æ¥ä¹‹åå®Œæˆhttp2 settting frameçš„äº¤äº’ã€‚ åœ¨rpcäº¤äº’é˜¶æ®µï¼š serverç­‰å¾…clientå‘æ¥çš„header frameï¼Œä»è€Œåˆ›å»ºå‡ºä¸€ä¸ªstreamå¯¹è±¡æ¥ç®¡ç†çœŸä¸ªäº¤äº’æµç¨‹ï¼Œæ ¹æ®header frameä¸­çš„ä¿¡æ¯ï¼ŒserverçŸ¥é“clientè¯·æ±‚çš„æ˜¯å“ªä¸€ä¸ªserviceçš„é‚£ä¸€ä¸ªmethod serveræ¥å—åˆ°clientå‘æ¥çš„data frameï¼Œå¹¶æ‰§è¡Œmethod serverå°†æ‰§è¡Œæ˜¯å¦æˆåŠŸç­‰ä¿¡æ¯æ”¾åœ¨header frameä¸­å‘é€ç»™client serverå°†methodæ‰§è¡Œçš„ç»“æœï¼ˆè¿”å›å€¼ï¼‰æ”¾åœ¨data frameä¸­å‘é€ç»™client ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:1:2","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc Serverçš„rpcè¿æ¥é˜¶æ®µ func main() { flag.Parse() lis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port)) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } s := grpc.NewServer() pb.RegisterGreeterServer(s, \u0026server{}) log.Printf(\"server listening at %v\", lis.Addr()) if err := s.Serve(lis); err != nil { log.Fatalf(\"failed to serve: %v\", err) } } å¦‚ä¸Šæ˜¯ä¸€ä¸ªç®€å•çš„æœåŠ¡ç«¯ç¨‹åºï¼Œæµç¨‹å¦‚ä¸‹ é¦–å…ˆé€šè¿‡net.Listenerç›‘å¬tcpç«¯å£ï¼Œæ¯•ç«ŸgrpcæœåŠ¡æ˜¯åŸºäºtcpçš„ åˆ›å»ºgrpc serverï¼Œå¹¶æ³¨å†ŒæœåŠ¡ï¼Œ\u0026server{}å®é™…ä¸Šå°±æ˜¯æœåŠ¡çš„å®ç° é˜»å¡ç­‰å¾…æ¥è‡ªclientçš„è®¿é—® func (s *Server) Serve(lis net.Listener) error { s.serve = true for { rawConn, err := lis.Accept() s.serveWG.Add(1) go func() { s.handleRawConn(lis.Addr().String(), rawConn) s.serveWG.Done() }() } grpcåœ¨ä¸€ä¸ªforå¾ªç¯ä¸­ç­‰å¾…æ¥è‡ªclientçš„è®¿é—®ï¼Œæ¯æ¬¡æœ‰æ–°çš„clientç«¯è®¿é—®ï¼Œåˆ›å»ºä¸€ä¸ªnet.Connï¼Œå¹¶åˆ›å»ºä¸€ä¸ªæ–°çš„goroutineå¤„ç†è¿™ä¸ªnet.Connï¼Œæ‰€ä»¥è¿™ä¸ªè¿æ¥ä¸Šçš„è¯·æ±‚ï¼Œæ— è®ºå®¢æˆ·ç«¯è°ƒç”¨å“ªä¸€ä¸ªè¿œç¨‹è®¿é—®æˆ–è€…è°ƒç”¨å‡ æ¬¡ï¼Œéƒ½ä¼šç”±è¿™ä¸ªgoroutineå¤„ç†ã€‚ func (s *Server) handleRawConn(lisAddr string, rawConn net.Conn) { // å¦‚æœgrpc serverå·²ç»å…³é—­ï¼Œé‚£ä¹ˆåŒæ ·å…³é—­è¿™ä¸ªtcpè¿æ¥ if s.quit.HasFired() { rawConn.Close() return } // è®¾ç½®tcpè¶…æ—¶æ—¶é—´ rawConn.SetDeadline(time.Now().Add(s.opts.connectionTimeout)) // Finish handshaking (HTTP2) st := s.newHTTP2Transport(rawConn) // æ¸…ç†tcpè¶…æ—¶æ—¶é—´ rawConn.SetDeadline(time.Time{}) // rpcäº¤äº’é˜¶æ®µï¼Œåˆ›å»ºæ–°çš„goroutineå¤„ç†æ¥è‡ªclientçš„æ•°æ® go func() { s.serveStreams(context.Background(), st, rawConn) s.removeConn(lisAddr, st) }() } åœ¨è¿™é‡Œï¼Œé¦–å…ˆåˆ¤æ–­gprc serveræ˜¯å¦å…³é—­ï¼Œå¦‚æœå…³é—­ï¼Œåˆ™åŒæ ·å…³é—­tcpè¿æ¥ã€‚ç„¶åè¿›è¡ŒHTTP2çš„æ¡æ‰‹ï¼Œè¿™é‡Œä¸“é—¨è®¾ç½®äº†tcpè¶…æ—¶æ—¶é—´ï¼Œé¿å…æ¡æ‰‹è¿Ÿè¿Ÿä¸ç»“æŸï¼Œå¯¼è‡´èµ„æºå ç”¨ï¼Œåœ¨æ¡æ‰‹ç»“æŸåï¼Œæ¸…ç†tcpè¶…æ—¶æ—¶é—´ï¼Œé¿å…å¯¹åé¢è¯·æ±‚çš„å½±å“ã€‚æœ€åæ–°å¯åŠ¨ä¸€ä¸ªgoroutineï¼Œç”¨æ¥å¤„ç†å®é™…çš„è¯·æ±‚ã€‚ ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:2:0","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpcæœåŠ¡ç«¯HTTP2æ¡æ‰‹ func (s *Server) newHTTP2Transport(c net.Conn) transport.ServerTransport { // ç»„è£… serverconfig config := \u0026transport.ServerConfig{ MaxStreams: s.opts.maxConcurrentStreams, ConnectionTimeout: s.opts.connectionTimeout, ... } // æ ¹æ®configçš„é…ç½®é¡¹ï¼Œå’Œclientè¿›è¡Œhttp2çš„æ¡æ‰‹ st, err := transport.NewServerTransport(c, config) æ ¹æ®ä½¿ç”¨è€…åœ¨å¯åŠ¨grpc serveræ—¶çš„é…ç½®é¡¹ï¼Œæˆ–è€…é»˜è®¤çš„é…ç½®é¡¹ï¼Œè°ƒç”¨transport.NewServerTransportå®Œæˆå’Œclientçš„http2æ¡æ‰‹ã€‚ writeBufSize := config.WriteBufferSize readBufSize := config.ReadBufferSize maxHeaderListSize := defaultServerMaxHeaderListSize if config.MaxHeaderListSize != nil { maxHeaderListSize = *config.MaxHeaderListSize } framer := newFramer(conn, writeBufSize, readBufSize, config.SharedWriteBuffer, maxHeaderListSize) é¦–å…ˆåˆ›å»ºframerï¼Œç”¨æ¥è´Ÿè´£æ¥å—å’Œå‘é€HTTP2 frameï¼Œæ˜¯serverå’Œclientäº¤æµçš„å®é™…æ¥å£ã€‚ // Send initial settings as connection preface to client. isettings := []http2.Setting{{ ID: http2.SettingMaxFrameSize, Val: http2MaxFrameLen, }} if config.MaxStreams != math.MaxUint32 { isettings = append(isettings, http2.Setting{ ID: http2.SettingMaxConcurrentStreams, Val: config.MaxStreams, }) } ... if err := framer.fr.WriteSettings(isettings...); err != nil { return nil, connectionErrorf(false, err, \"transport: %v\", err) } grpc serverç«¯é¦–å…ˆæ˜ç¡®è‡ªå·±çš„HTTP2çš„åˆå§‹é…ç½®ï¼Œæ¯”å¦‚MaxFrameSizeç­‰ï¼Œå¹¶å°†è¿™äº›é…ç½®ä¿¡æ¯é€šè¿‡frame.frå‘é€ç»™clientï¼Œframe.frå®é™…ä¸Šå°±æ˜¯golangåŸç”Ÿçš„http2.Framerï¼Œåœ¨åº•å±‚ï¼Œè¿™äº›é…ç½®ä¿¡æ¯ä¼šè¢«åŒ…è£…æˆä¸€ä¸ªSetting Frameå‘é€ç»™clientã€‚ clientåœ¨æ”¶åˆ°Setting Frameåï¼Œæ ¹æ®è‡ªèº«æƒ…å†µè°ƒæ•´å‚æ•°ï¼ŒåŒæ ·å‘é€ä¸€ä¸ªSetting Frameç»™severã€‚ t := \u0026http2Server{ done: done, conn: conn, peer: peer, framer: framer, readerDone: make(chan struct{}), loopyWriterDone: make(chan struct{}), maxStreams: config.MaxStreams, inTapHandle: config.InTapHandle, fc: \u0026trInFlow{limit: uint32(icwz)}, state: reachable, activeStreams: make(map[uint32]*ServerStream), stats: config.StatsHandlers, kp: kp, idle: time.Now(), kep: kep, initialWindowSize: iwz, bufferPool: config.BufferPool, } // controlbufç”¨æ¥ç¼“å­˜Setting Frameç­‰å’Œè®¾ç½®ç›¸å…³çš„Frameçš„ç¼“å­˜ t.controlBuf = newControlBuffer(t.done) // è‡ªå¢è¿æ¥id t.connectionID = atomic.AddUint64(\u0026serverConnectionCounter, 1) // flush framerï¼Œç¡®ä¿å‘clientå‘é€äº†setting frame t.framer.writer.Flush() grpc serveråœ¨å‘é€äº†setting frameä¹‹åï¼Œåˆ›å»ºå¥½http2Serverå¯¹è±¡ï¼Œå¹¶ç­‰å¾…clientçš„åç»­æ¶ˆæ¯ã€‚ // Check the validity of client preface. preface := make([]byte, len(clientPreface)) // è¯»å–å®¢æˆ·ç«¯å‘æ¥çš„client prefaceï¼Œå¹¶éªŒè¯æ˜¯å¦å’Œé¢„æœŸä¸€è‡´ if _, err := io.ReadFull(t.conn, preface); err != nil { // In deployments where a gRPC server runs behind a cloud load balancer // which performs regular TCP level health checks, the connection is // closed immediately by the latter. Returning io.EOF here allows the // grpc server implementation to recognize this scenario and suppress // logging to reduce spam. if err == io.EOF { return nil, io.EOF } return nil, connectionErrorf(false, err, \"transport: http2Server.HandleStreams failed to receive the preface from client: %v\", err) } if !bytes.Equal(preface, clientPreface) { return nil, connectionErrorf(false, nil, \"transport: http2Server.HandleStreams received bogus greeting from client: %q\", preface) } // è¯»å–clientç«¯å‘æ¥çš„frame frame, err := t.framer.fr.ReadFrame() if err == io.EOF || err == io.ErrUnexpectedEOF { return nil, err } if err != nil { return nil, connectionErrorf(false, err, \"transport: http2Server.HandleStreams failed to read initial settings frame: %v\", err) } atomic.StoreInt64(\u0026t.lastRead, time.Now().UnixNano()) // è½¬æˆSettingFrame sf, ok := frame.(*http2.SettingsFrame) if !ok { return nil, connectionErrorf(false, nil, \"transport: http2Server.HandleStreams saw invalid preface type %T from client\", frame) } // å¤„ç†SettingFrame t.handleSettings(sf) func (t *http2Server) handleSettings(f *http2.SettingsFrame) { // å¦‚æœæ˜¯ack frameï¼Œåˆ™ç›´æ¥è¿”å› if f.IsAck() { return } var ss []http2.Setting var updateFuncs []func() f.ForeachSetting(func(s http2.Setting) error { switch s.ID { // æ›´æ–°http2Serverä¸­çš„é…ç½®ä¿¡æ¯ case http2.SettingMaxHeaderListSize: updateFuncs = append(updateFuncs, func() { t.maxSendHeaderListSize = new(uint32) *t.maxSendHeaderListSize = s.Val }) default: ss = append(ss, s) } return nil }) // è¿™é‡Œåˆé‡åˆ°äº†controlBuf // æ‰§è¡ŒupdateFunc","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:2:1","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc serverçš„rpcäº¤äº’é˜¶æ®µ HTTP2ä¸­å®šä¹‰äº†å¾ˆå¤šç±»å‹çš„frameï¼ŒåŒ…æ‹¬data, headersç­‰ï¼Œå…·ä½“å¦‚ä¸‹ï¼Œå¯¹äºä¸åŒçš„frameç±»å‹ï¼ŒHTTP2 serveråº”è¯¥æœ‰ä¸åŒçš„å¤„ç†é€»è¾‘ã€‚åœ¨grpcä¸­ï¼Œå¯¹frameç±»å‹çš„åˆ†ç±»å’Œå¤„ç†ï¼Œè¢«åŒ…å«åœ¨func (s *Server) serveStreamsä¸­ã€‚ // FrameType represents the type of an HTTP/2 Frame. // See [Frame Type]. // // [Frame Type]: https://httpwg.org/specs/rfc7540.html#FrameType type FrameType uint8 // Frame types defined in the HTTP/2 Spec. const ( FrameTypeData FrameType = 0x0 FrameTypeHeaders FrameType = 0x1 FrameTypeRSTStream FrameType = 0x3 FrameTypeSettings FrameType = 0x4 FrameTypePing FrameType = 0x6 FrameTypeGoAway FrameType = 0x7 FrameTypeWindowUpdate FrameType = 0x8 FrameTypeContinuation FrameType = 0x9 ) func (s *Server) serveStreams(ctx context.Context, st transport.ServerTransport, rawConn net.Conn) { streamQuota := newHandlerQuota(s.opts.maxConcurrentStreams) // é˜»å¡å¹¶æ¥å—æ¥è‡ªclientçš„frame st.HandleStreams(ctx, func(stream *transport.ServerStream) { s.handlersWG.Add(1) streamQuota.acquire() f := func() { defer streamQuota.release() defer s.handlersWG.Done() // å½“ä¸€ä¸ªæ–°çš„streamè¢«åˆ›å»ºä¹‹åï¼Œè¿›è¡Œä¸€äº›é…ç½® s.handleStream(st, stream) } if s.opts.numServerWorkers \u003e 0 { select { case s.serverWorkerChannel \u003c- f: return default: // If all stream workers are busy, fallback to the default code path. } } go f() }) } st.HandleStreamsä¼šé˜»å¡å½“å‰goroutineï¼Œå¹¶ç­‰å¾…æ¥è‡ªclientçš„frameï¼Œåœ¨ä¸€ä¸ªforå¾ªç¯ä¸­ç­‰å¾…å¹¶è¯»å–æ¥è‡ªclientçš„frameï¼Œå¹¶é‡‡å–ä¸åŒçš„å¤„ç†æ–¹å¼ã€‚ grpcæœåŠ¡ç«¯ä½¿ç”¨ä¸€ä¸ªgoroutineå‘å¤–å‘é€æ•°æ®loopyWriterï¼Œä½¿ç”¨å¦ä¸€ä¸ªgoroutineè¯»å–æ•°æ®serverStreamsã€‚ func (t *http2Server) HandleStreams(ctx context.Context, handle func(*ServerStream)) { defer func() { close(t.readerDone) \u003c-t.loopyWriterDone }() // forå¾ªç¯ï¼ŒæŒç»­å¤„ç†ä¸€ä¸ªè¿æ¥çš„ä¸Šè¯·æ±‚ for { // é™æµ t.controlBuf.throttle() // è¯»å–frame frame, err := t.framer.fr.ReadFrame() atomic.StoreInt64(\u0026t.lastRead, time.Now().UnixNano()) // æ ¹æ®frameçš„ç±»å‹åˆ†åˆ«å¤„ç† switch frame := frame.(type) { // MetaHeaderFrameå¹¶ä¸æ˜¯http2çš„frameç±»å‹ï¼Œè€Œæ˜¯ç»è¿‡åŒ…è£…çš„ç±»å‹ // headers frame + zero or more continuation frame + hspackç¼–ç å†…å®¹çš„è§£ç  case *http2.MetaHeadersFrame: if err := t.operateHeaders(ctx, frame, handle); err != nil { // Any error processing client headers, e.g. invalid stream ID, // is considered a protocol violation. t.controlBuf.put(\u0026goAway{ code: http2.ErrCodeProtocol, debugData: []byte(err.Error()), closeConn: err, }) continue } case *http2.DataFrame: t.handleData(frame) case *http2.RSTStreamFrame: t.handleRSTStream(frame) case *http2.SettingsFrame: t.handleSettings(frame) case *http2.PingFrame: t.handlePing(frame) case *http2.WindowUpdateFrame: t.handleWindowUpdate(frame) case *http2.GoAwayFrame: // TODO: Handle GoAway from the client appropriately. default: if t.logger.V(logLevel) { t.logger.Infof(\"Received unsupported frame type %T\", frame) } } } } ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:3:0","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"Headers Frameçš„å¤„ç† func (t *http2Server) operateHeaders(ctx context.Context, frame *http2.MetaHeadersFrame, handle func(*ServerStream)) error { // Acquire max stream ID lock for entire duration t.maxStreamMu.Lock() defer t.maxStreamMu.Unlock() // ä»å®¢æˆ·ç«¯frameä¸­è·å–streamID streamID := frame.Header().StreamID // æ ¡éªŒstream id if streamID%2 != 1 || streamID \u003c= t.maxStreamID { // illegal gRPC stream id. return fmt.Errorf(\"received an illegal stream id: %v. headers frame: %+v\", streamID, frame) } // å°†è·å¾—çš„streamIDè®¾ç½®åˆ°http2Server t.maxStreamID = streamID // æ— ç•Œmessageç¼“å†² buf := newRecvBuffer() // åˆ›å»ºstream s := \u0026ServerStream{ Stream: \u0026Stream{ id: streamID, buf: buf, fc: \u0026inFlow{limit: uint32(t.initialWindowSize)}, }, st: t, headerWireLength: int(frame.Header().Length), } åœ¨grpc serverå’Œclientç«¯ï¼Œå­˜åœ¨è¿™ä¸€ä¸ªstreamçš„æ¦‚å¿µï¼Œç”¨æ¥è¡¨å¾ä¸€æ¬¡grpc callã€‚ä¸€ä¸ªgrpc callæ€»æ˜¯ä»¥ä¸€ä¸ªæ¥è‡ªclientçš„headers frameå¼€å§‹ï¼Œå› æ­¤serverä¼šåœ¨operateHeadersä¸­åˆ›å»ºä¸€ä¸ªStreamå¯¹è±¡ï¼Œstreamæœ‰ä¸€ä¸ªclientå’Œserverç«¯ä¸€è‡´çš„idï¼Œä¹Ÿæœ‰ä¸€ä¸ªbufç¼“å­˜ã€‚ for _, hf := range frame.Fields { switch hf.Name { case \"grpc-encoding\": s.recvCompress = hf.Value case \":method\": // POST, GETè¿™äº› httpMethod = hf.Value case \":path\": // ä½¿ç”¨grpcé‚£ä¸ªæœåŠ¡çš„é‚£ä¸ªæ–¹æ³• s.method = hf.Value case \"grpc-timeout\": timeoutSet = true var err error if timeout, err = decodeTimeout(hf.Value); err != nil { headerError = status.Newf(codes.Internal, \"malformed grpc-timeout: %v\", err) } } } grpc serverä¼šéå†frameä¸­çš„fieldï¼Œå¹¶å°†filedä¸­çš„ä¿¡æ¯è®°å½•åœ¨streamä¸­ã€‚:methodå’Œ:pathè¿™ä¸¤ä¸ªfieldéœ€è¦ç‰¹åˆ«æ³¨æ„ï¼Œclientç«¯éœ€è¦å¡«å†™å¥½è¿™ä¸¤ä¸ªfieldæ¥æ˜ç¡®åœ°æŒ‡å®šè¦è°ƒç”¨serverç«¯æä¾›çš„é‚£ä¸€ä¸ªæ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè°ƒç”¨å“ªä¸€ä¸ªserveræ–¹æ³•çš„ä¿¡æ¯æ˜¯å’Œè°ƒç”¨æ–¹æ³•çš„å‚æ•°åˆ†å¼€åœ¨ä¸åŒçš„frameä¸­çš„ã€‚ if frame.StreamEnded() { // s is just created by the caller. No lock needed. s.state = streamReadDone } // è¶…æ—¶è®¾ç½® if timeoutSet { s.ctx, s.cancel = context.WithTimeout(ctx, timeout) } else { s.ctx, s.cancel = context.WithCancel(ctx) } if uint32(len(t.activeStreams)) \u003e= t.maxStreams { t.mu.Unlock() t.controlBuf.put(\u0026cleanupStream{ streamID: streamID, rst: true, rstCode: http2.ErrCodeRefusedStream, onWrite: func() {}, }) s.cancel() return nil } // å°†streamåŠ å…¥activeStreams map t.activeStreams[streamID] = s if len(t.activeStreams) == 1 { t.idle = time.Time{} } // Start a timer to close the stream on reaching the deadline. if timeoutSet { // We need to wait for s.cancel to be updated before calling // t.closeStream to avoid data races. cancelUpdated := make(chan struct{}) timer := internal.TimeAfterFunc(timeout, func() { \u003c-cancelUpdated t.closeStream(s, true, http2.ErrCodeCancel, false) }) oldCancel := s.cancel s.cancel = func() { oldCancel() timer.Stop() } close(cancelUpdated) } s.trReader = \u0026transportReader{ reader: \u0026recvBufferReader{ ctx: s.ctx, ctxDone: s.ctxDone, recv: s.buf, }, windowHandler: func(n int) { t.updateWindow(s, uint32(n)) }, } // Register the stream with loopy. t.controlBuf.put(\u0026registerStream{ streamID: s.id, wq: s.wq, }) handle(s) è¿™ä¸ªæ–°å»ºçš„streamå¯¹è±¡ä¼šè¢«æ”¾åˆ°serverçš„activeStreams mapä¸­ï¼Œå¹¶è°ƒç”¨å›è°ƒå‡½æ•°handle(s)æ¥è¿›ä¸€æ­¥å¤„ç†è¿™ä¸ªstreamï¼Œå…¶ä¸­æœ€é‡è¦çš„æ˜¯è°ƒç”¨s.handleStreamã€‚ st.HandleStreams(ctx, func(stream *transport.ServerStream) { s.handlersWG.Add(1) streamQuota.acquire() f := func() { defer streamQuota.release() defer s.handlersWG.Done() s.handleStream(st, stream) } // å¦‚æœè®¾ç½®äº†workeræ± ï¼Œåˆ™å…ˆå°è¯•æäº¤ä»»åŠ¡åˆ°workeræ± ä¸­ï¼Œå¦‚æœä¸è¡Œï¼Œæ–°èµ·goroutineæ‰§è¡Œ if s.opts.numServerWorkers \u003e 0 { select { case s.serverWorkerChannel \u003c- f: return default: // If all stream workers are busy, fallback to the default code path. } } go f() }) // initServerWorkers creates worker goroutines and a channel to process incoming // connections to reduce the time spent overall on runtime.morestack. func (s *Server) initServerWorkers() { s.serverWorkerChannel = make(chan func()) s.serverWorkerChannelClose = sync.OnceFunc(func() { close(s.serverWorkerChannel) }) for i := uint32(0); i \u003c s.opts.numServerWorkers; i++ { go s.serverWorker() } } å›è°ƒå‡½æ•°ä¸­ä¼šå°†å¤„ç†streamçš„ä»»åŠ¡æäº¤åˆ°å…¶ä»–goroutineä¸­ï¼Œå¦‚æœå¯ç”¨çš„workerï¼Œåˆ™ç”±workeræ‰§è¡Œï¼Œå¦åˆ™å¦èµ·goroutineæ¥æ‰§è¡Œã€‚ func (s *Server) handleStream(t transport.ServerTransport, stream *transport.ServerStream) { // è·å–grpcè·¯å¾„ sm := stream.Method() pos := strings.LastIndex(sm, \"/\") // è°ƒç”¨çš„grpc service name service := sm[:pos] // è°ƒç”¨çš„grpc method name method := sm[pos","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:3:1","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"Data Frameçš„å¤„ç† func (t *http2Server) handleData(f *http2.DataFrame) { size := f.Header().Length // Select the right stream to dispatch. s, ok := t.getStream(f) if !ok { return } if s.getState() == streamReadDone { t.closeStream(s, true, http2.ErrCodeStreamClosed, false) return } if size \u003e 0 { if len(f.Data()) \u003e 0 { pool := t.bufferPool s.write(recvMsg{buffer: mem.Copy(f.Data(), pool)}) } } if f.StreamEnded() { // Received the end of stream from the client. s.compareAndSwapState(streamActive, streamReadDone) s.write(recvMsg{err: io.EOF}) } } åœ¨å¤„ç†data frameæ—¶ æ ¹æ®stream IDï¼Œä»serverçš„activeStreams mapä¸­æ‰¾åˆ°streamå¯¹è±¡ ä»bufferPoolä¸­æ‹¿åˆ°ä¸€å—bufferï¼Œå¹¶å°†frameçš„æ•°æ®å†™å…¥åˆ°buffer å°†è¿™å—bufferä¿å­˜åˆ°streamçš„recvBufferä¸­ å¦‚æœè¯»å–ç»“æŸï¼Œä¿®æ”¹æµçŠ¶æ€ä¸ºstreamReadDoneï¼Œå¹¶ä¸”å†™å…¥io.EOFæ ‡è®° recvBufferä¸­ç¼“å­˜çš„æ•°æ®ï¼Œæœ€ç»ˆä¼šè¢«å‰é¢æåˆ°çš„recvAndDecompresså‡½æ•°è¯»å–ï¼Œä»è€Œåœ¨serverç«¯é‡å»ºrpcçš„å‚æ•°ã€‚ ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:3:2","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"Setting Frameçš„å¤„ç† func (t *http2Server) handleSettings(f *http2.SettingsFrame) { if f.IsAck() { return } var ss []http2.Setting var updateFuncs []func() f.ForeachSetting(func(s http2.Setting) error { switch s.ID { case http2.SettingMaxHeaderListSize: updateFuncs = append(updateFuncs, func() { t.maxSendHeaderListSize = new(uint32) *t.maxSendHeaderListSize = s.Val }) default: ss = append(ss, s) } return nil }) t.controlBuf.executeAndPut(func() bool { for _, f := range updateFuncs { f() } return true }, \u0026incomingSettings{ ss: ss, }) } handleSettingså¹¶æ²¡æœ‰ç›´æ¥å°†settting frameçš„å‚æ•°åº”ç”¨åœ¨serverä¸Šï¼Œè€Œæ˜¯å°†å…¶æ”¾åˆ°äº†controlBufä¸­ã€‚ ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:3:3","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"serverå¦‚ä½•å‘é€frame grpc serveråœ¨æ¯æ¬¡æ”¶åˆ°ä¸€ä¸ªæ–°çš„æ¥è‡ªclientçš„è¿æ¥åï¼Œä¼šåˆ›å»ºä¸€ä¸ªFramerï¼Œè¿™ä¸ªFramerå°±æ˜¯å®é™…ä¸Šè´Ÿè´£å‘é€å’Œæ¥æ”¶HTTP2 frameçš„æ¥å£ï¼Œæ¯ä¸€ä¸ªclientéƒ½å¯¹åº”ä¸€ä¸ªFrameræ¥å¤„ç†æ¥è‡ªè¯¥clientçš„æ‰€æœ‰frameï¼Œä¸ç®¡è¿™äº›frameæ˜¯å¦å±äºåŒä¸€ä¸ªstreamã€‚ type framer struct { // ä¸€ä¸ªåŒ…å«äº†bufferçš„net.Connçš„writer writer *bufWriter // åŸç”Ÿçš„http2.Framerï¼Œè´Ÿè´£æ•°æ®è¯»å†™ fr *http2.Framer } framerå…¶å®å°±æ˜¯å¯¹golangåŸç”Ÿhttp2.Framerçš„å°è£…ã€‚ type bufWriter struct { pool *sync.Pool buf []byte offset int batchSize int conn net.Conn err error } func newBufWriter(conn net.Conn, batchSize int, pool *sync.Pool) *bufWriter { w := \u0026bufWriter{ batchSize: batchSize, conn: conn, pool: pool, } // this indicates that we should use non shared buf if pool == nil { w.buf = make([]byte, batchSize) } return w } func (w *bufWriter) Write(b []byte) (int, error) { // åœ¨writeä¹‹é—´æ£€æŸ¥ä¸Šä¸€æ¬¡writeæ˜¯å¦å‘ç”Ÿäº†é”™è¯¯ if w.err != nil { return 0, w.err } // å¦‚æœbatchsizeä¸º0ï¼Œè¯´æ˜ä¸éœ€è¦å†™ç¼“å­˜ï¼Œç›´æ¥å‘net.Connå†™æ•°æ® if w.batchSize == 0 { // Buffer has been disabled. n, err := w.conn.Write(b) return n, toIOError(err) } if w.buf == nil { b := w.pool.Get().(*[]byte) w.buf = *b } written := 0 // å¦‚æœå†™å…¥çš„æ•°æ®å°‘äºbatchSizeï¼Œåˆ™ç¼“å­˜ï¼Œæš‚æ—¶ä¸å†™å…¥conn // å¦‚æœå†™å…¥çš„æ•°æ®å¤šä½™batchSizeï¼Œåˆ™è°ƒç”¨flushKeepBufferä¸æ–­å†™æ•°æ® for len(b) \u003e 0 { copied := copy(w.buf[w.offset:], b) b = b[copied:] written += copied w.offset += copied if w.offset \u003c w.batchSize { continue } if err := w.flushKeepBuffer(); err != nil { return written, err } } return written, nil } func (w *bufWriter) Flush() error { // åˆ·æ–°æ•°æ®åˆ°conn err := w.flushKeepBuffer() // Only release the buffer if we are in a \"shared\" mode if w.buf != nil \u0026\u0026 w.pool != nil { b := w.buf w.pool.Put(\u0026b) w.buf = nil } return err } func (w *bufWriter) flushKeepBuffer() error { if w.err != nil { return w.err } if w.offset == 0 { return nil } _, w.err = w.conn.Write(w.buf[:w.offset]) w.err = toIOError(w.err) w.offset = 0 return w.err } grpc serverå®ç°äº†ä¸€ä¸ªç®€å•çš„ç¼“å­˜å†™ç»™http2.framerä½œä¸ºio.Writerã€‚ // å…¨å±€writeBufferPool var writeBufferPoolMap = make(map[int]*sync.Pool) var writeBufferMutex sync.Mutex func newFramer(conn net.Conn, writeBufferSize, readBufferSize int, sharedWriteBuffer bool, maxHeaderListSize uint32) *framer { if writeBufferSize \u003c 0 { writeBufferSize = 0 } var r io.Reader = conn if readBufferSize \u003e 0 { // è®¾ç½®io.Reader r = bufio.NewReaderSize(r, readBufferSize) } var pool *sync.Pool if sharedWriteBuffer { pool = getWriteBufferPool(writeBufferSize) } // è®¾ç½®io.Writer w := newBufWriter(conn, writeBufferSize, pool) // åˆ›å»ºframer f := \u0026framer{ writer: w, fr: http2.NewFramer(w, r), } f.fr.SetMaxReadFrameSize(http2MaxFrameLen) // Opt-in to Frame reuse API on framer to reduce garbage. // Frames aren't safe to read from after a subsequent call to ReadFrame. f.fr.SetReuseFrames() f.fr.MaxHeaderListSize = maxHeaderListSize f.fr.ReadMetaHeaders = hpack.NewDecoder(http2InitHeaderTableSize, nil) return f } // writeBuffer ä½¿ç”¨sync.Pool func getWriteBufferPool(size int) *sync.Pool { writeBufferMutex.Lock() defer writeBufferMutex.Unlock() pool, ok := writeBufferPoolMap[size] if ok { return pool } pool = \u0026sync.Pool{ New: func() any { b := make([]byte, size) return \u0026b }, } writeBufferPoolMap[size] = pool return pool } ä¼ é€’ç»™http2.framerçš„io.Readerä½¿ç”¨äº†bifio packageã€‚ writeBufferä½¿ç”¨äº†goæ ‡å‡†åº“ä¸­çš„sync.Poolï¼Œæ ¹æ®éœ€è¦çš„sizeè·å–å¯¹åº”çš„sync.Poolï¼Œå¦‚æœæ± ä¸­æœ‰å¯¹åº”çš„byte[]ï¼Œè·å–ç„¶åè¿”å›ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ›å»ºæ–°çš„byte[]å¹¶è¿”å›ã€‚æ± ä¸­å…ƒç´ çš„å›æ”¶æ—¶æœºï¼Œgoå…è®¸åœ¨ä»»ä½•æ—¶å€™è‡ªåŠ¨å›æ”¶æ± ä¸­çš„å…ƒç´ ï¼ˆgcï¼‰ã€‚ grpc serverä¸ºæ¯ä¸€ä¸ªclientåˆ›å»ºä¸€ä¸ªloopyWriterï¼Œæœ‰è¿™ä¸ªloopyWriterè´Ÿè´£å‘é€æ•°æ®ã€‚ type loopyWriter struct { // å®¢æˆ·ç«¯è¿˜æ˜¯æœåŠ¡ç«¯ side side // controlBuffer cbuf *controlBuffer // å‘é€é…é¢ sendQuota uint32 // å‘é€ç«¯åˆå§‹çª—å£å¤§å° outbound initial window size oiws uint32 // å·²ç»å»ºç«‹æœªæ¸…ç†çš„streamï¼Œåœ¨å®¢æˆ·ç«¯ï¼ŒæŒ‡æ‰€æœ‰å·²ç»å°†Headerså‘é€å‡ºå»çš„streamï¼Œ // åœ¨æœåŠ¡ç«¯ï¼ŒæŒ‡æ‰€æœ‰å·²ç»æ¥æ”¶åˆ°Headersçš„stream estdStreams map[uint32]*outStream // æ´»è·ƒstreamåˆ—è¡¨ï¼Œæœ‰æ•°æ®éœ€è¦å‘é€ä¸”åŒ…å«stream-levelæµæ§ï¼Œé‡Œé¢çš„æ¯ä¸ªstreamå†…éƒ¨éƒ½æœ‰ä¸€ä¸ªæ•°æ®åˆ—è¡¨ç”¨æ¥å­˜æ”¾å‘é€çš„æ•°æ® activeStreams *outStreamList // http2.Framerçš„åŒ…è£…ï¼Œç”¨æ¥å®é™…è¯»å†™æ•° framer *framer hBuf *bytes.Buffer // The buffer for HPACK encoding. hEnc *hpack.Encoder // HPACK encoder. bdpEst *bdpEstimator draining bool // åº•å±‚tcpè¿æ¥ conn net.Conn logger *grpclog.PrefixLogger bufferPool mem.BufferPool // Side-specific handlers ssGoAwayHandler func(*goAway) (bool, error) } loopyWriterä»control bufferä¸­æ¥æ”¶frameï¼Œæ¯ä¸ªframeè¢«å•ç‹¬","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:4:0","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc serverçš„æµé‡æ§åˆ¶ grpcåœ¨åº”ç”¨å±‚å®ç°äº†è‡ªå·±çš„æµé‡æ§åˆ¶ï¼Œå¹¶å°†æµé‡æ§åˆ¶åˆ†æˆäº†ä¸‰ä¸ªå±‚çº§ sample level æµé‡æ§åˆ¶ connection level æµé‡æ§åˆ¶ stream level æµé‡æ§åˆ¶ æµé‡æ§åˆ¶å¯ä»¥è¯´æ˜¯grpcé«˜æ€§èƒ½çš„å…³é”®ï¼Œé€šè¿‡åŠ¨æ€åœ°æ§åˆ¶æ•°æ®å‘é€å’Œæ¥æ”¶çš„é€Ÿç‡ï¼Œgrpcä¿è¯åœ¨ä»»ä½•ç½‘ç»œæƒ…å†µä¸‹éƒ½èƒ½å‘æŒ¥æœ€å¤§çš„æ€§èƒ½ï¼Œå°½é‡æé«˜ä¼ è¾“å¸¦å®½å¹¶é™ä½ä¼ è¾“å»¶è¿Ÿã€‚ ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:5:0","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"é‡‡æ ·æµé‡æ§åˆ¶ BDPä¼°ç®—å’ŒåŠ¨æ€æµé‡æ§åˆ¶çª—å£ BDPå’ŒåŠ¨æ€æµé‡æ§åˆ¶çª—å£ç¼©å°äº†grpcå’Œhttp1.1åœ¨é«˜å»¶è¿Ÿç½‘ç»œä¸­çš„æ€§èƒ½è¡¨ç°ã€‚å¸¦å®½å»¶è¿Ÿç§¯ï¼ˆBDPï¼ŒBandwidth Delay Productï¼‰æ˜¯ç½‘ç»œè¿æ¥çš„å¸¦å®½å’Œæ•°æ®å¾€è¿”å»¶è¿Ÿçš„ä¹˜ç§¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¨ç¤ºåœ¨ç½‘ç»œè¢«å®Œå…¨åˆ©ç”¨æ—¶ç½‘ç»œä¸Šæœ‰å¤šå°‘å­—èŠ‚æ•°æ®ã€‚ BDPç®—æ³•åŸºæœ¬æ€è·¯å¦‚ä¸‹ï¼š æ¯æ¬¡æ¥æ”¶æ–¹æ”¶åˆ°ä¸€ä¸ªdata frameæ—¶ï¼Œå®ƒå°±ä¼šå‘é€ä¸€ä¸ªBDP pingï¼ˆä¸€ä¸ªå¸¦æœ‰å”¯ä¸€æ•°æ®ã€ä»…ç”¨äºBDPä¼°ç®—çš„pingï¼‰ã€‚åœ¨è¿™ä¹‹åï¼Œæ¥æ”¶æ–¹å¼€å§‹ç»Ÿè®¡å®ƒæ¥æ”¶åˆ°çš„å­—èŠ‚æ•°ï¼ˆåŒ…æ‹¬è§¦å‘è¯¥BDP pingçš„é‚£éƒ¨åˆ†æ•°æ®ï¼‰ï¼Œç›´åˆ°å®ƒæ”¶åˆ°è¯¥pingçš„ackä¸ºæ­¢ã€‚è¿™ä¸ªåœ¨å¤§çº¦1.5ä¸ªRTTï¼ˆround-trip timeï¼‰å†…æ¥æ”¶åˆ°çš„å­—èŠ‚æ€»æ•°ï¼Œçº¦ä¸ºBDPçš„1.5å€ã€‚å¦‚æœè¿™ä¸ªæ€»å­—èŠ‚æ•°æ¥è¿‘å½“å‰çš„çª—å£ï¼ˆæ¯”å¦‚è¶…è¿‡çª—å£çš„2/3ï¼‰ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¿…é¡»å¢å¤§çª—å£ã€‚æˆ‘ä»¬å°†çª—å£å¤§å°ï¼ˆåŒ…æ‹¬stremaingå’Œconnectionçª—å£ï¼‰è®¾ä¸ºé‡‡æ ·å¾—åˆ°çš„BDPçš„ä¸¤å€ï¼ˆä¹Ÿå°±æ˜¯æ¥æ”¶åˆ°çš„å­—èŠ‚æ€»æ•°çš„ä¸¤å€ï¼‰ã€‚ åœ¨grpc serverç«¯å®šä¹‰äº†ä¸€ä¸ªbdpEstimatorï¼Œæ˜¯ç”¨æ¥è®¡ç®—BDPçš„æ ¸å¿ƒã€‚ const ( // bdpLimit is the maximum value the flow control windows will be increased // to. TCP typically limits this to 4MB, but some systems go up to 16MB. // Since this is only a limit, it is safe to make it optimistic. bdpLimit = (1 \u003c\u003c 20) * 16 // alpha is a constant factor used to keep a moving average // of RTTs. alpha = 0.9 // If the current bdp sample is greater than or equal to // our beta * our estimated bdp and the current bandwidth // sample is the maximum bandwidth observed so far, we // increase our bbp estimate by a factor of gamma. beta = 0.66 // To put our bdp to be smaller than or equal to twice the real BDP, // we should multiply our current sample with 4/3, however to round things out // we use 2 as the multiplication factor. gamma = 2 ) // Adding arbitrary data to ping so that its ack can be identified. // Easter-egg: what does the ping message say? var bdpPing = \u0026ping{data: [8]byte{2, 4, 16, 16, 9, 14, 7, 7}} type bdpEstimator struct { // sentAt is the time when the ping was sent. sentAt time.Time mu sync.Mutex // bdp is the current bdp estimate. bdp uint32 // sample is the number of bytes received in one measurement cycle. sample uint32 // bwMax is the maximum bandwidth noted so far (bytes/sec). bwMax float64 // bool to keep track of the beginning of a new measurement cycle. isSent bool // Callback to update the window sizes. updateFlowControl func(n uint32) // sampleCount is the number of samples taken so far. sampleCount uint64 // round trip time (seconds) rtt float64 } bdpEstimatoræœ‰ä¸¤ä¸ªä¸»è¦çš„æ–¹æ³•addå’Œcalculate // addçš„è¿”å›å€¼æŒ‡ç¤ºloopyWriteræ˜¯å¦å‘é€BDP ping frameç»™client func (b *bdpEstimator) add(n uint32) bool { b.mu.Lock() defer b.mu.Unlock() // å¦‚æœbdpå·²ç»è¾¾åˆ°ä¸Šé™ï¼Œå°±ä¸å†å‘é€bdp pingè¿›è¡Œé‡‡æ · if b.bdp == bdpLimit { return false } // å¦‚æœåœ¨å½“å‰æ—¶é—´ç‚¹æ²¡æœ‰bdp ping frameå‘é€å‡ºå»ï¼Œå°±åº”è¯¥å‘é€ï¼Œæ¥è¿›è¡Œé‡‡æ · if !b.isSent { b.isSent = true b.sample = n b.sentAt = time.Time{} b.sampleCount++ return true } // å·²ç»æœ‰bdp ping frameå‘é€å‡ºå»äº†ï¼Œä½†æ˜¯è¿˜æ²¡æœ‰æ”¶åˆ°ackï¼Œç´¯åŠ æ”¶åˆ°çš„å­—èŠ‚æ•° b.sample += n return false } addå‡½æ•°æœ‰ä¸¤ä¸ªä½œç”¨ï¼š å‘ŠçŸ¥loopyWriteræ˜¯å¦å¼€å§‹é‡‡æ · è®°å½•é‡‡æ ·å¼€å§‹çš„æ—¶é—´å’Œåˆå§‹æ•°æ®é‡ func (t *http2Server) handleData(f *http2.DataFrame) { size := f.Header().Length var sendBDPPing bool if t.bdpEst != nil { sendBDPPing = t.bdpEst.add(size) } if w := t.fc.onData(size); w \u003e 0 { t.controlBuf.put(\u0026outgoingWindowUpdate{ streamID: 0, increment: w, }) } if sendBDPPing { // Avoid excessive ping detection (e.g. in an L7 proxy) // by sending a window update prior to the BDP ping. if w := t.fc.reset(); w \u003e 0 { t.controlBuf.put(\u0026outgoingWindowUpdate{ streamID: 0, increment: w, }) } t.controlBuf.put(bdpPing) } // Select the right stream to dispatch. s, ok := t.getStream(f) } handleDataå‡½æ•°æ˜¯grpc serveæ”¶åˆ°æ¥è‡ªclientçš„http2 data frameä¹‹åæ‰§è¡Œçš„å‡½æ•°ï¼Œå¯ä»¥çœ‹åˆ°ï¼Œgrpc serverå’Œæ¯ä¸ªclientä¹‹é—´éƒ½ç»´æŠ¤ç€ä¸€ä¸ªbdpEstimatorï¼Œæ¯æ¬¡æ”¶åˆ°ä¸€ä¸ªdata frameï¼Œgrpc serveréƒ½ä¼šåˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œé‡‡æ ·ï¼Œå¦‚æœéœ€è¦é‡‡æ ·ï¼Œå°±å‘clientå‘é€ä¸€ä¸ªbdpPing frameï¼Œè¿™ä¸ªframeä¹Ÿæ˜¯åŠ å…¥controlBufferï¼Œå¼‚æ­¥å¤„ç†çš„ã€‚ è¿™é‡Œä¹Ÿå°†è¿æ¥çš„æµé‡æ§åˆ¶å’Œåº”ç”¨ç¨‹åºè¯»å–æ•°æ®çš„è¡Œä¸ºè§£è€¦ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè¿æ¥çº§åˆ«çš„çª—å£æ›´æ–°ä¸åº”è¯¥ä¾èµ–äºåº”ç”¨æ˜¯å¦è¯»å–äº†æ•°æ®ã€‚stream-levelæµæ§å·²ç»æœ‰è¿™ä¸ªé™åˆ¶ï¼ˆå¿…é¡»ç­‰å¾…åº”ç”¨è¯»å–åæ‰èƒ½æ›´æ–°çª—å£ï¼‰ï¼Œæ‰€ä»¥å¦‚æœæŸä¸ªstreamå¾ˆæ…¢ï¼Œå‘é€æ–¹å·²ç»è¢«é˜»å¡ï¼ˆå› ä¸ºçª—å£è€—å°½ï¼‰ã€‚è§£è€¦å¯ä»¥é¿å…ä¸‹é¢çš„æƒ…å†µå‘ç”Ÿï¼Œå½“æŸäº›stremaå¾ˆæ…¢ï¼ˆæˆ–è€…å‹æ ¹æ²¡æœ‰è¯»å–æ•°æ®ï¼‰æ—¶ï¼Œå¯¼è‡´å…¶ä»–æ´»è·ƒçš„streamç”±äºæ²¡æœ‰connection-levelæµæ§çª—å£è€Œè¢«é˜»å¡ã€‚ func (l *loopyWriter) pingHandler(p *ping) error { if !p.ack { l.bdpEst.timesnap(p.data) } return l.framer.fr.WritePing(p.ack, p.data) } func (b *bdpEstimator) timesnap(d [8]byte) { if bdpPing.data != d { return } b.sentAt = time.Now() } å‰é¢æåˆ°bdp ping frameæ˜¯é€šè¿‡control framerå¼‚æ­¥å‘é€å‡ºå»çš„ï¼Œè¿™ä¸ªæ—¶é—´ç‚¹å¯èƒ½å’Œä¹‹å‰å†³å®šå‘é€pingçš„æ—¶é—´ç‚¹æœ‰ä¸€å®šçš„è·ç¦»ï¼Œä¸ºäº†æ›´å‡†ç¡®çš„è®¡ç®—RTTï¼Œæ‰€ä»¥åœ¨ä½¿ç”¨http2.framerå®é™…å‘é€æ•°æ®å‰ï¼Œé‡æ–°æ›´æ–°äº†bdp ping frameçš„å‘é€æ—¶é—´ã€‚ Clientç«¯åœ¨æ”¶åˆ°ä¸€ä¸ªbdp ping frameä¹‹åï¼Œä¼šç«‹åˆ»è¿”å›ä¸€ä¸ªackï¼Œserverä¼šæ•æ‰åˆ°è¿™ä¸ªackã€‚ func (t *http2Server) handlePing(f *http2.PingFrame) { if f.IsAck() { if f.Dat","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:5:1","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"controlBufferæ•°æ®ç»“æ„ å…ˆä»‹ç»ä¸€ä¸ªé‡è¦çš„æ•°æ®ç»“æœcontrolBufferï¼Œè¿™ä¸ªåœ¨ä¹‹å‰å·²ç»æåˆ°è¿‡äº†ï¼Œåœ¨å‘å¤–å‘é€æ•°æ®å‰ï¼Œå…¶å®éƒ½ä¼šåŠ å…¥controlBufferä¸­ï¼Œç„¶åå†è¿›è¡Œå¤„ç†ã€‚ type controlBuffer struct { // wakeupchçš„ä½œç”¨æ˜¯åœ¨é˜»å¡è¯»å–ç¼“å­˜ä¸­çš„å†…å®¹æ—¶ï¼Œå½“æœ‰æ–°çš„frameåŠ å…¥itemListï¼Œå¯ä»¥è§£å†³é˜»å¡å¹¶è¿”å›itemListä¸­çš„frame wakeupCh chan struct{} // Unblocks readers waiting for something to read. // done \u003c-chan struct{} // Closed when the transport is done. // Mutex guards all the fields below, except trfChan which can be read // atomically without holding mu. mu sync.Mutex // å’ŒwakeupChé…ç½®ä½¿ç”¨ï¼Œç¡®ä¿ä¸å‘wakeupChä¸­æ”¾å…¥å¤šä½™çš„structï¼Œä¿è¯é˜»å¡è¯»å–ç¼“å­˜ä¸ä¼šå› ä¸ºwakeupChä¸­çš„å¤šä½™å…ƒç´ é”™è¯¯è§£é™¤é˜»å¡ consumerWaiting bool // True when readers are blocked waiting for new data. closed bool // True when the controlbuf is finished. list *itemList // List of queued control frames. // è®°å½•æ’é˜Ÿçš„å“åº”å¸§æ•°é‡ transportResponseFrames int // å½“transportResponseFrames \u003e= maxQueuedTransportResponseFramesæ—¶ï¼Œ // åˆ›å»ºtrfChanï¼Œç”¨äºæ§åˆ¶æ˜¯å¦ç»§ç»­ä»clientè¯»å–frame trfChan atomic.Pointer[chan struct{}] } controlBufferä¸­çš„æ•°æ®è¢«ç§°ä¸ºcontrol frameï¼Œä¸€ä¸ªcontrol frameä¸æ­¢è¡¨ç¤ºå‘å¤–å‘é€çš„dataã€messageã€headersï¼Œä¹Ÿè¢«ç”¨æ¥æŒ‡ç¤ºloopyWriteræ›´æ–°è‡ªèº«çš„å†…éƒ¨çŠ¶æ€ã€‚control frameå’Œhttp2 frameæ²¡æœ‰ç›´æ¥å…³ç³»ï¼Œå°½ç®¡æœ‰äº›control frameï¼Œæ¯”å¦‚è¯´ dataFrameå’ŒheaderFrameç¡®å®ä½œä¸ºhttp2 frameå‘å¤–ä¼ è¾“ã€‚ controlBufferç»´æŠ¤äº†ä¸€ä¸ªitemListï¼ˆå•å‘é“¾è¡¨ï¼‰ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€å—ç¼“å­˜åŒºï¼Œè¿™å—ç¼“å­˜åŒºä¸»è¦æœ‰ä¸¤ä¸ªä½œç”¨ï¼š ç¼“å­˜éœ€è¦å‘é€çš„frame æ ¹æ®ç¼“å­˜ä¸­transportResponseFrameçš„æ•°é‡ï¼Œå†³å®šæ˜¯å¦æš‚æ—¶åœæ­¢è¯»å–ä»clientå‘æ¥çš„frame ä¸‹é¢çœ‹controlBufferä¸­çš„ä¸€äº›ä¸»è¦å‡½æ•°ï¼ŒåŠ æ·±ç†è§£ func newControlBuffer(done \u003c-chan struct{}) *controlBuffer { return \u0026controlBuffer{ wakeupCh: make(chan struct{}, 1), list: \u0026itemList{}, done: done, } } newControlBufferç”¨äºåˆ›å»ºcontrolBufferå®ä¾‹ï¼Œå…¶ä¸­wakeupChæ˜¯ç¼“å†²åŒºä¸º1çš„channelã€‚ func (c *controlBuffer) throttle() { if ch := c.trfChan.Load(); ch != nil { select { case \u003c-(*ch): case \u003c-c.done: } } } throttleå‡½æ•°ä¼šè¢«é˜»å¡ï¼Œå¦‚æœcontrolBufferä¸­å­˜åœ¨å¤ªå¤šçš„å“åº”å¸§ï¼Œæ¯”å¦‚incommingSettingsã€cleanupStremaç­‰ã€‚åœ¨grpc serverçš„ä»£ç ä¸­ï¼Œthrottleå‡½æ•°é€šå¸¸å‡ºç°åœ¨grpc serveræ¥æ”¶client frameçš„å¼€å¤´ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå½“transportResponseFramesæ•°é‡è¿‡å¤šæ—¶ï¼Œgrpc serverä¼šæš‚åœæ¥å—æ¥è‡ªclientçš„frameï¼ŒmaxQueuedTransportResponseFramesä¸º50ã€‚ func (c *controlBuffer) executeAndPut(f func() bool, it cbItem) (bool, error) { c.mu.Lock() defer c.mu.Unlock() if c.closed { return false, ErrConnClosing } if f != nil { if !f() { // f wasn't successful return false, nil } } if it == nil { return true, nil } var wakeUp bool if c.consumerWaiting { wakeUp = true c.consumerWaiting = false } // å°†itemåŠ å…¥åˆ°bufferä¸­ c.list.enqueue(it) if it.isTransportResponseFrame() { c.transportResponseFrames++ if c.transportResponseFrames == maxQueuedTransportResponseFrames { // We are adding the frame that puts us over the threshold; create // a throttling channel. ch := make(chan struct{}) c.trfChan.Store(\u0026ch) } } if wakeUp { select { case c.wakeupCh \u003c- struct{}{}: default: } } return true, nil } executeAndPutè¿è¡Œfå‡½æ•°ï¼Œå¦‚æœfå‡½æ•°è¿”å›trueï¼Œæ·»åŠ ç»™å®šçš„itemåˆ°controlBufã€‚å¦‚æœconsumerWaitingä¸ºtrueï¼Œä¹Ÿå°±æ˜¯loopyWriterå‘ç°æ²¡æœ‰æ¶ˆæ¯å¯ä¾›å¤„ç†ï¼Œæ‰€ä»¥é˜»å¡è·å–control frameï¼Œè¿™é‡Œä¼šå‘wakeupChä¸­æ”¾å…¥ä¸€ä¸ªå…ƒç´ ï¼Œæ¥é€šçŸ¥æ¶ˆè´¹è€…å¯ä»¥è¯»å–frameäº†ã€‚åœ¨è¿™é‡Œä¹Ÿä¼šæ£€æŸ¥å“åº”å¸§çš„æ•°é‡ï¼Œå¦‚æœè¶…è¿‡é˜ˆå€¼ï¼Œåˆ™åˆ›å»ºtrfChanã€‚ func (c *controlBuffer) get(block bool) (any, error) { // forå¾ªç¯ for { c.mu.Lock() frame, err := c.getOnceLocked() if frame != nil || err != nil || !block { c.mu.Unlock() return frame, err } // è®¾ç½®çŠ¶æ€ä¸ºconsumerWaiting c.consumerWaiting = true c.mu.Unlock() // Release the lock above and wait to be woken up. select { // control bufferä¸­æ²¡æœ‰control frameï¼Œé˜»å¡ç­‰å¾… case \u003c-c.wakeupCh: case \u003c-c.done: return nil, errors.New(\"transport closed by client\") } } } func (c *controlBuffer) getOnceLocked() (any, error) { if c.closed { return false, ErrConnClosing } if c.list.isEmpty() { return nil, nil } h := c.list.dequeue().(cbItem) // å°†controlframeç§»é™¤å“åº”å¸§ï¼Œå¯èƒ½ä¼šè§£å°å¯¹clientè¯·æ±‚çš„è¯»å– if h.isTransportResponseFrame() { if c.transportResponseFrames == maxQueuedTransportResponseFrames { // We are removing the frame that put us over the // threshold; close and clear the throttling channel. ch := c.trfChan.Swap(nil) close(*ch) } c.transportResponseFrames-- } return h, nil } getä»control bufferä¸­è·å–ä¸‹ä¸€ä¸ªcontrol frameï¼Œå¦‚æœblockå‚æ•°ä¸ºtrueå¹¶ä¸”control bufferä¸­æ²¡æœ‰control frameï¼Œè°ƒç”¨è¢«é˜»å¡ç›´åˆ°æœ‰control frameæˆ–è€…bufferè¢«å…³é—­ã€‚ ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:5:2","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"connection levelæµé‡æ§åˆ¶ connection levelæµé‡æ§åˆ¶ä¼šæ§åˆ¶å¯¹äºæŸä¸ªclientæŸä¸€æ—¶åˆ»èƒ½å¤Ÿå‘é€çš„æ•°æ®æ€»é‡ã€‚ type loopyWriter struct { ...... sendQuota uint32 ...... } æ§åˆ¶çš„æ–¹å¼å°±æ˜¯åœ¨loopyWriterä¸­ç”¨ä¸€ä¸ªsendQuotaæ¥æ ‡è®°è¯¥clientç›®å‰å¯å‘é€æ•°æ®çš„é…é¢ã€‚ func (l *loopyWriter) processData() (bool, error) { ...... l.sendQuota -= uint32(size) ...... } sendQuotaä¼šè¢«åˆå§‹åŒ–ä¸º65535ï¼Œå¹¶ä¸”æ¯å½“æœ‰æ•°æ®è¢«grpc serverå‘é€ç»™clientçš„æ—¶å€™ï¼ŒsendQuotaéƒ½ä¼šå‡å°‘å’Œè¢«å‘é€æ•°æ®ç›¸ç­‰çš„å¤§å°ã€‚ ä¸ºäº†é…åˆserverç«¯çš„æµé‡æ§åˆ¶ï¼Œclientç«¯åœ¨è¿æ¥åˆå§‹åŒ–æ—¶è¢«åˆ†é…äº†ä¸€ä¸ªlimitï¼Œé»˜è®¤ä¸º65536å­—èŠ‚ï¼Œclientç«¯ä¼šè®°å½•æ”¶åˆ°çš„æ•°æ®é‡çš„æ€»å’Œunackedï¼Œå½“unackedè¶…è¿‡äº†limitçš„1/4åï¼Œclientå°±ä¼šå‘serveræ®µå‘é€ä¸€ä¸ªwindow updateï¼ˆæ•°å€¼ä¸ºunackedï¼‰,é€šçŸ¥serverå¯ä»¥å°†quotaåŠ å›æ¥ï¼ŒåŒæ—¶å°†unackedç½®é›¶ã€‚ å¯ä»¥çœ‹åˆ°ä¸ºäº†é¿å…é¢‘ç¹çš„å‘é€window updateå ç”¨ç½‘ç»œå¸¦å®½ï¼Œclientå¹¶ä¸ä¼šåœ¨æ¯æ¬¡æ¥æ”¶åˆ°æ•°æ®ä¹‹åå°±å‘é€window updateï¼Œè€Œæ˜¯ç­‰å¾…æ¥æ”¶çš„æ•°æ®é‡è¾¾åˆ°æŸä¸€é˜ˆå€¼åå†å‘é€ã€‚ // trInFlow æ˜¯ client ç«¯å†³å®šæ˜¯å¦å‘é€ window update ç»™ server çš„æ ¸å¿ƒ type trInFlow struct { // server ç«¯èƒ½å¤Ÿå‘é€æ•°æ®çš„ä¸Šé™, ä¼šè¢« server ç«¯æ ¹æ®é‡‡ç”¨æ§åˆ¶çš„ç»“æœæ›´æ–° limit uint32 // client ç«¯å·²ç»æ¥æ”¶åˆ°çš„æ•°æ® unacked uint32 // ç”¨äº metric è®°å½•, ä¸å½±å“æµé‡æ§åˆ¶ effectiveWindowSize uint32 } // å‚æ•° n æ˜¯ client æ¥æ”¶åˆ°çš„æ•°æ®å¤§å°, è¿”å›å€¼è¡¨ç¤ºéœ€è¦å‘ server å‘é€çš„ window update ä¸­çš„æ•°å€¼å¤§å°. // è¿”å› 0 ä»£è¡¨ä¸éœ€è¦å‘é€ window update func (f *trInFlow) onData(n uint32) uint32 { f.unacked += n // è¶…è¿‡ 1/4 * limit æ‰ä¼šå‘é€ window update, ä¸”æ•°å€¼ä¸ºå·²ç»æ¥æ”¶åˆ°çš„æ•°æ®æ€»é‡ if f.unacked \u003e= f.limit/4 { w := f.unacked f.unacked = 0 f.updateEffectiveWindowSize() return w } f.updateEffectiveWindowSize() return 0 } trInFlowæ˜¯clientç«¯æ§åˆ¶æ˜¯å¦å‘é€window updateçš„æ ¸å¿ƒï¼Œlimitä¼šéšserverç«¯å‘æ¥çš„window updateè€Œæ”¹å˜ã€‚ type outgoingWindowUpdate struct { streamID uint32 increment uint32 } æœ€ç»ˆå‘å¯¹ç«¯å‘é€çš„æ˜¯WindowUpdate Frameï¼Œå…¶ä¸­streamIDä¸º0ï¼Œè¡¨ç¤ºä½œç”¨äºæ•´ä¸ªè¿æ¥ï¼Œincrementè¡¨ç¤ºquotaçš„å¢é‡ã€‚ func (t *http2Server) handleWindowUpdate(f *http2.WindowUpdateFrame) { t.controlBuf.put(\u0026incomingWindowUpdate{ streamID: f.Header().StreamID, increment: f.Increment, }) } æœåŠ¡ç«¯æ”¶åˆ°WindowUpdateFrameåï¼Œä¼šå°†æ¶ˆæ¯åŒ…è£…æˆincomingWindowUpdateæ”¾å…¥controlBufä¸­ func (l *loopyWriter) incomingWindowUpdateHandler(w *incomingWindowUpdate) error { // Otherwise update the quota. if w.streamID == 0 { l.sendQuota += w.increment return nil } ...... } å½“grpc serveræ”¶åˆ°æ¥è‡ªclientçš„http2 FrameWindowUpdate frameæ—¶ï¼Œæ‰ä¼šå°†è¿™ä¸€quotaå¢åŠ ï¼Œä¹Ÿå°±æ˜¯è¯´sendQuotaä¼šåœ¨serverå‘å‡ºæ•°æ®æ—¶å‡å°‘ï¼Œåœ¨æ”¶åˆ°æ¥è‡ªclientçš„FrameWindowUpdate frameæ—¶å¢åŠ ï¼Œconnection levelçš„æµé‡æ§åˆ¶æ˜¯serverå’Œclientç›¸äº’äº¤äº’çš„ç»“æœï¼Œç”±åŒæ–¹å…±åŒå†³å®šçª—å£å¤§å°ã€‚ func (l *loopyWriter) processData() (bool, error) { if l.sendQuota == 0 { return true, nil } if maxSize \u003e int(l.sendQuota) { // connection-level flow control. maxSize = int(l.sendQuota) } å½“loopyWriteræ‰“ç®—å‘å¤–å‘é€æ•°æ®æ—¶ï¼Œå¦‚æœsendQuotaä¸ºé›¶ï¼Œå°±åœæ­¢å‘å¤–å‘é€æ•°æ®ï¼Œå¦‚æœæ‰“ç®—å‘å¤–å‘é€çš„æ•°æ®è¶…è¿‡sendquotaï¼Œåˆ™åªå‘é€sendQuotaå¤§å°çš„æ•°æ®ã€‚ ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:5:3","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"stream levelæµé‡æ§åˆ¶ ä¸€ä¸ªstreamçš„æµé‡æ§åˆ¶æœ‰ä¸‰ç§çŠ¶æ€ï¼Œåˆ†åˆ«æ˜¯ active: streamä¸­æœ‰æ•°æ®ä¸”æ•°æ®å¯ä»¥è¢«å‘é€ empty: streamä¸­æ²¡æœ‰æ•°æ® waitingOnStreamQuota: streamçš„quotaä¸è¶³ï¼Œç­‰å¾…æœ‰quotaæ—¶å†å‘é€æ•°æ® ä¸€ä¸ªstreamä¸€å¼€å§‹çš„çŠ¶æ€ä¸ºemptyï¼Œå› ä¸ºä¸€ä¸ªstreamåœ¨è¢«åˆ›å»ºå‡ºæ¥æ—¶è¿˜æ²¡æœ‰å¾…å‘é€çš„æ•°æ®ã€‚ func (l *loopyWriter) preprocessData(df *dataFrame) error { str, ok := l.estdStreams[df.streamID] if !ok { return nil } // If we got data for a stream it means that // stream was originated and the headers were sent out. str.itl.enqueue(df) if str.state == empty { str.state = active l.activeStreams.enqueue(str) } return nil } å½“serverå¤„ç†controlBufferæ—¶é‡åˆ°æŸä¸ªstreamçš„frameæ—¶ï¼Œä¼šå°†è¯¥streamè½¬æˆactiveçŠ¶æ€ï¼ŒactiveçŠ¶æ€çš„streamå¯ä»¥å‘é€æ•°æ®ã€‚ func (l *loopyWriter) processData() (bool, error) { ...... if strQuota := int(l.oiws) - str.bytesOutStanding; strQuota \u003c= 0 { // stream-level flow control. str.state = waitingOnStreamQuota return false, nil } ...... str.bytesOutStanding += size ...... } å‘é€æ•°æ®ä¹‹åï¼ŒbyteOutStandingä¼šå¢åŠ ç›¸åº”çš„æ•°æ®å¤§å°ï¼Œè¡¨æ˜è¯¥streamæœ‰è¿™äº›æ•°æ®è¢«å‘é€ç»™clientï¼Œè¿˜æ²¡æœ‰æ”¶åˆ°å›åº”ã€‚è€Œå½“byteOutStandingçš„å¤§å°è¶…è¿‡loopyWriter.oiwsï¼Œä¹Ÿå°±æ˜¯65535åï¼Œä¼šæ‹’ç»ä¸ºè¯¥streamç»§ç»­å‘é€æ•°æ®ï¼Œè¿™ç§ç­–ç•¥é¿å…äº†ä¸æ–­å‘ä¸€ä¸ªå¤±å»å›åº”çš„clientå‘é€æ•°æ®ï¼Œé¿å…æµªè´¹ç½‘ç»œå¸¦å®½ã€‚ stream levelçš„æµé‡æ§åˆ¶å’Œconnenction levelçš„æµé‡æ§åˆ¶åŸç†åŸºæœ¬ä¸Šä¸€ç›´ï¼Œä¸»è¦çš„åŒºåˆ«æœ‰ä¸¤ç‚¹ï¼š stream levelçš„æµé‡æ§åˆ¶ä¸­çš„quotaåªé’ˆå¯¹å•ä¸ªstreamï¼Œæ¯ä¸ªstreamæ—¢å—é™äºstream levelæµé‡æ§åˆ¶ï¼Œåˆå—é™äºconection levelæµé‡æ§åˆ¶ clientç«¯å†³å®šåé¦ˆç»™server windowUpdate frameçš„æ—¶æœºæ›´è´Ÿè´£ä¸€äº› // å…¥ç«™æµé‡æ§åˆ¶ï¼ˆinbound flow control type inFlow struct { mu sync.Mutex // streamèƒ½æ¥å—çš„æ•°æ®ä¸Šé™ï¼Œåˆå§‹ä¸º65535å­—èŠ‚ï¼Œå—åˆ°é‡‡æ ·æµé‡æ§åˆ¶çš„å½±å“ limit uint32 // æ”¶åˆ°ä½†æœªè¢«åº”ç”¨æ¶ˆè´¹ï¼ˆæœªè¢«è¯»å–ï¼‰çš„æ•°æ®é‡ pendingData uint32 // åº”ç”¨å·²ç»æ¶ˆè´¹ä½†è¿˜æœªå‘é€windowUpdate frameçš„æ•°æ®é‡ï¼Œç”¨äºå‡ä½windowUpdate frameçš„å‘é€é¢‘ç‡ pendingUpdate uint32 // æ˜¯åœ¨limitåŸºç¡€ä¸Šé¢å¤–å¢åŠ çš„æ•°æ®é‡ï¼Œå½“åº”ç”¨è¯•ç€è¯»å–è¶…è¿‡limitå¤§å°çš„æ•°æ®æ˜¯ï¼Œä¼šä¸´æ—¶åœ¨limitä¸Šå¢åŠ deltaï¼Œæ¥å…è®¸åº”ç”¨è¯»å–æ•°æ® delta uint32 } steam levelçš„æµé‡æ§åˆ¶ä¸å…‰è¦è®°å½•å·²ç»æ”¶åˆ°çš„æ•°æ®é‡ï¼Œè¿˜éœ€è¦è®°å½•è¢«streamæ¶ˆè´¹æ‰çš„æ•°æ®é‡ï¼Œä»¥è¾¾åˆ°æ›´ç²¾å‡†çš„æµé‡æ§åˆ¶ï¼Œå¯¹åº”çš„æ•°æ®ç»“æ„ä¸ºinFlowã€‚ // å½“data frameè¢«æ¥æ”¶æ—¶ï¼Œè°ƒç”¨onDataæ›´æ–°pendingData func (f *inFlow) onData(n uint32) error { f.mu.Lock() f.pendingData += n if f.pendingData+f.pendingUpdate \u003e f.limit+f.delta { limit := f.limit rcvd := f.pendingData + f.pendingUpdate f.mu.Unlock() return fmt.Errorf(\"received %d-bytes data exceeding the limit %d bytes\", rcvd, limit) } f.mu.Unlock() return nil } å½“clientæ¥æ”¶åˆ°æ¥è‡ªserverçš„data frameçš„æ—¶å€™ï¼ŒpendingDataå¢åŠ æ¥æ”¶åˆ°çš„æ•°æ®é‡ã€‚ // å½“åº”ç”¨è¯»å–æ•°æ®æ—¶è°ƒç”¨onReadï¼Œè¿”å›å¢åŠ çš„çª—å£å¤§å° func (f *inFlow) onRead(n uint32) uint32 { f.mu.Lock() if f.pendingData == 0 { f.mu.Unlock() return 0 } f.pendingData -= n if n \u003e f.delta { n -= f.delta f.delta = 0 } else { f.delta -= n n = 0 } f.pendingUpdate += n if f.pendingUpdate \u003e= f.limit/4 { wu := f.pendingUpdate f.pendingUpdate = 0 f.mu.Unlock() return wu } f.mu.Unlock() return 0 } å½“åº”ç”¨è¯»å–nå­—èŠ‚æ•°æ®æ—¶ï¼ŒpendingDataå‡å»nï¼ŒpendingUpdateå¢åŠ nï¼Œå¦‚æœå­˜åœ¨deltaï¼Œåˆ™éœ€è¦å…ˆè¿˜æ¸…ä¹‹å‰deltaçš„æ¬ å€ºï¼Œç„¶åæ‰èƒ½å°†ä½™é¢å¢åŠ åˆ°pengingUpdateï¼Œå¦‚æœpendignUpdateè¶…è¿‡1/4 limitï¼Œè¿”å›pendingUpdateä½œä¸ºå¢åŠ çš„çª—å£å¤§å°ï¼Œå¯¹ç«¯å¯ä»¥ç»§ç»­åœ¨streamä¸Šå‘é€æ•°æ®ï¼Œè¿™ä¸€åˆ‡éƒ½æ˜¯ä¸ºäº†æ¸æ¸æ¶ˆé™¤ä¹‹å‰ä¸ºäº†å…è®¸serverå‘é€å¤§é‡æ•°æ®è€Œä¸´æ—¶å¢åŠ çš„é¢åº¦ã€‚ func (f *inFlow) maybeAdjust(n uint32) uint32 { if n \u003e uint32(math.MaxInt32) { n = uint32(math.MaxInt32) } f.mu.Lock() defer f.mu.Unlock() // æ¥æ”¶è€…çš„è§†è§’ä¸‹å‘é€è€…å¯ä»¥ç»§ç»­å‘é€çš„æœ€å¤§å­—èŠ‚æ•° estSenderQuota := int32(f.limit - (f.pendingData + f.pendingUpdate)) // å‡è®¾è¦è¯»å–nå­—èŠ‚é•¿åº¦çš„grpc messageï¼ŒestUntransmittedDataè¡¨ç¤ºå‘é€è€…å¯èƒ½è¿˜æ²¡æœ‰å‘é€çš„æœ€å¤§å­—èŠ‚æ•° estUntransmittedData := int32(n - f.pendingData) // è¿™æ„å‘³ç€é™¤éæˆ‘ä»¬å‘é€ä¸€ä¸ªwindow update frameï¼Œå¦åˆ™å‘é€è€…å¯èƒ½æ— æ³•å‘é€messageçš„æ‰€æœ‰å­—èŠ‚ // ç”±äºæœ‰æ¥è‡ªåº”ç”¨çš„æ´»è·ƒè¯»è¯·æ±‚ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å‘é€window update frameï¼Œå…è®¸è¶…è¿‡åŸå…ˆçš„limit if estUntransmittedData \u003e estSenderQuota { if f.limit+n \u003e maxWindowSize { f.delta = maxWindowSize - f.limit } else { // è¿™é‡Œæ›´æ–°çª—å£åˆ°å¯ä»¥æ¥å—messageï¼Œä¸»è¦è€ƒè™‘åˆ°messageå¯èƒ½å­˜åœ¨padding f.delta = n } return f.delta } return 0 } maybeAdjustçš„æ ¸å¿ƒé€»è¾‘æ˜¯ä¿è¯grpc messageä¸€å®šæœ‰è¶³å¤Ÿçš„çª—å£èƒ½å¤Ÿè¢«å‘é€ï¼Œé¿å…é™·å…¥åœæ»ï¼Œå¦‚æœç”±äºmessageéœ€è¦ä¸´æ—¶å¢åŠ çª—å£å¤§å°ï¼Œåˆ™å¢åŠ deltaï¼Œè€Œä¸æ˜¯limitã€‚æœ€ç»ˆå‘å¯¹ç«¯å‘é€window update frameï¼Œæç¤ºå¯¹ç«¯å¯ä»¥ç»§ç»­å‘é€æ•°æ®ã€‚ ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:5:4","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpcæµé‡æ§åˆ¶å°ç»“ æµé‡æ§åˆ¶ï¼Œä¸€èˆ¬æ˜¯æŒ‡åœ¨ç½‘ç»œä¼ è¾“è¿‡ç¨‹ä¸­ï¼Œå‘é€è€…ä¸»åŠ¨é™åˆ¶è‡ªèº«å‘é€æ•°æ®çš„é€Ÿç‡æˆ–è€…å‘é€çš„æ•°æ®é‡ï¼Œä»¥é€‚åº”æ¥æ”¶è€…å¤„ç†æ•°æ®çš„é€Ÿåº¦ï¼Œå½“æ¥æ”¶è€…çš„å¤„ç†é€Ÿåº¦è¾ƒæ…¢æ˜¯ï¼Œæ¥ä¸åŠå¤„ç†çš„æ•°æ®ä¼šè¢«å­˜æ”¾åœ¨å†…å­˜ä¸­ï¼Œè€Œå½“å†…å­˜ä¸­çš„æ•°æ®ç¼“å­˜åŒºè¢«å¡«æ»¡åï¼Œæ–°æ”¶åˆ°çš„æ•°æ®å°±ä¼šè¢«æ‰”æ‰ï¼Œå¯¼è‡´å‘é€è€…ä¸å¾—ä¸é‡æ–°å‘é€ï¼Œé€ æˆç½‘ç»œå¸¦å®½çš„æµªè´¹ã€‚ æµé‡æ§åˆ¶æ˜¯ä¸€ä¸ªç½‘ç»œç»„ä»¶çš„åŸºæœ¬åŠŸèƒ½ï¼Œæˆ‘ä»¬ç†ŸçŸ¥çš„TCPåè®®å°±è§„å®šäº†æµé‡æ§åˆ¶ç®—æ³•ï¼Œgrpcå»ºç«‹åœ¨TCPä¹‹ä¸Šï¼Œä¹Ÿä¾èµ–äºhttp2 WindowUupdate Frameå®ç°äº†è‡ªå·±åœ¨åº”ç”¨å±‚çš„æµé‡æ§åˆ¶ã€‚ åœ¨grpcä¸­ï¼Œæµé‡æ§åˆ¶ä½“ç°åœ¨ä¸‰ä¸ªç»´åº¦ï¼š é‡‡æ ·æµé‡æ§åˆ¶ï¼šgrpcæ¥æ”¶è€…æ£€æµ‹ä¸€æ®µæ—¶é—´å†…æ”¶åˆ°çš„æ•°æ®é‡ï¼Œä»è€Œæ¨æµ‹å‡ºbdpï¼Œå¹¶æŒ‡å¯¼å‘é€è€…è°ƒæ•´æµé‡æ§åˆ¶çª—å£ connection levelæµé‡æ§åˆ¶ï¼šå‘é€è€…åœ¨åˆå§‹åŒ–æ—¶è¢«åˆ†é…ä¸€å®šçš„quotaï¼Œquotaéšæ•°æ®å‘é€è€Œé™ä½ï¼Œå¹¶åœ¨æ”¶åˆ°æ¥æ”¶è€…çš„åé¦ˆä¹‹åå¢åŠ ï¼Œå‘é€è€…åœ¨è€—å°½quotaä¹‹åä¸èƒ½å†å‘é€æ•°æ® stream levelæµé‡æ§åˆ¶ï¼šå’Œconnection levelçš„æµé‡æ§åˆ¶ç±»ä¼¼ï¼Œåªä¸è¿‡connection levelç®¡ç†çš„æ˜¯ä¸€ä¸ªè¿æ¥çš„æ‰€æœ‰æµé‡ï¼Œè€Œstream levelç®¡ç†çš„æ˜¯connectionä¸­è¯¸å¤šstreamä¸­çš„ä¸€ä¸ªã€‚ grpcä¸­çš„æµé‡æ§åˆ¶ä»…é’ˆå¯¹HTTP2 data frameã€‚ ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:5:5","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc timeoutå®ç° Deadlineå¯¹äºä¸€ä¸ªç½‘ç»œæœåŠ¡æ¥è¯´å¾ˆé‡è¦ï¼Œclientå¯ä»¥æŒ‡å®šä¸€ä¸ªdeadlineï¼Œä»è€Œå½“æ—¶é—´è¶…è¿‡åï¼Œå¯ä»¥åŠæ—¶æ”¾å¼ƒè¯·æ±‚ï¼Œè¿”å›DEADLINE_EXCEEDEDã€‚å¯ä»¥è§£å†³ç±»ä¼¼äº å°¾éƒ¨å»¶è¿Ÿï¼ŒæŸäº›è¯·æ±‚ç›¸æ¯”äºå…¶ä»–è¯·æ±‚èŠ±è´¹å¤ªå¤šæ—¶é—´æ‰è¿”å› é¿å…å®¢æˆ·ç«¯æ— æ„ä¹‰é˜»å¡ç­‰å¾…ï¼Œæ¯”å¦‚æœåŠ¡å™¨å·²ç»æŒ‚æ‰äº†ï¼Œç­‰å¾…å·²ç»æ²¡æœ‰æ„ä¹‰äº† é¿å…èµ„æºçš„ä¸åˆç†å ç”¨ï¼Œrpcè¯·æ±‚å¯èƒ½ä¼šæŒæœ‰ä¸€äº›èµ„æºï¼Œé€šè¿‡åŠæ—¶ä¸­æ–­ï¼Œå¯ä»¥é‡Šæ”¾è¿™äº›èµ„æº æ€ä¹ˆå¾—åˆ°ä¸€ä¸ªåˆç†çš„deadlinesï¼Œéœ€è¦è€ƒè™‘å¤šæ–¹é¢å› ç´ ï¼ŒåŒ…æ‹¬æ•´ä¸ªç³»ç»Ÿçš„ç«¯åˆ°ç«¯å»¶è¿Ÿï¼Œå“ªäº›RPCæ˜¯ä¸²è¡Œçš„ï¼Œå“ªäº›æ˜¯å¹¶è¡Œçš„ï¼Œç„¶åå°è¯•ä¼°ç®—æ¯ä¸€é˜¶æ®µçš„è€—æ—¶ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªç²—ç•¥çš„ä¼°è®¡ã€‚ åœ¨grpcä¸­ï¼Œclientå’Œserverä¼šåˆ†åˆ«ç‹¬ç«‹å’Œå±€åœ°çš„åˆ¤æ–­rpcè°ƒç”¨æ˜¯å¦æˆåŠŸï¼Œè¿™æ„å‘³ç€clientå’Œserverå¾—åˆ°çš„ç»“è®ºå¯èƒ½ä¸ä¸€è‡´ï¼Œä¸€ä¸ªåœ¨serverç«¯æˆåŠŸçš„rpcè°ƒç”¨å¯èƒ½åœ¨clientç«¯è¢«è®¤ä¸ºæ˜¯å¤±è´¥çš„ï¼Œæ¯”å¦‚æœåŠ¡å™¨å¯ä»¥å‘é€å“åº”ï¼Œä½†å“åº”è¾¾åˆ°æ˜¯clientçš„è¶…æ—¶å·²ç»è§¦å‘ï¼Œclientæœ€ç»ˆä¼šç»ˆæ­¢å½“å‰rpcè°ƒç”¨è°ƒç”¨å¹¶è¿”å›DEADLINE_EXCEEDEDã€‚ clientDeadline := time.Now().Add(time.Duration(*deadlineMs) * time.Millisecond) ctx, cancel := context.WithDeadline(ctx, clientDeadline) åœ¨goä¸­é€šè¿‡å¯¹ctxæŒ‡å®šè¶…æ—¶æ—¶é—´æ¥è®¾ç½®grpcè¶…æ—¶ã€‚ response = blockingStub.withDeadlineAfter(deadlineMs, TimeUnit.MILLISECONDS).sayHello(request); åœ¨javaä¸­é€šè¿‡è°ƒç”¨client stubçš„æ–¹æ³•withDeadLineAfteræ¥è®¾ç½®è¶…æ—¶æ—¶é—´ã€‚ åœ¨æœåŠ¡ç«¯ï¼Œserverå¯ä»¥æŸ¥è¯¢æŸä¸ªrpcæ˜¯å¦å·²ç»è¶…æ—¶ï¼Œåœ¨serverå¯ä»¥å¤„ç†rpcè¯·æ±‚æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦clientè¿˜åœ¨ç­‰å¾…éå¸¸é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨åšä¸€äº›å¾ˆè´¹æ—¶é—´çš„å¤„ç†æ—¶ã€‚ if ctx.Err() == context.Canceled { return status.New(codes.Canceled, \"Client cancelled, abandoning.\") } if (Context.current().isCancelled()) { responseObserver.onError(Status.CANCELLED.withDescription(\"Cancelled by client\").asRuntimeException()); return; } grpc over http2è§„å®šäº†deadlineæ˜¯é€šè¿‡åœ¨è¯·æ±‚çš„Headers frameä¸­æŒ‡å®šgrpc-timeoutå­—æ®µå®ç°çš„ï¼Œå…¶å€¼çš„æ ¼å¼åŒ…å«ä¸¤éƒ¨åˆ†ï¼š TimeoutValue asciiå½¢å¼çš„æ­£æ•´æ•°å­—ç¬¦ä¸²ï¼Œæœ€å¤š8ä½ TimeoutUnit å¯ä»¥ä¸ºHour -\u003e H / Minute -\u003e M / Second -\u003e S / Millisecond -\u003e m / Microsecond -\u003e u / Nanosecond -\u003e n func (t *http2Server) operateHeaders(ctx context.Context, frame *http2.MetaHeadersFrame, handle func(*ServerStream)) error { streamID := frame.Header().StreamID s := \u0026ServerStream{ Stream: \u0026Stream{ id: streamID, buf: buf, fc: \u0026inFlow{limit: uint32(t.initialWindowSize)}, }, st: t, headerWireLength: int(frame.Header().Length), } // æ‰¾åˆ°grpc-timeoutå­—æ®µ for _, hf := range frame.Fields { case \"grpc-timeout\": timeoutSet = true var err error if timeout, err = decodeTimeout(hf.Value); err != nil { headerError = status.Newf(codes.Internal, \"malformed grpc-timeout: %v\", err) } // ä¸ºstreamè®¾ç½®deadline if timeoutSet { s.ctx, s.cancel = context.WithTimeout(ctx, timeout) } else { s.ctx, s.cancel = context.WithCancel(ctx) } // å¯åŠ¨ä¸€ä¸ªtimeråœ¨è¶…æ—¶çš„æƒ…å†µä¸‹å…³é—­stream if timeoutSet { // We need to wait for s.cancel to be updated before calling // t.closeStream to avoid data races. cancelUpdated := make(chan struct{}) timer := internal.TimeAfterFunc(timeout, func() { \u003c-cancelUpdated // æœ€ç»ˆä¼šå‘é€http2 RST frameå…³é—­stream t.closeStream(s, true, http2.ErrCodeCancel, false) }) oldCancel := s.cancel s.cancel = func() { oldCancel() timer.Stop() } close(cancelUpdated) } serverç«¯è·å¾—headers frameåï¼Œåœ¨operateHeadersä¸­è¿›è¡Œå¤„ç†ã€‚ ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:6:0","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc deadlineåœ¨javaä¸­çš„å®ç° private static \u003cV\u003e V getUnchecked(Future\u003cV\u003e future) { try { return future.get(); } catch (InterruptedException e) { // æ¢å¤ä¸­æ–­ Thread.currentThread().interrupt(); // æŠ›å‡ºStatusRuntimeException throw Status.CANCELLED .withDescription(\"Thread interrupted\") .withCause(e) .asRuntimeException(); } catch (ExecutionException e) { throw toStatusRuntimeException(e.getCause()); } } è¿”å›future.get()çš„ç»“æœï¼Œè€Œä¸”æ˜¯å¯ä¸­æ–­çš„ï¼Œé€‚ç”¨äºä¸ä¼šæŠ›å‡ºå—æ£€å¼‚å¸¸çš„ä»»åŠ¡ï¼Œå¦‚æœå‘ç”Ÿä¸­æ–­ï¼Œçº¿ç¨‹åœ¨æŠ›å‡ºå¼‚å¸¸å‰ä¼šå…ˆæ¢å¤ä¸­æ–­ã€‚ å¦‚æœgetæŠ›å‡ºCancellationExceptionï¼ŒåŸæ ·æŠ›å‡ºå¼‚å¸¸ å¦‚æœgetæŠ›å‡ºExecutionExceptionæˆ–è€…InterruptedExceptionï¼Œåˆ™æŠ›å‡ºStatusRuntimeException public class StatusRuntimeException extends RuntimeException { private static final long serialVersionUID = 1950934672280720624L; private final Status status; private final Metadata trailers; public StatusRuntimeException(Status status) { this(status, (Metadata)null); } public StatusRuntimeException(Status status, @Nullable Metadata trailers) { super(Status.formatThrowableMessage(status), status.getCause()); this.status = status; this.trailers = trailers; } public final Status getStatus() { return this.status; } @Nullable public final Metadata getTrailers() { return this.trailers; } } StatusRuntimeExceptioniæ˜¯Statusçš„RuntimeExceptionå½¢å¼ï¼Œä¸ºäº†èƒ½å¤Ÿé€šè¿‡å¼‚å¸¸ä¼ æ’­Statusã€‚ public final class Status { private final Code code; private final String description; private final Throwable cause; Statuså®šä¹‰äº†æ“ä½œçš„çŠ¶æ€é€šè¿‡æä¾›æ ‡å‡†çš„Codeå’Œå¯é€‰çš„æè¿°ã€‚å¯¹äºå®¢æˆ·ç«¯ï¼Œæ¯ä¸ªè¿œç¨‹è°ƒç”¨è°ƒç”¨éƒ½ä¼šåœ¨å®Œæˆæ—¶è¿”å›ä¸€ä¸ªstatusï¼Œå¦‚æœå‘ç”Ÿé”™è¯¯statusä¼šè¢«ä¼ æ’­åˆ°blocking stubä½œä¸ºStatusRuntimeExcpetionï¼Œæˆ–è€…ä½œä¸ºlistenerçš„æ˜¾å¼å‚æ•°ï¼Œç±»ä¼¼çš„ï¼ŒæœåŠ¡ç«¯å¯ä»¥æŠ›å‡ºStatusRuntimeExceptionæˆ–è€…å°†statusä¼ é€’ç»™callbackå‡½æ•°ã€‚ Codeæ˜¯ä¸€ä¸ªenumç±»å‹ï¼Œè¿™é‡Œåˆ—å‡ºä¸€äº›å€¼å¾—æ›´å¤šå…³æ³¨çš„codeï¼š OK æ“ä½œæˆåŠŸç»“æŸ CALCELLED æ“ä½œè¢«å–æ¶ˆï¼Œä¸€èˆ¬æ˜¯è¢«è°ƒç”¨è€…å–æ¶ˆ UNKNOWN æœªçŸ¥é”™è¯¯ INVALID_ARGUMENT å®¢æˆ·å•æä¾›äº†æ— æ•ˆçš„å‚æ•° DEADLINE_EXCEEDED åœ¨æ“ä½œå®Œæˆå‰è¶…æ—¶ UNIMPLEMENTED æœåŠ¡ä¸­çš„çš„æ“ä½œæœªå®ç° INTERNAL å†…éƒ¨é”™è¯¯ï¼Œè¡¨ç¤ºåº•å±‚ç³»ç»Ÿæ‰€æœŸæœ›çš„ä¸€äº›ä¸å˜é‡é­åˆ°äº†ç ´å UNAVAILABLE æœåŠ¡å½“å‰ä¸å¯ç”¨ï¼Œå¯èƒ½æ˜¯ç¬æ—¶é”™è¯¯ï¼Œå¯ä»¥é€šè¿‡backoffé‡è¯•çº æ­£ï¼Œç„¶åå¯¹äºéå¹‚ç­‰æ“ä½œé‡è¯•ä¸ä¸€å®šå®‰å…¨ ListenableFuture public interface ListenableFuture\u003cV extends @Nullable Object\u003e extends Future\u003cV\u003e { void addListener(Runnable listener, Executor executor); } googleåœ¨ListenableFutureExplainedæ–‡ç« ä¸­æ¨èæ€»æ˜¯ä½¿ç”¨ListenableFutureè€Œä¸æ˜¯Futureï¼Œå¹¶ç»™å‡ºäº†ä»¥ä¸‹åŸå› ï¼š å¤§å¤šæ•°Futuresæ–¹æ³•éœ€è¦ListenableFutures çœå»åç»­æ›´æ”¹ä¸ºListenableFutureçš„éº»çƒ¦ æä¾›å·¥å…·æ–¹æ³•æ—¶ä¸å†éœ€è¦æä¾›Futureå’ŒListenableFutureä¸¤ç§å˜ä½“ ä¼ ç»Ÿçš„Futureè¡¨ç¤ºå¼‚æ­¥è®¡ç®—çš„ç»“æœï¼Œä¸€ä¸ªè®¡ç®—å¯èƒ½ä¹Ÿå¯èƒ½è¿˜æ²¡æœ‰äº§ç”Ÿç»“æœï¼ŒFutureå¯ä»¥ä½œä¸ºæ­£åœ¨è¿›è¡Œä¸­çš„è®¡ç®—çš„å¥æŸ„ï¼ŒæœåŠ¡æ‰¿è¯ºæœªæ¥æä¾›ç»“æœç»™æˆ‘ä»¬ã€‚ ListenableFutureå…è®¸æ³¨å†Œå›è°ƒå‡½æ•°ï¼Œä¸€æ—¦è®¡ç®—å®Œæˆï¼Œè¿™äº›å›è°ƒå‡½æ•°å°†è¢«æ‰§è¡Œï¼Œæˆ–è€…å¦‚æœæ³¨å†Œå›è°ƒå‡½æ•°æ—¶è®¡ç®—å·²ç»å¼€å§‹ï¼Œåˆ™ç«‹å³å¼€å§‹æ‰§è¡Œã€‚ addListenræ–¹æ³•è¡¨ç¤ºå½“futureå®Œæˆæ—¶ï¼Œæ³¨å†Œçš„å›è°ƒå‡½æ•°å°†åœ¨æä¾›çš„çº¿ç¨‹æ± ä¸­è¢«æ‰§è¡Œã€‚ æ¨èé€šè¿‡Futures.addCallback(ListenableFuture\u003cV\u003e, FutureCallback\u003cV\u003e, Executor)æ·»åŠ å›è°ƒå‡½æ•°ï¼ŒFutureCallback\u003cV\u003eå®ç°äº†ä¸¤ä¸ªæ–¹æ³• onSuccess(V) å½“futureæˆåŠŸååŸºäºæ‰§è¡Œç»“æœæ‰§è¡Œè¡ŒåŠ¨ onFailure(Throwable) åœ¨futureå¤±è´¥æ—¶åŸºäºå¤±è´¥åŸå› æ‰§è¡Œè¡ŒåŠ¨ å¯¹åº”äºJDKé€šè¿‡ExecutorService.submit(Callable)åˆå§‹åŒ–ä¸€ä¸ªå¼‚æ­¥è®¡ç®—ï¼Œguavaæä¾›äº†ListerningExecutorServiceæ¥å£ï¼Œåœ¨ä»»ä½•ExecutorServiceè¿”å›Futureçš„åœ°æ–¹éƒ½æ”¹æˆè¿”å›ListenableFutureã€‚å¯ä»¥é€šè¿‡ä½¿ç”¨MoreExecutors.listerningDecorator(ExecutorSerivce)å°†ExecutorServiceè½¬æ¢æˆListerningExecutorServiceã€‚ å¦‚æœä½ æ‰“ç®—è½¬æ¢FutureTaskï¼Œguavaæä¾›äº†ListenableFutureTask.create(Callable\u003cV\u003e)å’ŒListenableFutureTask.create(Runnable, V)ï¼Œä¸åŒäºjdkï¼ŒListenableFutureTaskä¸å¸Œæœ›è¢«ç›´æ¥ç»§æ‰¿ã€‚ å¦‚æœä½ éœ€è¦çš„futureæŠ½è±¡å¸Œæœ›ç›´æ¥è®¾ç½®futureçš„å€¼è€Œä¸æ˜¯å®ç°ä¸€ä¸ªæ–¹æ³•å»è®¡ç®—è¿™ä¸ªå€¼ï¼Œå¯ä»¥è€ƒè™‘æ‹“å±•AbstractFuture\u003cV\u003eæˆ–è€…ç›´æ¥ä½¿ç”¨SettableFutureã€‚ å¦‚æœä½ å¿…é¡»å°†å…¶ä»–APIæä¾›çš„futureè½¬æ¢æˆListenableFutureï¼Œé‚£ä¹ˆä½ å¯èƒ½åˆ«æ— é€‰æ‹©ï¼Œåªèƒ½ä½¿ç”¨è¾ƒä¸ºé‡é‡çº§çš„JdkFutureAdapters.listenInPoolThread(Future) æ–¹æ³•æ¥å®Œæˆè½¬æ¢ã€‚ä½†åœ¨å¯èƒ½çš„æƒ…å†µä¸‹ï¼Œå»ºè®®ä½ ä¿®æ”¹åŸå§‹ä»£ç ï¼Œä½¿å…¶ç›´æ¥è¿”å› ListenableFutureã€‚ æ¨èä½¿ç”¨ListenableFutureçš„æœ€é‡è¦åŸå› ä¸ºå®ƒä½¿å¾—æ„å»ºå¤æ‚çš„å¼‚æ­¥æ“ä½œé“¾å˜å¾—å¯è¡Œï¼Œç±»ä¼¼äºJDKä¸­æä¾›äº†CompletableFutureã€‚ GrpcFuture private static final class GrpcFuture\u003cRespT\u003e extends AbstractFuture\u003cRespT\u003e { private final ClientCall\u003c?, RespT\u003e call; // Non private to avoid synthetic class GrpcFuture(ClientCall\u003c?, RespT\u003e call) { this.call = call; } @Override protected void interruptTask() { call.cancel(\"GrpcFuture was cancelled\", null); } @Override protected boolean set(@Nullable RespT resp) { return super.set(resp); } @Override protected boolean setException(Throwable throwable) { return super.setException(throwable); } @SuppressWarnings(\"MissingOverride\") // Add @Override once Java 6 support is dropped protected String pendingToString() { return MoreObjects.toStringHelper(this).add(\"clientCall\", call).toString(); } } GrpcFutureç»§æ‰¿äº†AbstractFutureï¼Œå¯ä»¥é€šè¿‡interruptTaskå–æ¶ˆgrpcè°ƒç”¨ï¼Œé€šè¿‡setæˆ–è€…setExceptionæ–¹æ³•ç›´æ¥è®¾ç½®futureçš„","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:6:1","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc keepaliveå®ç° [grpc Keepaliveæ˜¯ä¸€ç§åœ¨http2è¿æ¥ç©ºé—²ï¼ˆæ²¡æœ‰æ•°æ®ä¼ è¾“ï¼‰æ˜¯ä¿æŒè¿æ¥æ´»åŠ¨çŠ¶æ€çš„æŠ€æœ¯ï¼Œé€šè¿‡å®šæœŸå‘é€pingå¸§æ¥å®ç°ã€‚http2ä¿æ´»æœºåˆ¶èƒ½å¤Ÿæå‡http2è¿æ¥çš„æ€§èƒ½å’Œå¯é æ€§ï¼Œä½†éœ€è¦ä»”ç»†é…ç½®ä¿æ´»é—´éš”æ—¶é—´ã€‚ ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:7:0","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"å‚è€ƒæ–‡çŒ® gprcæºç åˆ†æ zhengxinzx ä¸€ç³»åˆ—grpcæºç åˆ†æï¼Œä¸»è¦ä»‹ç»äº†grpcçš„åŸç†å’Œæµé‡æ§åˆ¶ï¼Œå¼ºçƒˆæ¨è grpc over http2 åŸºäºhttp2å®ç°grpcåè®®è§„èŒƒ ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:8:0","tags":["grpc"],"title":"Grpcæºç åˆ†æ","uri":"/posts/grpc-in-practice/"},{"categories":null,"content":"é˜…å‰é¡»çŸ¥ ","date":"2025-04-28","objectID":"/posts/java-concurrency-jmm/:1:0","tags":null,"title":"Javaå†…å­˜æ¨¡å‹","uri":"/posts/java-concurrency-jmm/"},{"categories":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"content":"é˜…å‰æç¤º å‚è€ƒæ–‡çŒ®ä¸­çš„æ–‡ç« éå¸¸çš„å¥½ï¼ŒåŸºæœ¬çœ‹å®Œäº†å°±èƒ½ç†è§£å¾ˆå¤šä¸œè¥¿ï¼Œæ¨èé˜…è¯» æºç ä¸­ä¹Ÿæä¾›äº†å¾ˆå¤šæ³¨é‡Šæ–‡æœ¬ï¼Œæ¨èå¯¹ç…§æºç å­¦ä¹ ã€‚ ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:1:0","tags":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"title":"Javaçº¿ç¨‹æ± æºç åˆ†æ","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"content":"é‡è¦æ¦‚å¿µå’Œæ¥å£ ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:2:0","tags":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"title":"Javaçº¿ç¨‹æ± æºç åˆ†æ","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"content":"Runnable æ¥å£ @FunctionalInterface public interface Runnable { public abstract void run(); } çº¿ç¨‹å¯ä»¥æ¥å—ä¸€ä¸ªå®ç° Runnable æ¥å£çš„å¯¹è±¡ï¼Œå¹¶æ‰§è¡Œå¯¹åº”çš„é€»è¾‘ã€‚ ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:2:1","tags":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"title":"Javaçº¿ç¨‹æ± æºç åˆ†æ","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"content":"Callable æ¥å£ @FunctionalInterface public interface Callable\u003cV\u003e { V call() throws Exception; } ç±»ä¼¼Runnalbeæ¥å£ï¼Œä½†å¯ä»¥è¿”å›ç»“æœå’ŒæŠ›å‡ºå¼‚å¸¸ ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:2:2","tags":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"title":"Javaçº¿ç¨‹æ± æºç åˆ†æ","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"content":"Future æ¥å£ è¡¨ç¤ºå¼‚æ­¥æ‰§è¡Œçš„ç»“æœï¼Œæä¾›äº†è·å–ç»“æœä»¥åŠå–æ¶ˆè®¡ç®—æ‰§è¡Œç­‰æ–¹æ³•ã€‚ public interface Future\u003cV\u003e { // å–æ¶ˆä»»åŠ¡æ‰§è¡Œï¼ŒmayInterruptIfRunningå‚æ•°ä¸ºtrueæ—¶å°†ä¸­æ–­æ­£åœ¨æ‰§è¡Œä»»åŠ¡çš„çº¿ç¨‹ï¼Œå¦åˆ™æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡å°†ç»§ç»­æ‰§è¡Œ boolean cancel(boolean mayInterruptIfRunning); // æ˜¯å¦ä»»åŠ¡åœ¨æ‰§è¡Œå®Œæˆå‰è¢«å–æ¶ˆ boolean isCancelled(); // ä»»åŠ¡æ˜¯å¦å®Œæˆï¼Œä¸ç®¡ä»»åŠ¡æ­£å¸¸ç»“æŸã€æŠ›å‡ºå¼‚å¸¸è¿˜æ˜¯è¢«å–æ¶ˆéƒ½è®¤ä¸ºä»»åŠ¡å®Œæˆ boolean isDone(); // ç­‰å¾…ä»»åŠ¡å®Œæˆå¹¶è·å¾—ç»“æœ // è®¡ç®—è¢«å–æ¶ˆæ—¶æŠ›å‡ºCalcellationException // è®¡ç®—æŠ›å‡ºå¼‚å¸¸æ—¶æŠ›å‡ºExecutionException // å½“å‰çº¿ç¨‹ç­‰å¾…æ—¶è¢«ä¸­æ–­æŠ›å‡ºInteruptedException V get() throws InterruptedException, ExecutionException; // è¶…æ—¶ç‰ˆæœ¬çš„getï¼Œå¦‚æœç­‰å¾…è¶…æ—¶æŠ›å‡ºTimeoutException V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; } ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:2:3","tags":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"title":"Javaçº¿ç¨‹æ± æºç åˆ†æ","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"content":"Executor æ¥å£ public interface Executor { void execute(Runnable command); } çº¿ç¨‹æ± åŸºç¡€æ¥å£ï¼Œæäº¤ä¸€ä¸ªä»»åŠ¡åˆ°çº¿ç¨‹æ± æ‰§è¡Œã€‚ Memory consistency effects: Actions in a thread prior to submitting a Runnable object to an Executor happen-before its execution begins, perhaps in another thread. ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:2:4","tags":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"title":"Javaçº¿ç¨‹æ± æºç åˆ†æ","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"content":"ExecutorSerivce æ¥å£ ExecutorServiceç»§æ‰¿äº†Executoræ¥å£ï¼Œé€šå¸¸æˆ‘ä»¬ä½¿ç”¨ExecutorServiceä½œä¸ºçº¿ç¨‹æ± æ¥å£ï¼Œå®ƒæä¾›äº†ä¸°å¯Œçš„åŠŸèƒ½ï¼Œä¸€èˆ¬èƒ½å¤Ÿæ»¡è¶³éœ€æ±‚ã€‚ public interface ExecutorService extends Executor { // å·²æäº¤çš„ä»»åŠ¡ç»§ç»­æ‰§è¡Œï¼Œä½†ä¸å†æ¥æ”¶æ–°ä»»åŠ¡ï¼Œç­‰å¾…æ­£åœ¨æ‰§è¡Œä»»åŠ¡ç»ˆæ­¢è¯·ä½¿ç”¨awaitTermination void shutdown(); // å°è¯•åœæ­¢æ‰€æœ‰æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ï¼Œç»ˆæ­¢æ‰€æœ‰ç­‰å¾…ä»»åŠ¡çš„å¤„ç†ï¼Œè¿”å›ç­‰å¾…ä»»åŠ¡åˆ—è¡¨ï¼Œç­‰å¾…æ­£åœ¨æ‰§è¡Œä»»åŠ¡ç»ˆæ­¢è¯·ä½¿ç”¨awaitTermination // æ— æ³•ä¿è¯æ‰€ä»¥ä»»åŠ¡éƒ½èƒ½ç»ˆæ­¢ï¼Œç»å…¸çš„å®ç°ä¼šé€šè¿‡`interrupt`å–æ¶ˆä»»åŠ¡ï¼Œä½†å¦‚æœä»»åŠ¡ä¸å“åº”ä¸­æ–­ï¼Œåˆ™å¯èƒ½æ°¸è¿œéƒ½ä¸ä¼šåœæ­¢ List\u003cRunnable\u003e shutdownNow(); // çº¿ç¨‹æ± æ˜¯å¦å…³é—­ boolean isShutdown(); // shutdownåæ‰€æœ‰ä»»åŠ¡æ˜¯å¦å·²ç»ç»“æŸ // è¿™ä¸ªæ–¹æ³•åªæœ‰åœ¨è°ƒç”¨shutdownæˆ–è€…shutdownNowåæ‰å¯èƒ½è¿”å›true boolean isTerminated(); // åœ¨shutdownåè°ƒç”¨ï¼Œé˜»å¡ç›´åˆ°æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæˆæˆ–è€…è¶…æ—¶å‘ç”Ÿæˆ–è€…å½“å‰çº¿ç¨‹è¢«ä¸­æ–­ boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; // æäº¤ä¸€ä¸ªæœ‰è¿”å›å€¼çš„ä»»åŠ¡ï¼Œé€šè¿‡Futureå¯¹è±¡è·å–è¿”å›å€¼ // taskä¸èƒ½ä¸ºnullï¼Œå¦åˆ™æŠ›å‡ºNullPointerException // å¦‚æœä»»åŠ¡ä¸èƒ½è¢«è°ƒç”¨æ‰§è¡Œï¼ŒæŠ›å‡º RejectedExecutionException \u003cT\u003e Future\u003cT\u003e submit(Callable\u003cT\u003e task); // ç±»ä¼¼ submit(Callable)ï¼Œè¿”å›å€¼å¯¹åº”ä¼ å…¥çš„resultå‚æ•° \u003cT\u003e Future\u003cT\u003e submit(Runnable task, T result); // ç±»ä¼¼ submit(Callable)ï¼Œè¿”å›å€¼ä¸ºnull Future\u003c?\u003e submit(Runnable task); // æ‰§è¡Œç»™å®šçš„ä»»åŠ¡åˆ—è¡¨ï¼Œå½“å…¨éƒ¨å®Œæˆæ—¶è¿”å›Futrueåˆ—è¡¨ï¼Œåœ¨æ“ä½œæ‰§è¡Œæ—¶ä¿®æ”¹ç»™å®šçš„é›†åˆä¼šå¯¼è‡´æœªå®šä¹‰è¡Œä¸º // åœ¨ç­‰å¾…æ—¶å‘ç”Ÿä¸­æ–­ï¼ŒæŠ›å‡ºInterruptedExceptionï¼Œå–æ¶ˆæœªå®Œæˆçš„ä»»åŠ¡ // ä»»åŠ¡åˆ—è¡¨å’Œå…¶ä¸­çš„ä»»åŠ¡éƒ½ä¸èƒ½ä¸ºnullï¼Œå¦åˆ™æŠ›å‡ºNullPointerException // å¦‚æœä»»ä½•ä»»åŠ¡ä¸èƒ½è¢«è°ƒåº¦æ‰§è¡Œï¼ŒæŠ›å‡ºRejectedExecutionException \u003cT\u003e List\u003cFuture\u003cT\u003e\u003e invokeAll(Collection\u003c? extends Callable\u003cT\u003e\u003e tasks) throws InterruptedException; // è¶…æ—¶ç‰ˆæœ¬çš„invokeAllï¼Œå¦‚æœè¶…æ—¶å‘ç”Ÿï¼Œå–æ¶ˆæœªå®Œæˆçš„ä»»åŠ¡ \u003cT\u003e List\u003cFuture\u003cT\u003e\u003e invokeAll(Collection\u003c? extends Callable\u003cT\u003e\u003e tasks, long timeout, TimeUnit unit) throws InterruptedException; // ç±»ä¼¼invokeAllï¼Œæ‰§è¡Œç»™å®šçš„ä»»åŠ¡åˆ—è¡¨ï¼Œè¿”å›ä¸€ä¸ªæˆåŠŸæ‰§è¡Œä»»åŠ¡çš„ç»“æœï¼ŒæŒ‡æ²¡æœ‰æŠ›å‡ºå¼‚å¸¸ï¼Œå–æ¶ˆæœªæ‰§è¡Œå®Œæˆçš„ä»»åŠ¡ // tasksä¸ºç©ºæ—¶æŠ›å‡ºIllegalArgumentException // å¦‚æœæ²¡æœ‰ä»»åŠ¡æˆåŠŸå®Œæˆï¼ŒæŠ›å‡ºExecutionException \u003cT\u003e T invokeAny(Collection\u003c? extends Callable\u003cT\u003e\u003e tasks) throws InterruptedException, ExecutionException; // è¶…æ—¶ç‰ˆæœ¬çš„invokeAny // è¶…æ—¶å‘ç”ŸæŠ›å‡ºTimeoutException \u003cT\u003e T invokeAny(Collection\u003c? extends Callable\u003cT\u003e\u003e tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; } Memory consistency effects: Actions in a thread prior to the submission of a Runnable or Callable task to an ExecutorService happen-before any actions taken by that task, which in turn happen-before the result is retrieved via Future. get(). Doug Lea ç»™äº†ä¸€ä¸ªç»ˆæ­¢çº¿ç¨‹æ± çš„ä¾‹å­ï¼Œé¦–å…ˆè°ƒç”¨shutdownæ‹’ç»æ¥å—æ–°ä»»åŠ¡ï¼Œç„¶åè°ƒç”¨shutdowNowï¼Œå–æ¶ˆé€—ç•™çš„ä»»åŠ¡ï¼Œè¿™é‡Œç‰¹åˆ«å¤„ç†äº†å½“å‰çº¿ç¨‹é‡åˆ°interruptçš„æƒ…å†µã€‚ void shutdownAndAwaitTermination(ExecutorService pool) { pool.shutdown(); // Disable new tasks from being submitted try { // Wait a while for existing tasks to terminate if (!pool.awaitTermination(60, TimeUnit.SECONDS)) { pool.shutdownNow(); // Cancel currently executing tasks // Wait a while for tasks to respond to being cancelled if (!pool.awaitTermination(60, TimeUnit.SECONDS)) System.err.println(\"Pool did not terminate\"); } } catch (InterruptedException ie) { // (Re-)Cancel if current thread also interrupted pool.shutdownNow(); // Preserve interrupt status Thread.currentThread().interrupt(); } } ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:2:5","tags":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"title":"Javaçº¿ç¨‹æ± æºç åˆ†æ","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"content":"FutureTask ç±»æºç è§£æ æ¥å£RunnableFutureæ˜¯Runnalbeçš„Futureï¼Œrunæ–¹æ³•çš„æˆåŠŸæ‰§è¡Œå¯¹åº”Futureçš„å®Œæˆï¼Œå¹¶å…è®¸è·å–ç»“æœã€‚ public interface RunnableFuture\u003cV\u003e extends Runnable, Future\u003cV\u003e { void run(); } FutureTaskç±»å®ç°äº†RunnableFutureï¼ŒFutureTaskçš„å…·ä½“å®ç°åŸç†ç•™åœ¨åé¢å†è®²ã€‚(todo) ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:3:0","tags":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"title":"Javaçº¿ç¨‹æ± æºç åˆ†æ","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"content":"AbstractExecutorService æºç è§£æ AbstractExecutorServiceæŠ½è±¡ç±»æ´¾ç”Ÿè‡ªExecutorServiceæ¥å£ï¼Œç„¶ååœ¨å…¶åŸºç¡€ä¸Šå®ç°äº†å‡ ä¸ªå®ç”¨çš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•æä¾›ç»™å­ç±»è¿›è¡Œè°ƒç”¨ã€‚ æŠ½è±¡ç±»å®ç°äº† invokeAny å’Œ invokeAll æ–¹æ³•ï¼ˆè¿™ä¸¤ä¸ªæ–¹æ³•å…ˆä¸çœ‹ todoï¼‰ï¼Œæ–¹æ³•newTaskForç”¨äºå°†Runnableæˆ–è€…CallableåŒ…è£…æˆFutureTaskã€‚æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± ä¸­æœ‰ä¸¤ç±»æ–¹æ³•ï¼Œsubmitç”¨äºéœ€è¦è¿”å›å€¼çš„åœºæ™¯ï¼Œexecuteç”¨äºä¸éœ€è¦è¿”å›å€¼çš„åœºæ™¯ï¼Œå½“ç„¶å¯ä»¥éƒ½åªç”¨submitæ–¹æ³•ï¼Œå½“ä¸éœ€è¦è¿”å›å€¼æ—¶è¿”å› null å³å¯ã€‚ public abstract class AbstractExecutorService implements ExecutorService { // å°†runnableåŒ…è£…æˆFutureTask protected \u003cT\u003e RunnableFuture\u003cT\u003e newTaskFor(Runnable runnable, T value) { return new FutureTask\u003cT\u003e(runnable, value); } // å°†CallableåŒ…è£…æˆFutureTask protected \u003cT\u003e RunnableFuture\u003cT\u003e newTaskFor(Callable\u003cT\u003e callable) { return new FutureTask\u003cT\u003e(callable); } // åŒ…è£…æˆFutureTaskï¼Œå¹¶äº¤ç»™åº•å±‚executeæ–¹æ³•æ‰§è¡Œ public Future\u003c?\u003e submit(Runnable task) { if (task == null) throw new NullPointerException(); RunnableFuture\u003cVoid\u003e ftask = newTaskFor(task, null); execute(ftask); return ftask; } public \u003cT\u003e Future\u003cT\u003e submit(Runnable task, T result) { if (task == null) throw new NullPointerException(); RunnableFuture\u003cT\u003e ftask = newTaskFor(task, result); execute(ftask); return ftask; } public \u003cT\u003e Future\u003cT\u003e submit(Callable\u003cT\u003e task) { if (task == null) throw new NullPointerException(); RunnableFuture\u003cT\u003e ftask = newTaskFor(task); execute(ftask); return ftask; } ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:4:0","tags":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"title":"Javaçº¿ç¨‹æ± æºç åˆ†æ","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"content":"ThreadPoolExecutor ThreadPoolExecutoræ˜¯ JDK ä¸­çš„çº¿ç¨‹æ± å®ç°ï¼Œå®ç°äº†ä»»åŠ¡æäº¤ã€çº¿ç¨‹ç®¡ç†ã€ç›‘æ§ç­‰æ–¹æ³•ã€‚ é€šè¿‡æ„é€ å‡½æ•°ï¼Œä»‹ç»ä¸€äº›é‡è¦çš„å±æ€§ï¼š corePoolSize æ ¸å¿ƒçº¿ç¨‹æ•°ï¼Œæ³¨æ„æœ‰æ—¶å°†æ ¸å¿ƒçº¿ç¨‹æ•°å†…çš„çº¿ç¨‹ç§°ä¸ºæ ¸å¿ƒçº¿ç¨‹ï¼Œä½†æ ¸å¿ƒçº¿ç¨‹æœ¬èº«å’Œå…¶ä»–çº¿ç¨‹ä¸€æ · maximumPoolSize æœ€å¤§çº¿ç¨‹æ•° workQueue ä»»åŠ¡é˜Ÿåˆ—ï¼ŒBlockingQueue æ¥å£çš„æŸä¸ªå®ç°ï¼ˆå¸¸ç”¨ ArrayBlockingQueue å’Œ LinkedBlockingQueueï¼‰ keepAliveTime ç©ºé—²çº¿ç¨‹çš„ä¿æ´»çº¿ç¨‹ï¼Œé»˜è®¤åªå¯¹éæ ¸å¿ƒçº¿ç¨‹ç”Ÿæ•ˆï¼Œå¯ä»¥é€šè¿‡è®¾ç½®allowCoreThreadTimeout(true)ä½¿æ ¸å¿ƒçº¿ç¨‹æ•°å†…çš„çº¿ç¨‹å¯ä»¥è¢«å›æ”¶ threadFactory ç”¨äºç”Ÿæˆçº¿ç¨‹ï¼Œæ¯”å¦‚è®¾ç½®çº¿ç¨‹çš„åå­— handler è®¾ç½®çº¿ç¨‹æ± çš„æ‹’ç»ç­–ç•¥ Doug Lea é‡‡ç”¨ä¸€ä¸ª 32 ä¸ºçš„æ•´æ•°æ¥å­˜æ”¾çº¿ç¨‹æ± çŠ¶æ€å’Œçº¿ç¨‹æ± ä¸­çš„çº¿ç¨‹æ•°ï¼Œå…¶ä¸­é«˜ 3 ä¸ºç”¨äºå­˜æ”¾çº¿ç¨‹æ± çŠ¶æ€ï¼Œä½ 29 ä½è¡¨ç¤ºçº¿ç¨‹æ•°ã€‚ private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); private static final int COUNT_BITS = Integer.SIZE - 3; private static final int CAPACITY = (1 \u003c\u003c COUNT_BITS) - 1; // runState is stored in the high-order bits private static final int RUNNING = -1 \u003c\u003c COUNT_BITS; private static final int SHUTDOWN = 0 \u003c\u003c COUNT_BITS; private static final int STOP = 1 \u003c\u003c COUNT_BITS; private static final int TIDYING = 2 \u003c\u003c COUNT_BITS; private static final int TERMINATED = 3 \u003c\u003c COUNT_BITS; // Packing and unpacking ctl private static int runStateOf(int c) { return c \u0026 ~CAPACITY; } private static int workerCountOf(int c) { return c \u0026 CAPACITY; } private static int ctlOf(int rs, int wc) { return rs | wc; } /* * Bit field accessors that don't require unpacking ctl. * These depend on the bit layout and on workerCount being never negative. */ private static boolean runStateLessThan(int c, int s) { return c \u003c s; } private static boolean runStateAtLeast(int c, int s) { return c \u003e= s; } private static boolean isRunning(int c) { return c \u003c SHUTDOWN; } çº¿ç¨‹æ± å„ç§çŠ¶æ€çš„ä»‹ç»ï¼š RUNNING: æ¥å—æ–°çš„ä»»åŠ¡ï¼Œå¤„ç†ç­‰å¾…é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡ SHUTDWON: ä¸æ¥å—æ–°çš„ä»»åŠ¡ï¼Œä½†ä¼šç»§ç»­å¤„ç†ç­‰å¾…é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡ STOP: ä¸æ¥å—æ–°çš„ä»»åŠ¡æäº¤ï¼Œä¸å†å¤„ç†ç­‰å¾…é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡ï¼Œä¸­æ–­æ­£åœ¨æ‰§è¡Œçš„çº¿ç¨‹ TIDYING: æ‰€æœ‰çš„ä»»åŠ¡éƒ½é”€æ¯äº†ï¼ŒworkCount ä¸º 0ï¼Œæ‰§è¡Œé’©å­æ–¹æ³• terminated() TERMINATED: terminated()æ–¹æ³•è°ƒç”¨ç»“æŸåï¼Œçº¿ç¨‹æ± çš„çŠ¶æ€åˆ‡æ¢ä¸ºæ­¤ RUNNING å®šä¹‰ä¸º-1ï¼ŒSHUTDOWN å®šä¹‰ä¸º 0ï¼Œå…¶ä»–éƒ½æ¯” 0 å¤§ï¼Œæ‰€ä»¥ç­‰äº 0 æ—¶ä¸èƒ½æäº¤ä»»åŠ¡ï¼Œå¤§äº 0 çš„è¯ï¼Œè¿æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ä¹Ÿè¦ä¸­æ–­ çŠ¶æ€è¿ç§»è¿‡ç¨‹ï¼š RUNNING -\u003e SHUTDOWNï¼Œè°ƒç”¨ shutdown() (RUNNING or SHUTDOWN) -\u003e STOP: è°ƒç”¨ shutdownNow() SHUTDOWN -\u003e TIDYING: å½“ä»»åŠ¡é˜Ÿåˆ—å’Œçº¿ç¨‹æ± éƒ½æ¸…ç©ºåï¼Œæœ‰ SHUTDOWN è½¬æ¢ä¸º TIDYING STOP -\u003e TIDYING: ä»»åŠ¡é˜Ÿåˆ—æ¸…ç©ºå TIDYING -\u003e TERMINATED: terminated()æ–¹æ³•ç»“æŸå Doug Lea å°†çº¿ç¨‹æ± ä¸­çš„çº¿ç¨‹åŒ…è£…æˆå†…éƒ¨ç±» Workerï¼Œæ‰€ä»¥ä»»åŠ¡æ˜¯ Runnable ï¼ˆå†…éƒ¨å˜é‡åå« task æˆ–è€… command)ï¼Œçº¿ç¨‹æ˜¯ worker AQS: todo worker çš„å®ç°åŒ…å«å¤æ‚çš„å¹¶å‘æ§åˆ¶ï¼Œè¿™äº›æš‚æ—¶ä¸è€ƒè™‘ private final class Worker extends AbstractQueuedSynchronizer implements Runnable { /** Thread this worker is running in. Null if factory fails. */ final Thread thread; /** Initial task to run. Possibly null. */ Runnable firstTask; /** Per-thread task counter */ volatile long completedTasks; /** * Creates with given first task and thread from ThreadFactory. * @param firstTask the first task (null if none) */ Worker(Runnable firstTask) { setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this); } /** Delegates main run loop to outer runWorker */ public void run() { runWorker(this); } executeæ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„æ–¹æ³•ï¼Œæ‰€æœ‰submitæ–¹æ³•åº•å±‚éƒ½ä¼šè°ƒç”¨executeæ–¹æ³•æäº¤ä»»åŠ¡ã€‚å¯ä»¥çœ‹åˆ°å°½ç®¡è¿™æ®µä»£ç éå¸¸çŸ­å°ï¼Œä½†ç”±äºå¹¶å‘é—®é¢˜å®ç°é€»è¾‘æ¯”è¾ƒç»•ã€‚ public void execute(Runnable command) { if (command == null) throw new NullPointerException(); int c = ctl.get(); // å¦‚æœå½“å‰çº¿ç¨‹æ•°å°‘äºæ ¸å¿ƒçº¿ç¨‹æ•°ï¼Œç›´æ¥æ·»åŠ ä¸€ä¸ªworkeræ¥æ‰§è¡Œä»»åŠ¡ï¼Œå°†å½“å‰ä»»åŠ¡ä½œä¸ºå®ƒçš„ç¬¬ä¸€ä¸ªä»»åŠ¡ // addWorkerè°ƒç”¨ä¼šåŸå­çš„æ£€æŸ¥runStateå’ŒworkerCountï¼Œé¿å…é”™è¯¯æ·»åŠ æ–°çš„çº¿ç¨‹ if (workerCountOf(c) \u003c corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } // å¦‚æœçº¿ç¨‹æ± å¤„äºRUNNINGçŠ¶æ€ï¼Œå°†è¿™ä¸ªä»»åŠ¡æ·»åŠ åˆ°ä»»åŠ¡é˜Ÿåˆ—workQueueä¸­ if (isRunning(c) \u0026\u0026 workQueue.offer(command)) { // double-check int recheck = ctl.get(); // å¦‚æœçº¿ç¨‹ä¸å¤„äºRUNNINGçŠ¶æ€ï¼Œç§»é™¤å·²ç»å…¥é˜Ÿçš„ä»»åŠ¡ï¼Œå¹¶æ‰§è¡Œæ‹’ç»ç­–ç•¥ if (! isRunning(recheck) \u0026\u0026 remove(command)) reject(command); // å¦‚æœçº¿ç¨‹æ± è¿˜æ˜¯RUNNINGçŠ¶æ€ï¼Œå¹¶ä¸”çº¿ç¨‹æ•°ä¸º0ï¼Œé‚£ä¹ˆå¼€å¯æ–°çš„çº¿ç¨‹ else if (workerCountOf(recheck) == 0) addWorker(null, false); } // å¦‚æœé˜Ÿåˆ—æ»¡äº†ï¼Œå°è¯•åˆ›å»ºæ–°çš„çº¿ç¨‹ï¼Œå¦‚æœå·²ç»è¾¾åˆ°æœ€å¤§çº¿ç¨‹æ•°ï¼Œæ‰§è¡Œæ‹’ç»ç­–ç•¥ else if (!addWorker(command, false)) reject(command); } addWorker æ–¹æ³•ç”¨æ¥åˆ›å»ºæ–°çš„çº¿ç¨‹ private boolean addWorker(Runnable firstTask, boolean core) { retry: for (;;) { int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. // çº¿ç¨‹æ± éRUNNINKGçŠ¶æ€ï¼Œåˆ™å…³é—­ // éœ€è¦æ’é™¤ä¸€ç§ç‰¹æ®Šæƒ…å†µï¼Œçº¿ç¨‹æ± å¤„äºSHUTDOWNçŠ¶æ€ï¼Œä¸”ç­‰å¾…é˜Ÿåˆ—éç©ºï¼Œè¿™ç§æƒ…å†µä¸‹åº”è¯¥å…è®¸è¿›ä¸€æ­¥åˆ¤æ–­æ˜¯å¦åˆ›å»ºæ–°çš„worker if (rs \u003e= SHUTDOWN \u0026\u0026 ! (rs == SHUTDOWN \u0026\u0026 firstTask == null \u0026\u0026 ! workQueue.isEmpty())) return false; for (;;) { int wc = ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:5:0","tags":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"title":"Javaçº¿ç¨‹æ± æºç åˆ†æ","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"content":"Executorså·¥å…·ç±» ç”Ÿæˆä¸€ä¸ªå›ºå®šå¤§å°çš„çº¿ç¨‹æ±  public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u003cRunnable\u003e()); } æœ€å¤§çº¿ç¨‹æ•°è®¾ç½®ä¸ºå’Œæ ¸å¿ƒçº¿ç¨‹æ•°ç›¸ç­‰ï¼Œæ­¤æ—¶keepAliveTimeè®¾ç½®ä¸º0ï¼ˆçº¿ç¨‹æ± é»˜è®¤ä¸ä¼šä¸ä¼šcorePoolSizeå†…çš„çº¿ç¨‹ï¼‰ï¼Œä»»åŠ¡é˜Ÿåˆ—é‡‡ç”¨LinkedBlockingQueueï¼Œæ— ç•Œé˜Ÿåˆ—ã€‚ å•çº¿ç¨‹çº¿ç¨‹æ± ï¼Œç±»ä¼¼ä¸Šé¢ï¼Œæ ¸å¿ƒçº¿ç¨‹æ•°ä¸º1 public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u003cRunnable\u003e())); } ç¼“å­˜çº¿ç¨‹æ±  public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue\u003cRunnable\u003e()); } æ ¸å¿ƒçº¿ç¨‹æ•°ä¸º0ï¼Œæœ€å¤§çº¿ç¨‹æ•°ä¸ºInteger.MAX_VALUEï¼ŒkeepAliveTimeä¸º60sï¼Œä»»åŠ¡é˜Ÿåˆ—é‡‡ç”¨SynchronousQueue çº¿ç¨‹æ•°ä¸è®¾ä¸Šé™ï¼Œä»»åŠ¡é˜Ÿåˆ—ä¸ºåŒæ­¥é˜Ÿåˆ—ï¼Œ60sè¶…æ—¶åç©ºé—²çº¿ç¨‹ä¼šè¢«å›æ”¶ ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:6:0","tags":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"title":"Javaçº¿ç¨‹æ± æºç åˆ†æ","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"content":"å‚è€ƒæ–‡çŒ® æ·±åº¦è§£è¯» java çº¿ç¨‹æ± è®¾è®¡æ€æƒ³åŠæºç å®ç° javadoop ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:7:0","tags":["JVM","Java","JDK11","çº¿ç¨‹æ± "],"title":"Javaçº¿ç¨‹æ± æºç åˆ†æ","uri":"/posts/java_thread_pool/"},{"categories":["Spark"],"content":"é€šè¿‡å¦‚ä¸‹çš„æ–¹æ³•åœ¨ideaä¸­é…ç½®sparkå¼€å‘ç¯å¢ƒï¼Œæœ€åå’Œä¸€èˆ¬çš„javaé¡¹ç›®ä¸€æ ·ï¼Œä½¿ç”¨mavené¢æ¿çš„ cleanå’Œpackageè¿›è¡Œç¼–è¯‘ã€‚ æˆ‘å®é™…ä½¿ç”¨çš„ç¼–è¯‘å™¨ä¸ºjava17ï¼Œideaä¼šæç¤ºé…ç½®scalaç¼–è¯‘å™¨ã€‚ The Maven-based build is the build of reference for Apache Spark. Building Spark using Maven requires Maven 3.9.6 and Java 8/11/17. Spark requires Scala 2.12/2.13; support for Scala 2.11 was removed in Spark 3.0.0. While many of the Spark developers use SBT or Maven on the command line, the most common IDE we use is IntelliJ IDEA. You can get the community edition for free (Apache committers can get free IntelliJ Ultimate Edition licenses) and install the JetBrains Scala plugin from Preferences \u003e Plugins. To create a Spark project for IntelliJ: Download IntelliJ and install the Scala plug-in for IntelliJ. Go to File -\u003e Import Project, locate the spark source directory, and select â€œMaven Projectâ€. In the Import wizard, itâ€™s fine to leave settings at their default. However it is usually useful to enable â€œImport Maven projects automaticallyâ€, since changes to the project structure will automatically update the IntelliJ project. As documented in Building Spark, some build configurations require specific profiles to be enabled. The same profiles that are enabled with -P[profile name] above may be enabled on the Profiles screen in the Import wizard. For example, if developing for Hadoop 2.7 with YARN support, enable profiles yarn and hadoop-2.7. These selections can be changed later by accessing the â€œMaven Projectsâ€ tool window from the View menu, and expanding the Profiles section. ","date":"2025-04-05","objectID":"/posts/spark_developement_envrionment/:0:0","tags":["Spark"],"title":"Sparkå¼€å‘ç¯å¢ƒæ­å»º","uri":"/posts/spark_developement_envrionment/"},{"categories":["Spark"],"content":"é‡åˆ°çš„é—®é¢˜ Exception in thread \"main\" java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/Filter at java.base/java.lang.Class.forName0(Native Method) at java.base/java.lang.Class.forName(Class.java:578) at java.base/java.lang.Class.forName(Class.java:557) at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41) at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36) at org.apache.spark.util.SparkClassUtils$.classForName(SparkClassUtils.scala:141) at org.apache.spark.sql.SparkSession$.lookupCompanion(SparkSession.scala:826) at org.apache.spark.sql.SparkSession$.CLASSIC_COMPANION$lzycompute(SparkSession.scala:816) at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$CLASSIC_COMPANION(SparkSession.scala:815) at org.apache.spark.sql.SparkSession$.$anonfun$DEFAULT_COMPANION$1(SparkSession.scala:820) at scala.util.Try$.apply(Try.scala:217) at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$DEFAULT_COMPANION(SparkSession.scala:820) at org.apache.spark.sql.SparkSession$Builder.\u003cinit\u003e(SparkSession.scala:854) at org.apache.spark.sql.SparkSession$.builder(SparkSession.scala:833) at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:28) at org.apache.spark.examples.SparkPi.main(SparkPi.scala) Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.Filter at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:528) ... 16 more åœ¨idea Run/Debug Configurationä¸­æ·»åŠ Add dependencies with provided scope to classpath org.apache.spark.SparkException: A master URL must be set in your configuration at org.apache.spark.SparkContext.\u003cinit\u003e(SparkContext.scala:421) at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:3062) at org.apache.spark.sql.classic.SparkSession$Builder.$anonfun$build$2(SparkSession.scala:911) at scala.Option.getOrElse(Option.scala:201) at org.apache.spark.sql.classic.SparkSession$Builder.build(SparkSession.scala:902) at org.apache.spark.sql.classic.SparkSession$Builder.getOrCreate(SparkSession.scala:931) at org.apache.spark.sql.classic.SparkSession$Builder.getOrCreate(SparkSession.scala:804) at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:923) at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:30) at org.apache.spark.examples.SparkPi.main(SparkPi.scala) æ·»åŠ jvmå‚æ•°-Dspark.master=localï¼Œæœ¬åœ°è¿è¡Œ ä¸çŸ¥é“ä¸ºä»€ä¹ˆideaä¸èƒ½ç›´æ¥æ‰¾åˆ°parallelizeçš„å®šä¹‰è€Œé£˜çº¢ï¼Œè¿™é‡Œç›´æ¥å¯¼å…¥SparkContextå¹¶ä¸”é€šè¿‡asInstanceOf[SparkContext]æ˜ç¤ºideaã€‚ // scalastyle:off println package org.apache.spark.examples import scala.math.random import org.apache.spark.SparkContext import org.apache.spark.sql.SparkSession /** Computes an approximation to pi */ object SparkPi { def main(args: Array[String]): Unit = { val spark = SparkSession .builder() .appName(\"Spark Pi\") .getOrCreate() val slices = if (args.length \u003e 0) args(0).toInt else 2 val n = math.min(100000L * slices, Int.MaxValue).toInt // avoid overflow val count = spark.sparkContext.asInstanceOf[SparkContext].parallelize( 1 until n, slices).map { i =\u003e val x = random() * 2 - 1 val y = random() * 2 - 1 if (x*x + y*y \u003c= 1) 1 else 0 }.reduce(_ + _) println(s\"Pi is roughly ${4.0 * count / (n - 1)}\") spark.stop() } } ","date":"2025-04-05","objectID":"/posts/spark_developement_envrionment/:1:0","tags":["Spark"],"title":"Sparkå¼€å‘ç¯å¢ƒæ­å»º","uri":"/posts/spark_developement_envrionment/"},{"categories":["Spark","å†…å­˜ç®¡ç†"],"content":"å…³é”®é—®é¢˜ å†…å­˜è¢«åˆ†æˆå“ªäº›åŒºåŸŸï¼Œå„åˆ†åŒºä¹‹é—´çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Œé€šè¿‡ä»€ä¹ˆå‚æ•°æ§åˆ¶ å†…å­˜ä¸ŠæŠ¥å’Œé‡Šæ”¾çš„å•ä½æ˜¯ä»€ä¹ˆï¼Œä¸ŠæŠ¥å’Œé‡Šæ”¾æ˜¯å¦‚ä½•å®ç°çš„ å¦‚ä½•é¿å…å†…å­˜æ²¡æœ‰é‡Šæ”¾å¯¼è‡´èµ„æºæ³„éœ² å¦‚ä½•é¿å…é‡å¤ä¸ŠæŠ¥å’Œæ¼ä¸ŠæŠ¥é—®é¢˜ å¯¹è±¡çš„ç”Ÿå‘½å‘¨æœŸå’Œå†…å­˜ä¸ŠæŠ¥é‡Šæ”¾ä¹‹é—´çš„å…³ç³» å“ªäº›å¯¹è±¡ä¼šè¢«ä¸ŠæŠ¥ï¼Œä¸ºä»€ä¹ˆé€‰æ‹©è¿™äº›å¯¹è±¡ä¸ŠæŠ¥ å†…å­˜ä¸ŠæŠ¥æ˜¯å¦æŒæœ‰å¯¹è±¡å¼•ç”¨ ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:1:0","tags":["Spark","å†…å­˜ç®¡ç†"],"title":"Sparkå†…å­˜ç®¡ç†","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","å†…å­˜ç®¡ç†"],"content":"é™æ€å†…å­˜ç®¡ç†æ¨¡å‹ Sparkæ—©æœŸç‰ˆæœ¬ï¼ˆSpark 1.6ä¹‹å‰çš„ç‰ˆæœ¬ï¼‰ä½¿ç”¨é™æ€å†…å­˜ç®¡ç†æ¨¡å‹ï¼ˆStaticfMemoryManagerï¼‰å°†å†…å­˜ç©ºé—´åˆ’åˆ†ä¸º3ä¸ªåˆ†åŒºï¼š æ•°æ®ç¼“å­˜ç©ºé—´ï¼ˆStorage Memoryï¼‰ï¼šçº¦å 60%çš„å†…å­˜ç©ºé—´ï¼Œç”¨äºå­˜å‚¨RDDç¼“å­˜æ•°æ®ã€å¹¿æ’­æ•°æ®ã€taskçš„ä¸€äº›è®¡ç®—ç»“æœç­‰ æ¡†æ¶æ‰§è¡Œç©ºé—´ï¼ˆExecution Memoryï¼‰ï¼šçº¦å 20%çš„å†…å­˜ç©ºé—´ï¼Œç”¨äºå­˜å‚¨Shuffleæœºåˆ¶ä¸­çš„ä¸­é—´æ•°æ® ç”¨æˆ·ä»£ç ç©ºé—´ï¼ˆUser Memoryï¼‰ï¼šçº¦å 20%çš„å†…å­˜ç©ºé—´ï¼Œç”¨äºå­˜å‚¨ç”¨æˆ·ä»£ç çš„ä¸­é—´è®¡ç®—ç»“æœã€Sparkæ¡†æ¶è‡ªèº«äº§ç”Ÿçš„å†…éƒ¨å¯¹è±¡ï¼Œä»¥åŠEexecutor JVMè‡ªèº«çš„ä¸€äº›å†…å­˜å¯¹è±¡ç­‰ ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:2:0","tags":["Spark","å†…å­˜ç®¡ç†"],"title":"Sparkå†…å­˜ç®¡ç†","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","å†…å­˜ç®¡ç†"],"content":"ç»Ÿä¸€å†…å­˜ç®¡ç†æ¨¡å‹ Executor JVMçš„æ•´ä¸ªå†…å­˜ç©ºé—´åˆ’åˆ†ä¸ºä»¥ä¸‹3ä¸ªéƒ¨åˆ† ç³»ç»Ÿä¿ç•™å†…å­˜ï¼ˆReserved Memory) ç³»ç»Ÿä¿ç•™å†…å­˜ä½¿ç”¨è¾ƒå°çš„ç©ºé—´å­˜å‚¨Sparkæ¡†æ¶äº§ç”Ÿçš„å†…éƒ¨å¯¹è±¡ï¼ˆå¦‚Spark Executorå¯¹è±¡ï¼ŒTaskMemoryManagerå¯¹è±¡ç­‰Sparkå†…éƒ¨å¯¹è±¡ï¼‰ï¼Œç³»ç»Ÿä¿ç•™å†…å­˜å¤§å°é€šè¿‡spark.testing.ReservedMemoryé»˜è®¤è®¾ç½®ä¸º300MB ç”¨æˆ·ä»£ç ç©ºé—´ï¼ˆUser Memoryï¼‰ç”¨æˆ·ä»£ç ç©ºé—´è¢«ç”¨äºå­˜å‚¨ç”¨æˆ·ä»£ç ç”Ÿæˆçš„å¯¹è±¡ï¼Œå¦‚mapä¸­ç”¨æˆ·è‡ªå®šä¹‰çš„æ•°æ®ç»“æ„ï¼Œç”¨æˆ·ä»£ç ç©ºé—´æ¨¡æ‹Ÿçƒ­çº¦ä¸º40%çš„å†…å­˜ç©ºé—´ æ¡†æ¶å†…å­˜ç©ºé—´ ï¼ˆFrameworke Memoryï¼‰æ¡†æ¶ç¼“å­˜ç©ºé—´åŒ…æ‹¬æ¡†æ¶æ‰§è¡Œç©ºé—´ï¼ˆExecution Memoryï¼‰å’Œæ•°æ®ç¼“å­˜ç©ºé—´ï¼ˆStorage Memoryï¼‰ã€‚æ€»å¤§å°ä¸ºspark.memory.fraction (default 0.6) * (heap - ReservedMemory)ï¼Œçº¦ç­‰äº60%çš„å†…å­˜ç©ºé—´ã€‚ä¸¤è€…å…±äº«è¿™ä¸ªç©ºé—´ï¼Œå…¶ä¸­ä¸€æ–¹ç©ºé—´ä¸è¶³æ—¶å¯ä»¥åŠ¨æ€å‘å¦ä¸€æ–¹å€Ÿç”¨ã€‚å…·ä½“åœ°ï¼Œå½“æ•°æ®ç¼“å­˜ç©ºé—´ä¸è¶³æ—¶ï¼Œå¯ä»¥å‘æ¡†æ¶æ‰§è¡Œç©ºé—´å€Ÿç”¨å…¶ç©ºé—²ç©ºé—´ï¼Œåç»­å½“æ¡†æ¶æ‰§è¡Œéœ€è¦æ›´å¤šç©ºé—´æ—¶ï¼Œæ•°æ®ç¼“å­˜ç©ºé—´éœ€è¦å½’è¿˜å€Ÿç”¨çš„ç©ºé—´ï¼Œè¿™æ—¶å€™Sparkå¯èƒ½å°†éƒ¨åˆ†ç¼“å­˜æ•°æ®ç§»é™¤å†…å­˜æ¥å½’è¿˜ç©ºé—´ã€‚åŒæ ·ï¼Œå½“æ¡†æ¶æ‰§è¡Œç©ºé—´ä¸è¶³æ—¶ï¼Œå¯ä»¥å‘æ•°æ®ç¼“å­˜ç©ºé—´å€Ÿç”¨ç©ºé—´ï¼Œä½†è‡³å°‘è¦ä¿è¯æ•°æ®ç¼“å­˜ç©ºé—´å…·æœ‰çº¦50%å·¦å³ï¼ˆspark.memory.storageFraction (default 0.5) * Framework memory) çš„ç©ºé—´ã€‚åœ¨æ¡†æ¶æ‰§è¡Œæ—¶å€Ÿèµ°çš„ç©ºé—´ä¸ä¼šå½’è¿˜ç»™æ•°æ®ç¼“å­˜ç©ºé—´ï¼Œæ„¿æ„æ˜¯éš¾ä»¥ä»£ç å®ç°ã€‚ å¦‚æœç”¨æˆ·å®šä¹‰äº†å †å¤–å†…å­˜ï¼Œå…¶å¤§å°é€šè¿‡spark.memory.offHeap.sizeè®¾ç½®ï¼Œé‚£ä¹ˆSparkä»ç„¶ä¼šæŒ‰ç…§å †å†…å†…å­˜ä½¿ç”¨çš„spark.memory.storageFractionå°†å †å¤–å†…å­˜åˆ†ä¸ºæ¡†æ¶æ‰§è¡Œç©ºé—´å’Œæ•°æ®ç¼“å­˜ç©ºé—´ï¼Œè€Œä¸”å †å¤–å†…å­˜çš„ç®¡ç†æ–¹å¼å’ŒåŠŸèƒ½ä¸å †å¤–å†…å­˜çš„Framework Memoryä¸€æ ·ã€‚åœ¨è¿è¡Œåº”ç”¨æ—¶ï¼ŒSparkä¼šæ ¹æ®åº”ç”¨çš„Shuffleæ–¹å¼åŠç”¨æˆ·è®¾å®šçš„æ•°æ®ç¼“å­˜çº§åˆ«æ¥å†³å®šä½¿ç”¨å †å¤–å†…å­˜è¿˜æ˜¯å †å¤–å†…å­˜ã€‚å¦‚SerializedShuffleæ–¹æ³•å¯ä»¥åˆ©ç”¨å †å¤–å†…å­˜æ¥è¿›è¡ŒShuffle Writeï¼Œç”¨æˆ·ä½¿ç”¨rdd.persist(OFF_HEAP)å¯ä»¥å°†rddå­˜å‚¨åˆ°å †å¤–å†…å­˜ã€‚ ç”±äºExecutorä¸­å­˜åœ¨å¤šä¸ªtaskï¼Œå› æ­¤æ¡†æ¶æ‰§è¡Œç©ºé—´å®é™…ä¸Šæ˜¯ç”±å¤šä¸ªtaskï¼ˆShuffleMapTaskæˆ–ResultTaskï¼‰å…±äº«çš„ã€‚åœ¨è¿è¡Œè¿‡ç¨‹ä¸­ï¼ŒExecutorä¸­æ´»è·ƒçš„taskæ•°ç›®åœ¨[0, #ExecutorCores]å†…å˜åŒ–ï¼Œ#ExecutorCoresè¡¨ç¤ºä¸ºæ¯ä¸ªExecutoråˆ†é…çš„cpuä¸ªæ•°ã€‚ä¸ºäº†å…¬å¹³æ€§ï¼Œæ¯ä¸ªtaskå¯ä½¿ç”¨çš„å†…å­˜ç©ºé—´è¢«å‡åˆ†ï¼Œä¹Ÿå°±æ˜¯ç©ºé—´å¤§å°è¢«æ§åˆ¶åœ¨[1/2N, 1/N] * ExecutorMemoryå†…ï¼ŒNæ˜¯å½“å‰æ´»è·ƒçš„taskæ•°ç›®ã€‚å‡è®¾ä¸€ä¸ªExecutorä¸­æœ€åˆæœ‰4ä¸ªæ´»è·ƒçš„taskï¼Œä¸”åªä½¿ç”¨å †å†…å†…å­˜ï¼Œé‚£ä¹ˆæ¯ä¸ªtaskæœ€å¤šå¯ä»¥å ç”¨1/4çš„On-heap Execution Memoryï¼Œå½“å…¶ä¸­2ä¸ªtaskå®Œæˆè€Œåˆæ–°åŠ å…¥4ä¸ªtaskåï¼Œæ´»è·ƒtaskå˜ä¸º6ä¸ªï¼Œé‚£ä¹ˆååŠ å…¥çš„æ¯ä¸ªtaskæœ€å¤šä½¿ç”¨1/6çš„On-heap Execution Memoryï¼Œè¿™ä¸ªç­–ç•¥ä¹Ÿé€‚ç”¨äºå †å¤–å†…å­˜çš„Execution Memoryã€‚ è¿™é‡Œé‡ç‚¹ä»‹ç»Shuffleæœºåˆ¶ä¸­çš„Serialized Shuffleï¼ŒSerialized Shuffleç”¨æ¥ä¸éœ€è¦mapç«¯èšåˆã€ä¸éœ€è¦æŒ‰ç…§Keyè¿›è¡Œæ’åºï¼Œä¸”åˆ†åŒºä¸ªæ•°è¾ƒå¤§çš„æƒ…å½¢ï¼Œå°†recordå¯¹è±¡åºåˆ—åŒ–åå­˜æ”¾åˆ°å¯åˆ†é¡µå­˜å‚¨çš„æ•°ç»„ä¸­ï¼Œåºåˆ—åŒ–å¯ä»¥å‡å°‘å­˜å‚¨å¼€é”€ï¼Œåˆ†é¡µå¯ä»¥åˆ©ç”¨ä¸è¿ç»­çš„ç©ºé—´ã€‚é¦–å…ˆå°†æ–°æ¥çš„\u003cK, V\u003e recordåºåˆ—åŒ–å†™å…¥ä¸€ä¸ª1MBçš„ç¼“å†²åŒºï¼ˆserBufferï¼‰ï¼Œç„¶åå°†serBufferä¸­åºåˆ—åŒ–çš„recordæ”¾åˆ°ShuffleExternalSorterçš„Pageä¸­è¿›è¡Œæ’åºã€‚æ’å…¥å’Œæ’åºçš„æ–¹æ³•æ˜¯ï¼Œé¦–å…ˆåˆ†é…ä¸€ä¸ªLongArrayæ¥ä¿å­˜recordçš„æŒ‡é’ˆï¼ŒæŒ‡é’ˆä¸º64ä½ï¼Œå‰24ä½å­˜å‚¨recordçš„partitionIdï¼Œä¸­é—´13ä¸ºå­˜å‚¨recordæ‰€åœ¨çš„Page Numï¼Œå27ä½å­˜å‚¨recordåœ¨è¯¥Pageä¸­çš„åç§»é‡ã€‚ä¹Ÿå°±æ˜¯è¯´LongArrayæœ€å¤šå¯ä»¥ç®¡ç†1TBçš„å†…å­˜ï¼Œéšç€recordä¸æ–­åœ°æ’å…¥Pageä¸­ï¼Œå¦‚æœLongArrayä¸å¤Ÿç”¨æˆ–Pageä¸å¤Ÿç”¨ï¼Œåˆ™ä¼šé€šè¿‡allocatePageå‘TaskMemoryManagerç”³è¯·ï¼Œå¦‚æœç”³è¯·ä¸åˆ°ï¼Œå°±å¯åŠ¨spillç¨‹åºï¼Œå°†ä¸­é—´ç»“æœspillåˆ°ç£ç›˜ä¸Šï¼Œæœ€åå†ç”±UnsafeShuffleWriterè¿›è¡Œç»Ÿä¸€çš„mergeã€‚Pageç”±TaskMemoryManagerç®¡ç†å’Œåˆ†é…ï¼Œå¯ä»¥å­˜æ”¾åœ¨å †å†…å†…å­˜æˆ–è€…å †å¤–å†…å­˜ã€‚ æ•°æ®ç¼“å­˜ç©ºé—´ä¸»è¦ç”¨äºå­˜æ”¾3ç§æ•°æ®ï¼šRDDç¼“å­˜æ•°æ®ï¼ˆRDD partitionï¼‰ã€å¹¿æ’­æ•°æ®ï¼ˆBroadcast dataï¼‰ï¼Œä»¥åŠtaskçš„è®¡ç®—ç»“æœï¼ˆTaskResultï¼‰ã€‚ Broadcasté»˜è®¤ä½¿ç”¨ç±»ä¼¼BTä¸‹è½½çš„TorrentBroadcastæ–¹å¼ï¼Œéœ€è¦å¹¿æ’­çš„æ•°æ®ä¸€èˆ¬é¢„å…ˆå­˜å‚¨åœ¨Driverç«¯ï¼ŒSparkåœ¨Driverç«¯å°†è¦å¹¿æ’­çš„æ•°æ®åˆ’åˆ†å¤§å°ä¸ºspark.Broadcast.blockSize = 4MBçš„æ•°æ®å—ï¼ˆblockï¼‰ï¼Œç„¶åèµ‹äºˆæ¯ä¸ªæ•°æ®å—ä¸€ä¸ªblockIdä¸ºBroadcastblockId(id, â€œpieceâ€ + i) ï¼Œidè¡¨ç¤ºblockçš„ç¼–å·ï¼Œpieceè¡¨ç¤ºè¢«åˆ’åˆ†åçš„ç¬¬å‡ ä¸ªblockã€‚ä¹‹åä½¿ç”¨ç±»ä¼¼BTçš„æ–¹å¼å°†æ¯ä¸ªblockå¹¿æ’­åˆ°æ¯ä¸ªExecutorä¸­ï¼ŒExecutoræ¥æ”¶åˆ°æ¯ä¸ªblockæ•°æ®å—åï¼Œå°†å…¶æ”¾åˆ°å †å†…çš„æ•°æ®ç¼“å­˜ç©ºé—´çš„ChunkedByteBufferé‡Œé¢ï¼Œç¼“å­˜æ¨¡å¼ä¸ºMEMORY_AND_DISK_SERï¼Œå› æ­¤ï¼Œè¿™é‡Œçš„ChunkedByteBufferæ„é€ ä¸MEMORY_ONLY_SERæ¨¡å¼ä¸­çš„ä¸€æ ·ï¼Œéƒ½æ˜¯ç”¨ä¸è¿ç»­çš„ç©ºé—´æ¥å­˜å‚¨åºåˆ—åŒ–æ•°æ®ã€‚ è®¸å¤šåº”ç”¨éœ€è¦åœ¨Driverç«¯æ”¶é›†taskçš„è®¡ç®—ç»“æœå¹¶è¿›è¡Œå¤„ç†ï¼Œå¦‚è°ƒç”¨äº†rdd.collectçš„åº”ç”¨ï¼Œå½“taskçš„è¾“å‡ºç»“æœå¤§å°è¶…è¿‡spark.task.maxDirectResultSize = 1MBä¸”å°äº1GBä½¿ï¼Œéœ€è¦å…ˆå°†æ¯ä¸ªtaskçš„è¾“å‡ºç»“æœç¼“å­˜åˆ°æ‰§è¡Œè¯¥taskçš„Executorä¸­ï¼Œå­˜æ”¾æ¨¡å¼æ˜¯MEMORY_AND_DISK_SERï¼Œç„¶åExecutorå°†taskçš„è¾“å‡ºç»“æœå‘é€åˆ°Driverç«¯è¿›ä¸€æ­¥å¤„ç†ã€‚ ç›®å‰ï¼Œé’ˆå¯¹RDDæ“ä½œï¼ŒSparkåªæä¾›äº†Serialized Shuffle Writeræ–¹å¼ï¼Œæ²¡æœ‰æä¾›Serialized Shuffle Readæ–¹å¼ï¼Œå®é™…ä¸Šï¼Œ åœ¨SparkSQLé¡¹ç›®ä¸­ï¼ŒSparkåˆ©ç”¨SQLæ“ä½œçš„ç‰¹ç‚¹ï¼ˆå¦‚SUMã€AVGè®¡ç®—ç»“æœçš„ç­‰å®½æ€§ï¼‰ï¼Œæä¾›äº†æ›´å¤šçš„Serialized Shuffleæ–¹å¼ï¼Œç›´æ¥åœ¨åºåˆ—åŒ–çš„æ•°æ®ä¸Šå®ç°èšåˆç­‰è®¡ç®—ï¼Œè¯¦æƒ…å¯ä»¥å‚è€ƒUnsafeFixedAggregationMapã€ObjectAggregationMapç­‰æ•°æ®ç»“æ„çš„å®ç°ã€‚ ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:3:0","tags":["Spark","å†…å­˜ç®¡ç†"],"title":"Sparkå†…å­˜ç®¡ç†","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","å†…å­˜ç®¡ç†"],"content":"æºç åˆ†æ ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:4:0","tags":["Spark","å†…å­˜ç®¡ç†"],"title":"Sparkå†…å­˜ç®¡ç†","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","å†…å­˜ç®¡ç†"],"content":"MemoryBlock MemoryBlockè¡¨ç¤ºä¸€æ®µè¿ç»­çš„å†…å­˜ç©ºé—´ï¼Œç±»ä¼¼äºæ“ä½œç³»ç»Ÿä¸­pageçš„æ¦‚å¿µã€‚ MemoryBlockç»§æ‰¿è‡ªMemoryLocationï¼Œå½“è¿½è¸ªå †å¤–åˆ†é…æ—¶ï¼Œobjä¸ºç©ºï¼Œoffsetè¡¨ç¤ºå †å¤–å†…å­˜åœ°å€ï¼Œå½“è¿½è¸ªå †å†…å†…å­˜åˆ†é…æ—¶ï¼Œobjä¸ºå¯¹è±¡å¼•ç”¨ï¼Œoffsetä¸ºå¯¹è±¡å†…åç§»é‡ï¼Œå¯ä»¥çœ‹åˆ°MemoryLocationåªæ˜¯è®°å½•äº†å¯¹è±¡çš„ä½ç½®ä¿¡æ¯ï¼Œæ²¡æœ‰è®°å½•å¯¹è±¡å†…å­˜å ç”¨çš„ä¿¡æ¯ã€‚ public class MemoryLocation { @Nullable Object obj; long offset; public class MemoryBlock extends MemoryLocation { private final long length; public int pageNumber = NO_PAGE_NUMBER; MemoryBlockæ–°å¢ä¸¤ä¸ªå­—æ®µï¼Œlengthè¡¨ç¤ºpageçš„å¤§å°ï¼ŒpageNumberå¾ˆå¥½ç†è§£ï¼ŒTaskMemoryManagerä¼šç»™æ¯ä¸ªé¡µåˆ†é…ä¸€ä¸ªé¡µå·ï¼Œæœ‰ä»¥ä¸‹å‡ ç§ç‰¹æ®Šæƒ…å†µ NO_PAGE_NUMBER è¡¨ç¤ºæ²¡æœ‰è¢«TaskMemoryManageråˆ†é…ï¼Œåˆå§‹å€¼ FREED_IN_TMM_PAGE_NUMBER è¡¨ç¤ºè¢«TaskMemoryManageré‡Šæ”¾ï¼ŒTaskMemoryManager.freeæ“ä½œä¸­ä¼šå°†é¡µå·è®¾ç½®ä¸ºæ­¤å€¼ï¼ŒMemoryAllocator.freeé‡åˆ°æ²¡æœ‰è¢«TaskMemoryManangeré‡Šæ”¾çš„é¡µæ—¶ï¼Œä¼šæŠ¥é”™ FREED_IN_ALLOCATOR_PAGE_NUMBER è¢«MemoryAllocatoré‡Šæ”¾ï¼Œå¯ä»¥æ£€æµ‹å¤šæ¬¡é‡Šæ”¾ ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:4:1","tags":["Spark","å†…å­˜ç®¡ç†"],"title":"Sparkå†…å­˜ç®¡ç†","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","å†…å­˜ç®¡ç†"],"content":"MemoryAllocator MemoryAllocatoræ¥å£å®šä¹‰äº†ç”³è¯·å’Œé‡Šæ”¾MemoryBlockçš„æ–¹æ³•ï¼ŒHeapMemoryAllocatorå’ŒUnsafeMemoryAllocatoråˆ†åˆ«å®ç°äº†å †å†…å’Œå †å¤–çš„å†…å­˜åˆ†é…å™¨ã€‚ public interface MemoryAllocator { /** * Allocates a contiguous block of memory. Note that the allocated memory is not guaranteed * to be zeroed out (call `fill(0)` on the result if this is necessary). */ MemoryBlock allocate(long size) throws OutOfMemoryError; void free(MemoryBlock memory); MemoryAllocator UNSAFE = new UnsafeMemoryAllocator(); MemoryAllocator HEAP = new HeapMemoryAllocator(); } HeapMemoryAllocator public class HeapMemoryAllocator implements MemoryAllocator { @GuardedBy(\"this\") private final Map\u003cLong, LinkedList\u003cWeakReference\u003clong[]\u003e\u003e\u003e bufferPoolsBySize = new HashMap\u003c\u003e(); private static final int POOLING_THRESHOLD_BYTES = 1024 * 1024; å¯ä»¥çœ‹åˆ°å®é™…åˆ†é…çš„å¯¹è±¡å°±æ˜¯longæ•°ç»„ï¼Œå¹¶ä¸”åšäº†æ± åŒ–ï¼Œå¯¹äº1MBä»¥ä¸Šçš„å†…å­˜å°è¯•æ”¾å…¥æ± ä¸­ï¼Œè¿™é‡Œæ²¡æœ‰é™åˆ¶æ± çš„å¤§å°ï¼ŒæŒæœ‰çš„æ˜¯longæ•°ç»„çš„å¼±å¼•ç”¨ï¼Œå‡å°‘é¢‘ç¹ç”³è¯·å’Œé‡Šæ”¾å¤§å†…å­˜é€ æˆçš„å¼€é”€ã€‚ å¦‚æœç”³è¯·ä¸åˆ°å†…å­˜ï¼Œä¼šæŠ›å‡ºOutOfMemoryError UnsafeMemoryAllocator å®ç°æ²¡æœ‰ä»€ä¹ˆç‰¹æ®Šçš„åœ°æ–¹ï¼Œç›´æ¥è°ƒç”¨SparkåŒ…è£…è¿‡çš„Unsafe APIï¼Œç›´æ¥è°ƒç”¨UnsafeåŒ…ä¸­çš„APIï¼Œæ‰€ä»¥ä¸å—MaxDirectMemorySizeçš„æ§åˆ¶ public long allocateMemory(long bytes) { beforeMemoryAccess(); return theInternalUnsafe.allocateMemory(bytes); } ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:4:2","tags":["Spark","å†…å­˜ç®¡ç†"],"title":"Sparkå†…å­˜ç®¡ç†","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","å†…å­˜ç®¡ç†"],"content":"MemoryManager MemoryManageræŠ½è±¡ç±»è´Ÿè´£ç®¡ç†å†…å­˜ï¼Œåœ¨è®¡ç®—å’Œå­˜å‚¨ä¹‹é—´å…±äº«å†…å­˜ï¼Œè®¡ç®—å†…å­˜æŒ‡åœ¨shuffles, joins, sorts and aggregations ä¸­è®¡ç®—è¿‡ç¨‹æ‰€ä½¿ç”¨çš„å†…å­˜ï¼Œè€Œå­˜å‚¨å†…å­˜æŒ‡è¢«ç”¨äºç¼“å­˜æˆ–è€…åœ¨é›†ç¾¤ä¸­ä¼ æ’­å†…éƒ¨æ•°æ®æ‰€å ç”¨çš„å†…å­˜ï¼Œæ¯ä¸ªJVMåªæœ‰ä¸€ä¸ªMemoryManagerã€‚ Sparkå†…å­˜å‚æ•° spark.memory.offHeap.enabled å¦‚æœå¼€å¯ï¼ŒæŸäº›è®¡ç®—å°†ä½¿ç”¨å †å¤–å†…å­˜ï¼Œè¦æ±‚spark.memory.offHeap.sizeå¿…é¡»ä¸ºæ­£æ•°ï¼Œé»˜è®¤å…³é—­ spark.memory.fraction (å †å†…å­˜ - 300MB)è¢«ç”¨äºè®¡ç®—å’Œå­˜å‚¨çš„æ¯”ä¾‹ï¼Œè¿™ä¸ªå€¼è¶Šä½ï¼Œåç£ç›˜ä»¥åŠç¼“å­˜é©±é€å‘ç”Ÿçš„è¶Šé¢‘ç¹ï¼Œè¿™ä¸ªè®¾ç½®çš„ä¸»è¦ç›®çš„æ˜¯ç•™å‡ºç©ºé—´ç»™ç”¨æˆ·æ•°æ®ç»“æ„ä»¥åŠæ¯”å¦‚ç¨€ç–ã€ä¸å¯»å¸¸çš„å¤§å†…å­˜è®°å½•å¯¼è‡´çš„å†…å­˜ä¼°ç®—ä¸å‡†ç¡®ã€‚é»˜è®¤å€¼ä¸º0.6 spark.memory.offHeap.sizeæŒ‡å®šäº†sparkå †å¤–ä½¿ç”¨çš„å†…å­˜å¤§å° saprk.memory.storageFractionå…äºé©±é€çš„å­˜å‚¨å†…å­˜å ç”¨å†…å­˜å¤§å°ï¼Œè¿™é‡Œè¡¨ç¤ºä¸ºspark.memory.fractionç•™å‡ºçš„å†…å­˜çš„ç™¾åˆ†æ¯”ã€‚é»˜è®¤ä¸º0.5 å †å¤–å†…å­˜ç”±spark.memory.offHeap.sizeè§„å®šï¼Œå †å¤–å­˜å‚¨å†…å­˜ä¸º$spark.memory.offHeap.size * spark.memory.storageFractioin$ï¼Œå‰©ä½™çš„å†…å­˜ä¸ºå †å¤–è®¡ç®—å†…å­˜ã€‚ ä¸»è¦å­—æ®µå’Œæ–¹æ³• @GuardedBy(\"this\") protected val onHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(\"this\") protected val offHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.OFF_HEAP) @GuardedBy(\"this\") protected val onHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(\"this\") protected val offHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.OFF_HEAP) StorageMemoryPoolå®é™…ç®¡ç†å­˜å‚¨å†…å­˜ï¼ŒExecutionMemoryPoolå®é™…ç®¡ç†è®¡ç®—å†…å­˜ï¼Œè¿™ä¸¤è€…åœ¨å¤„ç†å…³é”®æ“ä½œæ˜¯éƒ½éœ€è¦æŒæœ‰MemoryManagerå¯¹è±¡é”ï¼Œä»è€Œå®ç°åœ¨å­˜å‚¨å’Œè®¡ç®—ä¹‹é—´å…±äº«å†…å­˜çš„æ“ä½œã€‚ acquireStorageMemory:è·å¾—å­˜å‚¨å†…å­˜ç”¨æ¥ç¼“å­˜blockç­‰ acquireUnrollMemory: è·å–å±•å¼€å†…å­˜ç”¨æ¥å±•å¼€ç»™å®šçš„block acquireExecutionMemory: è·å¾—è®¡ç®—å†…å­˜ï¼Œè°ƒç”¨å¯èƒ½é˜»å¡ï¼Œç¡®ä¿æ¯ä¸ªä»»åŠ¡è‡³å°‘æœ‰æœºä¼šè·å¾—$1/ 2N$å†…å­˜æ± å¤§å°ï¼ŒNè¡¨ç¤ºå½“å‰æ´»è·ƒä»»åŠ¡æ•°é‡ï¼Œæ¯”å¦‚è€çš„ä»»åŠ¡å·²ç»å ç”¨äº†å¾ˆå¤šå†…å­˜è€Œä»»åŠ¡æ•°å¢åŠ  releaseExecutionMemory é‡Šæ”¾è®¡ç®—å†…å­˜ releaseAllExecutionMemoryForTask é‡Šæ”¾å½“å‰ä»»åŠ¡çš„æ‰€æœ‰è®¡ç®—å†…å­˜ releaseStorageMemory é‡Šæ”¾å­˜å‚¨å†…å­˜ releaseAllStorageMemory é‡Šæ”¾æ‰€æœ‰å­˜å‚¨å†…å­˜ releaseUnrollMemory é‡Šæ”¾å±•å¼€å†…å­˜ ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:4:3","tags":["Spark","å†…å­˜ç®¡ç†"],"title":"Sparkå†…å­˜ç®¡ç†","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","å†…å­˜ç®¡ç†"],"content":"UnifiedMemoryManager å †å†…çš„å†…å­˜æ€»é‡ç”±spark.testing.memoryæŒ‡å®šï¼Œé»˜è®¤ä¸ºjvmå †å¤§å°ï¼Œä¿ç•™å†…å­˜ä¸º300MB $$ å¯ç”¨äºå­˜å‚¨æˆ–è€…è®¡ç®—çš„å†…å­˜ = (spark.testing.memory - reserved\\ memory) * spark.memory.fraction $$ åˆå§‹å­˜å‚¨å†…å­˜å¤§å°å æ¯”ç”±spark.memory.storageFractionæŒ‡å®šï¼Œå­˜å‚¨å’Œè®¡ç®—å¯ä»¥ç›¸äº’å€Ÿç”¨å¯¹æ–¹çš„å†…å­˜ï¼Œéµå¾ªä»¥ä¸‹è§„åˆ™ï¼š å¦‚æœè®¡ç®—å†…å­˜ä¸è¶³ï¼Œå¯æœ€å¤šå¯ä»¥è®©å­˜å‚¨å°†å ç”¨è¶…è¿‡åˆå§‹å­˜å‚¨å†…å­˜å¤§å°çš„ç©ºé—´è¿”è¿˜ç»™è®¡ç®—å†…å­˜ å¦‚æœå­˜å‚¨ç©ºé—´ä¸è¶³ï¼Œå¯ä»¥å€Ÿç”¨è®¡ç®—å†…å­˜çš„å¤šä½™ç©ºé—´ acquireExecutionMemory: å®é™…è°ƒç”¨executionPool.acquireMemoryï¼Œä¾èµ–äºå›è°ƒå‡½æ•°maybeGrowExecutionPoolå’ŒcomputeMaxExecutionPoolSizeï¼Œå‰è€…å¯èƒ½å°†éƒ¨åˆ†å­˜å‚¨å†…å­˜è½¬ç§»åˆ°è®¡ç®—å†…å­˜ï¼Œåè€…è®¡ç®—å½“å‰æƒ…å†µä¸‹æœ€å¤§è®¡ç®—å†…å­˜ï¼Œç­‰äºå¯ç”¨å†…å­˜å‡å»å­˜å‚¨å†…å­˜å½“å‰å ç”¨å’Œå­˜å‚¨å†…å­˜åˆå§‹å¤§å°çš„æœ€å°å€¼ã€‚ acquiredStorageMemory: å¦‚æœå­˜å‚¨å†…å­˜ç©ºé—´ä¸è¶³ï¼Œåˆ™å°è¯•å€Ÿç”¨éƒ¨åˆ†è®¡ç®—å†…å­˜ç©ºé—´ï¼Œæœ€åè°ƒç”¨storagePool.acquireMemoryå®é™…æ‰§è¡Œæ“ä½œ acquireUnrollMemory: å®é™…è°ƒç”¨acquiredStorageMemory ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:4:4","tags":["Spark","å†…å­˜ç®¡ç†"],"title":"Sparkå†…å­˜ç®¡ç†","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","å†…å­˜ç®¡ç†"],"content":"MemoryPool ç®¡ç†ä¸€å—å¯ä»¥è°ƒæ•´å¤§å°çš„å†…å­˜åŒºåŸŸçš„å†…éƒ¨çŠ¶æ€å’Œä½¿ç”¨è®°å½•ã€‚ ExecutionMemoryPool å­—æ®µmemoryForTaskè®°å½•äº†æ¯ä¸ªtask id (long)å¯¹åº”çš„å†…å­˜æ¶ˆè€—(long)ã€‚ æ¯ä¸ªä»»åŠ¡æœ€å°‘å¯ä»¥å ç”¨ $1 / 2N * poolSize$ï¼Œè€Œæ¯ä¸ªä»»åŠ¡æœ€å¤šå ç”¨$1 / N * maxPoolsize$ acquireMemory: å¦‚æœæ˜¯æ–°çš„ä»»åŠ¡ï¼ŒåŠ å…¥memoryForTaskï¼Œå¹¶ä¸”é€šçŸ¥æ‰€æœ‰ç­‰å¾…è·å–è®¡ç®—å†…å­˜çš„ä»»åŠ¡ï¼Œå½“å‰ä»»åŠ¡æ•°å¢åŠ  å¾ªç¯ï¼Œç›´åˆ°ä»»åŠ¡å ç”¨è¶…è¿‡äº†ä¸Šé™1/Nï¼Œæˆ–è€…æœ‰ç©ºé—²å†…å­˜ï¼Œä»¥ä¸‹æ­¥éª¤å‡åœ¨å¾ªç¯ä½“æ¥ è°ƒç”¨maybeGrowPoolå°è¯•ä»å­˜å‚¨ç©ºé—´è·å–å†…å­˜ è®¡ç®—æ¯ä¸ªä»»åŠ¡çš„æœ€å°‘å†…å­˜å ç”¨å’Œæœ€é«˜å†…å­˜å ç”¨ å¦‚æœè·å¾—çš„è®¡ç®—å†…å­˜åŠ ä¸Šå½“å‰å†…å­˜å ç”¨ä½äºæœ€å°‘å†…å­˜å ç”¨ï¼Œåˆ™ç­‰å¾…é€šçŸ¥ å¦åˆ™æ›´æ–°çŠ¶æ€ï¼Œå¹¶è¿”å›è·å–åˆ°çš„å†…å­˜å¤§å° releaseMemory: é‡Šæ”¾å†…å­˜ï¼Œå¦‚æœé‡Šæ”¾åå½“å‰å†…å­˜ä¸º0ï¼Œåˆ™ç§»é™¤å½“å‰ä»»åŠ¡ï¼Œåªè¦é‡Šæ”¾å†…å­˜ï¼Œåˆ™é€šçŸ¥åœ¨acquiredMemoryç­‰å¾…çš„ä»»åŠ¡å†…å­˜å·²ç»é‡Šæ”¾ StorageMemoryPool acquireMemory: å¦‚æœå­˜å‚¨ç©ºé—´ä¸è¶³ï¼Œåˆ™è°ƒç”¨memoryStore.evictBlocksToFreeSpaceé‡Šæ”¾éƒ¨åˆ†ç©ºé—´ï¼Œåˆ¤æ–­éœ€è¦çš„å†…å­˜å¤§å°æ˜¯å¦å°äºç­‰äºå½“å‰ç©ºé—²å†…å­˜ releaseMemory: é‡Šæ”¾å†…å­˜ freeSpaceToShrinkPool: é‡Šæ”¾å†…å­˜æ¥å‡å°‘å­˜å‚¨ç©ºé—´çš„å ç”¨ï¼Œå¿…è¦æ—¶è°ƒç”¨memoryStore.evictBlocksToFreeSpaceé©±é€block ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:4:5","tags":["Spark","å†…å­˜ç®¡ç†"],"title":"Sparkå†…å­˜ç®¡ç†","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","å†…å­˜ç®¡ç†"],"content":"TaskMemoryManager å†…å­˜åœ°å€ç¼–ç  å½“éœ€è¦å°†ä¸€ä¸ªintæˆ–è€…longä¹‹ç±»çš„å…ƒç´ æ’å…¥åˆ°æ•°ç»„æˆ–è€…å †å¤–çš„æŒ‡å®šä½ç½®æ—¶ å¯¹äºå †å†…ï¼Œéœ€è¦çŸ¥é“æ•°æ®çš„å¼•ç”¨ä»¥åŠåç§»é‡ï¼Œåœ¨TaskMemoryManagerä¸­ä¿å­˜äº†pageNumberå’ŒMemoryBlockçš„æ˜ å°„ï¼Œè€ŒMemoryBlockä¿å­˜äº†å¯¹è±¡çš„å¼•ç”¨ï¼Œæ‰€ä»¥ä½¿ç”¨64ä½ç¼–ç å†…å­˜åœ°å€æ—¶ï¼Œå‰13ä½ç”¨æ¥å‚¨å­˜pageNumberï¼Œå51ä½ç”¨æ¥å­˜å‚¨æ•°ç»„ä¸­çš„åç§»é‡ã€‚ï¼ˆå¯¹è±¡çš„åœ°å€ä¼šç”±äºgcçš„åŸå› è€Œå˜åŠ¨ï¼Œæ‰€ä»¥ä¸èƒ½ç›´æ¥ä½¿ç”¨å¯¹è±¡åœ°å€ï¼‰ å¯¹äºå †å¤–ï¼Œéœ€è¦çŸ¥é“ç”³è¯·åˆ°å †å¤–å†…å­˜çš„èµ·å§‹åœ°å€å’Œåç§»é‡ï¼Œä¾ç„¶ä½¿ç”¨å‰13ä½å­˜å‚¨pageNumberï¼Œä½¿ç”¨å51ä½å­˜å‚¨åç§»é‡ã€‚è¿™é‡Œå¦‚æœç›´æ¥ä½¿ç”¨å†…å­˜åœ°å€ï¼Œåˆ™ä¸èƒ½çŸ¥é“å¯¹åº”çš„pageæ˜¯é‚£ä¸ªï¼Œå½“ä½¿ç”¨å‰13ä½å‚¨å­˜pageNumberåï¼Œå51ä½æ˜¾ç„¶ä¸èƒ½å‚¨å­˜å†…å­˜çš„ç»å¯¹åœ°å€ï¼Œè€Œåº”è¯¥å­˜å‚¨å†…å­˜ç›¸å¯¹äºèµ·å§‹åœ°å€çš„åç§»é‡ã€‚ ä¸»è¦å­—æ®µä½œç”¨ pageTable: é¡µè¡¨ï¼Œä¿å­˜pageNumberåˆ°MemoryBlockçš„æ˜ å°„ï¼ŒMemoryBlock[PAGE_TABLE_SIZE] memoryManager: TaskMemoryManagerå…±äº«MemoryManagerçš„å†…å­˜èµ„æº taskAttemptId: task Id tungtenMemoryMode: ä½¿ç”¨å †å†…è¿˜æ˜¯å †å¤–å†…å­˜ï¼Œå’ŒMemoryMangerä¿æŒä¸€è‡´ consumersï¼šå†…å­˜æ¶ˆè´¹è€…ï¼Œæ”¯æŒåç£ç›˜ï¼ŒHashSet\u003cMemoryConsumer\u003e acquiredButNotUsed: å‘å†…å­˜ç®¡ç†æ¡†æ¶ç”³è¯·å†…å­˜æˆåŠŸï¼Œä½†å®é™…ç”³è¯·å†…å­˜æ—¶å‘ç”ŸOOMï¼Œè®¤ä¸ºMemoryManagerå¯èƒ½é«˜ä¼°äº†å®é™…çš„å¯ç”¨å†…å­˜ï¼Œå°†è¿™éƒ¨åˆ†å†…å­˜é…é¢ä¿å­˜åœ¨æ­¤å­—æ®µï¼Œæ–¹ä¾¿åç»­è§¦å‘åç£ç›˜ï¼Œlong currentOffHeapMemory: ä»»åŠ¡å½“å‰å †å¤–å†…å­˜å ç”¨ï¼Œlong currentOnHeapMemoryï¼šä»»åŠ¡å½“å‰å †å†…å†…å­˜å ç”¨ï¼Œlong peakOffHeapMemoryï¼šä»»åŠ¡æœ€é«˜å †å¤–å†…å­˜å ç”¨ï¼Œlong peakOnHeapMemoryï¼šä»»åŠ¡æœ€é«˜å †å†…å†…å­˜å ç”¨ï¼Œlong ä¸»è¦æ–¹æ³• acquireExecutionMemoryä¸ºæŒ‡å®šçš„MemoryConsumerè·å–å†…å­˜ï¼Œå¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„å†…å­˜ï¼Œè§¦å‘åç£ç›˜é‡Šæ”¾å†…å­˜ï¼Œè¿”å›æˆåŠŸè·å¾—çš„è®¡ç®—å†…å­˜(\u003c=N)ã€‚ public long acquireExecutionMemory(long required, MemoryConsumer requestingConsumer) { é¦–å…ˆè°ƒç”¨MemoryManager.acquireExecutionMemoryå°è¯•è·å–è®¡ç®—å†…å­˜ å¦‚æœè·å–åˆ°è¶³å¤Ÿçš„å†…å­˜ï¼Œåˆ™è·³è¿‡åç£ç›˜é€»è¾‘ å¦‚æœæ²¡æœ‰è·å–åˆ°è¶³å¤Ÿçš„å†…å­˜ï¼Œå°è¯•åç£ç›˜é‡Šæ”¾å†…å­˜ï¼Œå¹¶å°è¯•è·å–è®¡ç®—å†…å­˜ åç£ç›˜æœ‰ä¸¤ä¸ªä¼˜åŒ–çš„ç›®æ ‡ï¼š æœ€å°åŒ–åç£ç›˜è°ƒç”¨çš„æ¬¡æ•°ï¼Œå‡å°‘åç£ç›˜æ–‡ä»¶çš„æ•°é‡å¹¶ä¸”é¿å…å°çš„åç£ç›˜æ–‡ä»¶ é¿å…åç£ç›˜é‡Šæ”¾å†…å­˜è¶…è¿‡æ‰€éœ€ï¼Œå¦‚æœæˆ‘ä»¬åªæ˜¯æƒ³è¦ä¸€ä¸ç‚¹å†…å­˜ï¼Œä¸å¸Œæœ›å°½å¯èƒ½å¤šçš„åç£ç›˜ï¼Œå¾ˆå¤šå†…å­˜æ¶ˆè´¹è€…åç£ç›˜æ—¶ä¼šé‡Šæ”¾æ¯”è¯·æ±‚å¤šçš„å†…å­˜ æ‰€ä»¥è¿™é‡Œé‡‡ç”¨ä¸€ç§å¯å‘å¼çš„ç®—æ³•ï¼Œé€‰æ‹©å†…å­˜å ç”¨è¶…è¿‡æ‰€éœ€å†…å­˜çš„MemoryConsumerä¸­æœ€å°çš„MemoryConsumeræ¥å¹³è¡¡è¿™äº›å› ç´ ï¼Œå½“åªæœ‰å°‘é‡å¤§å†…å­˜è¯·æ±‚æ—¶ï¼Œè¿™ç§æ–¹æ³•æ•ˆç‡å¾ˆå¥½ï¼Œä½†å¦‚æœåœºæ™¯ä¸­æœ‰å¤§é‡å°å†…å­˜è¯·æ±‚ï¼Œè¿™ç§æ–¹æ³•ä¼šå¯¼è‡´äº§ç”Ÿå¤§é‡å°çš„spillæ–‡ä»¶ å…·ä½“å®ç°ï¼Œå°†æ‰€æœ‰çš„MemoryConsumeræ”¾å…¥ä¸€ä¸ªTreeMapä¸­ï¼Œæ ¹æ®å†…å­˜å ç”¨æ’åºï¼Œå¦‚æœæ˜¯å½“å‰MemoryConsumerï¼Œåˆ™è®¤ä¸ºå†…å­˜å ç”¨ä¸º0ï¼Œè¿™æ ·å½“å‰MemoryConsumerè¢«spillçš„ä¼˜å…ˆçº§æœ€ä½ã€‚ ç„¶åé€‰æ‹©å†…å­˜å ç”¨è¶…è¿‡æ‰€éœ€å†…å­˜çš„MemoryConsumerä¸­æœ€å°çš„MemoryConsumerè¿›è¡Œåç£ç›˜æ“ä½œå¹¶ä¸”å°è¯•è·å–è®¡ç®—å†…å­˜ï¼Œå¦‚æœæ²¡æœ‰ç¬¦åˆè¿™ä¸€æ¡ä»¶çš„MemoryConsumerï¼Œåˆ™ç›´æ¥é€‰æ‹©å†…å­˜å ç”¨æœ€å¤§çš„MemoryCosumerè¿›è¡Œåç£ç›˜å¹¶å°è¯•è·å–è®¡ç®—å†…å­˜trySpillAndAcquireã€‚ å¦‚æœè·å–åˆ°çš„å†…å­˜ä¾ç„¶ä¸æ»¡è¶³éœ€æ±‚ï¼Œåˆ™ç»§ç»­åç£ç›˜æµç¨‹ï¼Œé€‰æ‹©ä¸‹ä¸€ä¸ªMemoryConsumerï¼Œé‡å¤ä¸Šè¿°æµç¨‹ã€‚ æœ€ç»ˆä¸ç®¡æ˜¯å¦è·å–åˆ°äº†æ‰€éœ€çš„å†…å­˜ï¼Œéƒ½å°†MemoryConsumeråŠ å…¥consumersä¸­ï¼Œå¹¶æ›´æ–°å½“å‰å’Œæœ€é«˜çš„ä»»åŠ¡å†…å­˜å ç”¨ trySpillAndAcquireå¯¹é€‰ä¸­çš„MemoryConsumeræ‰§è¡Œåç£ç›˜æ“ä½œé‡Šæ”¾å†…å­˜ï¼Œå¹¶å°è¯•è·å–æ‰€éœ€çš„è®¡ç®—å†…å­˜ * @return number of bytes acquired (\u003c= requested) * @throws RuntimeException if task is interrupted * @throws SparkOutOfMemoryError if an IOException occurs during spilling */ private long trySpillAndAcquire(MemoryConsumer requestingConsumer, long requested, List\u003cMemoryConsumer\u003e cList, int idx) é¦–å…ˆè°ƒç”¨MemoryConsumer#spillæ–¹æ³•å°è¯•é‡Šæ”¾å†…å­˜ï¼Œå¦‚æœé‡Šæ”¾å†…å­˜ä¸º0ï¼Œåˆ™ç›´æ¥è¿”å›0 å¦‚æœé‡Šæ”¾å†…å­˜å¤§äº0ï¼Œè°ƒç”¨MemoryManager#acquireExecutionMemoryå°è¯•è·å–è®¡ç®—å†…å­˜ï¼Œè¿™é‡Œéœ€è¦æ³¨æ„ï¼Œåç£ç›˜é‡Šæ”¾çš„å†…å­˜ä¼šè¢«æ‰€æœ‰ä»»åŠ¡å…¬å¹³ç«äº‰ï¼Œæ‰€ä»¥å¯èƒ½æ— æ³•è·å–åˆ°è¿™æ¬¡åç£ç›˜é‡Šæ”¾çš„æ‰€æœ‰å†…å­˜ï¼Œéœ€è¦åœ¨ä¸‹ä¸€æ¬¡å¾ªç¯ä¸­ç»§ç»­å°è¯•åç£ç›˜ ä¸¤ç§å¼‚å¸¸åœºæ™¯ï¼Œå½“ä»»åŠ¡è¢«ä¸­æ–­æ—¶ï¼ŒæŠ›å‡ºRuntimeExceptionï¼Œåç£ç›˜é‡åˆ°IOExceptionæ—¶ï¼ŒæŠ›å‡ºSparkOutOfMemoryError releaseExecutionMemory ä¸ºä¸€ä¸ªMemoryConsumeré‡Šæ”¾Nå­—èŠ‚çš„è®¡ç®—å†…å­˜ï¼Œå®é™…è°ƒç”¨äº†MemoryManager#releaseExecutionMemoryï¼Œå¹¶æ›´æ–°å½“å‰å†…å­˜å ç”¨ showMemoryUsage dumpæ‰€æœ‰Consumerçš„å†…å­˜å ç”¨ allocatePage åˆ†é…å†…å­˜ï¼Œå¹¶æ›´æ–°é¡µè¡¨ï¼Œè¯¥æ“ä½œæ—¨åœ¨ä¸ºå¤šä¸ªç®—å­ä¹‹é—´å…±äº«çš„å¤§å—å†…å­˜åˆ†é…ç©ºé—´ public MemoryBlock allocatePage(long size, MemoryConsumer consumer) é¦–å…ˆè°ƒç”¨TaskMemoryManager#acquiredExectionMemoryè·å–è®¡ç®—å†…å­˜ï¼Œå¦‚æœæ²¡æœ‰è·å–åˆ°å†…å­˜ï¼Œåˆ™è¿”å›null ç„¶åé€šè¿‡MemoryManager#tungstenMemoryAllocator#allocateå®é™…ç”³è¯·å†…å­˜ï¼Œå¦‚æœé‡åˆ°OutOfMemoryErrorï¼Œåˆ™è®¤ä¸ºå®é™…ä¸Šæ²¡æœ‰è¶³å¤Ÿå¤šçš„å†…å­˜ï¼Œå®é™…çš„ç©ºé—²å†…å­˜è¦æ¯”MemoryManagerè®¤ä¸ºçš„å°‘ä¸€äº›ï¼Œæ‰€ä»¥å°†ä»å†…å­˜ç®¡ç†æ¡†æ¶ä¸­è·å¾—çš„å†…å­˜é…é¢æ·»åŠ åˆ°acquiredButNotUsedå­—æ®µä¸­ï¼Œå¹¶å†æ¬¡è°ƒç”¨å½“å‰å‡½æ•°ï¼Œè¿™æ¬¡å°†è§¦å‘åç£ç›˜æ“ä½œé‡Šæ”¾å†…å­˜ï¼ˆp.s. æ„Ÿè§‰å¤„ç†OutOfMemoryErrorçš„æ„ä¹‰ä¸å¤§ï¼ŒOutOfMeomryErrorå‘ç”Ÿæ—¶åº”è¯¥ç›´æ¥ç»“æŸç¨‹åºï¼Œå› ä¸ºç¨‹åºå·²ç»è¿›å…¥äº†å¼‚å¸¸çŠ¶æ€ï¼Œæ— æ³•é¢„æ–™OutOfMemoryErrorå¯¹ç¨‹åºçš„å½±å“ï¼‰ å¦‚æœæˆåŠŸè·å–åˆ°å†…å­˜ï¼Œåˆ™éœ€è¦æ›´æ–°é¡µè¡¨ï¼Œå¹¶è¿”å›å¯¹åº”çš„é¡µï¼Œå…¶å®å°±æ˜¯MemoryBlock freePageé‡Šæ”¾é¡µå ç”¨çš„å†…å­˜ï¼Œæ›´æ–°pageNumberä¸ºFREED_IN_TMM_PAGE_NUMBERï¼Œæ¸…ç†é¡µè¡¨ï¼Œè°ƒç”¨MemoryManager.tunstenMemoryAllocator#freeå®é™…é‡Šæ”¾å†…å­˜ï¼Œè°ƒç”¨releaseExecutionMemoryé‡Šæ”¾å†…å­˜ç®¡ç†æ¡†æ¶å¯¹åº”çš„å†…å­˜é…é¢ã€‚ ä¼¼ä¹ç”¨é€»è¾‘å†…å­˜æŒ‡ä»£å†…å­˜ç®¡ç†æ¡†æ¶ä¸­çš„å†…å­˜é…é¢ï¼Œè€Œç”¨ç‰©ç†å†…å­˜æŒ‡ä»£å®é™…çš„å†…å­˜æ›´åŠ å¥½ä¸€äº› public void freePage(MemoryBlock page, MemoryConsumer consumer) { cleanUpAllAllocatedMemoryæ¸…ç†æ‰€æœ‰ç”³è¯·çš„å†…å­˜å’Œé¡µ è°ƒç”¨MemoryManager#tungstenMemoryAllocator#freeé‡Šæ”¾æ¯ä¸ªé¡µçš„å†…å­˜ è°ƒç”¨MemoryManager#releaseExectionMemoryé‡Šæ”¾acquiredButNotUsedå†…å­˜ è°ƒç”¨MemoryManager#ReleaseAllExecutionMemoryForTaské‡Šæ”¾ä»»åŠ¡çš„æ‰€æœ‰è®¡ç®—å†…å­˜ï¼Œå¹¶è¿”å›é‡Šæ”¾çš„å†…å­˜å¤§å°ï¼Œé0å€¼å¯ä»¥ç”¨æ¥æ£€æµ‹å†…å­˜æ³„éœ² ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:4:6","tags":["Spark","å†…å­˜ç®¡ç†"],"title":"Sparkå†…å­˜ç®¡ç†","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","å†…å­˜ç®¡ç†"],"content":"å‚è€ƒèµ„æ–™ Deep Dive into Spark Memory Management Apache Spark Memory Management: Deep Dive ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:5:0","tags":["Spark","å†…å­˜ç®¡ç†"],"title":"Sparkå†…å­˜ç®¡ç†","uri":"/posts/spark_memory_manager/"},{"categories":["kubernetes"],"content":"Pods Pod æ˜¯ Kubernetes ä¸­æœ€å°çš„å¯éƒ¨ç½²è®¡ç®—å•å…ƒï¼Œä½ å¯ä»¥åˆ›å»ºå’Œç®¡ç†å®ƒä»¬ã€‚ Podï¼ˆç±»ä¼¼äºä¸€ç¾¤é²¸é±¼çš„â€œpodâ€æˆ–è±Œè±†èšâ€œpea podâ€ï¼‰ æ˜¯ä¸€ç»„ä¸€ä¸ªæˆ–å¤šä¸ªå®¹å™¨ï¼Œè¿™äº›å®¹å™¨å…±äº«å­˜å‚¨å’Œç½‘ç»œèµ„æºï¼Œå¹¶ä¸”æœ‰ä¸€ä¸ªè§„èŒƒæ¥å®šä¹‰å¦‚ä½•è¿è¡Œå®ƒä»¬ã€‚Pod å†…éƒ¨çš„å†…å®¹å§‹ç»ˆæ˜¯ å…±åŒè°ƒåº¦ï¼ˆco-scheduledï¼‰å¹¶åœ¨ç›¸åŒçš„ä¸Šä¸‹æ–‡ä¸­è¿è¡Œ çš„ã€‚Pod å……å½“ä¸€ä¸ªç‰¹å®šåº”ç”¨çš„â€œé€»è¾‘ä¸»æœºâ€ï¼ˆlogical hostï¼‰ï¼šå®ƒåŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ª ç›¸å¯¹ç´§å¯†è€¦åˆçš„åº”ç”¨å®¹å™¨ã€‚åœ¨éäº‘ç¯å¢ƒä¸‹ï¼Œè¿è¡Œåœ¨åŒä¸€å°ç‰©ç†æœºæˆ–è™šæ‹Ÿæœºä¸Šçš„åº”ç”¨ç¨‹åºï¼Œå¯ä»¥ç±»æ¯”äºåœ¨ Kubernetes ä¸­è¿è¡Œåœ¨åŒä¸€é€»è¾‘ä¸»æœºä¸Šçš„åº”ç”¨ã€‚ é™¤äº†åº”ç”¨å®¹å™¨ä¹‹å¤–ï¼ŒPod è¿˜å¯ä»¥åŒ…å« Init å®¹å™¨ï¼ˆinit containersï¼‰ï¼Œè¿™äº›å®¹å™¨åœ¨ Pod å¯åŠ¨æ—¶è¿è¡Œã€‚æ­¤å¤–ï¼Œä½ è¿˜å¯ä»¥ æ³¨å…¥ä¸´æ—¶å®¹å™¨ï¼ˆephemeral containersï¼‰ æ¥è°ƒè¯•æ­£åœ¨è¿è¡Œçš„ Podã€‚ ","date":"2025-02-07","objectID":"/posts/k8s_storage/:1:0","tags":null,"title":"k8s å­˜å‚¨","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"Volumes k8s Volumes ä¸ºpodä¸­çš„å®¹å™¨æä¾›äº†é€šè¿‡æ–‡ä»¶ç³»ç»Ÿè®¿é—®å’Œå…±äº«æ•°æ®çš„æ–¹å¼ï¼Œæ•°æ®å…±äº«å¯ä»¥åœ¨ä¸€ä¸ªå®¹å™¨ä¸­çš„ä¸åŒè¿›ç¨‹æˆ–è€…å®¹å™¨é—´ç”šè‡³ä¸åŒçš„podã€‚ volumeèƒ½å¤Ÿè§£å†³æ•°æ®çš„æŒä¹…åŒ–ä»¥åŠå…±äº«å­˜å‚¨çš„é—®é¢˜ã€‚ k8sæ”¯æŒå¤šç§volumesï¼Œpodå¯ä»¥åŒæ—¶ä½¿ç”¨ä»»æ„æ•°é‡çš„ä¸åŒç±»å‹volumeï¼ŒEphemeral volume çš„ç”Ÿå‘½å‘¨æœŸå’Œpodç›¸åŒï¼Œpersistent volumes å¯ä»¥è¶…å‡ºä¸€ä¸ªpodçš„ç”Ÿå‘½å‘¨æœŸã€‚å½“podæŒ‚æ‰æ—¶ï¼ŒK8sä¼šæ‘§æ¯ ephemeral volume ä½†ä¸ä¼šæ‘§æ¯ persistent volume ã€‚å¯¹äºåœ¨ç»™å®špodä¸­çš„ä»»æ„ç±»å‹çš„volumeï¼Œæ•°æ®åœ¨å®¹å™¨é‡å¯æ—¶éƒ½ä¼šè¢«ä¿ç•™ã€‚ æœ¬è´¨ä¸Šï¼Œå·ï¼ˆVolumeï¼‰æ˜¯ä¸€ä¸ªç›®å½•ï¼Œå…¶ä¸­å¯èƒ½åŒ…å«ä¸€äº›æ•°æ®ï¼Œå¹¶ä¸”å¯ä¾› Pod å†…çš„å®¹å™¨è®¿é—®ã€‚è¯¥ç›®å½•å¦‚ä½•åˆ›å»ºã€ç”±ä½•ç§å­˜å‚¨ä»‹è´¨æ”¯æŒä»¥åŠå…¶å†…å®¹ï¼Œå–å†³äºæ‰€ä½¿ç”¨çš„ç‰¹å®šå·ç±»å‹ã€‚ ä¸ºäº†ä½¿ç”¨ä¸€ä¸ªå·ï¼Œå£°æ˜è¦è¢«æä¾›ç»™podçš„å·åœ¨.spec.volumesä¸‹ï¼Œå£°æ˜åœ¨å®¹å™¨çš„å“ªé‡ŒæŒ‚è½½è¿™äº›å·åœ¨spec.containers[*].volumeMountsä¸­ã€‚ å½“ä¸€ä¸ªpodè¢«å¯åŠ¨æ—¶ï¼Œå®¹å™¨ä¸­çš„è¿›ç¨‹çœ‹åˆ°çš„æ–‡ä»¶ç³»ç»Ÿè§†å›¾æœ‰ä¸¤éƒ¨åˆ†ç»„æˆï¼Œä¸€éƒ¨åˆ†æ˜¯å®¹å™¨é•œåƒçš„åˆå§‹å†…å®¹ï¼Œå¦ä¸€éƒ¨åˆ†æ˜¯æŒ‚è½½åˆ°å®¹å™¨ä¸­çš„å·ã€‚å¯¹äºpodä¸­çš„æ¯ä¸ªå®¹å™¨ï¼Œéœ€è¦ç‹¬ç«‹çš„å£°æ˜ä¸åŒå®¹å™¨çš„æŒ‚è½½ç‚¹ã€‚ ","date":"2025-02-07","objectID":"/posts/k8s_storage/:2:0","tags":null,"title":"k8s å­˜å‚¨","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"Storage Classes StorageClass ä¸ºç®¡ç†å‘˜æä¾›äº†ä¸€ç§æè¿°å…¶æä¾›çš„å­˜å‚¨ç±»åˆ«çš„æ–¹æ³•ã€‚ä¸åŒçš„å­˜å‚¨ç±»åˆ«å¯èƒ½å¯¹åº”ä¸åŒçš„ æœåŠ¡è´¨é‡ï¼ˆQoSï¼‰çº§åˆ«ã€å¤‡ä»½ç­–ç•¥ï¼Œæˆ–è€…ç”±é›†ç¾¤ç®¡ç†å‘˜è‡ªå®šä¹‰çš„å…¶ä»–ç­–ç•¥ã€‚Kubernetes æœ¬èº«å¹¶ä¸å¯¹è¿™äº›å­˜å‚¨ç±»åˆ«çš„å…·ä½“å«ä¹‰åšä»»ä½•è§„å®šã€‚ æ¯ä¸ªStorageClass åŒ…å«å­—æ®µ provisioner, parameterså’ŒraclaimPolicyï¼Œå½“ä¸€ä¸ªå±äºæŸä¸ªstorage class çš„persistent volumeéœ€è¦è¢«åŠ¨æ€æä¾›ç»™ persistent volume claimæ—¶è¢«ä½¿ç”¨ã€‚ Storage Classçš„åå­—éå¸¸é‡è¦ï¼Œç”¨æˆ·é€šè¿‡åå­—è¯·æ±‚æŸç±»å­˜å‚¨ï¼Œç®¡ç†å‘˜åœ¨åˆ›å»ºstorage classå¯¹è±¡æ˜¯è®¾ç½®åå­—ä»¥åŠç±»åˆ«çš„å…¶ä»–å‚æ•°ã€‚ ä½œä¸ºç®¡ç†å‘˜ï¼Œä½ å¯ä»¥å£°æ˜ä¸€ä¸ªé»˜è®¤çš„storage classç”¨äºæ²¡æœ‰æŒ‡å®šç±»åˆ«çš„ä»»ä½•PVCã€‚ apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: low-latency annotations: storageclass.kubernetes.io/is-default-class: \"false\" provisioner: csi-driver.example-vendor.example reclaimPolicy: Retain # default value is Delete allowVolumeExpansion: true mountOptions: - discard # this might enable UNMAP / TRIM at the block storage layer volumeBindingMode: WaitForFirstConsumer parameters: guaranteedReadWriteLatency: \"true\" # provider-specific ","date":"2025-02-07","objectID":"/posts/k8s_storage/:3:0","tags":null,"title":"k8s å­˜å‚¨","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"Persistent Volumes PersistentVolume (PV) æ˜¯é›†ç¾¤ä¸­çš„ä¸€å—å­˜å‚¨ï¼Œå¯ä»¥ç”±ç®¡ç†å‘˜æä¾›æˆ–è€…é€šè¿‡ Storage ClassesåŠ¨æ€æä¾›ã€‚ **PersistentVolumeClaim (PVC)**æ˜¯ç”¨æˆ·å¯¹å­˜å‚¨çš„é’ç§€åŒºï¼Œç±»ä¼¼äºpodï¼Œpodæ¶ˆè´¹èŠ‚ç‚¹èµ„æºè€ŒPVCsæ¶ˆè´¹PVèµ„æº æœ‰ä¸¤ç§æ–¹å¼æä¾›PVsï¼š é™æ€ï¼šç®¡ç†å‘˜ç›´æ¥åˆ›å»ºPV åŠ¨æ€ï¼šå½“æ²¡æœ‰é™æ€PVæ»¡è¶³PVCï¼Œé›†ç¾¤å¯èƒ½å°è¯•åŠ¨æ€çš„æä¾›å·ï¼ŒPVCå¿…é¡»è¦æ±‚Storage Classï¼Œç®¡ç†å‘˜å¿…é¡»åˆ›å»ºå¹¶ä¸”é…ç½®storage classï¼ŒStorage Class \"\"å…³é—­åŠ¨æ€è·å–å· Podä½¿ç”¨PVCä½œä¸ºå·ï¼Œé›†ç¾¤æ£€æŸ¥PVCå¾—åˆ°å¯¹åº”çš„å·å¹¶å°†å·ç»‘å®šåˆ°podã€‚ ","date":"2025-02-07","objectID":"/posts/k8s_storage/:4:0","tags":null,"title":"k8s å­˜å‚¨","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"å›æ”¶ç­–ç•¥ å½“ç”¨æˆ·ä½¿ç”¨å®Œå·åï¼Œå¯ä»¥å°†PVCå¯¹è±¡åˆ é™¤ä»è€Œå…è®¸èµ„æºçš„å›æ”¶ï¼ŒPVçš„å›æ”¶ç­–ç•¥å‘Šè¯‰é›†ç¾¤å½“å·è¢«é‡Šæ”¾ååº”è¯¥æ€ä¹ˆæ ·å¤„ç†å·ï¼Œç›®å‰æœ‰ä¸‰ç§ç­–ç•¥ï¼šRetained, Recycledå’Œ Deletedã€‚ è¿™é‡Œåªä»‹ç»Retainç­–ç•¥ï¼š Retainå›æ”¶ç­–ç•¥å…è®¸æ‰‹åŠ¨çš„èµ„æºå›æ”¶ï¼Œå½“PVCè¢«åˆ é™¤æ—¶ï¼Œ PVä¾ç„¶å­˜åœ¨ï¼Œå·è¢«è®¤ä¸ºæ˜¯é‡Šæ”¾çŠ¶æ€ï¼Œä½†å®ƒå¹¶ä¸èƒ½å¤Ÿè¢«å¦ä¸€ä¸ªPVCè¯·æ±‚ç›´æ¥ä½¿ç”¨å› ä¸ºå‰ä»»çš„æ•°æ®è¿˜åœ¨ä¸Šé¢ã€‚ç®¡ç†å‘˜å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ‰‹åŠ¨å›æ”¶å·ï¼š åˆ é™¤PV æ‰‹åŠ¨æ¸…ç†æ•°æ® æ‰‹åŠ¨åˆ é™¤å¯¹åº”çš„storage asset å¦‚æœæƒ³è¦é‡ç”¨ç›¸åŒçš„storage assetï¼Œä½¿ç”¨ç›¸åŒçš„storage asset definition åˆ›å»ºä¸€ä¸ªæ–°çš„PV ","date":"2025-02-07","objectID":"/posts/k8s_storage/:4:1","tags":null,"title":"k8s å­˜å‚¨","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"PVçš„ç±»å‹ PVç±»å‹ä½œä¸ºæ’ä»¶å®ç°ï¼Œè¿™é‡Œç»™å‡ºk8sæ”¯æŒçš„ä¸€äº›æ’ä»¶ï¼š csi : Container Storage Interface local: æŒ‚è½½åœ¨èŠ‚ç‚¹ä¸Šçš„æœ¬åœ°å­˜å‚¨è®¾å¤‡ æ¯ä¸ªPVåŒ…å«ä¸€ä¸ªè§„èŒƒ(spec) å’ŒçŠ¶æ€ (status)ï¼ŒPVå¯¹è±¡çš„åå­—å¿…é¡»æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ DNS subdomain nameï¼Œè¿™æ„å‘³ç€ åŒ…å«ä¸è¶…è¿‡253ä¸ªå­—ç¬¦ åªåŒ…å«å°å†™å­—ç¬¦ã€æ•°å­—ã€-æˆ–è€…. å­—ç¬¦æˆ–è€…æ•°å­—å¼€å¤´ å­—ç¬¦æˆ–è€…æ•°å­—ç»“å°¾ apiVersion: v1 kind: PersistentVolume metadata: name: pv0003 spec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: slow mountOptions: - hard - nfsvers=4.1 nfs: path: /tmp server: 172.17.0.2 è®¿é—®æ¨¡å¼ ReadWriteOnceï¼Œå·å¯ä»¥è¢«æŒ‚è½½ä¸ºå•ä¸ªèŠ‚ç‚¹å¯è¯»å†™ï¼ŒReadWriteOnceä»ç„¶å…è®¸å¤šä¸ªpodè®¿é—®ï¼Œåªè¦è¿™äº›podåœ¨ç›¸åŒçš„èŠ‚ç‚¹ä¸Šã€‚å¯¹äºå•podè®¿é—®ï¼Œå¯ä»¥ä½¿ç”¨ReadWriteOncePodã€‚ èŠ‚ç‚¹äº²å’Œåº¦ Node Affinityï¼Œå¯¹äºå¤§å¤šæ•°å·ç±»å‹ï¼Œä¸éœ€è¦è®¾ç½®è¿™ä¸ªå­—æ®µï¼Œå¯¹äºlocalå·éœ€è¦æ˜¾ç¤ºè®¾ç½®è¿™ä¸ªå­—æ®µã€‚ ä¸€ä¸ªPVå¯ä»¥å£°æ˜èŠ‚ç‚¹äº²å’Œåº¦æ¥é™åˆ¶åœ¨é‚£ä¸ªèŠ‚ç‚¹ä¸Šè¿™ä¸ªå·å¯ä»¥è¢«è®¿é—®ï¼Œä½¿ç”¨æŸä¸ªPVçš„Podå°†åªä¼šè¢«è°ƒåº¦åˆ°æ»¡è¶³èŠ‚ç‚¹äº²å’Œåº¦çš„èŠ‚ç‚¹ä¸Šã€‚ ","date":"2025-02-07","objectID":"/posts/k8s_storage/:4:2","tags":null,"title":"k8s å­˜å‚¨","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"PersistentVolumeClaims apiVersion: v1 kind: PersistentVolumeClaim metadata: name: myclaim spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: slow selector: matchLabels: release: \"stable\" matchExpressions: - {key: environment, operator: In, values: [dev]} ","date":"2025-02-07","objectID":"/posts/k8s_storage/:4:3","tags":null,"title":"k8s å­˜å‚¨","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"å‚è€ƒæ–‡çŒ® https://kubernetes.io/docs/concepts/storage/persistent-volumes/ ","date":"2025-02-07","objectID":"/posts/k8s_storage/:5:0","tags":null,"title":"k8s å­˜å‚¨","uri":"/posts/k8s_storage/"},{"categories":null,"content":"Prometheus https://www.youtube.com/watch?v=h4Sl21AKiDg Prometheus Server, Pushgateway, Alertmanager https://prometheus.io/docs/concepts/metric_types/ https://itnext.io/prometheus-for-beginners-5f20c2e89b6c Prometheus is essentially just another metrics collection and analysis tool, and at its core it is made up of 3 components: A time series database that will store all our metrics data A data retrieval worker that is responsible for pulling/scraping metrics from external sources and pushing them into the database A web server that provides a simple web interface for configuration and querying of the data stored. https://prometheus.io/docs/practices/naming/#metric-names https://prometheus.io/docs/practices/naming/#base-units ","date":"2025-02-01","objectID":"/posts/prometheus_and_grafana/:1:0","tags":["prometheus","grafana"],"title":"Prometheus_and_grafana","uri":"/posts/prometheus_and_grafana/"},{"categories":["MinIO"],"content":"xl.meta æ•°æ®ç»“æ„ å½“å¯¹è±¡å¤§å°è¶…è¿‡ 128KiB åï¼Œæ¯”å¦‚a.txtï¼Œæ•°æ®å’Œå…ƒæ•°æ®åˆ†å¼€å­˜å‚¨ MinIO æä¾›äº†å‘½ä»¤è¡Œå·¥å…·xl-metaç”¨æ¥æŸ¥çœ‹xl.metaæ–‡ä»¶ { \"Versions\": [ { \"Header\": { \"EcM\": 1, \"EcN\": 0, \"Flags\": 2, \"ModTime\": \"2025-01-23T15:27:45.311572+08:00\", \"Signature\": \"d0c2b58b\", \"Type\": 1, \"VersionID\": \"00000000000000000000000000000000\" }, \"Idx\": 0, \"Metadata\": { \"Type\": 1, \"V2Obj\": { \"CSumAlgo\": 1, \"DDir\": \"74hQxU7FTrq56ShK8pjqAA==\", \"EcAlgo\": 1, \"EcBSize\": 1048576, \"EcDist\": [1], \"EcIndex\": 1, \"EcM\": 1, \"EcN\": 0, \"ID\": \"AAAAAAAAAAAAAAAAAAAAAA==\", \"MTime\": 1737617265311572000, \"MetaSys\": {}, \"MetaUsr\": { \"content-type\": \"text/plain\", \"etag\": \"90a1a2b65a4e40d55d758f2a59fe33b4\" }, \"PartASizes\": [2097152], \"PartETags\": null, \"PartNums\": [1], \"PartSizes\": [2097152], \"Size\": 2097152 }, \"v\": 1734527744 } } ] } . â”œâ”€â”€ a.txt â”‚Â â”œâ”€â”€ ef8850c5-4ec5-4eba-b9e9-284af298ea00 â”‚Â â”‚Â â””â”€â”€ part.1 â”‚Â â””â”€â”€ xl.meta â””â”€â”€ b.txt â””â”€â”€ xl.meta ","date":"2025-01-22","objectID":"/posts/minio-get-started/:1:0","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"minio çš„å¯åŠ¨æµç¨‹ minio å¯åŠ¨æ ¸å¿ƒçš„æ ¸å¿ƒå‘½ä»¤ä¸º minio server https://minio{1...4}.example.net:9000/mnt/disk{1...4}/minioï¼Œè¡¨ç¤º minio æœåŠ¡åˆ†å¸ƒéƒ¨ç½²åœ¨ 4 å°æœåŠ¡å™¨ä¸Šæ€»å…± 16 å—ç£ç›˜ä¸Šï¼Œ...è¿™ç§å†™æ³•ç§°ä¹‹ä¸ºæ‹“å±•è¡¨è¾¾å¼ï¼Œæ¯”å¦‚ http://minio{1â€¦4}.example.net:9000å®é™…ä¸Šè¡¨ç¤ºhttp://minio1.example.net:9000åˆ°http://minio4.example.net:9000`çš„4å°ä¸»æœºã€‚ go ç¨‹åºçš„å…¥å£ä¸ºmain#main()å‡½æ•°ï¼Œç›´æ¥è°ƒç”¨äº†cmd#Main,å…¶ä¸­åšäº†ä¸€äº›å‘½ä»¤è¡Œç¨‹åºçš„ç›¸å…³æ“ä½œï¼ŒåŒ…æ‹¬æ³¨å†Œå‘½ä»¤ï¼Œå…¶ä¸­registerCommand(serverCmd)æ³¨å†ŒæœåŠ¡ç›¸å…³å‘½ä»¤ï¼Œcmd#ServerMainæ˜¯ä¸»è¦å¯åŠ¨æµç¨‹å‡½æ•°ã€‚ // Run the app - exit on error. if err := newApp(appName).Run(args); err != nil { os.Exit(1) //nolint:gocritic } var serverCmd = cli.Command{ Name: \"server\", Usage: \"start object storage server\", Flags: append(ServerFlags, GlobalFlags...), Action: serverMain, ","date":"2025-01-22","objectID":"/posts/minio-get-started/:2:0","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"ServerMain server http://127.0.0.1:/Users/hanjing/mnt/minio0{1...3} http://127.0.0.1:/Users/hanjing/mnt/minio0{4...6} å¤„ç†ç³»ç»Ÿç»ˆæ­¢æˆ–è€…é‡å¯ç›¸å…³çš„ä¿¡å·ç­‰ signal.Notify(globalOSSignalCh, os.Interrupt, syscall.SIGTERM, syscall.SIGQUIT) go handleSignals() buildServerCtxtå†³å®šç£ç›˜å¸ƒå±€ä»¥åŠæ˜¯å¦ä½¿ç”¨ legacy æ–¹å¼ï¼Œè°ƒç”¨å‡½æ•°cmd#mergeDisksLayoutFromArgsåˆ¤æ–­æ˜¯å¦ä½¿ç”¨äº†æ‹“å±•è¡¨è¾¾å¼ï¼Œå¦‚æœæ²¡æœ‰ï¼Œlegacy = trueï¼Œå¦åˆ™legacy =false, legacyå‚æ•°çš„ä½œç”¨æˆ‘ä»¬åœ¨åé¢å°±èƒ½çœ‹åˆ°äº†ã€‚ serverHandleCmdArgså‡½æ•°ä¸­è°ƒç”¨ createServerEndpointsï¼Œ // Handle all server command args and build the disks layout bootstrapTrace(\"serverHandleCmdArgs\", func() { // è¿™é‡Œç¡®å®šäº†erasure set sizeçš„å¤§å° err := buildServerCtxt(ctx, \u0026globalServerCtxt) logger.FatalIf(err, \"Unable to prepare the list of endpoints\") serverHandleCmdArgs(globalServerCtxt) }) ","date":"2025-01-22","objectID":"/posts/minio-get-started/:2:1","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"MinIO çš„ DNS ç¼“å­˜ MinIO ä¸ºäº†é¿å…å‘å¤–å‘é€è¿‡å¤šçš„ DNS æŸ¥è¯¢ï¼Œæ‰€ä»¥å®ç°äº† DNS ç¼“å­˜ï¼Œé»˜è®¤ä½¿ç”¨net.DefaultResolverå®é™…æ‰§è¡Œ DNS æŸ¥è¯¢ï¼Œè®¾ç½®çš„ DNS æŸ¥è¯¢è¶…æ—¶æ—¶é—´ä¸º5sï¼Œç¼“å­˜çš„åˆ·æ–°æ—¶é—´åœ¨å®¹å™¨ç¯å¢ƒä¸‹é»˜è®¤ä¸º30sï¼Œåœ¨å…¶ä»–ç¯å¢ƒä¸‹ä¸º10minï¼Œå¯ä»¥é€šè¿‡dns-cache-ttlæŒ‡å®šã€‚ type Resolver struct { // Timeout defines the maximum allowed time allowed for a lookup. Timeout time.Duration // Resolver is used to perform actual DNS lookup. If nil, // net.DefaultResolver is used instead. Resolver DNSResolver once sync.Once mu sync.RWMutex cache map[string]*cacheEntry } globalDNSCache = \u0026dnscache.Resolver{ Timeout: 5 * time.Second, } func runDNSCache(ctx *cli.Context) { dnsTTL := ctx.Duration(\"dns-cache-ttl\") // Check if we have configured a custom DNS cache TTL. if dnsTTL \u003c= 0 { if orchestrated { dnsTTL = 30 * time.Second } else { dnsTTL = 10 * time.Minute } } // Call to refresh will refresh names in cache. go func() { // Baremetal setups set DNS refresh window up to dnsTTL duration. t := time.NewTicker(dnsTTL) defer t.Stop() for { select { case \u003c-t.C: globalDNSCache.Refresh() case \u003c-GlobalContext.Done(): return } } }() } ","date":"2025-01-22","objectID":"/posts/minio-get-started/:3:0","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"æ„é€ æ‹“æ‰‘å…³ç³» (buildServerCtxt) // serverCtxtä¿å­˜äº†ç£ç›˜å¸ƒå±€ type disksLayout struct { // æ˜¯å¦ä½¿ç”¨æ‹“å±•è¡¨è¾¾å¼ legacy bool // server poolçš„é›†åˆ pools []poolDisksLayout } type poolDisksLayout struct { // server poolå¯¹åº”çš„å‘½ä»¤è¡Œå‘½ä»¤ cmdline string // layoutçš„ç¬¬ä¸€ä½è¡¨ç¤ºä¸åŒçš„erasure setï¼Œç¬¬äºŒç»´è¡¨ç¤ºåŒä¸€ä¸ªerasure setä¸­ä¸åŒçš„ç£ç›˜è·¯å¾„ layout [][]string } æ„é€ æ‹“æ‰‘å…³ç³»çš„ä¸»è¦å‡½æ•°å®ç°æ˜¯mergeDisksLayoutFromArgsï¼Œåˆ¤æ–­ç¯å¢ƒå˜é‡MINIO_ERASURE_SET_DRIVE_COUNTæ˜¯å¦å­˜åœ¨ï¼Œç¯å¢ƒå˜é‡MINIO_ERASURE_SET_DRIVE_COUNTè¡¨ç¤º erasure set ä¸­æŒ‡å®šçš„ç£ç›˜æ•°é‡ï¼Œå¦åˆ™é»˜è®¤ä¸º 0ï¼Œè¡¨ç¤ºè‡ªåŠ¨è®¾ç½®æœ€ä¼˜ç»“æœã€‚æ ¹æ®æ˜¯å¦ä½¿ç”¨æ‹“å±•è¡¨è¾¾å¼ä¼šèµ°ä¸åŒçš„é€»è¾‘ã€‚è¿™é‡Œæˆ‘ä»¬ä¸»è¦å…³å¿ƒä½¿ç”¨æ‹“å±•è¡¨è¾¾å¼çš„åœºæ™¯GetAllSets(setDriveCount, arg)ã€‚ï¼ˆé¡ºå¸¦ä¸€æï¼Œlegacy style ä¼šèµ°GetAllSets(setDriveCount, args...)ï¼Œå¯ä»¥çœ‹åˆ° legacy style åªèƒ½æŒ‡å®šä¸€ä¸ªserver poolï¼‰ // mergeDisksLayoutFromArgs supports with and without ellipses transparently. // æ„é€ ç½‘ç»œæ‹“æ‰‘ func mergeDisksLayoutFromArgs(args []string, ctxt *serverCtxt) (err error) { if len(args) == 0 { return errInvalidArgument } ok := true // ok è¡¨ç¤ºæ˜¯å¦ä½¿ç”¨æ‹“å±•è¡¨è¾¾å¼ï¼Œtrueè¡¨ç¤ºä¸ä½¿ç”¨æ‹“å±•è¡¨è¾¾å¼ // åªè¦åœ¨å…¶ä¸­ä¸€ä¸ªargä¸­ä½¿ç”¨æ‹“å±•è¡¨è¾¾å¼ï¼Œç»“æœå‡ä¸ºfalse for _, arg := range args { ok = ok \u0026\u0026 !ellipses.HasEllipses(arg) } var setArgs [][]string // é€šè¿‡ç¯å¢ƒå˜é‡å¾—åˆ°erasure setçš„å¤§å°ï¼Œé»˜è®¤ä¸º0 v, err := env.GetInt(EnvErasureSetDriveCount, 0) if err != nil { return err } setDriveCount := uint64(v) // None of the args have ellipses use the old style. if ok { setArgs, err = GetAllSets(setDriveCount, args...) if err != nil { return err } // æ‰€æœ‰çš„å‚æ•°ç»„æˆä¸€ä¸ªserver pool ctxt.Layout = disksLayout{ legacy: true, pools: []poolDisksLayout{{layout: setArgs, cmdline: strings.Join(args, \" \")}}, } return } for _, arg := range args { if !ellipses.HasEllipses(arg) \u0026\u0026 len(args) \u003e 1 { // TODO: support SNSD deployments to be decommissioned in future return fmt.Errorf(\"all args must have ellipses for pool expansion (%w) args: %s\", errInvalidArgument, args) } setArgs, err = GetAllSets(setDriveCount, arg) if err != nil { return err } ctxt.Layout.pools = append(ctxt.Layout.pools, poolDisksLayout{cmdline: arg, layout: setArgs}) } return } GetAllSetsä¸»è¦è°ƒç”¨äº†parseEndpointSetï¼Œé€šè¿‡æ­£åˆ™è¡¨è¾¾å¼è§£æå¸¦æœ‰æ‹“å±•è¡¨è¾¾å¼çš„è¾“å…¥å‚æ•°ï¼Œå¹¶è¿”å›ä¸€ä¸ª[][]stringï¼Œè¡¨ç¤ºä¸åŒ erasure set ä¸­çš„ç£ç›˜è·¯å¾„ã€‚è¿™é‡Œä¸»è¦å¯¹åº”çš„æ•°æ®ç»“æ„æ˜¯endpointSetï¼Œä¸»è¦å®ç°ä¸¤ä»¶äº‹æƒ…ï¼Œç¬¬ä¸€ç¡®å®š setSizeï¼Œç¬¬äºŒç¡®å®šå¦‚ä½•å°† endpoints åˆ†å¸ƒåˆ°ä¸åŒçš„ erasure set ä¸­ã€‚ // Endpoint set represents parsed ellipses values, also provides // methods to get the sets of endpoints. type endpointSet struct { // è§£æç»ˆç«¯å­—ç¬¦ä¸²å¾—åˆ°çš„arg patternï¼Œå¦‚æœæœ‰å¤šä¸ªellipsesï¼Œå¯¹åº”å¤šä¸ª`Pattern` argPatterns []ellipses.ArgPattern endpoints []string // Endpoints saved from previous GetEndpoints(). // å¯¹äºellipses-styleçš„å‚æ•° // setIndexeså¯¹åº”ä¸€è¡Œï¼Œè®°å½•äº†server pool size /setSize ä¸ª setSizeå€¼ setIndexes [][]uint64 // All the sets. } type ArgPattern []Pattern // Pattern - ellipses pattern, describes the range and also the // associated prefix and suffixes. type Pattern struct { Prefix string Suffix string Seq []string } å‡½æ•°getSetIndexesçš„ç›®çš„æ˜¯æ‰¾åˆ°åˆé€‚çš„setSizeï¼ŒMinIO è§„å®šåˆ†å¸ƒå¼éƒ¨ç½² setSize çš„å–å€¼å¿…é¡»å±äºvar setSizes = []uint64{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}ï¼Œé¦–å…ˆä»SetSizesä¸­æ‰¾åˆ°èƒ½å¤Ÿè¢«server pool sizeæ•´é™¤çš„setCountsé›†åˆï¼Œå¦‚æœè‡ªå®šä¹‰äº†setSizeåˆ™åˆ¤æ–­è‡ªå®šä¹‰çš„setSizeæ˜¯å¦å±äºsetCountsé›†åˆï¼Œå¦‚æœå±äºåˆ™setSizeè®¾ç½®æˆåŠŸï¼Œå¦åˆ™è¿”å›é”™è¯¯ã€‚å¦‚æœæ²¡æœ‰è®¾ç½®è‡ªå®šä¹‰çš„setSizeï¼Œå‡½æ•°possibleSetCountsWithSymmetryä»setCountsé›†åˆä¸­æ‰¾åˆ°å…·æœ‰symmetryå±æ€§çš„å€¼ï¼ŒMinIO ä¸­è¾“å…¥å¸¦æ‹“å±•è¡¨è¾¾å¼çš„å‚æ•°å¯¹åº”çš„ pattern åˆ—è¡¨å’Œå‚æ•°ä¸­çš„é¡ºåºæ˜¯ç›¸åçš„ï¼Œsymmetryè¿‡æ»¤å‡ºèƒ½å¤Ÿè¢« pattern ä¸­æœ€åä¸€ä¸ª pattern å¯¹åº”çš„æ•°é‡æ•´é™¤æˆ–è€…è¢«æ•´é™¤çš„setCountsä¸­çš„å€¼ï¼Œè¿™é‡Œä¸¾ä¸€ä¸ªä¾‹å­http://127.0.0.{1...4}:9000/Users/hanjing/mnt/minio{1...32}ï¼Œæ˜¾ç„¶symmetryå‡½æ•°ä¼šåˆ¤æ–­ 4 å’ŒsetCountsä¸­å€¼çš„å…³ç³»ï¼Œè€Œä¸æ˜¯ 32 å’ŒsetCountsä¸­å€¼çš„å…³ç³»ï¼Œè¿™å¯èƒ½ä¸ MinIO å¸Œæœ›å°½å¯èƒ½å°† erasure set çš„ä¸­ä¸åŒç£ç›˜åˆ†å¸ƒåˆ°ä¸åŒçš„èŠ‚ç‚¹ä¸Šæœ‰å…³ã€‚æœ€åå–å‡ºå‰©ä½™å€™é€‰å€¼ä¸­æœ€å¤§çš„å€¼ä½œä¸ºæœ€ç»ˆçš„setSizeã€‚ func (s endpointSet) Get() (sets [][]string) { k := uint64(0) endpoints := s.getEndpoints() for i := range s.setIndexes { for j := range s.setIndexes[i] { sets = append(sets, endpoints[k:s.setIndexes[i][j]+k]) k = s.setIndexes[i][j] + k } } return sets } endpointSet#Getæ–¹æ³•è¿”å›ä¸€ä¸ªäºŒç»´æ•°æ®ï¼Œç¬¬ä¸€ç»´è¡¨ç¤º ä¸åŒçš„ erasure setï¼Œç¬¬äºŒä½è¡¨ç¤º erasure set ä¸­çš„ä¸åŒç£ç›˜ã€‚è¿™é‡ŒgetEndpointså¤šé‡å¾ªç¯è¿­ä»£ ellipses-style å¯¹åº”çš„ patternï¼Œå¦‚æœè¿˜è®°å¾—çš„è¯ï¼Œpattern çš„é¡ºåºå’Œå®é™…åœ¨å‚æ•°ä¸­å‡ºç°çš„é¡ºåºç›¸åï¼Œè¿™æ ·å¾—åˆ°çš„endpointsåˆ—è¡¨å°†ä¸åŒèŠ‚ç‚¹ä¸Šçš„ç£ç›˜å‡åŒ€åˆ†å¸ƒï¼Œåé¢è¿ç»­å–åˆ—è¡¨ä¸­çš„ä¸€æ®µç»„æˆerasure setæ—¶ï¼Œå¾—åˆ°çš„erasure setä¸­çš„ç£ç›˜ä¹Ÿåˆ†å¸ƒåœ¨ä¸åŒçš„èŠ‚ç‚¹ä¸Šã€‚ ","date":"2025-01-22","objectID":"/posts/minio-get-started/:4:0","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"serverHandleCmdArgs å‡½æ•° globalEndpoints, setupType, err = createServerEndpoints(globalMinioAddr, ctxt.Layout.pools, ctxt.Layout.legacy) logger.FatalIf(err, \"Invalid command line arguments\") globalNodes = globalEndpoints.GetNodes() globalIsErasure = (setupType == ErasureSetupType) globalIsDistErasure = (setupType == DistErasureSetupType) if globalIsDistErasure { globalIsErasure = true } globalIsErasureSD = (setupType == ErasureSDSetupType) if globalDynamicAPIPort \u0026\u0026 globalIsDistErasure { logger.FatalIf(errInvalidArgument, \"Invalid --address=\\\"%s\\\", port '0' is not allowed in a distributed erasure coded setup\", ctxt.Addr) } globalLocalNodeName = GetLocalPeer(globalEndpoints, globalMinioHost, globalMinioPort) nodeNameSum := sha256.Sum256([]byte(globalLocalNodeName)) globalLocalNodeNameHex = hex.EncodeToString(nodeNameSum[:]) // Initialize, see which NIC the service is running on, and save it as global value setGlobalInternodeInterface(ctxt.Interface) é‡Œé¢æœ‰ä¸€ä¸ªæ¯”è¾ƒé‡è¦çš„å·¥å…·å‡½æ•°isLocalHostï¼Œé€šè¿‡ DNS æŸ¥è¯¢ host å¯¹åº”çš„ ipï¼Œå’Œæ‰€æœ‰ç½‘å¡å¯¹åº”çš„æ‰€æœ‰æœ¬åœ° ip å–äº¤é›†,å¦‚æœäº¤é›†ä¸ºç©ºï¼Œè¯´æ˜ä¸æ˜¯æœ¬åœ°æœåŠ¡å™¨ï¼Œå¦åˆ™æ˜¯æœ¬åœ°æœåŠ¡å™¨ã€‚ å‡½æ•°createServerEndpointså°†æ•°æ®ç»“æ„[]poolDisksLayoutè½¬æ¢æˆEndpointServerPoolsï¼Œå¹¶æŒ‡å®šå¯¹åº”çš„SetupType å¯¹äºå•ç£ç›˜éƒ¨ç½²ï¼Œè¦æ±‚ä½¿ç”¨ç›®å½•è·¯å¾„æŒ‡å®šè¾“å…¥å‚æ•°ï¼ŒIsLocalä¸€å®šä¸ºtrueï¼ŒSetupTypeä¸ºErasureSDSetupTypeã€‚å…¶ä»–æƒ…å†µä¸‹æ ¹æ®ï¼Œæ ¹æ®æœ¬åœ° ip å’Œç»™å®šçš„ hostï¼Œåˆ¤æ–­IsLocalï¼Œå¦‚æœ host ä¸ºç©ºï¼ˆMinIO ç§°ä¸ºPathEndpointType)ï¼Œåˆ™setupType = ErasureSetupTypeï¼Œå¦åˆ™ä¸ºURLEndpointTypeæƒ…å†µï¼Œå¦‚æœä¸åŒhost:portçš„æ•°é‡ç­‰äº 1ï¼Œåˆ™æ˜¯ErasureSetupTypeï¼Œå¦åˆ™å¯¹åº”DistErasureSetupTypeï¼Œæ ¹æ®å¾—åˆ°çš„SetTypeè®¾ç½®å…¨å±€å‚æ•°ã€‚ EndpointServerPoolså®é™…ä¸Šæ˜¯[][]EndPointï¼Œç¬¬ä¸€ä½ // EndpointServerPoolsæ˜¯ PoolEndpointsçš„é›†åˆï¼Œå®é™…ä¸Šæè¿°æ•´ä¸ªéƒ¨ç½²çš„æ‹“æ‰‘ç»“æ„ type EndpointServerPools []PoolEndpoints // PoolEndpoints represent endpoints in a given pool // along with its setCount and setDriveCount. // PoolEndpointsè¡¨ç¤ºä¸€ä¸ªserver poolçš„ç»“æ„ type PoolEndpoints struct { // indicates if endpoints are provided in non-ellipses style // legacy è¡¨ç¤º æ˜¯å¦ä½¿ç”¨é—ç•™çš„æ–¹æ³•è¡¨ç¤ºç»ˆç«¯ï¼Œè€Œä¸ä½¿ç”¨çœç•¥å·è¡¨è¾¾å¼ Legacy bool // SetCountè¡¨ç¤º server poolä¸­çš„ erasure setçš„æ•°é‡ SetCount int // DrivesPerSet è¡¨ç¤ºä¸€ä¸ªerasure setä¸­çš„ç£ç›˜æ•°é‡ DrivesPerSet int // type Endpoints []Endpoint // è¡¨ç¤ºä¸€ä¸ªserver poolä¸­çš„æ‰€æœ‰disk Endpoints Endpoints // server poolå¯¹åº”çš„å‘½ä»¤è¡ŒæŒ‡ä»¤ CmdLine string // æ“ä½œç³»ç»Ÿä¿¡æ¯ Platform string } type Endpoint struct { *url.URL // å¦‚æœæ˜¯å•ä¸ªç›®å½•çš„è¾“å…¥ï¼Œåˆ™ IsLocalä¸ºtrue // å¦‚æœè¾“å…¥å‚æ•°ipæ˜¯æœ¬åœ°ipï¼ŒIsLocalä¹Ÿä¸ºtrue // å…¶ä»–æƒ…å†µä¸‹ä¸ºfalse IsLocal bool PoolIdx, SetIdx, DiskIdx int } // SetupType - enum for setup type. type SetupType int const ( // UnknownSetupType - starts with unknown setup type. UnknownSetupType SetupType = iota // FSSetupType - FS setup type enum. FSSetupType // ErasureSDSetupType - Erasure single drive setup enum. ErasureSDSetupType // ErasureSetupType - Erasure setup type enum. ErasureSetupType // DistErasureSetupType - Distributed Erasure setup type enum. DistErasureSetupType ) ä»¥ä¸‹å‡½æ•°åˆ—å‡ºäº† Minio æ”¯æŒçš„ä¸åŒæ¨¡å¼ï¼Œå’Œä¸Šé¢çš„SetTypeä¹‹é—´å­˜åœ¨å¯¹åº”å…³ç³»ã€‚ // Returns the mode in which MinIO is running func getMinioMode() string { switch { case globalIsDistErasure: return globalMinioModeDistErasure case globalIsErasure: return globalMinioModeErasure case globalIsErasureSD: return globalMinioModeErasureSD default: return globalMinioModeFS } } ","date":"2025-01-22","objectID":"/posts/minio-get-started/:4:1","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"HTTP æœåŠ¡å™¨æ³¨å†Œ API æ³¨å†Œåˆ†å¸ƒå¼å‘½åç©ºé—´é” registerAPIRouteræ³¨å†Œ s3 ç›¸å…³çš„ä¸»è¦ api // Configure server. bootstrapTrace(\"configureServer\", func() { handler, err := configureServerHandler(globalEndpoints) if err != nil { logger.Fatal(config.ErrUnexpectedError(err), \"Unable to configure one of server's RPC services\") } // Allow grid to start after registering all services. close(globalGridStart) close(globalLockGridStart) httpServer := xhttp.NewServer(getServerListenAddrs()). UseHandler(setCriticalErrorHandler(corsHandler(handler))). UseTLSConfig(newTLSConfig(getCert)). UseIdleTimeout(globalServerCtxt.IdleTimeout). UseReadTimeout(globalServerCtxt.IdleTimeout). UseWriteTimeout(globalServerCtxt.IdleTimeout). UseReadHeaderTimeout(globalServerCtxt.ReadHeaderTimeout). UseBaseContext(GlobalContext). UseCustomLogger(log.New(io.Discard, \"\", 0)). // Turn-off random logging by Go stdlib UseTCPOptions(globalTCPOptions) httpServer.TCPOptions.Trace = bootstrapTraceMsg go func() { serveFn, err := httpServer.Init(GlobalContext, func(listenAddr string, err error) { bootLogIf(GlobalContext, fmt.Errorf(\"Unable to listen on `%s`: %v\", listenAddr, err)) }) if err != nil { globalHTTPServerErrorCh \u003c- err return } globalHTTPServerErrorCh \u003c- serveFn() }() setHTTPServer(httpServer) }) // configureServer handler returns final handler for the http server. func configureServerHandler(endpointServerPools EndpointServerPools) (http.Handler, error) { // Initialize router. `SkipClean(true)` stops minio/mux from // normalizing URL path minio/minio#3256 router := mux.NewRouter().SkipClean(true).UseEncodedPath() // Initialize distributed NS lock. if globalIsDistErasure { registerDistErasureRouters(router, endpointServerPools) } // Add Admin router, all APIs are enabled in server mode. registerAdminRouter(router, true) // Add healthCheck router registerHealthCheckRouter(router) // Add server metrics router registerMetricsRouter(router) // Add STS router always. registerSTSRouter(router) // Add KMS router registerKMSRouter(router) // Add API router registerAPIRouter(router) router.Use(globalMiddlewares...) return router, nil } registerAPIRouterä¼šæ³¨å†Œä¸»è¦çš„ s3 APIï¼Œè¿™é‡Œä¸¾GetObjectæ“ä½œä¸ºä¾‹è¿›è¡Œè¯´æ˜ï¼Œå½“ http method ä¸ºGETæ—¶ï¼Œå¦‚æœæ²¡æœ‰å‘½ä¸­å…¶ä»–çš„è·¯ç”±ï¼Œåˆ™è®¤ä¸ºæ˜¯GetObjectæ“ä½œï¼Œä» Path ä¸­è·å–objectåå­—ï¼Œå¹¶ä½¿ç”¨api.GetObjectHandlerè¿›è¡Œå¤„ç†å’Œå“åº”ï¼Œs3APIMiddlewareä½œä¸ºä¸­é—´ä»¶ï¼Œå¯ä»¥åšä¸€äº›é¢å¤–çš„æ“ä½œï¼Œæ¯”å¦‚ç›‘æ§å’Œè®°å½•æ—¥å¿—ã€‚ apiå¯¹è±¡ä¸­ä¿å­˜äº†ä¸€ä¸ªå‡½æ•°å¼•ç”¨ï¼Œé€šè¿‡è¿™ä¸ªå‡½æ•°å¼•ç”¨ï¼Œèƒ½å¤Ÿå¾—åˆ°å…¨å±€çš„ObjectLayerå¯¹è±¡ï¼ŒObjectLayerå®ç°äº†å¯¹è±¡ API å±‚çš„åŸºæœ¬æ“ä½œã€‚ // GetObject router.Methods(http.MethodGet).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.GetObjectHandler, traceHdrsS3HFlag)) // Initialize API. api := objectAPIHandlers{ ObjectAPI: newObjectLayerFn, } // objectAPIHandlers implements and provides http handlers for S3 API. type objectAPIHandlers struct { ObjectAPI func() ObjectLayer } func newObjectLayerFn() ObjectLayer { globalObjLayerMutex.RLock() defer globalObjLayerMutex.RUnlock() return globalObjectAPI } ","date":"2025-01-22","objectID":"/posts/minio-get-started/:5:0","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"ObjectLayer çš„åˆå§‹åŒ–æµç¨‹ var newObject ObjectLayer bootstrapTrace(\"newObjectLayer\", func() { var err error newObject, err = newObjectLayer(GlobalContext, globalEndpoints) if err != nil { logFatalErrs(err, Endpoint{}, true) } }) storageclass.LookupConfigå‡½æ•°æ ¹æ®ç¯å¢ƒå˜é‡ç­‰åˆå§‹åŒ–Standard Storage Classã€Reduced Redundancy Storage Classä»¥åŠOptimized Storage Classã€ä»¥åŠ inline dataçš„å¤§å° Standard Storage Classï¼šé€šè¿‡ç¯å¢ƒå˜é‡MINIO_STORAGE_CLASS_STANDARDæŒ‡å®šï¼Œå¦åˆ™ä¼šæ ¹æ®erasure setçš„å¤§å°æŒ‡å®š // DefaultParityBlocks returns default parity blocks for 'drive' count func DefaultParityBlocks(drive int) int { switch drive { case 1: return 0 case 3, 2: return 1 case 4, 5: return 2 case 6, 7: return 3 default: return 4 } } Reduced Redundancy Storage Class: é€šè¿‡ç¯å¢ƒå˜é‡MINIO_STORAGE_CLASS_RRSæŒ‡å®šï¼Œå¦åˆ™é»˜è®¤ä¸º 1 Optimized Storage Classï¼šé€šè¿‡ç¯å¢ƒå˜é‡MINIO_STORAGE_CLASS_OPTIMIZEæŒ‡å®šï¼Œé»˜è®¤ä¸º\"\" inline block size: é€šè¿‡ç¯å¢ƒå˜é‡MINIO_STORAGE_CLASS_INLINE_BLOCKæŒ‡å®šï¼Œé»˜è®¤ä¸º128KiB,å¦‚æœ shard æ•°æ®çš„å¤§å°å°äºinline block sizeï¼Œåˆ™ä¼šç›´æ¥å°†æ•°æ®å’Œå…ƒæ•°æ®å†™åˆ°åŒä¸€ä¸ªæ–‡ä»¶ï¼Œå³xl.meta ","date":"2025-01-22","objectID":"/posts/minio-get-started/:6:0","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"MinIO çš„å­˜å‚¨åˆ†å±‚ erasureServerPools // erasureServerPools // minio æœåŠ¡å¯ä»¥ç”±å¤šä¸ªserver pool ç»„æˆï¼Œç”¨æ¥æ°´å¹³æ‹“å±• // erasureServerPoolsæ˜¯server poolçš„é›†åˆ type erasureServerPools struct { poolMetaMutex sync.RWMutex poolMeta poolMeta rebalMu sync.RWMutex rebalMeta *rebalanceMeta deploymentID [16]byte distributionAlgo string // server pool ç”±å¤šä¸ªerasure set ç»„æˆ // è¿™é‡Œçš„erasureSetsç»“æ„å®é™…ä¸ŠæŒ‡å•ä¸ª server pool serverPools []*erasureSets // Active decommission canceler decommissionCancelers []context.CancelFunc s3Peer *S3PeerSys mpCache *xsync.MapOf[string, MultipartInfo] } erasureSets // erasureSets implements ObjectLayer combining a static list of erasure coded // object sets. NOTE: There is no dynamic scaling allowed or intended in // current design. // server pool ç”±å¤šä¸ªerasure set ç»„æˆ // è¿™é‡Œçš„erasureSetsç»“æ„å®é™…ä¸ŠæŒ‡å•ä¸ª server pool // ä¸Šé¢è¿™æ®µè¯çš„æ„æ€æ˜¯ä¸èƒ½åŠ¨æ€æ‰©å±•server poolï¼Œåˆå§‹æŒ‡å®šåå°±ä¸èƒ½å†ä¿®æ”¹äº† type erasureSets struct { sets []*erasureObjects // Reference format. format *formatErasureV3 // erasureDisks mutex to lock erasureDisks. erasureDisksMu sync.RWMutex // Re-ordered list of disks per set. erasureDisks [][]StorageAPI // Distributed locker clients. erasureLockers setsDsyncLockers // Distributed lock owner (constant per running instance). erasureLockOwner string // List of endpoints provided on the command line. endpoints PoolEndpoints // String version of all the endpoints, an optimization // to avoid url.String() conversion taking CPU on // large disk setups. endpointStrings []string // Total number of sets and the number of disks per set. setCount, setDriveCount int defaultParityCount int poolIndex int // Distribution algorithm of choice. distributionAlgo string deploymentID [16]byte lastConnectDisksOpTime time.Time } erasureObjects // erasureObjects - Implements ER object layer. // è¡¨ç¤ºä¸€ä¸ªerasure Set type erasureObjects struct { setDriveCount int defaultParityCount int setIndex int poolIndex int // getDisks returns list of storageAPIs. getDisks func() []StorageAPI // getLockers returns list of remote and local lockers. getLockers func() ([]dsync.NetLocker, string) // getEndpoints returns list of endpoint belonging this set. // some may be local and some remote. getEndpoints func() []Endpoint // getEndpoints returns list of endpoint strings belonging this set. // some may be local and some remote. getEndpointStrings func() []string // Locker mutex map. nsMutex *nsLockMap } ","date":"2025-01-22","objectID":"/posts/minio-get-started/:6:1","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"StorageAPI StorageAPIä¸»è¦æœ‰ä¸¤ä¸ªå®ç° xlStorageè¡¨ç¤ºæœ¬åœ°å­˜å‚¨ storageRESTClientè¡¨ç¤ºè¿œç¨‹ä¸»æœºä¸Šçš„å­˜å‚¨ // Depending on the disk type network or local, initialize storage API. func newStorageAPI(endpoint Endpoint, opts storageOpts) (storage StorageAPI, err error) { if endpoint.IsLocal { storage, err := newXLStorage(endpoint, opts.cleanUp) if err != nil { return nil, err } return newXLStorageDiskIDCheck(storage, opts.healthCheck), nil } return newStorageRESTClient(endpoint, opts.healthCheck, globalGrid.Load()) } newXLStorageå‡½æ•°è°ƒç”¨äº†getDiskInfoå‡½æ•°ï¼Œå¹¶è¦æ±‚è·¯å¾„ä¸èƒ½åœ¨rootDriveä¸Šã€‚åˆ¤æ–­ç£ç›˜æ˜¯å¦æ”¯æŒO_DIRECTï¼Œåœ¨åˆ†å¸ƒå¼éƒ¨ç½²ä¸‹ï¼Œå¦‚æœä¸æ”¯æŒO_DIRECTï¼Œåˆ™ç›´æ¥æŠ¥é”™ã€‚ // Return an error if ODirect is not supported. Single disk will have // oDirect off. // åœ¨ç±»ä¼¼unixçš„å¹³å°ä¸Š disk.ODirectPlatformåº”è¯¥ä¸ºtrue if globalIsErasureSD || !disk.ODirectPlatform { s.oDirect = false } else if err := s.checkODirectDiskSupport(info.FSType); err == nil { s.oDirect = true } else { return s, err } // getDiskInfo returns given disk information. func getDiskInfo(drivePath string) (di disk.Info, rootDrive bool, err error) { if err = checkPathLength(drivePath); err == nil { di, err = disk.GetInfo(drivePath, false) if !globalIsCICD \u0026\u0026 !globalIsErasureSD { if globalRootDiskThreshold \u003e 0 { // Use MINIO_ROOTDISK_THRESHOLD_SIZE to figure out if // this disk is a root disk. treat those disks with // size less than or equal to the threshold as rootDrives. rootDrive = di.Total \u003c= globalRootDiskThreshold } else { rootDrive, err = disk.IsRootDisk(drivePath, SlashSeparator) } } } // StorageAPI interface. // å¯¹åº”ç£ç›˜ type StorageAPI interface { // Stringified version of disk. String() string // Storage operations. // Returns true if disk is online and its valid i.e valid format.json. // This has nothing to do with if the drive is hung or not responding. // For that individual storage API calls will fail properly. The purpose // of this function is to know if the \"drive\" has \"format.json\" or not // if it has a \"format.json\" then is it correct \"format.json\" or not. IsOnline() bool // Returns the last time this disk (re)-connected LastConn() time.Time // Indicates if disk is local or not. IsLocal() bool // Returns hostname if disk is remote. Hostname() string // Returns the entire endpoint. Endpoint() Endpoint // Close the disk, mark it purposefully closed, only implemented for remote disks. Close() error // Returns the unique 'uuid' of this disk. GetDiskID() (string, error) // Set a unique 'uuid' for this disk, only used when // disk is replaced and formatted. SetDiskID(id string) // Returns healing information for a newly replaced disk, // returns 'nil' once healing is complete or if the disk // has never been replaced. Healing() *healingTracker DiskInfo(ctx context.Context, opts DiskInfoOptions) (info DiskInfo, err error) NSScanner(ctx context.Context, cache dataUsageCache, updates chan\u003c- dataUsageEntry, scanMode madmin.HealScanMode, shouldSleep func() bool) (dataUsageCache, error) // Volume operations. MakeVol(ctx context.Context, volume string) (err error) MakeVolBulk(ctx context.Context, volumes ...string) (err error) ListVols(ctx context.Context) (vols []VolInfo, err error) StatVol(ctx context.Context, volume string) (vol VolInfo, err error) DeleteVol(ctx context.Context, volume string, forceDelete bool) (err error) // WalkDir will walk a directory on disk and return a metacache stream on wr. WalkDir(ctx context.Context, opts WalkDirOptions, wr io.Writer) error // Metadata operations DeleteVersion(ctx context.Context, volume, path string, fi FileInfo, forceDelMarker bool, opts DeleteOptions) error DeleteVersions(ctx context.Context, volume string, versions []FileInfoVersions, opts DeleteOptions) []error DeleteBulk(ctx context.Context, volume string, paths ...string) error WriteMetadata(ctx context.Context, origvolume, volume, path string, fi FileInfo) error UpdateMetadata(ctx context.Context, volume, path string, fi FileInfo, opts UpdateMetadataOpts) error ReadVersion(ctx context.Context, origvolume, volume, path, versionID string, opts ReadOptions) (FileInfo, err","date":"2025-01-22","objectID":"/posts/minio-get-started/:6:2","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"ObjectLayer å”¯ä¸€ä¸€ä¸ªå®ç°å°±æ˜¯erasureServerPools ObjectLayer å°±æ˜¯ Minio æä¾›çš„é¢å‘ Object çš„æ¥å£ï¼Œè€ŒStorageAPIåˆ™æ˜¯å…·ä½“çš„æœ¬åœ°æˆ–è€…è¿œç¨‹å­˜å‚¨ç£ç›˜ã€‚ // ObjectLayer implements primitives for object API layer. // é‡è¦æ¥å£ type ObjectLayer interface { // Locking operations on object. NewNSLock(bucket string, objects ...string) RWLocker // Storage operations. Shutdown(context.Context) error NSScanner(ctx context.Context, updates chan\u003c- DataUsageInfo, wantCycle uint32, scanMode madmin.HealScanMode) error BackendInfo() madmin.BackendInfo Legacy() bool // Only returns true for deployments which use CRCMOD as its object distribution algorithm. StorageInfo(ctx context.Context, metrics bool) StorageInfo LocalStorageInfo(ctx context.Context, metrics bool) StorageInfo // Bucket operations. MakeBucket(ctx context.Context, bucket string, opts MakeBucketOptions) error GetBucketInfo(ctx context.Context, bucket string, opts BucketOptions) (bucketInfo BucketInfo, err error) ListBuckets(ctx context.Context, opts BucketOptions) (buckets []BucketInfo, err error) DeleteBucket(ctx context.Context, bucket string, opts DeleteBucketOptions) error ListObjects(ctx context.Context, bucket, prefix, marker, delimiter string, maxKeys int) (result ListObjectsInfo, err error) ListObjectsV2(ctx context.Context, bucket, prefix, continuationToken, delimiter string, maxKeys int, fetchOwner bool, startAfter string) (result ListObjectsV2Info, err error) ListObjectVersions(ctx context.Context, bucket, prefix, marker, versionMarker, delimiter string, maxKeys int) (result ListObjectVersionsInfo, err error) // Walk lists all objects including versions, delete markers. Walk(ctx context.Context, bucket, prefix string, results chan\u003c- itemOrErr[ObjectInfo], opts WalkOptions) error // Object operations. // GetObjectNInfo returns a GetObjectReader that satisfies the // ReadCloser interface. The Close method runs any cleanup // functions, so it must always be called after reading till EOF // // IMPORTANTLY, when implementations return err != nil, this // function MUST NOT return a non-nil ReadCloser. GetObjectNInfo(ctx context.Context, bucket, object string, rs *HTTPRangeSpec, h http.Header, opts ObjectOptions) (reader *GetObjectReader, err error) GetObjectInfo(ctx context.Context, bucket, object string, opts ObjectOptions) (objInfo ObjectInfo, err error) PutObject(ctx context.Context, bucket, object string, data *PutObjReader, opts ObjectOptions) (objInfo ObjectInfo, err error) CopyObject(ctx context.Context, srcBucket, srcObject, destBucket, destObject string, srcInfo ObjectInfo, srcOpts, dstOpts ObjectOptions) (objInfo ObjectInfo, err error) DeleteObject(ctx context.Context, bucket, object string, opts ObjectOptions) (ObjectInfo, error) DeleteObjects(ctx context.Context, bucket string, objects []ObjectToDelete, opts ObjectOptions) ([]DeletedObject, []error) TransitionObject(ctx context.Context, bucket, object string, opts ObjectOptions) error RestoreTransitionedObject(ctx context.Context, bucket, object string, opts ObjectOptions) error // Multipart operations. ListMultipartUploads(ctx context.Context, bucket, prefix, keyMarker, uploadIDMarker, delimiter string, maxUploads int) (result ListMultipartsInfo, err error) NewMultipartUpload(ctx context.Context, bucket, object string, opts ObjectOptions) (result *NewMultipartUploadResult, err error) CopyObjectPart(ctx context.Context, srcBucket, srcObject, destBucket, destObject string, uploadID string, partID int, startOffset int64, length int64, srcInfo ObjectInfo, srcOpts, dstOpts ObjectOptions) (info PartInfo, err error) PutObjectPart(ctx context.Context, bucket, object, uploadID string, partID int, data *PutObjReader, opts ObjectOptions) (info PartInfo, err error) GetMultipartInfo(ctx context.Context, bucket, object, uploadID string, opts ObjectOptions) (info MultipartInfo, err error) ListObjectParts(ctx context.Context, bucket, object, uploadID string, partNumberMarker int, maxParts int, opts ObjectOptions) (result ListPartsInfo, err error) AbortMultipartUpload(ctx contex","date":"2025-01-22","objectID":"/posts/minio-get-started/:6:3","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"é‡è¦å¸¸é‡ const ( // Represents Erasure backend. formatBackendErasure = \"xl\" // Represents Erasure backend - single drive formatBackendErasureSingle = \"xl-single\" // formatErasureV1.Erasure.Version - version '1'. formatErasureVersionV1 = \"1\" // formatErasureV2.Erasure.Version - version '2'. formatErasureVersionV2 = \"2\" // formatErasureV3.Erasure.Version - version '3'. formatErasureVersionV3 = \"3\" // Distribution algorithm used, legacy formatErasureVersionV2DistributionAlgoV1 = \"CRCMOD\" // Distributed algorithm used, with N/2 default parity formatErasureVersionV3DistributionAlgoV2 = \"SIPMOD\" // Distributed algorithm used, with EC:4 default parity formatErasureVersionV3DistributionAlgoV3 = \"SIPMOD+PARITY\" ) ","date":"2025-01-22","objectID":"/posts/minio-get-started/:7:0","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"s3 API: PutObject // PutObject // httpå¤„ç†å‡½æ•° router.Methods(http.MethodPut).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.PutObjectHandler, traceHdrsS3HFlag)) // objectLayerå±‚æ¥å£ putObject = objectAPI.PutObject // Validate storage class metadata if present // x-amz-storage-class minio åªæ”¯æŒ REDUCED_REDUNDANCY å’Œ Standard if sc := r.Header.Get(xhttp.AmzStorageClass); sc != \"\" { if !storageclass.IsValid(sc) { writeErrorResponse(ctx, w, errorCodes.ToAPIErr(ErrInvalidStorageClass), r.URL) return } } // maximum Upload size for objects in a single operation // 5TiB if isMaxObjectSize(size) { writeErrorResponse(ctx, w, errorCodes.ToAPIErr(ErrEntityTooLarge), r.URL) return } objInfo, err := putObject(ctx, bucket, object, pReader, opts) åŒä¸€ä¸ªå¯¹è±¡å¯¹åº”åˆ°çš„erasure setæ€»æ˜¯åŒä¸€ä¸ªï¼Œè¿™æ˜¯é€šè¿‡ç¡®å®šæ€§çš„ hash ç®—æ³•å¾—åˆ°çš„ï¼Œæ‰€ä»¥ server pool ä¸èƒ½è¢«ä¿®æ”¹ï¼Œå¦åˆ™ hash æ˜ å°„å…³ç³»å¯èƒ½å‘ç”Ÿå˜åŒ–ã€‚ // Returns always a same erasure coded set for a given input. func (s *erasureSets) getHashedSetIndex(input string) int { // é€šè¿‡hashå¾—åˆ°å¯¹åº”erasure setï¼Œä»¥ä¸‹è¦ç´ å¯èƒ½å½±å“hashç»“æœ return hashKey(s.distributionAlgo, input, len(s.sets), s.deploymentID) } // PutObject - creates an object upon reading from the input stream // until EOF, erasure codes the data across all disk and additionally // writes `xl.meta` which carries the necessary metadata for future // object operations. func (er erasureObjects) PutObject(ctx context.Context, bucket string, object string, data *PutObjReader, opts ObjectOptions) (objInfo ObjectInfo, err error) { return er.putObject(ctx, bucket, object, data, opts) } ","date":"2025-01-22","objectID":"/posts/minio-get-started/:8:0","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"block å’Œ shard block ï¼ˆå—ï¼‰ blockSize ä»£è¡¨åŸå§‹æ•°æ®åœ¨å­˜å‚¨æ—¶è¢«åˆ‡åˆ†çš„æœ€å°å•ä½ã€‚ åœ¨ MinIO ä¸­ï¼Œæ•°æ®åœ¨å­˜å‚¨å‰è¢«åˆ†å‰²æˆå¤šä¸ª blockã€‚ è¿™äº› block ç»è¿‡ çº åˆ ç ï¼ˆErasure Codingï¼‰ è®¡ç®—åï¼Œç”Ÿæˆ æ•°æ®å—ï¼ˆdata blocksï¼‰ å’Œ æ ¡éªŒå—ï¼ˆparity blocksï¼‰ã€‚ Shard (åˆ†ç‰‡) shard æ˜¯ MinIO å­˜å‚¨åœ¨ç£ç›˜ä¸Šçš„ç‰©ç†å•ä½ï¼ŒåŒ…å« æ•°æ®å— å’Œ æ ¡éªŒå—ã€‚ åœ¨ N+M çº åˆ ç ï¼ˆN ä¸ªæ•°æ®å— + M ä¸ªæ ¡éªŒå—ï¼‰ä¸­ï¼Œæ¯ä¸ª shard å¯¹åº” ä¸€ä¸ªæ•°æ®å—æˆ–ä¸€ä¸ªæ ¡éªŒå—ã€‚ ä¾‹å¦‚ï¼Œ EC: 4+2 ï¼ˆ4 ä¸ªæ•°æ®å— + 2 ä¸ªæ ¡éªŒå—ï¼‰è¡¨ç¤ºï¼š æ•°æ®è¢«åˆ†æˆ 4 ä¸ª blockã€‚ è®¡ç®—å‡º 2 ä¸ªé¢å¤–çš„ parity blockï¼ˆç”¨äºæ¢å¤æ•°æ®ï¼‰ã€‚ æœ€ç»ˆå­˜å‚¨ 6 ä¸ª shardï¼Œæ¯ä¸ª shard åˆ†åˆ«å­˜æ”¾åœ¨ä¸åŒçš„ç£ç›˜ä¸Šã€‚ å‡è®¾æ•°æ®ä¸º 300MiBï¼Œblocksize ä¸º 10MiB, éµå¾ª EC: 4 + 2, shardsize = ceil(10MiB / 4) =2.5MiBï¼Œæœ€ç»ˆæ¯ä¸ª blocksize å­˜å‚¨åœ¨ç£ç›˜ä¸Šä¸º 6 ä¸ª shardï¼Œ4 ä¸ª data shardï¼Œ6 ä¸ª parity shard ","date":"2025-01-22","objectID":"/posts/minio-get-started/:8:1","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"putObject çš„ä¸»è¦æµç¨‹ åˆ›å»ºä¸´æ—¶ç›®å½•ï¼Œå†™å…¥åˆ†ç‰‡æ•°æ® å¦‚æœæ²¡æœ‰åŠ é”ï¼Œè·å–åå­—ç©ºé—´é”ï¼Œå®ç°åŸå­æ“ä½œï¼Œé¿å…æ•°æ®ç«äº‰ rename æ“ä½œï¼ŒåŒ…å«å°†åˆ†ç‰‡ç§»åŠ¨åˆ°ç›®æ ‡ç›®å½•ä»¥åŠå†™å…¥ xl.metaå…ƒæ•°æ® æœ€åå¥½åƒæœ‰æäº¤æ“ä½œï¼Œæ²¡æœ‰çœ‹æ‡‚ ","date":"2025-01-22","objectID":"/posts/minio-get-started/:8:2","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"putObjecté€‰æ‹©serverPool // PutObject - writes an object to least used erasure pool. func (z *erasureServerPools) PutObject(ctx context.Context, bucket string, object string, data *PutObjReader, opts ObjectOptions) (ObjectInfo, error) { // Validate put object input args. if err := checkPutObjectArgs(ctx, bucket, object); err != nil { return ObjectInfo{}, err } object = encodeDirObject(object) if z.SinglePool() { return z.serverPools[0].PutObject(ctx, bucket, object, data, opts) } idx, err := z.getPoolIdx(ctx, bucket, object, data.Size()) if err != nil { return ObjectInfo{}, err } if opts.DataMovement \u0026\u0026 idx == opts.SrcPoolIdx { return ObjectInfo{}, DataMovementOverwriteErr{ Bucket: bucket, Object: object, VersionID: opts.VersionID, Err: errDataMovementSrcDstPoolSame, } } return z.serverPools[idx].PutObject(ctx, bucket, object, data, opts) } å¦‚æœåªæœ‰ä¸€ä¸ªserver poolï¼Œé‚£åªèƒ½ä½¿ç”¨å½“å‰çš„server poolã€‚ // getPoolIdx returns the found previous object and its corresponding pool idx, // if none are found falls back to most available space pool, this function is // designed to be only used by PutObject, CopyObject (newObject creation) and NewMultipartUpload. func (z *erasureServerPools) getPoolIdx(ctx context.Context, bucket, object string, size int64) (idx int, err error) { idx, err = z.getPoolIdxExistingWithOpts(ctx, bucket, object, ObjectOptions{ SkipDecommissioned: true, SkipRebalancing: true, }) if err != nil \u0026\u0026 !isErrObjectNotFound(err) { return idx, err } if isErrObjectNotFound(err) { idx = z.getAvailablePoolIdx(ctx, bucket, object, size) if idx \u003c 0 { return -1, toObjectErr(errDiskFull) } } return idx, nil } å¦‚æœå¯¹è±¡åœ¨æŸä¸ªserver poolä¸­å·²ç»å­˜åœ¨ï¼Œåˆ™è¿”å›å¯¹åº”çš„server poolï¼Œå¦åˆ™é€‰æ‹©ç©ºé—²å®¹é‡æœ€å¤šçš„server poolã€‚ func (z *erasureServerPools) getPoolInfoExistingWithOpts(ctx context.Context, bucket, object string, opts ObjectOptions) (PoolObjInfo, []poolErrs, error) { var noReadQuorumPools []poolErrs poolObjInfos := make([]PoolObjInfo, len(z.serverPools)) poolOpts := make([]ObjectOptions, len(z.serverPools)) for i := range z.serverPools { poolOpts[i] = opts } var wg sync.WaitGroup for i, pool := range z.serverPools { wg.Add(1) go func(i int, pool *erasureSets, opts ObjectOptions) { defer wg.Done() // remember the pool index, we may sort the slice original index might be lost. pinfo := PoolObjInfo{ Index: i, } // do not remove this check as it can lead to inconsistencies // for all callers of bucket replication. if !opts.MetadataChg { opts.VersionID = \"\" } pinfo.ObjInfo, pinfo.Err = pool.GetObjectInfo(ctx, bucket, object, opts) poolObjInfos[i] = pinfo }(i, pool, poolOpts[i]) } wg.Wait() // Sort the objInfos such that we always serve latest // this is a defensive change to handle any duplicate // content that may have been created, we always serve // the latest object. sort.Slice(poolObjInfos, func(i, j int) bool { mtime1 := poolObjInfos[i].ObjInfo.ModTime mtime2 := poolObjInfos[j].ObjInfo.ModTime return mtime1.After(mtime2) }) defPool := PoolObjInfo{Index: -1} for _, pinfo := range poolObjInfos { // skip all objects from suspended pools if asked by the // caller. if opts.SkipDecommissioned \u0026\u0026 z.IsSuspended(pinfo.Index) { continue } // Skip object if it's from pools participating in a rebalance operation. if opts.SkipRebalancing \u0026\u0026 z.IsPoolRebalancing(pinfo.Index) { continue } if pinfo.Err == nil { // found a pool return pinfo, z.poolsWithObject(poolObjInfos, opts), nil } if isErrReadQuorum(pinfo.Err) \u0026\u0026 !opts.MetadataChg { // read quorum is returned when the object is visibly // present but its unreadable, we simply ask the writes to // schedule to this pool instead. If there is no quorum // it will fail anyways, however if there is quorum available // with enough disks online but sufficiently inconsistent to // break parity threshold, allow them to be overwritten // or allow new versions to be added. return pinfo, z.poolsWithObject(poolObjInfos, opts), nil } defPool = pinfo if !isErrObjectNotFound(pinfo.Err) \u0026\u0026 !isErrVersionNotFound(pinfo.Err) { return pinfo, noReadQuorumPools, pinfo.Err } //","date":"2025-01-22","objectID":"/posts/minio-get-started/:8:3","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"åå­—ç©ºé—´é”çš„å®ç°åŸç† ï¼ˆTODO) // func (er erasureObjects) putObject if !opts.NoLock { lk := er.NewNSLock(bucket, object) lkctx, err := lk.GetLock(ctx, globalOperationTimeout) if err != nil { return ObjectInfo{}, err } ctx = lkctx.Context() defer lk.Unlock(lkctx) } erasureObjectsä¸­æœ‰ä¸¤ä¸ªå…³é”®çš„å­—æ®µgetLockerså’ŒnsMutexç”¨äºåå­—ç©ºé—´åŠ é”ã€‚ type erasureObjects struct { // getLockers returns list of remote and local lockers. getLockers func() ([]dsync.NetLocker, string) // Locker mutex map. nsMutex *nsLockMap } type erasureSets struct { sets []*erasureObjects // Distributed locker clients. erasureLockers setsDsyncLockers // Distributed lock owner (constant per running instance). erasureLockOwner string // setsDsyncLockers is encapsulated type for Close() type setsDsyncLockers [][]dsync.NetLocker func (s *erasureSets) GetLockers(setIndex int) func() ([]dsync.NetLocker, string) { return func() ([]dsync.NetLocker, string) { lockers := make([]dsync.NetLocker, len(s.erasureLockers[setIndex])) copy(lockers, s.erasureLockers[setIndex]) // erasureLockerOwnerå®é™…ä¸Šæ˜¯globalLocalNodeName // The name of this local node, fetched from arguments // globalLocalNodeName string return lockers, s.erasureLockOwner } } // nsLockMap - namespace lock map, provides primitives to Lock, // Unlock, RLock and RUnlock. type nsLockMap struct { // Indicates if namespace is part of a distributed setup. isDistErasure bool lockMap map[string]*nsLock lockMapMutex sync.Mutex } // newNSLock - return a new name space lock map. func newNSLock(isDistErasure bool) *nsLockMap { nsMutex := nsLockMap{ isDistErasure: isDistErasure, } if isDistErasure { return \u0026nsMutex } nsMutex.lockMap = make(map[string]*nsLock) return \u0026nsMutex } ","date":"2025-01-22","objectID":"/posts/minio-get-started/:9:0","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"ä»€ä¹ˆæ˜¯ dsyncï¼Ÿ dsync æ˜¯ MinIO å®ç°çš„åˆ†å¸ƒå¼é”ï¼ˆdistributed lockingï¼‰åº“ï¼Œç”¨äºåœ¨å¤šèŠ‚ç‚¹ç¯å¢ƒä¸‹è¿›è¡ŒåŒæ­¥é”å®šï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§ã€‚ ä¸»è¦ä½œç”¨ï¼š åœ¨ MinIO é›†ç¾¤ ä¸­ï¼Œç¡®ä¿å¤šä¸ª MinIO æœåŠ¡å™¨èŠ‚ç‚¹åœ¨ å¹¶å‘è®¿é—®åŒä¸€èµ„æº æ—¶ï¼Œæ­£ç¡®ç®¡ç†è¯»/å†™é”ã€‚ æä¾› ç±»ä¼¼ sync.Mutex å’Œ sync.RWMutex çš„åˆ†å¸ƒå¼ç‰ˆæœ¬ï¼Œä½†é€‚ç”¨äºåˆ†å¸ƒå¼ç³»ç»Ÿï¼Œè€Œä¸æ˜¯å•æœºç¯å¢ƒã€‚ é¿å…æ•°æ®ç«äº‰å’Œä¸ä¸€è‡´æ€§ï¼Œä¿è¯å¤šä¸ª MinIO æœåŠ¡å™¨ä¸ä¼šå‘ç”Ÿå¹¶å‘å†²çªã€‚ NetLocker æ¥å£ ä½ æä¾›çš„ NetLocker æ¥å£å®šä¹‰äº†ä¸€ç§åˆ†å¸ƒå¼é”ç®¡ç†æœºåˆ¶ï¼Œä¸ dsync å…¼å®¹ï¼Œæ ¸å¿ƒæ–¹æ³•åŒ…æ‹¬ï¼š Lock() / Unlock() â€”â€” å†™é” RLock() / RUnlock() â€”â€” è¯»é” Refresh() â€”â€” ç»­çº¦é”ï¼Œé˜²æ­¢é”è¿‡æœŸ ForceUnlock() â€”â€” å¼ºåˆ¶è§£é” IsOnline() / IsLocal() â€”â€” æ£€æŸ¥é”æœåŠ¡æ˜¯å¦åœ¨çº¿ï¼Œæœ¬åœ°è¿˜æ˜¯è¿œç¨‹ String() / Close() â€”â€” è¿”å›é”çš„æ ‡è¯† \u0026 å…³é—­è¿æ¥ è¿™å¥—æœºåˆ¶å…è®¸ MinIO åœ¨å¤šä¸ªæœåŠ¡å™¨èŠ‚ç‚¹é—´è¿›è¡Œåˆ†å¸ƒå¼é”ç®¡ç†ï¼Œç¡®ä¿ä¸€è‡´æ€§ã€‚ ","date":"2025-01-22","objectID":"/posts/minio-get-started/:9:1","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"dsync æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ dsync é‡‡ç”¨åŸºäº n/2+1 å¤šæ•°å†³æœºåˆ¶çš„åˆ†å¸ƒå¼é”ï¼Œé€‚ç”¨äº MinIO åˆ†å¸ƒå¼å¯¹è±¡å­˜å‚¨é›†ç¾¤ã€‚ æ ¸å¿ƒç‰¹ç‚¹ï¼š åˆ†å¸ƒå¼é”ï¼ˆç±»ä¼¼ sync.Mutexï¼‰ Lock() / Unlock() å®ç°äº’æ–¥é”ï¼Œç¡®ä¿å¤šä¸ª MinIO èŠ‚ç‚¹ä¸ä¼šåŒæ—¶å†™å…¥ç›¸åŒæ•°æ®ã€‚ RLock() / RUnlock() å…è®¸å¤šä¸ªè¯»å–è€…å¹¶å‘è®¿é—®ï¼Œä½†ä¸èƒ½åŒæ—¶æœ‰å†™å…¥è€…ã€‚ åŸºäº Raft çš„ä¸€è‡´æ€§ç®—æ³• ä¸å­˜å‚¨é”çš„æŒä¹…åŒ–çŠ¶æ€ï¼Œè€Œæ˜¯é‡‡ç”¨ n/2+1 æœºåˆ¶ å¦‚æœå¤§å¤šæ•°ï¼ˆn/2+1ï¼‰MinIO èŠ‚ç‚¹åŒæ„åŠ é”ï¼Œåˆ™é”æˆåŠŸã€‚ å¦‚æœæœªè¾¾åˆ°å¤šæ•°å†³ï¼ˆå¦‚éƒ¨åˆ†èŠ‚ç‚¹å®•æœºï¼‰ï¼ŒåŠ é”å¤±è´¥ï¼Œé˜²æ­¢æ•°æ®ä¸ä¸€è‡´ã€‚ è¿™ç±»ä¼¼äº Paxos/Raft é€‰ä¸¾æœºåˆ¶ï¼Œä¿è¯æ•°æ®ä¸€è‡´æ€§ã€‚ è¶…æ—¶ \u0026 ç»­çº¦ï¼ˆé¿å…æ­»é”ï¼‰ é”ä¼šè‡ªåŠ¨è¶…æ—¶ï¼Œé˜²æ­¢æ­»é”é—®é¢˜ã€‚ Refresh() å…è®¸æŒæœ‰é”çš„è¿›ç¨‹ ç»­çº¦ï¼Œé˜²æ­¢é”è¿‡æœŸè¢«å…¶ä»–è¿›ç¨‹è·å–ã€‚ æ”¯æŒæœ¬åœ° \u0026 è¿œç¨‹é” å•æœºæ¨¡å¼ï¼šç±»ä¼¼ sync.Mutexï¼Œé”æ˜¯æœ¬åœ°çš„ã€‚ åˆ†å¸ƒå¼æ¨¡å¼ï¼ˆdsyncï¼‰ï¼šé”è¯·æ±‚ä¼šè¢«å‘é€åˆ°å¤šä¸ª MinIO æœåŠ¡å™¨ï¼Œç¡®ä¿æ•´ä¸ªé›†ç¾¤åŒæ­¥åŠ é”ã€‚ MinIO ä¸ºä»€ä¹ˆéœ€è¦ dsyncï¼Ÿ åœ¨ MinIO åˆ†å¸ƒå¼å¯¹è±¡å­˜å‚¨ ä¸­ï¼Œå¤šä¸ªèŠ‚ç‚¹å¯èƒ½åŒæ—¶æ“ä½œåŒä¸€ä¸ªå¯¹è±¡ï¼ˆå¦‚ PUT/DELETE æ“ä½œï¼‰ã€‚ å¦‚æœæ²¡æœ‰é”ï¼Œå¯èƒ½ä¼šå‡ºç° æ•°æ®è¦†ç›–ã€æŸåæˆ–ä¸ä¸€è‡´ çš„é—®é¢˜ã€‚ ä½¿ç”¨ dsync è¿›è¡Œåˆ†å¸ƒå¼é”ç®¡ç†ï¼ŒMinIO è§£å†³äº†è¿™äº›é—®é¢˜ï¼š ç¡®ä¿å¤šä¸ªèŠ‚ç‚¹ä¸ä¼šåŒæ—¶å†™å…¥åŒä¸€å¯¹è±¡ï¼Œé˜²æ­¢æ•°æ®æŸåã€‚ å…è®¸å¤šä¸ªèŠ‚ç‚¹åŒæ—¶è¯»å–æ•°æ®ï¼Œæé«˜å¹¶å‘æ€§èƒ½ã€‚ é˜²æ­¢æ­»é” \u0026 å…è®¸é”ç»­çº¦ï¼Œç¡®ä¿é”ä¸ä¼šæ°¸ä¹…å ç”¨èµ„æºã€‚ ","date":"2025-01-22","objectID":"/posts/minio-get-started/:9:2","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"æ€»ç»“ dsync æ˜¯ MinIO çš„åˆ†å¸ƒå¼é”åº“ï¼Œç”¨äºå¤šèŠ‚ç‚¹åŒæ­¥ï¼Œç¡®ä¿ä¸€è‡´æ€§ã€‚ é‡‡ç”¨ n/2+1 å¤šæ•°å†³æœºåˆ¶ï¼Œé˜²æ­¢æ•°æ®ç«äº‰ \u0026 ä¿è¯é”å®‰å…¨ã€‚ æä¾› è¯»/å†™é”ã€å¼ºåˆ¶è§£é”ã€é”ç»­çº¦ç­‰åŠŸèƒ½ï¼Œé€‚ç”¨äºé«˜å¹¶å‘åœºæ™¯ã€‚ MinIO é€šè¿‡ dsync ç¡®ä¿å¤šä¸ªæœåŠ¡å™¨ä¸ä¼šå¹¶å‘å†™å…¥ç›¸åŒå¯¹è±¡ï¼Œä¿è¯æ•°æ®ä¸€è‡´æ€§ã€‚ ","date":"2025-01-22","objectID":"/posts/minio-get-started/:9:3","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"O_DIRECT çš„å®é™…ç”¨é€” ","date":"2025-01-22","objectID":"/posts/minio-get-started/:10:0","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"s3 API: GetObject // GetObject router.Methods(http.MethodGet).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.GetObjectHandler, traceHdrsS3HFlag)) é¦–å…ˆåŠ åˆ†å¸ƒå¼è¯»é” é€šè¿‡è¯»å–xl.metaè·å–å¯¹è±¡çš„å…ƒæ•°æ®ä¿¡æ¯ï¼Œxl.metaä¿å­˜äº†partå’Œverisonçš„å…¨éƒ¨ä¿¡æ¯ï¼Œæ³¨æ„å¯èƒ½å­˜åœ¨æŸäº›ç£ç›˜ä¸Šçš„xl.metaç”±äºæ•…éšœè€Œä¿®æ”¹è½åï¼Œæ‰€ä»¥ä¾ç„¶éœ€è¦è¯»å–æ³•å®šäººæ•°çš„ç£ç›˜ï¼Œä»è€Œç¡®å®šå®é™…çš„å…ƒæ•°æ® å¦‚æœ http è¯·æ±‚é€šè¿‡partæˆ–è€…rangeè¦æ±‚è¯»å–éƒ¨åˆ†æ•°æ®ï¼Œæœ€ç»ˆéƒ½ä¼šè½¬æ¢æˆå¯¹å¤šä¸ª part çš„è¯»å–ï¼Œæ¯ä¸ª part éƒ½ä¼šåˆ’åˆ†æˆä¸åŒçš„blockè¿›è¡Œæ“ä½œã€‚ func (z *erasureServerPools) GetObjectNInfo(ctx context.Context, bucket, object string, rs *HTTPRangeSpec, h http.Header, opts ObjectOptions) (gr *GetObjectReader, err error) { if err = checkGetObjArgs(ctx, bucket, object); err != nil { return nil, err } // This is a special call attempted first to check for SOS-API calls. gr, err = veeamSOSAPIGetObject(ctx, bucket, object, rs, opts) if err == nil { return gr, nil } // reset any error to 'nil' and any reader to be 'nil' gr = nil err = nil object = encodeDirObject(object) if z.SinglePool() { return z.serverPools[0].GetObjectNInfo(ctx, bucket, object, rs, h, opts) } var unlockOnDefer bool nsUnlocker := func() {} defer func() { if unlockOnDefer { nsUnlocker() } }() // Acquire lock if !opts.NoLock { lock := z.NewNSLock(bucket, object) lkctx, err := lock.GetRLock(ctx, globalOperationTimeout) if err != nil { return nil, err } ctx = lkctx.Context() nsUnlocker = func() { lock.RUnlock(lkctx) } unlockOnDefer = true } checkPrecondFn := opts.CheckPrecondFn opts.CheckPrecondFn = nil // do not need to apply pre-conditions at lower layer. opts.NoLock = true // no locks needed at lower levels for getObjectInfo() objInfo, zIdx, err := z.getLatestObjectInfoWithIdx(ctx, bucket, object, opts) if err != nil { if objInfo.DeleteMarker { if opts.VersionID == \"\" { return \u0026GetObjectReader{ ObjInfo: objInfo, }, toObjectErr(errFileNotFound, bucket, object) } // Make sure to return object info to provide extra information. return \u0026GetObjectReader{ ObjInfo: objInfo, }, toObjectErr(errMethodNotAllowed, bucket, object) } return nil, err } // check preconditions before reading the stream. if checkPrecondFn != nil \u0026\u0026 checkPrecondFn(objInfo) { return nil, PreConditionFailed{} } opts.NoLock = true gr, err = z.serverPools[zIdx].GetObjectNInfo(ctx, bucket, object, rs, h, opts) if err != nil { return nil, err } if unlockOnDefer { unlockOnDefer = gr.ObjInfo.Inlined } if !unlockOnDefer { return gr.WithCleanupFuncs(nsUnlocker), nil } return gr, nil } // getLatestObjectInfoWithIdx returns the objectInfo of the latest object from multiple pools (this function // is present in-case there were duplicate writes to both pools, this function also returns the // additional index where the latest object exists, that is used to start the GetObject stream. func (z *erasureServerPools) getLatestObjectInfoWithIdx(ctx context.Context, bucket, object string, opts ObjectOptions) (ObjectInfo, int, error) { object = encodeDirObject(object) results := make([]struct { zIdx int oi ObjectInfo err error }, len(z.serverPools)) var wg sync.WaitGroup for i, pool := range z.serverPools { wg.Add(1) go func(i int, pool *erasureSets) { defer wg.Done() results[i].zIdx = i results[i].oi, results[i].err = pool.GetObjectInfo(ctx, bucket, object, opts) }(i, pool) } wg.Wait() // Sort the objInfos such that we always serve latest // this is a defensive change to handle any duplicate // content that may have been created, we always serve // the latest object. sort.Slice(results, func(i, j int) bool { a, b := results[i], results[j] if a.oi.ModTime.Equal(b.oi.ModTime) { // On tiebreak, select the lowest pool index. return a.zIdx \u003c b.zIdx } return a.oi.ModTime.After(b.oi.ModTime) }) for _, res := range results { err := res.err if err == nil { return res.oi, res.zIdx, nil } if !isErrObjectNotFound(err) \u0026\u0026 !isErrVersionNotFound(err) { // some errors such as MethodNotAllowed for delete marker // should be returned upwards. return res.oi, res.zIdx, err } // When its a delete marker and versionID is empty // we should simply return the error right away. if res.oi.DeleteMarker \u0026\u0026 opts.VersionID == \"\" { return res.oi, res.zI","date":"2025-01-22","objectID":"/posts/minio-get-started/:11:0","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"çº åˆ ç çš„åŸºæœ¬åŸç† https://p0kt65jtu2p.feishu.cn/docx/LZ36dMN3LoZCuUxFadccNOXGnKb å‡è®¾å°†æ•°æ®åˆ†æˆ 4 å—ï¼Œé‡‡ç”¨ EC:2 å†—ä½™æ¯”ä¾‹ï¼Œå¯ä»¥å°†åŸæ¥çš„æ•°æ®ç»„åˆæˆä¸€ä¸ªè¾“å…¥çŸ©é˜µ P = [][]byteï¼Œ ç¬¬ä¸€ç»´è¡¨ç¤ºä¸åŒçš„æ•°æ®å—ï¼Œç¬¬äºŒç»´è¡¨ç¤ºæ•°æ®å—çš„æ•°æ®ï¼Œæ‰€ä»¥è¿™é‡Œçš„ P çš„å¤§å°ä¸º 4 * nï¼Œn ä¸ºæ¯ä¸ªæ•°æ®å—çš„å¤§å° ç¼–ç çŸ©é˜µ E çš„å¤§å°ä¸º 6 * 4ï¼Œè¦æ±‚ ç¼–ç çŸ©é˜µçš„å‰ 4 è¡Œç»„æˆçš„çŸ©é˜µä¸ºå•ä½çŸ©é˜µï¼Œä¿æŒåŸæ¥æ•°æ®å—æ•°æ®ä¸å˜ï¼Œåä¸¤è¡Œç”¨æ¥ç”Ÿæˆå†—ä½™æ•°æ®ã€‚ E * P = C ï¼ˆC è¡¨ç¤ºç”Ÿæˆçš„æ•°æ®å—å’Œå†—ä½™å—ï¼‰ å‡è®¾æœ‰ä¸¤è¡Œæ•°æ®ä¸æ…ä¸¢å¤±ï¼Œæ­¤æ—¶å»æ‰é‚£ä¸¤è¡Œå¯¹åº”çš„æ•°æ®åä¾ç„¶æœ‰å…³ç³» $Eâ€™ * P = Câ€™$ æˆç«‹ï¼Œæ­¤æ—¶é€šè¿‡æ±‚é€†å¯ä»¥å¾—åˆ°åŸå…ˆçš„ Pï¼Œä¹Ÿå°±ä»æ•°æ®ä¸¢å¤±ä¸­æ¢å¤äº†åŸæ¥çš„æ•°æ®ã€‚ ","date":"2025-01-22","objectID":"/posts/minio-get-started/:12:0","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"åˆ†ç‰‡ä¸Šä¼ å’Œæ–­ç‚¹ç»­ä¼  åˆ†ç‰‡ä¸‹è½½å¯ä»¥é€šè¿‡å‰é¢è¯´è¿‡çš„ http è¯·æ±‚ä¸­çš„rangeæˆ–è€…partnumberå®ç°ã€‚ ä¸»è¦æ¶‰åŠçš„ s3 APIï¼ˆå®¢æˆ·ç«¯ï¼‰: InitiateMultipartUpload UploadPart AbortMultipartUpload CompleteMultipartUpload åœ¨ minio çš„å®¢æˆ·ç«¯ä»£ç ä¸­å®ç°äº†åˆ†ç‰‡ä¸Šä¼ ï¼Œå¹¶ä¸”æ”¯æŒå¹¶å‘ä¸Šä¼  // PutObject creates an object in a bucket. // // You must have WRITE permissions on a bucket to create an object. // // - For size smaller than 16MiB PutObject automatically does a // single atomic PUT operation. // // - For size larger than 16MiB PutObject automatically does a // multipart upload operation. // // - For size input as -1 PutObject does a multipart Put operation // until input stream reaches EOF. Maximum object size that can // be uploaded through this operation will be 5TiB. // // WARNING: Passing down '-1' will use memory and these cannot // be reused for best outcomes for PutObject(), pass the size always. // // NOTE: Upon errors during upload multipart operation is entirely aborted. func (c *Client) PutObject(ctx context.Context, bucketName, objectName string, reader io.Reader, objectSize int64, opts PutObjectOptions, ) // putObjectMultipartStreamParallel uploads opts.NumThreads parts in parallel. // This is expected to take opts.PartSize * opts.NumThreads * (GOGC / 100) bytes of buffer. func (c *Client) putObjectMultipartStreamParallel(ctx context.Context, bucketName, objectName string, reader io.Reader, opts PutObjectOptions, ) (info UploadInfo, err error) { // PutObjectPart router.Methods(http.MethodPut).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.PutObjectPartHandler, traceHdrsS3HFlag)). Queries(\"partNumber\", \"{partNumber:.*}\", \"uploadId\", \"{uploadId:.*}\") // ListObjectParts router.Methods(http.MethodGet).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.ListObjectPartsHandler)). Queries(\"uploadId\", \"{uploadId:.*}\") // CompleteMultipartUpload router.Methods(http.MethodPost).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.CompleteMultipartUploadHandler)). Queries(\"uploadId\", \"{uploadId:.*}\") // NewMultipartUpload router.Methods(http.MethodPost).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.NewMultipartUploadHandler)). Queries(\"uploads\", \"\") // AbortMultipartUpload router.Methods(http.MethodDelete).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.AbortMultipartUploadHandler)). Queries(\"uploadId\", \"{uploadId:.*}\") NewMultipartUpload ç”Ÿæˆ uuid ä½œä¸º uploadId å°†å…ƒæ•°æ®å†™å…¥ .minio.sys/multipart uploadId è·¯å¾„ä¸‹ PutObjectPart ç±»ä¼¼äº PutObejct CompleteMultipartUpload å¹¶æ²¡æœ‰åˆå¹¶ partï¼Œä»ç„¶ä¿ç•™æ¯ä¸ª part ","date":"2025-01-22","objectID":"/posts/minio-get-started/:13:0","tags":["MinIO"],"title":"MinIOå¤§æ‚çƒ©","uri":"/posts/minio-get-started/"},{"categories":[],"content":"è”ç³»æ–¹å¼ github: https://github.com/ShadowUnderMoon ","date":"2025-01-17","objectID":"/about/:0:0","tags":[],"title":"About","uri":"/about/"}]