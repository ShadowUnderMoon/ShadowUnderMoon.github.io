[{"categories":["grpc"],"content":"gprc流程概括 grpc的流程可以大致分成两个阶段，分别为grpc连接阶段和grpc交互阶段，如图[^1]所示。 在RPC连接阶段，client和server之间建立起TCP连接，grpc底层依赖于HTTP2，因此client和server还需要协调frame的相关设置，例如frame的大小，滑动窗口的大小等。 在RPC交互阶段，client将数据发送给server，并等待server执行执行method之后返回结果。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:1:0","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"Client的流程 在RPC连接阶段，client接收到一个目标地址和一系列的DialOptions，然后 配置连接参数，interceptor等，启动resolver Rosovler根据目标地址获取server的地址列表（比如一个DNS name可能会指向多个server ip，dnsResolver是grpc内置的resolver之一），启动balancer Balancer根据平衡策略，从诸多server地址中选择一个或者多个建立TCP连接 client在TCP连接建立完成之后，等待server发来的HTTP2 Setting frame，并调整自身的HTTP2相关配置，随后向server发送HTTP2 Setting frame 在RPC交互阶段，某个rpc方法被调用后 Client创建一个stream对象用来管理整个交互流程 Client将service name, method name等信息放到header frame中并发送给server client将method的参数信息放到data frame中并发送给server client等待server传回的header frame和data frame，一次rpc call的result status会被包含在header frame中，而method的返回值被包含在data frame中 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:1:1","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"Server流程 在rpc连接阶段，server在完成一些初始化的配置之后，开始监听某个tcp端口，在和某个client建立了tcp连接之后完成http2 settting frame的交互。 在rpc交互阶段： server等待client发来的header frame，从而创建出一个stream对象来管理真个交互流程，根据header frame中的信息，server知道client请求的是哪一个service的那一个method server接受到client发来的data frame，并执行method server将执行是否成功等信息放在header frame中发送给client server将method执行的结果（返回值）放在data frame中发送给client ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:1:2","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc Server的rpc连接阶段 func main() { flag.Parse() lis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port)) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } s := grpc.NewServer() pb.RegisterGreeterServer(s, \u0026server{}) log.Printf(\"server listening at %v\", lis.Addr()) if err := s.Serve(lis); err != nil { log.Fatalf(\"failed to serve: %v\", err) } } 如上是一个简单的服务端程序，流程如下 首先通过net.Listener监听tcp端口，毕竟grpc服务是基于tcp的 创建grpc server，并注册服务，\u0026server{}实际上就是服务的实现 阻塞等待来自client的访问 func (s *Server) Serve(lis net.Listener) error { s.serve = true for { rawConn, err := lis.Accept() s.serveWG.Add(1) go func() { s.handleRawConn(lis.Addr().String(), rawConn) s.serveWG.Done() }() } grpc在一个for循环中等待来自client的访问，每次有新的client端访问，创建一个net.Conn，并创建一个新的goroutine处理这个net.Conn，所以这个连接上的请求，无论客户端调用哪一个远程访问或者调用几次，都会由这个goroutine处理。 func (s *Server) handleRawConn(lisAddr string, rawConn net.Conn) { // 如果grpc server已经关闭，那么同样关闭这个tcp连接 if s.quit.HasFired() { rawConn.Close() return } // 设置tcp超时时间 rawConn.SetDeadline(time.Now().Add(s.opts.connectionTimeout)) // Finish handshaking (HTTP2) st := s.newHTTP2Transport(rawConn) // 清理tcp超时时间 rawConn.SetDeadline(time.Time{}) // rpc交互阶段，创建新的goroutine处理来自client的数据 go func() { s.serveStreams(context.Background(), st, rawConn) s.removeConn(lisAddr, st) }() } 在这里，首先判断gprc server是否关闭，如果关闭，则同样关闭tcp连接。然后进行HTTP2的握手，这里专门设置了tcp超时时间，避免握手迟迟不结束，导致资源占用，在握手结束后，清理tcp超时时间，避免对后面请求的影响。最后新启动一个goroutine，用来处理实际的请求。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:2:0","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc服务端HTTP2握手 func (s *Server) newHTTP2Transport(c net.Conn) transport.ServerTransport { // 组装 serverconfig config := \u0026transport.ServerConfig{ MaxStreams: s.opts.maxConcurrentStreams, ConnectionTimeout: s.opts.connectionTimeout, ... } // 根据config的配置项，和client进行http2的握手 st, err := transport.NewServerTransport(c, config) 根据使用者在启动grpc server时的配置项，或者默认的配置项，调用transport.NewServerTransport完成和client的http2握手。 writeBufSize := config.WriteBufferSize readBufSize := config.ReadBufferSize maxHeaderListSize := defaultServerMaxHeaderListSize if config.MaxHeaderListSize != nil { maxHeaderListSize = *config.MaxHeaderListSize } framer := newFramer(conn, writeBufSize, readBufSize, config.SharedWriteBuffer, maxHeaderListSize) 首先创建framer，用来负责接受和发送HTTP2 frame，是server和client交流的实际接口。 // Send initial settings as connection preface to client. isettings := []http2.Setting{{ ID: http2.SettingMaxFrameSize, Val: http2MaxFrameLen, }} if config.MaxStreams != math.MaxUint32 { isettings = append(isettings, http2.Setting{ ID: http2.SettingMaxConcurrentStreams, Val: config.MaxStreams, }) } ... if err := framer.fr.WriteSettings(isettings...); err != nil { return nil, connectionErrorf(false, err, \"transport: %v\", err) } grpc server端首先明确自己的HTTP2的初始配置，比如MaxFrameSize等，并将这些配置信息通过frame.fr发送给client，frame.fr实际上就是golang原生的http2.Framer，在底层，这些配置信息会被包装成一个Setting Frame发送给client。 client在收到Setting Frame后，根据自身情况调整参数，同样发送一个Setting Frame给sever。 t := \u0026http2Server{ done: done, conn: conn, peer: peer, framer: framer, readerDone: make(chan struct{}), loopyWriterDone: make(chan struct{}), maxStreams: config.MaxStreams, inTapHandle: config.InTapHandle, fc: \u0026trInFlow{limit: uint32(icwz)}, state: reachable, activeStreams: make(map[uint32]*ServerStream), stats: config.StatsHandlers, kp: kp, idle: time.Now(), kep: kep, initialWindowSize: iwz, bufferPool: config.BufferPool, } // controlbuf用来缓存Setting Frame等和设置相关的Frame的缓存 t.controlBuf = newControlBuffer(t.done) // 自增连接id t.connectionID = atomic.AddUint64(\u0026serverConnectionCounter, 1) // flush framer，确保向client发送了setting frame t.framer.writer.Flush() grpc server在发送了setting frame之后，创建好http2Server对象，并等待client的后续消息。 // Check the validity of client preface. preface := make([]byte, len(clientPreface)) // 读取客户端发来的client preface，并验证是否和预期一致 if _, err := io.ReadFull(t.conn, preface); err != nil { // In deployments where a gRPC server runs behind a cloud load balancer // which performs regular TCP level health checks, the connection is // closed immediately by the latter. Returning io.EOF here allows the // grpc server implementation to recognize this scenario and suppress // logging to reduce spam. if err == io.EOF { return nil, io.EOF } return nil, connectionErrorf(false, err, \"transport: http2Server.HandleStreams failed to receive the preface from client: %v\", err) } if !bytes.Equal(preface, clientPreface) { return nil, connectionErrorf(false, nil, \"transport: http2Server.HandleStreams received bogus greeting from client: %q\", preface) } // 读取client端发来的frame frame, err := t.framer.fr.ReadFrame() if err == io.EOF || err == io.ErrUnexpectedEOF { return nil, err } if err != nil { return nil, connectionErrorf(false, err, \"transport: http2Server.HandleStreams failed to read initial settings frame: %v\", err) } atomic.StoreInt64(\u0026t.lastRead, time.Now().UnixNano()) // 转成SettingFrame sf, ok := frame.(*http2.SettingsFrame) if !ok { return nil, connectionErrorf(false, nil, \"transport: http2Server.HandleStreams saw invalid preface type %T from client\", frame) } // 处理SettingFrame t.handleSettings(sf) func (t *http2Server) handleSettings(f *http2.SettingsFrame) { // 如果是ack frame，则直接返回 if f.IsAck() { return } var ss []http2.Setting var updateFuncs []func() f.ForeachSetting(func(s http2.Setting) error { switch s.ID { // 更新http2Server中的配置信息 case http2.SettingMaxHeaderListSize: updateFuncs = append(updateFuncs, func() { t.maxSendHeaderListSize = new(uint32) *t.maxSendHeaderListSize = s.Val }) default: ss = append(ss, s) } return nil }) // 这里又遇到了controlBuf // 执行updateFunc","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:2:1","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc server的rpc交互阶段 HTTP2中定义了很多类型的frame，包括data, headers等，具体如下，对于不同的frame类型，HTTP2 server应该有不同的处理逻辑。在grpc中，对frame类型的分类和处理，被包含在func (s *Server) serveStreams中。 // FrameType represents the type of an HTTP/2 Frame. // See [Frame Type]. // // [Frame Type]: https://httpwg.org/specs/rfc7540.html#FrameType type FrameType uint8 // Frame types defined in the HTTP/2 Spec. const ( FrameTypeData FrameType = 0x0 FrameTypeHeaders FrameType = 0x1 FrameTypeRSTStream FrameType = 0x3 FrameTypeSettings FrameType = 0x4 FrameTypePing FrameType = 0x6 FrameTypeGoAway FrameType = 0x7 FrameTypeWindowUpdate FrameType = 0x8 FrameTypeContinuation FrameType = 0x9 ) func (s *Server) serveStreams(ctx context.Context, st transport.ServerTransport, rawConn net.Conn) { streamQuota := newHandlerQuota(s.opts.maxConcurrentStreams) // 阻塞并接受来自client的frame st.HandleStreams(ctx, func(stream *transport.ServerStream) { s.handlersWG.Add(1) streamQuota.acquire() f := func() { defer streamQuota.release() defer s.handlersWG.Done() // 当一个新的stream被创建之后，进行一些配置 s.handleStream(st, stream) } if s.opts.numServerWorkers \u003e 0 { select { case s.serverWorkerChannel \u003c- f: return default: // If all stream workers are busy, fallback to the default code path. } } go f() }) } st.HandleStreams会阻塞当前goroutine，并等待来自client的frame，在一个for循环中等待并读取来自client的frame，并采取不同的处理方式。 func (t *http2Server) HandleStreams(ctx context.Context, handle func(*ServerStream)) { defer func() { close(t.readerDone) \u003c-t.loopyWriterDone }() // for循环，持续处理一个连接的上请求 for { // 限流 t.controlBuf.throttle() // 读取frame frame, err := t.framer.fr.ReadFrame() atomic.StoreInt64(\u0026t.lastRead, time.Now().UnixNano()) // 根据frame的类型分别处理 switch frame := frame.(type) { // MetaHeaderFrame并不是http2的frame类型，而是经过包装的类型 // headers frame + zero or more continuation frame + hspack编码内容的解码 case *http2.MetaHeadersFrame: if err := t.operateHeaders(ctx, frame, handle); err != nil { // Any error processing client headers, e.g. invalid stream ID, // is considered a protocol violation. t.controlBuf.put(\u0026goAway{ code: http2.ErrCodeProtocol, debugData: []byte(err.Error()), closeConn: err, }) continue } case *http2.DataFrame: t.handleData(frame) case *http2.RSTStreamFrame: t.handleRSTStream(frame) case *http2.SettingsFrame: t.handleSettings(frame) case *http2.PingFrame: t.handlePing(frame) case *http2.WindowUpdateFrame: t.handleWindowUpdate(frame) case *http2.GoAwayFrame: // TODO: Handle GoAway from the client appropriately. default: if t.logger.V(logLevel) { t.logger.Infof(\"Received unsupported frame type %T\", frame) } } } } ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:3:0","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"Headers Frame的处理 func (t *http2Server) operateHeaders(ctx context.Context, frame *http2.MetaHeadersFrame, handle func(*ServerStream)) error { // Acquire max stream ID lock for entire duration t.maxStreamMu.Lock() defer t.maxStreamMu.Unlock() // 从客户端frame中获取streamID streamID := frame.Header().StreamID // 校验stream id if streamID%2 != 1 || streamID \u003c= t.maxStreamID { // illegal gRPC stream id. return fmt.Errorf(\"received an illegal stream id: %v. headers frame: %+v\", streamID, frame) } // 将获得的streamID设置到http2Server t.maxStreamID = streamID // 无界message缓冲 buf := newRecvBuffer() // 创建stream s := \u0026ServerStream{ Stream: \u0026Stream{ id: streamID, buf: buf, fc: \u0026inFlow{limit: uint32(t.initialWindowSize)}, }, st: t, headerWireLength: int(frame.Header().Length), } 在grpc server和client端，存在这一个stream的概念，用来表征一次grpc call。一个grpc call总是以一个来自client的headers frame开始，因此server会在operateHeaders中创建一个Stream对象，stream有一个client和server端一致的id，也有一个buf缓存。 for _, hf := range frame.Fields { switch hf.Name { case \"grpc-encoding\": s.recvCompress = hf.Value case \":method\": // POST, GET这些 httpMethod = hf.Value case \":path\": // 使用grpc那个服务的那个方法 s.method = hf.Value case \"grpc-timeout\": timeoutSet = true var err error if timeout, err = decodeTimeout(hf.Value); err != nil { headerError = status.Newf(codes.Internal, \"malformed grpc-timeout: %v\", err) } } } grpc server会遍历frame中的field，并将filed中的信息记录在stream中。:method和:path这两个field需要特别注意，client端需要填写好这两个field来明确地指定要调用server端提供的那一个方法，也就是说，调用哪一个server方法的信息是和调用方法的参数分开在不同的frame中的。 if frame.StreamEnded() { // s is just created by the caller. No lock needed. s.state = streamReadDone } // 超时设置 if timeoutSet { s.ctx, s.cancel = context.WithTimeout(ctx, timeout) } else { s.ctx, s.cancel = context.WithCancel(ctx) } if uint32(len(t.activeStreams)) \u003e= t.maxStreams { t.mu.Unlock() t.controlBuf.put(\u0026cleanupStream{ streamID: streamID, rst: true, rstCode: http2.ErrCodeRefusedStream, onWrite: func() {}, }) s.cancel() return nil } // 将stream加入activeStreams map t.activeStreams[streamID] = s if len(t.activeStreams) == 1 { t.idle = time.Time{} } // Start a timer to close the stream on reaching the deadline. if timeoutSet { // We need to wait for s.cancel to be updated before calling // t.closeStream to avoid data races. cancelUpdated := make(chan struct{}) timer := internal.TimeAfterFunc(timeout, func() { \u003c-cancelUpdated t.closeStream(s, true, http2.ErrCodeCancel, false) }) oldCancel := s.cancel s.cancel = func() { oldCancel() timer.Stop() } close(cancelUpdated) } s.trReader = \u0026transportReader{ reader: \u0026recvBufferReader{ ctx: s.ctx, ctxDone: s.ctxDone, recv: s.buf, }, windowHandler: func(n int) { t.updateWindow(s, uint32(n)) }, } // Register the stream with loopy. t.controlBuf.put(\u0026registerStream{ streamID: s.id, wq: s.wq, }) handle(s) 这个新建的stream对象会被放到server的activeStreams map中，并调用回调函数handle(s)来进一步处理这个stream，其中最重要的是调用s.handleStream。 st.HandleStreams(ctx, func(stream *transport.ServerStream) { s.handlersWG.Add(1) streamQuota.acquire() f := func() { defer streamQuota.release() defer s.handlersWG.Done() s.handleStream(st, stream) } // 如果设置了worker池，则先尝试提交任务到worker池中，如果不行，新起goroutine执行 if s.opts.numServerWorkers \u003e 0 { select { case s.serverWorkerChannel \u003c- f: return default: // If all stream workers are busy, fallback to the default code path. } } go f() }) // initServerWorkers creates worker goroutines and a channel to process incoming // connections to reduce the time spent overall on runtime.morestack. func (s *Server) initServerWorkers() { s.serverWorkerChannel = make(chan func()) s.serverWorkerChannelClose = sync.OnceFunc(func() { close(s.serverWorkerChannel) }) for i := uint32(0); i \u003c s.opts.numServerWorkers; i++ { go s.serverWorker() } } 回调函数中会将处理stream的任务提交到其他goroutine中，如果可用的worker，则由worker执行，否则另起goroutine来执行。 func (s *Server) handleStream(t transport.ServerTransport, stream *transport.ServerStream) { // 获取grpc路径 sm := stream.Method() pos := strings.LastIndex(sm, \"/\") // 调用的grpc service name service := sm[:pos] // 调用的grpc method name method := sm[pos","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:3:1","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"Data Frame的处理 func (t *http2Server) handleData(f *http2.DataFrame) { size := f.Header().Length // Select the right stream to dispatch. s, ok := t.getStream(f) if !ok { return } if s.getState() == streamReadDone { t.closeStream(s, true, http2.ErrCodeStreamClosed, false) return } if size \u003e 0 { if len(f.Data()) \u003e 0 { pool := t.bufferPool s.write(recvMsg{buffer: mem.Copy(f.Data(), pool)}) } } if f.StreamEnded() { // Received the end of stream from the client. s.compareAndSwapState(streamActive, streamReadDone) s.write(recvMsg{err: io.EOF}) } } 在处理data frame时 根据stream ID，从server的activeStreams map中找到stream对象 从bufferPool中拿到一块buffer，并将frame的数据写入到buffer 将这块buffer保存到stream的recvBuffer中 如果读取结束，修改流状态为streamReadDone，并且写入io.EOF标记 recvBuffer中缓存的数据，最终会被前面提到的recvAndDecompress函数读取，从而在server端重建rpc的参数。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:3:2","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"Setting Frame的处理 func (t *http2Server) handleSettings(f *http2.SettingsFrame) { if f.IsAck() { return } var ss []http2.Setting var updateFuncs []func() f.ForeachSetting(func(s http2.Setting) error { switch s.ID { case http2.SettingMaxHeaderListSize: updateFuncs = append(updateFuncs, func() { t.maxSendHeaderListSize = new(uint32) *t.maxSendHeaderListSize = s.Val }) default: ss = append(ss, s) } return nil }) t.controlBuf.executeAndPut(func() bool { for _, f := range updateFuncs { f() } return true }, \u0026incomingSettings{ ss: ss, }) } handleSettings并没有直接将settting frame的参数应用在server上，而是将其放到了controlBuf中。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:3:3","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"server如何发送frame grpc server在每次收到一个新的来自client的连接后，会创建一个Framer，这个Framer就是实际上负责发送和接收HTTP2 frame的接口，每一个client都对应一个Framer来处理来自该client的所有frame，不管这些frame是否属于同一个stream。 type framer struct { // 一个包含了buffer的net.Conn的writer writer *bufWriter // 原生的http2.Framer，负责数据读写 fr *http2.Framer } framer其实就是对golang原生http2.Framer的封装。 type bufWriter struct { pool *sync.Pool buf []byte offset int batchSize int conn net.Conn err error } func newBufWriter(conn net.Conn, batchSize int, pool *sync.Pool) *bufWriter { w := \u0026bufWriter{ batchSize: batchSize, conn: conn, pool: pool, } // this indicates that we should use non shared buf if pool == nil { w.buf = make([]byte, batchSize) } return w } func (w *bufWriter) Write(b []byte) (int, error) { // 在write之间检查上一次write是否发生了错误 if w.err != nil { return 0, w.err } // 如果batchsize为0，说明不需要写缓存，直接向net.Conn写数据 if w.batchSize == 0 { // Buffer has been disabled. n, err := w.conn.Write(b) return n, toIOError(err) } if w.buf == nil { b := w.pool.Get().(*[]byte) w.buf = *b } written := 0 // 如果写入的数据少于batchSize，则缓存，暂时不写入conn // 如果写入的数据多余batchSize，则调用flushKeepBuffer不断写数据 for len(b) \u003e 0 { copied := copy(w.buf[w.offset:], b) b = b[copied:] written += copied w.offset += copied if w.offset \u003c w.batchSize { continue } if err := w.flushKeepBuffer(); err != nil { return written, err } } return written, nil } func (w *bufWriter) Flush() error { // 刷新数据到conn err := w.flushKeepBuffer() // Only release the buffer if we are in a \"shared\" mode if w.buf != nil \u0026\u0026 w.pool != nil { b := w.buf w.pool.Put(\u0026b) w.buf = nil } return err } func (w *bufWriter) flushKeepBuffer() error { if w.err != nil { return w.err } if w.offset == 0 { return nil } _, w.err = w.conn.Write(w.buf[:w.offset]) w.err = toIOError(w.err) w.offset = 0 return w.err } grpc server实现了一个简单的缓存写给http2.framer作为io.Writer。 // 全局writeBufferPool var writeBufferPoolMap = make(map[int]*sync.Pool) var writeBufferMutex sync.Mutex func newFramer(conn net.Conn, writeBufferSize, readBufferSize int, sharedWriteBuffer bool, maxHeaderListSize uint32) *framer { if writeBufferSize \u003c 0 { writeBufferSize = 0 } var r io.Reader = conn if readBufferSize \u003e 0 { // 设置io.Reader r = bufio.NewReaderSize(r, readBufferSize) } var pool *sync.Pool if sharedWriteBuffer { pool = getWriteBufferPool(writeBufferSize) } // 设置io.Writer w := newBufWriter(conn, writeBufferSize, pool) // 创建framer f := \u0026framer{ writer: w, fr: http2.NewFramer(w, r), } f.fr.SetMaxReadFrameSize(http2MaxFrameLen) // Opt-in to Frame reuse API on framer to reduce garbage. // Frames aren't safe to read from after a subsequent call to ReadFrame. f.fr.SetReuseFrames() f.fr.MaxHeaderListSize = maxHeaderListSize f.fr.ReadMetaHeaders = hpack.NewDecoder(http2InitHeaderTableSize, nil) return f } // writeBuffer 使用sync.Pool func getWriteBufferPool(size int) *sync.Pool { writeBufferMutex.Lock() defer writeBufferMutex.Unlock() pool, ok := writeBufferPoolMap[size] if ok { return pool } pool = \u0026sync.Pool{ New: func() any { b := make([]byte, size) return \u0026b }, } writeBufferPoolMap[size] = pool return pool } 传递给http2.framer的io.Reader使用了bifio package。 writeBuffer使用了go标准库中的sync.Pool，根据需要的size获取对应的sync.Pool，如果池中有对应的byte[]，获取然后返回，如果没有，创建新的byte[]并返回。池中元素的回收时机，go允许在任何时候自动回收池中的元素（gc）。 grpc server为每一个client创建一个loopyWriter，有这个loopyWriter负责发送数据。 type loopyWriter struct { // 客户端还是服务端 side side // controlBuffer cbuf *controlBuffer // 发送配额 sendQuota uint32 // 发送端初始窗口大小 outbound initial window size oiws uint32 // 已经建立未清理的stream，在客户端，指所有已经将Headers发送出去的stream， // 在服务端，指所有已经接收到Headers的stream estdStreams map[uint32]*outStream // 活跃stream列表，有数据需要发送且包含stream-level流控，里面的每个stream内部都有一个数据列表用来存放发送的数据 activeStreams *outStreamList // http2.Framer的包装，用来实际读写数 framer *framer hBuf *bytes.Buffer // The buffer for HPACK encoding. hEnc *hpack.Encoder // HPACK encoder. bdpEst *bdpEstimator draining bool // 底层tcp连接 conn net.Conn logger *grpclog.PrefixLogger bufferPool mem.BufferPool // Side-specific handlers ssGoAwayHandler func(*goAway) (bool, error) } loopyWriter从control buffer中接收frame，每个frame被单独","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:4:0","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc server的流量控制 grpc在应用层实现了自己的流量控制，并将流量控制分成了三个层级 sample level 流量控制 connection level 流量控制 stream level 流量控制 流量控制可以说是grpc高性能的关键，通过动态地控制数据发送和接收的速率，grpc保证在任何网络情况下都能发挥最大的性能，尽量提高传输带宽并降低传输延迟。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:5:0","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"采样流量控制 BDP估算和动态流量控制窗口 BDP和动态流量控制窗口缩小了grpc和http1.1在高延迟网络中的性能表现。带宽延迟积（BDP，Bandwidth Delay Product）是网络连接的带宽和数据往返延迟的乘积，能够有效地表示在网络被完全利用时网络上有多少字节数据。 BDP算法基本思路如下： 每次接收方收到一个data frame时，它就会发送一个BDP ping（一个带有唯一数据、仅用于BDP估算的ping）。在这之后，接收方开始统计它接收到的字节数（包括触发该BDP ping的那部分数据），直到它收到该ping的ack为止。这个在大约1.5个RTT（round-trip time）内接收到的字节总数，约为BDP的1.5倍。如果这个总字节数接近当前的窗口（比如超过窗口的2/3），那么我们必须增大窗口。我们将窗口大小（包括stremaing和connection窗口）设为采样得到的BDP的两倍（也就是接收到的字节总数的两倍）。 在grpc server端定义了一个bdpEstimator，是用来计算BDP的核心。 const ( // bdpLimit is the maximum value the flow control windows will be increased // to. TCP typically limits this to 4MB, but some systems go up to 16MB. // Since this is only a limit, it is safe to make it optimistic. bdpLimit = (1 \u003c\u003c 20) * 16 // alpha is a constant factor used to keep a moving average // of RTTs. alpha = 0.9 // If the current bdp sample is greater than or equal to // our beta * our estimated bdp and the current bandwidth // sample is the maximum bandwidth observed so far, we // increase our bbp estimate by a factor of gamma. beta = 0.66 // To put our bdp to be smaller than or equal to twice the real BDP, // we should multiply our current sample with 4/3, however to round things out // we use 2 as the multiplication factor. gamma = 2 ) // Adding arbitrary data to ping so that its ack can be identified. // Easter-egg: what does the ping message say? var bdpPing = \u0026ping{data: [8]byte{2, 4, 16, 16, 9, 14, 7, 7}} type bdpEstimator struct { // sentAt is the time when the ping was sent. sentAt time.Time mu sync.Mutex // bdp is the current bdp estimate. bdp uint32 // sample is the number of bytes received in one measurement cycle. sample uint32 // bwMax is the maximum bandwidth noted so far (bytes/sec). bwMax float64 // bool to keep track of the beginning of a new measurement cycle. isSent bool // Callback to update the window sizes. updateFlowControl func(n uint32) // sampleCount is the number of samples taken so far. sampleCount uint64 // round trip time (seconds) rtt float64 } bdpEstimator有两个主要的方法add和calculate // add的返回值指示loopyWriter是否发送BDP ping frame给client func (b *bdpEstimator) add(n uint32) bool { b.mu.Lock() defer b.mu.Unlock() // 如果bdp已经达到上限，就不再发送bdp ping进行采样 if b.bdp == bdpLimit { return false } // 如果在当前时间点没有bdp ping frame发送出去，就应该发送，来进行采样 if !b.isSent { b.isSent = true b.sample = n b.sentAt = time.Time{} b.sampleCount++ return true } // 已经有bdp ping frame发送出去了，但是还没有收到ack，累加收到的字节数 b.sample += n return false } add函数有两个作用： 告知loopyWriter是否开始采样 记录采样开始的时间和初始数据量 func (t *http2Server) handleData(f *http2.DataFrame) { size := f.Header().Length var sendBDPPing bool if t.bdpEst != nil { sendBDPPing = t.bdpEst.add(size) } if w := t.fc.onData(size); w \u003e 0 { t.controlBuf.put(\u0026outgoingWindowUpdate{ streamID: 0, increment: w, }) } if sendBDPPing { // Avoid excessive ping detection (e.g. in an L7 proxy) // by sending a window update prior to the BDP ping. if w := t.fc.reset(); w \u003e 0 { t.controlBuf.put(\u0026outgoingWindowUpdate{ streamID: 0, increment: w, }) } t.controlBuf.put(bdpPing) } // Select the right stream to dispatch. s, ok := t.getStream(f) } handleData函数是grpc serve收到来自client的http2 data frame之后执行的函数，可以看到，grpc server和每个client之间都维护着一个bdpEstimator，每次收到一个data frame，grpc server都会判断是否需要进行采样，如果需要采样，就向client发送一个bdpPing frame，这个frame也是加入controlBuffer，异步处理的。 这里也将连接的流量控制和应用程序读取数据的行为解耦，也就是说，连接级别的窗口更新不应该依赖于应用是否读取了数据。stream-level流控已经有这个限制（必须等待应用读取后才能更新窗口），所以如果某个stream很慢，发送方已经被阻塞（因为窗口耗尽）。解耦可以避免下面的情况发生，当某些strema很慢（或者压根没有读取数据）时，导致其他活跃的stream由于没有connection-level流控窗口而被阻塞。 func (l *loopyWriter) pingHandler(p *ping) error { if !p.ack { l.bdpEst.timesnap(p.data) } return l.framer.fr.WritePing(p.ack, p.data) } func (b *bdpEstimator) timesnap(d [8]byte) { if bdpPing.data != d { return } b.sentAt = time.Now() } 前面提到bdp ping frame是通过control framer异步发送出去的，这个时间点可能和之前决定发送ping的时间点有一定的距离，为了更准确的计算RTT，所以在使用http2.framer实际发送数据前，重新更新了bdp ping frame的发送时间。 Client端在收到一个bdp ping frame之后，会立刻返回一个ack，server会捕捉到这个ack。 func (t *http2Server) handlePing(f *http2.PingFrame) { if f.IsAck() { if f.Dat","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:5:1","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"controlBuffer数据结构 先介绍一个重要的数据结果controlBuffer，这个在之前已经提到过了，在向外发送数据前，其实都会加入controlBuffer中，然后再进行处理。 type controlBuffer struct { // wakeupch的作用是在阻塞读取缓存中的内容时，当有新的frame加入itemList，可以解决阻塞并返回itemList中的frame wakeupCh chan struct{} // Unblocks readers waiting for something to read. // done \u003c-chan struct{} // Closed when the transport is done. // Mutex guards all the fields below, except trfChan which can be read // atomically without holding mu. mu sync.Mutex // 和wakeupCh配置使用，确保不向wakeupCh中放入多余的struct，保证阻塞读取缓存不会因为wakeupCh中的多余元素错误解除阻塞 consumerWaiting bool // True when readers are blocked waiting for new data. closed bool // True when the controlbuf is finished. list *itemList // List of queued control frames. // 记录排队的响应帧数量 transportResponseFrames int // 当transportResponseFrames \u003e= maxQueuedTransportResponseFrames时， // 创建trfChan，用于控制是否继续从client读取frame trfChan atomic.Pointer[chan struct{}] } controlBuffer中的数据被称为control frame，一个control frame不止表示向外发送的data、message、headers，也被用来指示loopyWriter更新自身的内部状态。control frame和http2 frame没有直接关系，尽管有些control frame，比如说 dataFrame和headerFrame确实作为http2 frame向外传输。 controlBuffer维护了一个itemList（单向链表），本质上是一块缓存区，这块缓存区主要有两个作用： 缓存需要发送的frame 根据缓存中transportResponseFrame的数量，决定是否暂时停止读取从client发来的frame 下面看controlBuffer中的一些主要函数，加深理解 func newControlBuffer(done \u003c-chan struct{}) *controlBuffer { return \u0026controlBuffer{ wakeupCh: make(chan struct{}, 1), list: \u0026itemList{}, done: done, } } newControlBuffer用于创建controlBuffer实例，其中wakeupCh是缓冲区为1的channel。 func (c *controlBuffer) throttle() { if ch := c.trfChan.Load(); ch != nil { select { case \u003c-(*ch): case \u003c-c.done: } } } throttle函数会被阻塞，如果controlBuffer中存在太多的响应帧，比如incommingSettings、cleanupStrema等。在grpc server的代码中，throttle函数通常出现在grpc server接收client frame的开头，也就是说，当transportResponseFrames数量过多时，grpc server会暂停接受来自client的frame，maxQueuedTransportResponseFrames为50。 func (c *controlBuffer) executeAndPut(f func() bool, it cbItem) (bool, error) { c.mu.Lock() defer c.mu.Unlock() if c.closed { return false, ErrConnClosing } if f != nil { if !f() { // f wasn't successful return false, nil } } if it == nil { return true, nil } var wakeUp bool if c.consumerWaiting { wakeUp = true c.consumerWaiting = false } // 将item加入到buffer中 c.list.enqueue(it) if it.isTransportResponseFrame() { c.transportResponseFrames++ if c.transportResponseFrames == maxQueuedTransportResponseFrames { // We are adding the frame that puts us over the threshold; create // a throttling channel. ch := make(chan struct{}) c.trfChan.Store(\u0026ch) } } if wakeUp { select { case c.wakeupCh \u003c- struct{}{}: default: } } return true, nil } executeAndPut运行f函数，如果f函数返回true，添加给定的item到controlBuf。如果consumerWaiting为true，也就是loopyWriter发现没有消息可供处理，所以阻塞获取control frame，这里会向wakeupCh中放入一个元素，来通知消费者可以读取frame了。在这里也会检查响应帧的数量，如果超过阈值，则创建trfChan。 func (c *controlBuffer) get(block bool) (any, error) { // for循环 for { c.mu.Lock() frame, err := c.getOnceLocked() if frame != nil || err != nil || !block { c.mu.Unlock() return frame, err } // 设置状态为consumerWaiting c.consumerWaiting = true c.mu.Unlock() // Release the lock above and wait to be woken up. select { // control buffer中没有control frame，阻塞等待 case \u003c-c.wakeupCh: case \u003c-c.done: return nil, errors.New(\"transport closed by client\") } } } func (c *controlBuffer) getOnceLocked() (any, error) { if c.closed { return false, ErrConnClosing } if c.list.isEmpty() { return nil, nil } h := c.list.dequeue().(cbItem) // 将controlframe移除响应帧，可能会解封对client请求的读取 if h.isTransportResponseFrame() { if c.transportResponseFrames == maxQueuedTransportResponseFrames { // We are removing the frame that put us over the // threshold; close and clear the throttling channel. ch := c.trfChan.Swap(nil) close(*ch) } c.transportResponseFrames-- } return h, nil } get从control buffer中获取下一个control frame，如果block参数为true并且control buffer中没有control frame，调用被阻塞直到有control frame或者buffer被关闭。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:5:2","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"connection level流量控制 connection level流量控制会控制对于某个client某一时刻能够发送的数据总量。 type loopyWriter struct { ...... sendQuota uint32 ...... } 控制的方式就是在loopyWriter中用一个sendQuota来标记该client目前可发送数据的配额。 func (l *loopyWriter) processData() (bool, error) { ...... l.sendQuota -= uint32(size) ...... } sendQuota会被初始化为65535，并且每当有数据被grpc server发送给client的时候，sendQuota都会减少和被发送数据相等的大小。 func (l *loopyWriter) incomingWindowUpdateHandler(w *incomingWindowUpdate) error { // Otherwise update the quota. if w.streamID == 0 { l.sendQuota += w.increment return nil } ...... } 当grpc server收到来自client的http2 FrameWindowUpdate frame时，才会将这一quota增加，也就是说sendQuota会在server发出数据时减少，在收到来自client的FrameWindowUpdate frame时增加，connection level的流量控制是server和client相互交互的结果，由双方共同决定窗口大小。 为了配合server端的流量控制，client端在连接初始化时被分配了一个limit，默认为65536字节，client端会记录收到的数据量的总和unacked，当unacked超过了limit的1/4后，client就会向server段发送一个window update（数值为unacked）,通知server可以将quota加回来，同时将unacked置零。 可以看到为了避免频繁的发送window update占用网络带宽，client并不会在每次接收到数据之后就发送window update，而是等待接收的数据量达到某一阈值后再发送。 // trInFlow 是 client 端决定是否发送 window update 给 server 的核心 type trInFlow struct { // server 端能够发送数据的上限, 会被 server 端根据采用控制的结果更新 limit uint32 // client 端已经接收到的数据 unacked uint32 // 用于 metric 记录, 不影响流量控制 effectiveWindowSize uint32 } // 参数 n 是 client 接收到的数据大小, 返回值表示需要向 server 发送的 window update 中的数值大小. // 返回 0 代表不需要发送 window update func (f *trInFlow) onData(n uint32) uint32 { f.unacked += n // 超过 1/4 * limit 才会发送 window update, 且数值为已经接收到的数据总量 if f.unacked \u003e= f.limit/4 { w := f.unacked f.unacked = 0 f.updateEffectiveWindowSize() return w } f.updateEffectiveWindowSize() return 0 } trInFlow是client端控制是否发送window update的核心，limit会随server端发来的window update而改变。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:5:3","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"stream level流量控制 一个stream的流量控制有三种状态，分别是 active: stream中有数据且数据可以被发送 empty: stream中没有数据 waitingOnStreamQuota: stream的quota不足，等待有quota时再发送数据 一个stream一开始的状态为empty，因为一个stream在被创建出来时还没有待发送的数据。 func (l *loopyWriter) preprocessData(df *dataFrame) error { str, ok := l.estdStreams[df.streamID] if !ok { return nil } // If we got data for a stream it means that // stream was originated and the headers were sent out. str.itl.enqueue(df) if str.state == empty { str.state = active l.activeStreams.enqueue(str) } return nil } 当server处理controlBuffer时遇到某个stream的frame时，会将该stream转成active状态，active状态的stream可以发送数据。 func (l *loopyWriter) processData() (bool, error) { ...... if strQuota := int(l.oiws) - str.bytesOutStanding; strQuota \u003c= 0 { // stream-level flow control. str.state = waitingOnStreamQuota return false, nil } ...... str.bytesOutStanding += size ...... } 发送数据之后，byteOutStanding会增加相应的数据大小，表明该stream有这些数据被发送给client，还没有收到回应。而当byteOutStanding的大小超过loopyWriter.oiws，也就是65535后，会拒绝为该strema继续发送数据，这种策略避免了不断向一个失去回应的client发送数据，避免浪费网络带宽。 TODO； 客户端处理 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:5:4","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc流量控制小结 流量控制，一般是指在网络传输过程中，发送者主动限制自身发送数据的速率或者发送的数据量，以适应接收者处理数据的速度，当接收者的处理速度较慢是，来不及处理的数据会被存放在内存中，而当内存中的数据缓存区被填满后，新收到的数据就会被扔掉，导致发送者不得不重新发送，造成网络带宽的浪费。 流量控制是一个网络组件的基本功能，我们熟知的TCP协议就规定了流量控制算法，grpc建立在TCP之上，也依赖于http2 WindowUupdate Frame实现了自己在应用层的流量控制。 在grpc中，流量控制体现在三个维度： 采样流量控制：grpc接收者检测一段时间内收到的数据量，从而推测出bdp，并指导发送者调整流量控制窗口 connection level流量控制：发送者在初始化时被分配一定的quota，quota随数据发送而降低，并在收到接收者的反馈之后增加，发送者在耗尽quota之后不能再发送数据 stream level流量控制：和connection level的流量控制类似，只不过connection level管理的是一个连接的所有流量，而stream level管理的是connection中诸多stream中的一个。 grpc中的流量控制仅针对HTTP2 data frame。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:5:5","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"参考文献 [^1] gprc源码分析 zhengxinzx 一系列grpc源码分析，主要介绍了grpc的原理和流量控制 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:6:0","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":null,"content":"阅前须知 ","date":"2025-04-28","objectID":"/posts/java-concurrency-jmm/:1:0","tags":null,"title":"Java内存模型","uri":"/posts/java-concurrency-jmm/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"阅前提示 参考文献中的文章非常的好，基本看完了就能理解很多东西，推荐阅读 源码中也提供了很多注释文本，推荐对照源码学习。 ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:1:0","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"重要概念和接口 ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:2:0","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"Runnable 接口 @FunctionalInterface public interface Runnable { public abstract void run(); } 线程可以接受一个实现 Runnable 接口的对象，并执行对应的逻辑。 ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:2:1","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"Callable 接口 @FunctionalInterface public interface Callable\u003cV\u003e { V call() throws Exception; } 类似Runnalbe接口，但可以返回结果和抛出异常 ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:2:2","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"Future 接口 表示异步执行的结果，提供了获取结果以及取消计算执行等方法。 public interface Future\u003cV\u003e { // 取消任务执行，mayInterruptIfRunning参数为true时将中断正在执行任务的线程，否则正在执行的任务将继续执行 boolean cancel(boolean mayInterruptIfRunning); // 是否任务在执行完成前被取消 boolean isCancelled(); // 任务是否完成，不管任务正常结束、抛出异常还是被取消都认为任务完成 boolean isDone(); // 等待任务完成并获得结果 // 计算被取消时抛出CalcellationException // 计算抛出异常时抛出ExecutionException // 当前线程等待时被中断抛出InteruptedException V get() throws InterruptedException, ExecutionException; // 超时版本的get，如果等待超时抛出TimeoutException V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; } ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:2:3","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"Executor 接口 public interface Executor { void execute(Runnable command); } 线程池基础接口，提交一个任务到线程池执行。 Memory consistency effects: Actions in a thread prior to submitting a Runnable object to an Executor happen-before its execution begins, perhaps in another thread. ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:2:4","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"ExecutorSerivce 接口 ExecutorService继承了Executor接口，通常我们使用ExecutorService作为线程池接口，它提供了丰富的功能，一般能够满足需求。 public interface ExecutorService extends Executor { // 已提交的任务继续执行，但不再接收新任务，等待正在执行任务终止请使用awaitTermination void shutdown(); // 尝试停止所有正在执行的任务，终止所有等待任务的处理，返回等待任务列表，等待正在执行任务终止请使用awaitTermination // 无法保证所以任务都能终止，经典的实现会通过`interrupt`取消任务，但如果任务不响应中断，则可能永远都不会停止 List\u003cRunnable\u003e shutdownNow(); // 线程池是否关闭 boolean isShutdown(); // shutdown后所有任务是否已经结束 // 这个方法只有在调用shutdown或者shutdownNow后才可能返回true boolean isTerminated(); // 在shutdown后调用，阻塞直到所有任务执行完成或者超时发生或者当前线程被中断 boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; // 提交一个有返回值的任务，通过Future对象获取返回值 // task不能为null，否则抛出NullPointerException // 如果任务不能被调用执行，抛出 RejectedExecutionException \u003cT\u003e Future\u003cT\u003e submit(Callable\u003cT\u003e task); // 类似 submit(Callable)，返回值对应传入的result参数 \u003cT\u003e Future\u003cT\u003e submit(Runnable task, T result); // 类似 submit(Callable)，返回值为null Future\u003c?\u003e submit(Runnable task); // 执行给定的任务列表，当全部完成时返回Futrue列表，在操作执行时修改给定的集合会导致未定义行为 // 在等待时发生中断，抛出InterruptedException，取消未完成的任务 // 任务列表和其中的任务都不能为null，否则抛出NullPointerException // 如果任何任务不能被调度执行，抛出RejectedExecutionException \u003cT\u003e List\u003cFuture\u003cT\u003e\u003e invokeAll(Collection\u003c? extends Callable\u003cT\u003e\u003e tasks) throws InterruptedException; // 超时版本的invokeAll，如果超时发生，取消未完成的任务 \u003cT\u003e List\u003cFuture\u003cT\u003e\u003e invokeAll(Collection\u003c? extends Callable\u003cT\u003e\u003e tasks, long timeout, TimeUnit unit) throws InterruptedException; // 类似invokeAll，执行给定的任务列表，返回一个成功执行任务的结果，指没有抛出异常，取消未执行完成的任务 // tasks为空时抛出IllegalArgumentException // 如果没有任务成功完成，抛出ExecutionException \u003cT\u003e T invokeAny(Collection\u003c? extends Callable\u003cT\u003e\u003e tasks) throws InterruptedException, ExecutionException; // 超时版本的invokeAny // 超时发生抛出TimeoutException \u003cT\u003e T invokeAny(Collection\u003c? extends Callable\u003cT\u003e\u003e tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; } Memory consistency effects: Actions in a thread prior to the submission of a Runnable or Callable task to an ExecutorService happen-before any actions taken by that task, which in turn happen-before the result is retrieved via Future. get(). Doug Lea 给了一个终止线程池的例子，首先调用shutdown拒绝接受新任务，然后调用shutdowNow，取消逗留的任务，这里特别处理了当前线程遇到interrupt的情况。 void shutdownAndAwaitTermination(ExecutorService pool) { pool.shutdown(); // Disable new tasks from being submitted try { // Wait a while for existing tasks to terminate if (!pool.awaitTermination(60, TimeUnit.SECONDS)) { pool.shutdownNow(); // Cancel currently executing tasks // Wait a while for tasks to respond to being cancelled if (!pool.awaitTermination(60, TimeUnit.SECONDS)) System.err.println(\"Pool did not terminate\"); } } catch (InterruptedException ie) { // (Re-)Cancel if current thread also interrupted pool.shutdownNow(); // Preserve interrupt status Thread.currentThread().interrupt(); } } ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:2:5","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"FutureTask 类源码解析 接口RunnableFuture是Runnalbe的Future，run方法的成功执行对应Future的完成，并允许获取结果。 public interface RunnableFuture\u003cV\u003e extends Runnable, Future\u003cV\u003e { void run(); } FutureTask类实现了RunnableFuture，FutureTask的具体实现原理留在后面再讲。(todo) ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:3:0","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"AbstractExecutorService 源码解析 AbstractExecutorService抽象类派生自ExecutorService接口，然后在其基础上实现了几个实用的方法，这些方法提供给子类进行调用。 抽象类实现了 invokeAny 和 invokeAll 方法（这两个方法先不看 todo），方法newTaskFor用于将Runnable或者Callable包装成FutureTask。提交任务到线程池中有两类方法，submit用于需要返回值的场景，execute用于不需要返回值的场景，当然可以都只用submit方法，当不需要返回值时返回 null 即可。 public abstract class AbstractExecutorService implements ExecutorService { // 将runnable包装成FutureTask protected \u003cT\u003e RunnableFuture\u003cT\u003e newTaskFor(Runnable runnable, T value) { return new FutureTask\u003cT\u003e(runnable, value); } // 将Callable包装成FutureTask protected \u003cT\u003e RunnableFuture\u003cT\u003e newTaskFor(Callable\u003cT\u003e callable) { return new FutureTask\u003cT\u003e(callable); } // 包装成FutureTask，并交给底层execute方法执行 public Future\u003c?\u003e submit(Runnable task) { if (task == null) throw new NullPointerException(); RunnableFuture\u003cVoid\u003e ftask = newTaskFor(task, null); execute(ftask); return ftask; } public \u003cT\u003e Future\u003cT\u003e submit(Runnable task, T result) { if (task == null) throw new NullPointerException(); RunnableFuture\u003cT\u003e ftask = newTaskFor(task, result); execute(ftask); return ftask; } public \u003cT\u003e Future\u003cT\u003e submit(Callable\u003cT\u003e task) { if (task == null) throw new NullPointerException(); RunnableFuture\u003cT\u003e ftask = newTaskFor(task); execute(ftask); return ftask; } ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:4:0","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"ThreadPoolExecutor ThreadPoolExecutor是 JDK 中的线程池实现，实现了任务提交、线程管理、监控等方法。 通过构造函数，介绍一些重要的属性： corePoolSize 核心线程数，注意有时将核心线程数内的线程称为核心线程，但核心线程本身和其他线程一样 maximumPoolSize 最大线程数 workQueue 任务队列，BlockingQueue 接口的某个实现（常用 ArrayBlockingQueue 和 LinkedBlockingQueue） keepAliveTime 空闲线程的保活线程，默认只对非核心线程生效，可以通过设置allowCoreThreadTimeout(true)使核心线程数内的线程可以被回收 threadFactory 用于生成线程，比如设置线程的名字 handler 设置线程池的拒绝策略 Doug Lea 采用一个 32 为的整数来存放线程池状态和线程池中的线程数，其中高 3 为用于存放线程池状态，低 29 位表示线程数。 private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); private static final int COUNT_BITS = Integer.SIZE - 3; private static final int CAPACITY = (1 \u003c\u003c COUNT_BITS) - 1; // runState is stored in the high-order bits private static final int RUNNING = -1 \u003c\u003c COUNT_BITS; private static final int SHUTDOWN = 0 \u003c\u003c COUNT_BITS; private static final int STOP = 1 \u003c\u003c COUNT_BITS; private static final int TIDYING = 2 \u003c\u003c COUNT_BITS; private static final int TERMINATED = 3 \u003c\u003c COUNT_BITS; // Packing and unpacking ctl private static int runStateOf(int c) { return c \u0026 ~CAPACITY; } private static int workerCountOf(int c) { return c \u0026 CAPACITY; } private static int ctlOf(int rs, int wc) { return rs | wc; } /* * Bit field accessors that don't require unpacking ctl. * These depend on the bit layout and on workerCount being never negative. */ private static boolean runStateLessThan(int c, int s) { return c \u003c s; } private static boolean runStateAtLeast(int c, int s) { return c \u003e= s; } private static boolean isRunning(int c) { return c \u003c SHUTDOWN; } 线程池各种状态的介绍： RUNNING: 接受新的任务，处理等待队列中的任务 SHUTDWON: 不接受新的任务，但会继续处理等待队列中的任务 STOP: 不接受新的任务提交，不再处理等待队列中的任务，中断正在执行的线程 TIDYING: 所有的任务都销毁了，workCount 为 0，执行钩子方法 terminated() TERMINATED: terminated()方法调用结束后，线程池的状态切换为此 RUNNING 定义为-1，SHUTDOWN 定义为 0，其他都比 0 大，所以等于 0 时不能提交任务，大于 0 的话，连正在执行的任务也要中断 状态迁移过程： RUNNING -\u003e SHUTDOWN，调用 shutdown() (RUNNING or SHUTDOWN) -\u003e STOP: 调用 shutdownNow() SHUTDOWN -\u003e TIDYING: 当任务队列和线程池都清空后，有 SHUTDOWN 转换为 TIDYING STOP -\u003e TIDYING: 任务队列清空后 TIDYING -\u003e TERMINATED: terminated()方法结束后 Doug Lea 将线程池中的线程包装成内部类 Worker，所以任务是 Runnable （内部变量名叫 task 或者 command)，线程是 worker AQS: todo worker 的实现包含复杂的并发控制，这些暂时不考虑 private final class Worker extends AbstractQueuedSynchronizer implements Runnable { /** Thread this worker is running in. Null if factory fails. */ final Thread thread; /** Initial task to run. Possibly null. */ Runnable firstTask; /** Per-thread task counter */ volatile long completedTasks; /** * Creates with given first task and thread from ThreadFactory. * @param firstTask the first task (null if none) */ Worker(Runnable firstTask) { setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this); } /** Delegates main run loop to outer runWorker */ public void run() { runWorker(this); } execute是一个非常重要的方法，所有submit方法底层都会调用execute方法提交任务。可以看到尽管这段代码非常短小，但由于并发问题实现逻辑比较绕。 public void execute(Runnable command) { if (command == null) throw new NullPointerException(); int c = ctl.get(); // 如果当前线程数少于核心线程数，直接添加一个worker来执行任务，将当前任务作为它的第一个任务 // addWorker调用会原子的检查runState和workerCount，避免错误添加新的线程 if (workerCountOf(c) \u003c corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } // 如果线程池处于RUNNING状态，将这个任务添加到任务队列workQueue中 if (isRunning(c) \u0026\u0026 workQueue.offer(command)) { // double-check int recheck = ctl.get(); // 如果线程不处于RUNNING状态，移除已经入队的任务，并执行拒绝策略 if (! isRunning(recheck) \u0026\u0026 remove(command)) reject(command); // 如果线程池还是RUNNING状态，并且线程数为0，那么开启新的线程 else if (workerCountOf(recheck) == 0) addWorker(null, false); } // 如果队列满了，尝试创建新的线程，如果已经达到最大线程数，执行拒绝策略 else if (!addWorker(command, false)) reject(command); } addWorker 方法用来创建新的线程 private boolean addWorker(Runnable firstTask, boolean core) { retry: for (;;) { int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. // 线程池非RUNNINKG状态，则关闭 // 需要排除一种特殊情况，线程池处于SHUTDOWN状态，且等待队列非空，这种情况下应该允许进一步判断是否创建新的worker if (rs \u003e= SHUTDOWN \u0026\u0026 ! (rs == SHUTDOWN \u0026\u0026 firstTask == null \u0026\u0026 ! workQueue.isEmpty())) return false; for (;;) { int wc = ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:5:0","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"Executors工具类 生成一个固定大小的线程池 public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u003cRunnable\u003e()); } 最大线程数设置为和核心线程数相等，此时keepAliveTime设置为0（线程池默认不会不会corePoolSize内的线程），任务队列采用LinkedBlockingQueue，无界队列。 单线程线程池，类似上面，核心线程数为1 public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u003cRunnable\u003e())); } 缓存线程池 public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue\u003cRunnable\u003e()); } 核心线程数为0，最大线程数为Integer.MAX_VALUE，keepAliveTime为60s，任务队列采用SynchronousQueue 线程数不设上限，任务队列为同步队列，60s超时后空闲线程会被回收 ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:6:0","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"参考文献 深度解读 java 线程池设计思想及源码实现 javadoop ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:7:0","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["Spark","优化器"],"content":"遇到的问题 Exception in thread \"main\" java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/Filter at java.base/java.lang.Class.forName0(Native Method) at java.base/java.lang.Class.forName(Class.java:578) at java.base/java.lang.Class.forName(Class.java:557) at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41) at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36) at org.apache.spark.util.SparkClassUtils$.classForName(SparkClassUtils.scala:141) at org.apache.spark.sql.SparkSession$.lookupCompanion(SparkSession.scala:826) at org.apache.spark.sql.SparkSession$.CLASSIC_COMPANION$lzycompute(SparkSession.scala:816) at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$CLASSIC_COMPANION(SparkSession.scala:815) at org.apache.spark.sql.SparkSession$.$anonfun$DEFAULT_COMPANION$1(SparkSession.scala:820) at scala.util.Try$.apply(Try.scala:217) at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$DEFAULT_COMPANION(SparkSession.scala:820) at org.apache.spark.sql.SparkSession$Builder.\u003cinit\u003e(SparkSession.scala:854) at org.apache.spark.sql.SparkSession$.builder(SparkSession.scala:833) at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:28) at org.apache.spark.examples.SparkPi.main(SparkPi.scala) Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.Filter at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:528) ... 16 more 在idea Run/Debug Configuration中添加Add dependencies with provided scope to classpath org.apache.spark.SparkException: A master URL must be set in your configuration at org.apache.spark.SparkContext.\u003cinit\u003e(SparkContext.scala:421) at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:3062) at org.apache.spark.sql.classic.SparkSession$Builder.$anonfun$build$2(SparkSession.scala:911) at scala.Option.getOrElse(Option.scala:201) at org.apache.spark.sql.classic.SparkSession$Builder.build(SparkSession.scala:902) at org.apache.spark.sql.classic.SparkSession$Builder.getOrCreate(SparkSession.scala:931) at org.apache.spark.sql.classic.SparkSession$Builder.getOrCreate(SparkSession.scala:804) at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:923) at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:30) at org.apache.spark.examples.SparkPi.main(SparkPi.scala) 添加jvm参数-Dspark.master=local，本地运行 不知道为什么idea不能直接找到parallelize的定义而飘红，这里直接导入SparkContext并且通过asInstanceOf[SparkContext]明示idea。 // scalastyle:off println package org.apache.spark.examples import scala.math.random import org.apache.spark.SparkContext import org.apache.spark.sql.SparkSession /** Computes an approximation to pi */ object SparkPi { def main(args: Array[String]): Unit = { val spark = SparkSession .builder() .appName(\"Spark Pi\") .getOrCreate() val slices = if (args.length \u003e 0) args(0).toInt else 2 val n = math.min(100000L * slices, Int.MaxValue).toInt // avoid overflow val count = spark.sparkContext.asInstanceOf[SparkContext].parallelize( 1 until n, slices).map { i =\u003e val x = random() * 2 - 1 val y = random() * 2 - 1 if (x*x + y*y \u003c= 1) 1 else 0 }.reduce(_ + _) println(s\"Pi is roughly ${4.0 * count / (n - 1)}\") spark.stop() } } ","date":"2025-04-05","objectID":"/posts/spark_optimizer/:1:0","tags":["Spark","优化器"],"title":"Spark_optimizer","uri":"/posts/spark_optimizer/"},{"categories":["Spark","内存管理"],"content":"关键问题 内存被分成哪些区域，各分区之间的关系是什么，通过什么参数控制 内存上报和释放的单位是什么，上报和释放是如何实现的 如何避免内存没有释放导致资源泄露 如何避免重复上报和漏上报问题 对象的生命周期和内存上报释放之间的关系 哪些对象会被上报，为什么选择这些对象上报 内存上报是否持有对象引用 ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:1:0","tags":["Spark","内存管理"],"title":"Spark内存管理","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","内存管理"],"content":"源码分析 ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:2:0","tags":["Spark","内存管理"],"title":"Spark内存管理","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","内存管理"],"content":"MemoryBlock MemoryBlock表示一段连续的内存空间，类似于操作系统中page的概念。 MemoryBlock继承自MemoryLocation，当追踪堆外分配时，obj为空，offset表示堆外内存地址，当追踪堆内内存分配时，obj为对象引用，offset为对象内偏移量，可以看到MemoryLocation只是记录了对象的位置信息，没有记录对象内存占用的信息。 public class MemoryLocation { @Nullable Object obj; long offset; public class MemoryBlock extends MemoryLocation { private final long length; public int pageNumber = NO_PAGE_NUMBER; MemoryBlock新增两个字段，length表示page的大小，pageNumber很好理解，TaskMemoryManager会给每个页分配一个页号，有以下几种特殊情况 NO_PAGE_NUMBER 表示没有被TaskMemoryManager分配，初始值 FREED_IN_TMM_PAGE_NUMBER 表示被TaskMemoryManager释放，TaskMemoryManager.free操作中会将页号设置为此值，MemoryAllocator.free遇到没有被TaskMemoryMananger释放的页时，会报错 FREED_IN_ALLOCATOR_PAGE_NUMBER 被MemoryAllocator释放，可以检测多次释放 ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:2:1","tags":["Spark","内存管理"],"title":"Spark内存管理","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","内存管理"],"content":"MemoryAllocator MemoryAllocator接口定义了申请和释放MemoryBlock的方法，HeapMemoryAllocator和UnsafeMemoryAllocator分别实现了堆内和堆外的内存分配器。 public interface MemoryAllocator { /** * Allocates a contiguous block of memory. Note that the allocated memory is not guaranteed * to be zeroed out (call `fill(0)` on the result if this is necessary). */ MemoryBlock allocate(long size) throws OutOfMemoryError; void free(MemoryBlock memory); MemoryAllocator UNSAFE = new UnsafeMemoryAllocator(); MemoryAllocator HEAP = new HeapMemoryAllocator(); } HeapMemoryAllocator public class HeapMemoryAllocator implements MemoryAllocator { @GuardedBy(\"this\") private final Map\u003cLong, LinkedList\u003cWeakReference\u003clong[]\u003e\u003e\u003e bufferPoolsBySize = new HashMap\u003c\u003e(); private static final int POOLING_THRESHOLD_BYTES = 1024 * 1024; 可以看到实际分配的对象就是long数组，并且做了池化，对于1MB以上的内存尝试放入池中，这里没有限制池的大小，持有的是long数组的弱引用，减少频繁申请和释放大内存造成的开销。 如果申请不到内存，会抛出OutOfMemoryError UnsafeMemoryAllocator 实现没有什么特殊的地方，直接调用Spark包装过的Unsafe API，直接调用Unsafe包中的API，所以不受MaxDirectMemorySize的控制 public long allocateMemory(long bytes) { beforeMemoryAccess(); return theInternalUnsafe.allocateMemory(bytes); } ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:2:2","tags":["Spark","内存管理"],"title":"Spark内存管理","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","内存管理"],"content":"MemoryManager MemoryManager抽象类负责管理内存，在计算和存储之间共享内存，计算内存指在shuffles, joins, sorts and aggregations 中计算过程所使用的内存，而存储内存指被用于缓存或者在集群中传播内部数据所占用的内存，每个JVM只有一个MemoryManager。 Spark内存参数 spark.memory.offHeap.enabled 如果开启，某些计算将使用堆外内存，要求spark.memory.offHeap.size必须为正数，默认关闭 spark.memory.fraction (堆内存 - 300MB)被用于计算和存储的比例，这个值越低，吐磁盘以及缓存驱逐发生的越频繁，这个设置的主要目的是留出空间给内部晕啊数据，用户数据结构以及比如稀疏、不寻常的大内存记录导致的内存估算不准确。默认值为0.6 spark.memory.offHeap.size指定了spark堆外使用的内存大小 saprk.memory.storageFraction免于驱逐的存储内存占用内存大小，这里表示为spark.memory.fraction留出的内存的百分比。默认为0.5 堆外内存由spark.memory.offHeap.size规定，堆外存储内存为$spark.memory.offHeap.size * spark.memory.storageFractioin$，剩余的内存为堆外计算内存。 主要字段和方法 @GuardedBy(\"this\") protected val onHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(\"this\") protected val offHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.OFF_HEAP) @GuardedBy(\"this\") protected val onHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(\"this\") protected val offHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.OFF_HEAP) StorageMemoryPool实际管理存储内存，ExecutionMemoryPool实际管理计算内存，这两者在处理关键操作是都需要持有MemoryManager对象锁，从而实现在存储和计算之间共享内存的操作。 acquireStorageMemory:获得存储内存用来缓存block等 acquireUnrollMemory: 获取展开内存用来展开给定的block acquireExecutionMemory: 获得计算内存，调用可能阻塞，确保每个任务至少有机会获得$1/ 2N$内存池大小，N表示当前活跃任务数量，比如老的任务已经占用了很多内存而任务数增加 releaseExecutionMemory 释放计算内存 releaseAllExecutionMemoryForTask 释放当前任务的所有计算内存 releaseStorageMemory 释放存储内存 releaseAllStorageMemory 释放所有存储内存 releaseUnrollMemory 释放展开内存 ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:2:3","tags":["Spark","内存管理"],"title":"Spark内存管理","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","内存管理"],"content":"UnifiedMemoryManager 堆内的内存总量由spark.testing.memory指定，默认为jvm堆大小，保留内存为300MB $$ 可用于存储或者计算的内存 = (spark.testing.memory - reserved\\ memory) * spark.memory.fraction $$ 初始存储内存大小占比由spark.memory.storageFraction指定，存储和计算可以相互借用对方的内存，遵循以下规则： 如果计算内存不足，可最多可以让存储将占用超过初始存储内存大小的空间返还给计算内存 如果存储空间不足，可以借用计算内存的多余空间 acquireExecutionMemory: 实际调用executionPool.acquireMemory，依赖于回调函数maybeGrowExecutionPool和computeMaxExecutionPoolSize，前者可能将部分存储内存转移到计算内存，后者计算当前情况下最大计算内存，等于可用内存减去存储内存当前占用和存储内存初始大小的最小值。 acquiredStorageMemory: 如果存储内存空间不足，则尝试借用部分计算内存空间，最后调用storagePool.acquireMemory实际执行操作 acquireUnrollMemory: 实际调用acquiredStorageMemory ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:2:4","tags":["Spark","内存管理"],"title":"Spark内存管理","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","内存管理"],"content":"MemoryPool 管理一块可以调整大小的内存区域的内部状态和使用记录。 ExecutionMemoryPool 字段memoryForTask记录了每个task id (long)对应的内存消耗(long)。 每个任务最少可以占用 $1 / 2N * poolSize$，而每个任务最多占用$1 / N * maxPoolsize$ acquireMemory: 如果是新的任务，加入memoryForTask，并且通知所有等待获取计算内存的任务，当前任务数增加 循环，直到任务占用超过了上限1/N，或者有空闲内存，以下步骤均在循环体来 调用maybeGrowPool尝试从存储空间获取内存 计算每个任务的最少内存占用和最高内存占用 如果获得的计算内存加上当前内存占用低于最少内存占用，则等待通知 否则更新状态，并返回获取到的内存大小 releaseMemory: 释放内存，如果释放后当前内存为0，则移除当前任务，只要释放内存，则通知在acquiredMemory等待的任务内存已经释放 StorageMemoryPool acquireMemory: 如果存储空间不足，则调用memoryStore.evictBlocksToFreeSpace释放部分空间，判断需要的内存大小是否小于等于当前空闲内存 releaseMemory: 释放内存 freeSpaceToShrinkPool: 释放内存来减少存储空间的占用，必要时调用memoryStore.evictBlocksToFreeSpace驱逐block ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:2:5","tags":["Spark","内存管理"],"title":"Spark内存管理","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","内存管理"],"content":"TaskMemoryManager 内存地址编码 当需要将一个int或者long之类的元素插入到数组或者堆外的指定位置时 对于堆内，需要知道数据的引用以及偏移量，在TaskMemoryManager中保存了pageNumber和MemoryBlock的映射，而MemoryBlock保存了对象的引用，所以使用64位编码内存地址时，前13位用来储存pageNumber，后51位用来存储数组中的偏移量。（对象的地址会由于gc的原因而变动，所以不能直接使用对象地址） 对于堆外，需要知道申请到堆外内存的起始地址和偏移量，依然使用前13位存储pageNumber，使用后51位存储偏移量。这里如果直接使用内存地址，则不能知道对应的page是那个，当使用前13位储存pageNumber后，后51位显然不能储存内存的绝对地址，而应该存储内存相对于起始地址的偏移量。 主要字段作用 pageTable: 页表，保存pageNumber到MemoryBlock的映射，MemoryBlock[PAGE_TABLE_SIZE] memoryManager: TaskMemoryManager共享MemoryManager的内存资源 taskAttemptId: task Id tungtenMemoryMode: 使用堆内还是堆外内存，和MemoryManger保持一致 consumers：内存消费者，支持吐磁盘，HashSet\u003cMemoryConsumer\u003e acquiredButNotUsed: 向内存管理框架申请内存成功，但实际申请内存时发生OOM，认为MemoryManager可能高估了实际的可用内存，将这部分内存配额保存在此字段，方便后续触发吐磁盘，long currentOffHeapMemory: 任务当前堆外内存占用，long currentOnHeapMemory：任务当前堆内内存占用，long peakOffHeapMemory：任务最高堆外内存占用，long peakOnHeapMemory：任务最高堆内内存占用，long 主要方法 acquireExecutionMemory为指定的MemoryConsumer获取内存，如果没有足够的内存，触发吐磁盘释放内存，返回成功获得的计算内存(\u003c=N)。 public long acquireExecutionMemory(long required, MemoryConsumer requestingConsumer) { 首先调用MemoryManager.acquireExecutionMemory尝试获取计算内存 如果获取到足够的内存，则跳过吐磁盘逻辑 如果没有获取到足够的内存，尝试吐磁盘释放内存，并尝试获取计算内存 吐磁盘有两个优化的目标： 最小化吐磁盘调用的次数，减少吐磁盘文件的数量并且避免小的吐磁盘文件 避免吐磁盘释放内存超过所需，如果我们只是想要一丁点内存，不希望尽可能多的吐磁盘，很多内存消费者吐磁盘时会释放比请求多的内存 所以这里采用一种启发式的算法，选择内存占用超过所需内存的MemoryConsumer中最小的MemoryConsumer来平衡这些因素，当只有少量大内存请求时，这种方法效率很好，但如果场景中有大量小内存请求，这种方法会导致产生大量小的spill文件 具体实现，将所有的MemoryConsumer放入一个TreeMap中，根据内存占用排序，如果是当前MemoryConsumer，则认为内存占用为0，这样当前MemoryConsumer被spill的优先级最低。 然后选择内存占用超过所需内存的MemoryConsumer中最小的MemoryConsumer进行吐磁盘操作并且尝试获取计算内存，如果没有符合这一条件的MemoryConsumer，则直接选择内存占用最大的MemoryCosumer进行吐磁盘并尝试获取计算内存trySpillAndAcquire。 如果获取到的内存依然不满足需求，则继续吐磁盘流程，选择下一个MemoryConsumer，重复上述流程。 最终不管是否获取到了所需的内存，都将MemoryConsumer加入consumers中，并更新当前和最高的任务内存占用 trySpillAndAcquire对选中的MemoryConsumer执行吐磁盘操作释放内存，并尝试获取所需的计算内存 * @return number of bytes acquired (\u003c= requested) * @throws RuntimeException if task is interrupted * @throws SparkOutOfMemoryError if an IOException occurs during spilling */ private long trySpillAndAcquire(MemoryConsumer requestingConsumer, long requested, List\u003cMemoryConsumer\u003e cList, int idx) 首先调用MemoryConsumer#spill方法尝试释放内存，如果释放内存为0，则直接返回0 如果释放内存大于0，调用MemoryManager#acquireExecutionMemory尝试获取计算内存，这里需要注意，吐磁盘释放的内存会被所有任务公平竞争，所以可能无法获取到这次吐磁盘释放的所有内存，需要在下一次循环中继续尝试吐磁盘 两种异常场景，当任务被中断时，抛出RuntimeException，吐磁盘遇到IOException时，抛出SparkOutOfMemoryError releaseExecutionMemory 为一个MemoryConsumer释放N字节的计算内存，实际调用了MemoryManager#releaseExecutionMemory，并更新当前内存占用 showMemoryUsage dump所有Consumer的内存占用 allocatePage 分配内存，并更新页表，该操作旨在为多个算子之间共享的大块内存分配空间 public MemoryBlock allocatePage(long size, MemoryConsumer consumer) 首先调用TaskMemoryManager#acquiredExectionMemory获取计算内存，如果没有获取到内存，则返回null 然后通过MemoryManager#tungstenMemoryAllocator#allocate实际申请内存，如果遇到OutOfMemoryError，则认为实际上没有足够多的内存，实际的空闲内存要比MemoryManager认为的少一些，所以将从内存管理框架中获得的内存配额添加到acquiredButNotUsed字段中，并再次调用当前函数，这次将触发吐磁盘操作释放内存（p.s. 感觉处理OutOfMemoryError的意义不大，OutOfMeomryError发生时应该直接结束程序，因为程序已经进入了异常状态，无法预料OutOfMemoryError对程序的影响） 如果成功获取到内存，则需要更新页表，并返回对应的页，其实就是MemoryBlock freePage释放页占用的内存，更新pageNumber为FREED_IN_TMM_PAGE_NUMBER，清理页表，调用MemoryManager.tunstenMemoryAllocator#free实际释放内存，调用releaseExecutionMemory释放内存管理框架对应的内存配额。 似乎用逻辑内存指代内存管理框架中的内存配额，而用物理内存指代实际的内存更加好一些 public void freePage(MemoryBlock page, MemoryConsumer consumer) { cleanUpAllAllocatedMemory清理所有申请的内存和页 调用MemoryManager#tungstenMemoryAllocator#free释放每个页的内存 调用MemoryManager#releaseExectionMemory释放acquiredButNotUsed内存 调用MemoryManager#ReleaseAllExecutionMemoryForTask释放任务的所有计算内存，并返回释放的内存大小，非0值可以用来检测内存泄露 ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:2:6","tags":["Spark","内存管理"],"title":"Spark内存管理","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","内存管理"],"content":"参考资料 Deep Dive into Spark Memory Management Apache Spark Memory Management: Deep Dive ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:3:0","tags":["Spark","内存管理"],"title":"Spark内存管理","uri":"/posts/spark_memory_manager/"},{"categories":["kubernetes"],"content":"Pods Pod 是 Kubernetes 中最小的可部署计算单元，你可以创建和管理它们。 Pod（类似于一群鲸鱼的“pod”或豌豆荚“pea pod”） 是一组一个或多个容器，这些容器共享存储和网络资源，并且有一个规范来定义如何运行它们。Pod 内部的内容始终是 共同调度（co-scheduled）并在相同的上下文中运行 的。Pod 充当一个特定应用的“逻辑主机”（logical host）：它包含一个或多个 相对紧密耦合的应用容器。在非云环境下，运行在同一台物理机或虚拟机上的应用程序，可以类比于在 Kubernetes 中运行在同一逻辑主机上的应用。 除了应用容器之外，Pod 还可以包含 Init 容器（init containers），这些容器在 Pod 启动时运行。此外，你还可以 注入临时容器（ephemeral containers） 来调试正在运行的 Pod。 ","date":"2025-02-07","objectID":"/posts/k8s_storage/:1:0","tags":null,"title":"k8s 存储","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"Volumes k8s Volumes 为pod中的容器提供了通过文件系统访问和共享数据的方式，数据共享可以在一个容器中的不同进程或者容器间甚至不同的pod。 volume能够解决数据的持久化以及共享存储的问题。 k8s支持多种volumes，pod可以同时使用任意数量的不同类型volume，Ephemeral volume 的生命周期和pod相同，persistent volumes 可以超出一个pod的生命周期。当pod挂掉时，K8s会摧毁 ephemeral volume 但不会摧毁 persistent volume 。对于在给定pod中的任意类型的volume，数据在容器重启时都会被保留。 本质上，卷（Volume）是一个目录，其中可能包含一些数据，并且可供 Pod 内的容器访问。该目录如何创建、由何种存储介质支持以及其内容，取决于所使用的特定卷类型。 为了使用一个卷，声明要被提供给pod的卷在.spec.volumes下，声明在容器的哪里挂载这些卷在spec.containers[*].volumeMounts中。 当一个pod被启动时，容器中的进程看到的文件系统视图有两部分组成，一部分是容器镜像的初始内容，另一部分是挂载到容器中的卷。对于pod中的每个容器，需要独立的声明不同容器的挂载点。 ","date":"2025-02-07","objectID":"/posts/k8s_storage/:2:0","tags":null,"title":"k8s 存储","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"Storage Classes StorageClass 为管理员提供了一种描述其提供的存储类别的方法。不同的存储类别可能对应不同的 服务质量（QoS）级别、备份策略，或者由集群管理员自定义的其他策略。Kubernetes 本身并不对这些存储类别的具体含义做任何规定。 每个StorageClass 包含字段 provisioner, parameters和raclaimPolicy，当一个属于某个storage class 的persistent volume需要被动态提供给 persistent volume claim时被使用。 Storage Class的名字非常重要，用户通过名字请求某类存储，管理员在创建storage class对象是设置名字以及类别的其他参数。 作为管理员，你可以声明一个默认的storage class用于没有指定类别的任何PVC。 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: low-latency annotations: storageclass.kubernetes.io/is-default-class: \"false\" provisioner: csi-driver.example-vendor.example reclaimPolicy: Retain # default value is Delete allowVolumeExpansion: true mountOptions: - discard # this might enable UNMAP / TRIM at the block storage layer volumeBindingMode: WaitForFirstConsumer parameters: guaranteedReadWriteLatency: \"true\" # provider-specific ","date":"2025-02-07","objectID":"/posts/k8s_storage/:3:0","tags":null,"title":"k8s 存储","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"Persistent Volumes PersistentVolume (PV) 是集群中的一块存储，可以由管理员提供或者通过 Storage Classes动态提供。 **PersistentVolumeClaim (PVC)**是用户对存储的青秀区，类似于pod，pod消费节点资源而PVCs消费PV资源 有两种方式提供PVs： 静态：管理员直接创建PV 动态：当没有静态PV满足PVC，集群可能尝试动态的提供卷，PVC必须要求Storage Class，管理员必须创建并且配置storage class，Storage Class \"\"关闭动态获取卷 Pod使用PVC作为卷，集群检查PVC得到对应的卷并将卷绑定到pod。 ","date":"2025-02-07","objectID":"/posts/k8s_storage/:4:0","tags":null,"title":"k8s 存储","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"回收策略 当用户使用完卷后，可以将PVC对象删除从而允许资源的回收，PV的回收策略告诉集群当卷被释放后应该怎么样处理卷，目前有三种策略：Retained, Recycled和 Deleted。 这里只介绍Retain策略： Retain回收策略允许手动的资源回收，当PVC被删除时， PV依然存在，卷被认为是释放状态，但它并不能够被另一个PVC请求直接使用因为前任的数据还在上面。管理员可以通过以下方式手动回收卷： 删除PV 手动清理数据 手动删除对应的storage asset 如果想要重用相同的storage asset，使用相同的storage asset definition 创建一个新的PV ","date":"2025-02-07","objectID":"/posts/k8s_storage/:4:1","tags":null,"title":"k8s 存储","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"PV的类型 PV类型作为插件实现，这里给出k8s支持的一些插件： csi : Container Storage Interface local: 挂载在节点上的本地存储设备 每个PV包含一个规范(spec) 和状态 (status)，PV对象的名字必须是一个有效的 DNS subdomain name，这意味着 包含不超过253个字符 只包含小写字符、数字、-或者. 字符或者数字开头 字符或者数字结尾 apiVersion: v1 kind: PersistentVolume metadata: name: pv0003 spec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: slow mountOptions: - hard - nfsvers=4.1 nfs: path: /tmp server: 172.17.0.2 访问模式 ReadWriteOnce，卷可以被挂载为单个节点可读写，ReadWriteOnce仍然允许多个pod访问，只要这些pod在相同的节点上。对于单pod访问，可以使用ReadWriteOncePod。 节点亲和度 Node Affinity，对于大多数卷类型，不需要设置这个字段，对于local卷需要显示设置这个字段。 一个PV可以声明节点亲和度来限制在那个节点上这个卷可以被访问，使用某个PV的Pod将只会被调度到满足节点亲和度的节点上。 ","date":"2025-02-07","objectID":"/posts/k8s_storage/:4:2","tags":null,"title":"k8s 存储","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"PersistentVolumeClaims apiVersion: v1 kind: PersistentVolumeClaim metadata: name: myclaim spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: slow selector: matchLabels: release: \"stable\" matchExpressions: - {key: environment, operator: In, values: [dev]} ","date":"2025-02-07","objectID":"/posts/k8s_storage/:4:3","tags":null,"title":"k8s 存储","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"参考文献 https://kubernetes.io/docs/concepts/storage/persistent-volumes/ ","date":"2025-02-07","objectID":"/posts/k8s_storage/:5:0","tags":null,"title":"k8s 存储","uri":"/posts/k8s_storage/"},{"categories":null,"content":"Prometheus https://www.youtube.com/watch?v=h4Sl21AKiDg Prometheus Server, Pushgateway, Alertmanager https://prometheus.io/docs/concepts/metric_types/ https://itnext.io/prometheus-for-beginners-5f20c2e89b6c Prometheus is essentially just another metrics collection and analysis tool, and at its core it is made up of 3 components: A time series database that will store all our metrics data A data retrieval worker that is responsible for pulling/scraping metrics from external sources and pushing them into the database A web server that provides a simple web interface for configuration and querying of the data stored. https://prometheus.io/docs/practices/naming/#metric-names https://prometheus.io/docs/practices/naming/#base-units ","date":"2025-02-01","objectID":"/posts/prometheus_and_grafana/:1:0","tags":["prometheus","grafana"],"title":"Prometheus_and_grafana","uri":"/posts/prometheus_and_grafana/"},{"categories":["MinIO"],"content":"xl.meta 数据结构 当对象大小超过 128KiB 后，比如a.txt，数据和元数据分开存储 MinIO 提供了命令行工具xl-meta用来查看xl.meta文件 { \"Versions\": [ { \"Header\": { \"EcM\": 1, \"EcN\": 0, \"Flags\": 2, \"ModTime\": \"2025-01-23T15:27:45.311572+08:00\", \"Signature\": \"d0c2b58b\", \"Type\": 1, \"VersionID\": \"00000000000000000000000000000000\" }, \"Idx\": 0, \"Metadata\": { \"Type\": 1, \"V2Obj\": { \"CSumAlgo\": 1, \"DDir\": \"74hQxU7FTrq56ShK8pjqAA==\", \"EcAlgo\": 1, \"EcBSize\": 1048576, \"EcDist\": [1], \"EcIndex\": 1, \"EcM\": 1, \"EcN\": 0, \"ID\": \"AAAAAAAAAAAAAAAAAAAAAA==\", \"MTime\": 1737617265311572000, \"MetaSys\": {}, \"MetaUsr\": { \"content-type\": \"text/plain\", \"etag\": \"90a1a2b65a4e40d55d758f2a59fe33b4\" }, \"PartASizes\": [2097152], \"PartETags\": null, \"PartNums\": [1], \"PartSizes\": [2097152], \"Size\": 2097152 }, \"v\": 1734527744 } } ] } . ├── a.txt │ ├── ef8850c5-4ec5-4eba-b9e9-284af298ea00 │ │ └── part.1 │ └── xl.meta └── b.txt └── xl.meta ","date":"2025-01-22","objectID":"/posts/minio-get-started/:1:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"minio 的启动流程 minio 启动核心的核心命令为 minio server https://minio{1...4}.example.net:9000/mnt/disk{1...4}/minio，表示 minio 服务分布部署在 4 台服务器上总共 16 块磁盘上，...这种写法称之为拓展表达式，比如 http://minio{1…4}.example.net:9000实际上表示http://minio1.example.net:9000到http://minio4.example.net:9000`的4台主机。 go 程序的入口为main#main()函数，直接调用了cmd#Main,其中做了一些命令行程序的相关操作，包括注册命令，其中registerCommand(serverCmd)注册服务相关命令，cmd#ServerMain是主要启动流程函数。 // Run the app - exit on error. if err := newApp(appName).Run(args); err != nil { os.Exit(1) //nolint:gocritic } var serverCmd = cli.Command{ Name: \"server\", Usage: \"start object storage server\", Flags: append(ServerFlags, GlobalFlags...), Action: serverMain, ","date":"2025-01-22","objectID":"/posts/minio-get-started/:2:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"ServerMain server http://127.0.0.1:/Users/hanjing/mnt/minio0{1...3} http://127.0.0.1:/Users/hanjing/mnt/minio0{4...6} 处理系统终止或者重启相关的信号等 signal.Notify(globalOSSignalCh, os.Interrupt, syscall.SIGTERM, syscall.SIGQUIT) go handleSignals() buildServerCtxt决定磁盘布局以及是否使用 legacy 方式，调用函数cmd#mergeDisksLayoutFromArgs判断是否使用了拓展表达式，如果没有，legacy = true，否则legacy =false, legacy参数的作用我们在后面就能看到了。 serverHandleCmdArgs函数中调用 createServerEndpoints， // Handle all server command args and build the disks layout bootstrapTrace(\"serverHandleCmdArgs\", func() { // 这里确定了erasure set size的大小 err := buildServerCtxt(ctx, \u0026globalServerCtxt) logger.FatalIf(err, \"Unable to prepare the list of endpoints\") serverHandleCmdArgs(globalServerCtxt) }) ","date":"2025-01-22","objectID":"/posts/minio-get-started/:2:1","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"MinIO 的 DNS 缓存 MinIO 为了避免向外发送过多的 DNS 查询，所以实现了 DNS 缓存，默认使用net.DefaultResolver实际执行 DNS 查询，设置的 DNS 查询超时时间为5s，缓存的刷新时间在容器环境下默认为30s，在其他环境下为10min，可以通过dns-cache-ttl指定。 type Resolver struct { // Timeout defines the maximum allowed time allowed for a lookup. Timeout time.Duration // Resolver is used to perform actual DNS lookup. If nil, // net.DefaultResolver is used instead. Resolver DNSResolver once sync.Once mu sync.RWMutex cache map[string]*cacheEntry } globalDNSCache = \u0026dnscache.Resolver{ Timeout: 5 * time.Second, } func runDNSCache(ctx *cli.Context) { dnsTTL := ctx.Duration(\"dns-cache-ttl\") // Check if we have configured a custom DNS cache TTL. if dnsTTL \u003c= 0 { if orchestrated { dnsTTL = 30 * time.Second } else { dnsTTL = 10 * time.Minute } } // Call to refresh will refresh names in cache. go func() { // Baremetal setups set DNS refresh window up to dnsTTL duration. t := time.NewTicker(dnsTTL) defer t.Stop() for { select { case \u003c-t.C: globalDNSCache.Refresh() case \u003c-GlobalContext.Done(): return } } }() } ","date":"2025-01-22","objectID":"/posts/minio-get-started/:3:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"构造拓扑关系 (buildServerCtxt) // serverCtxt保存了磁盘布局 type disksLayout struct { // 是否使用拓展表达式 legacy bool // server pool的集合 pools []poolDisksLayout } type poolDisksLayout struct { // server pool对应的命令行命令 cmdline string // layout的第一位表示不同的erasure set，第二维表示同一个erasure set中不同的磁盘路径 layout [][]string } 构造拓扑关系的主要函数实现是mergeDisksLayoutFromArgs，判断环境变量MINIO_ERASURE_SET_DRIVE_COUNT是否存在，环境变量MINIO_ERASURE_SET_DRIVE_COUNT表示 erasure set 中指定的磁盘数量，否则默认为 0，表示自动设置最优结果。根据是否使用拓展表达式会走不同的逻辑。这里我们主要关心使用拓展表达式的场景GetAllSets(setDriveCount, arg)。（顺带一提，legacy style 会走GetAllSets(setDriveCount, args...)，可以看到 legacy style 只能指定一个server pool） // mergeDisksLayoutFromArgs supports with and without ellipses transparently. // 构造网络拓扑 func mergeDisksLayoutFromArgs(args []string, ctxt *serverCtxt) (err error) { if len(args) == 0 { return errInvalidArgument } ok := true // ok 表示是否使用拓展表达式，true表示不使用拓展表达式 // 只要在其中一个arg中使用拓展表达式，结果均为false for _, arg := range args { ok = ok \u0026\u0026 !ellipses.HasEllipses(arg) } var setArgs [][]string // 通过环境变量得到erasure set的大小，默认为0 v, err := env.GetInt(EnvErasureSetDriveCount, 0) if err != nil { return err } setDriveCount := uint64(v) // None of the args have ellipses use the old style. if ok { setArgs, err = GetAllSets(setDriveCount, args...) if err != nil { return err } // 所有的参数组成一个server pool ctxt.Layout = disksLayout{ legacy: true, pools: []poolDisksLayout{{layout: setArgs, cmdline: strings.Join(args, \" \")}}, } return } for _, arg := range args { if !ellipses.HasEllipses(arg) \u0026\u0026 len(args) \u003e 1 { // TODO: support SNSD deployments to be decommissioned in future return fmt.Errorf(\"all args must have ellipses for pool expansion (%w) args: %s\", errInvalidArgument, args) } setArgs, err = GetAllSets(setDriveCount, arg) if err != nil { return err } ctxt.Layout.pools = append(ctxt.Layout.pools, poolDisksLayout{cmdline: arg, layout: setArgs}) } return } GetAllSets主要调用了parseEndpointSet，通过正则表达式解析带有拓展表达式的输入参数，并返回一个[][]string，表示不同 erasure set 中的磁盘路径。这里主要对应的数据结构是endpointSet，主要实现两件事情，第一确定 setSize，第二确定如何将 endpoints 分布到不同的 erasure set 中。 // Endpoint set represents parsed ellipses values, also provides // methods to get the sets of endpoints. type endpointSet struct { // 解析终端字符串得到的arg pattern，如果有多个ellipses，对应多个`Pattern` argPatterns []ellipses.ArgPattern endpoints []string // Endpoints saved from previous GetEndpoints(). // 对于ellipses-style的参数 // setIndexes对应一行，记录了server pool size /setSize 个 setSize值 setIndexes [][]uint64 // All the sets. } type ArgPattern []Pattern // Pattern - ellipses pattern, describes the range and also the // associated prefix and suffixes. type Pattern struct { Prefix string Suffix string Seq []string } 函数getSetIndexes的目的是找到合适的setSize，MinIO 规定分布式部署 setSize 的取值必须属于var setSizes = []uint64{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}，首先从SetSizes中找到能够被server pool size整除的setCounts集合，如果自定义了setSize则判断自定义的setSize是否属于setCounts集合，如果属于则setSize设置成功，否则返回错误。如果没有设置自定义的setSize，函数possibleSetCountsWithSymmetry从setCounts集合中找到具有symmetry属性的值，MinIO 中输入带拓展表达式的参数对应的 pattern 列表和参数中的顺序是相反的，symmetry过滤出能够被 pattern 中最后一个 pattern 对应的数量整除或者被整除的setCounts中的值，这里举一个例子http://127.0.0.{1...4}:9000/Users/hanjing/mnt/minio{1...32}，显然symmetry函数会判断 4 和setCounts中值的关系，而不是 32 和setCounts中值的关系，这可能与 MinIO 希望尽可能将 erasure set 的中不同磁盘分布到不同的节点上有关。最后取出剩余候选值中最大的值作为最终的setSize。 func (s endpointSet) Get() (sets [][]string) { k := uint64(0) endpoints := s.getEndpoints() for i := range s.setIndexes { for j := range s.setIndexes[i] { sets = append(sets, endpoints[k:s.setIndexes[i][j]+k]) k = s.setIndexes[i][j] + k } } return sets } endpointSet#Get方法返回一个二维数据，第一维表示 不同的 erasure set，第二位表示 erasure set 中的不同磁盘。这里getEndpoints多重循环迭代 ellipses-style 对应的 pattern，如果还记得的话，pattern 的顺序和实际在参数中出现的顺序相反，这样得到的endpoints列表将不同节点上的磁盘均匀分布，后面连续取列表中的一段组成erasure set时，得到的erasure set中的磁盘也分布在不同的节点上。 ","date":"2025-01-22","objectID":"/posts/minio-get-started/:4:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"serverHandleCmdArgs 函数 globalEndpoints, setupType, err = createServerEndpoints(globalMinioAddr, ctxt.Layout.pools, ctxt.Layout.legacy) logger.FatalIf(err, \"Invalid command line arguments\") globalNodes = globalEndpoints.GetNodes() globalIsErasure = (setupType == ErasureSetupType) globalIsDistErasure = (setupType == DistErasureSetupType) if globalIsDistErasure { globalIsErasure = true } globalIsErasureSD = (setupType == ErasureSDSetupType) if globalDynamicAPIPort \u0026\u0026 globalIsDistErasure { logger.FatalIf(errInvalidArgument, \"Invalid --address=\\\"%s\\\", port '0' is not allowed in a distributed erasure coded setup\", ctxt.Addr) } globalLocalNodeName = GetLocalPeer(globalEndpoints, globalMinioHost, globalMinioPort) nodeNameSum := sha256.Sum256([]byte(globalLocalNodeName)) globalLocalNodeNameHex = hex.EncodeToString(nodeNameSum[:]) // Initialize, see which NIC the service is running on, and save it as global value setGlobalInternodeInterface(ctxt.Interface) 里面有一个比较重要的工具函数isLocalHost，通过 DNS 查询 host 对应的 ip，和所有网卡对应的所有本地 ip 取交集,如果交集为空，说明不是本地服务器，否则是本地服务器。 函数createServerEndpoints将数据结构[]poolDisksLayout转换成EndpointServerPools，并指定对应的SetupType 对于单磁盘部署，要求使用目录路径指定输入参数，IsLocal一定为true，SetupType为ErasureSDSetupType。其他情况下根据，根据本地 ip 和给定的 host，判断IsLocal，如果 host 为空（MinIO 称为PathEndpointType)，则setupType = ErasureSetupType，否则为URLEndpointType情况，如果不同host:port的数量等于 1，则是ErasureSetupType，否则对应DistErasureSetupType，根据得到的SetType设置全局参数。 EndpointServerPools实际上是[][]EndPoint，第一位 // EndpointServerPools是 PoolEndpoints的集合，实际上描述整个部署的拓扑结构 type EndpointServerPools []PoolEndpoints // PoolEndpoints represent endpoints in a given pool // along with its setCount and setDriveCount. // PoolEndpoints表示一个server pool的结构 type PoolEndpoints struct { // indicates if endpoints are provided in non-ellipses style // legacy 表示 是否使用遗留的方法表示终端，而不使用省略号表达式 Legacy bool // SetCount表示 server pool中的 erasure set的数量 SetCount int // DrivesPerSet 表示一个erasure set中的磁盘数量 DrivesPerSet int // type Endpoints []Endpoint // 表示一个server pool中的所有disk Endpoints Endpoints // server pool对应的命令行指令 CmdLine string // 操作系统信息 Platform string } type Endpoint struct { *url.URL // 如果是单个目录的输入，则 IsLocal为true // 如果输入参数ip是本地ip，IsLocal也为true // 其他情况下为false IsLocal bool PoolIdx, SetIdx, DiskIdx int } // SetupType - enum for setup type. type SetupType int const ( // UnknownSetupType - starts with unknown setup type. UnknownSetupType SetupType = iota // FSSetupType - FS setup type enum. FSSetupType // ErasureSDSetupType - Erasure single drive setup enum. ErasureSDSetupType // ErasureSetupType - Erasure setup type enum. ErasureSetupType // DistErasureSetupType - Distributed Erasure setup type enum. DistErasureSetupType ) 以下函数列出了 Minio 支持的不同模式，和上面的SetType之间存在对应关系。 // Returns the mode in which MinIO is running func getMinioMode() string { switch { case globalIsDistErasure: return globalMinioModeDistErasure case globalIsErasure: return globalMinioModeErasure case globalIsErasureSD: return globalMinioModeErasureSD default: return globalMinioModeFS } } ","date":"2025-01-22","objectID":"/posts/minio-get-started/:4:1","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"HTTP 服务器注册 API 注册分布式命名空间锁 registerAPIRouter注册 s3 相关的主要 api // Configure server. bootstrapTrace(\"configureServer\", func() { handler, err := configureServerHandler(globalEndpoints) if err != nil { logger.Fatal(config.ErrUnexpectedError(err), \"Unable to configure one of server's RPC services\") } // Allow grid to start after registering all services. close(globalGridStart) close(globalLockGridStart) httpServer := xhttp.NewServer(getServerListenAddrs()). UseHandler(setCriticalErrorHandler(corsHandler(handler))). UseTLSConfig(newTLSConfig(getCert)). UseIdleTimeout(globalServerCtxt.IdleTimeout). UseReadTimeout(globalServerCtxt.IdleTimeout). UseWriteTimeout(globalServerCtxt.IdleTimeout). UseReadHeaderTimeout(globalServerCtxt.ReadHeaderTimeout). UseBaseContext(GlobalContext). UseCustomLogger(log.New(io.Discard, \"\", 0)). // Turn-off random logging by Go stdlib UseTCPOptions(globalTCPOptions) httpServer.TCPOptions.Trace = bootstrapTraceMsg go func() { serveFn, err := httpServer.Init(GlobalContext, func(listenAddr string, err error) { bootLogIf(GlobalContext, fmt.Errorf(\"Unable to listen on `%s`: %v\", listenAddr, err)) }) if err != nil { globalHTTPServerErrorCh \u003c- err return } globalHTTPServerErrorCh \u003c- serveFn() }() setHTTPServer(httpServer) }) // configureServer handler returns final handler for the http server. func configureServerHandler(endpointServerPools EndpointServerPools) (http.Handler, error) { // Initialize router. `SkipClean(true)` stops minio/mux from // normalizing URL path minio/minio#3256 router := mux.NewRouter().SkipClean(true).UseEncodedPath() // Initialize distributed NS lock. if globalIsDistErasure { registerDistErasureRouters(router, endpointServerPools) } // Add Admin router, all APIs are enabled in server mode. registerAdminRouter(router, true) // Add healthCheck router registerHealthCheckRouter(router) // Add server metrics router registerMetricsRouter(router) // Add STS router always. registerSTSRouter(router) // Add KMS router registerKMSRouter(router) // Add API router registerAPIRouter(router) router.Use(globalMiddlewares...) return router, nil } registerAPIRouter会注册主要的 s3 API，这里举GetObject操作为例进行说明，当 http method 为GET时，如果没有命中其他的路由，则认为是GetObject操作，从 Path 中获取object名字，并使用api.GetObjectHandler进行处理和响应，s3APIMiddleware作为中间件，可以做一些额外的操作，比如监控和记录日志。 api对象中保存了一个函数引用，通过这个函数引用，能够得到全局的ObjectLayer对象，ObjectLayer实现了对象 API 层的基本操作。 // GetObject router.Methods(http.MethodGet).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.GetObjectHandler, traceHdrsS3HFlag)) // Initialize API. api := objectAPIHandlers{ ObjectAPI: newObjectLayerFn, } // objectAPIHandlers implements and provides http handlers for S3 API. type objectAPIHandlers struct { ObjectAPI func() ObjectLayer } func newObjectLayerFn() ObjectLayer { globalObjLayerMutex.RLock() defer globalObjLayerMutex.RUnlock() return globalObjectAPI } ","date":"2025-01-22","objectID":"/posts/minio-get-started/:5:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"ObjectLayer 的初始化流程 var newObject ObjectLayer bootstrapTrace(\"newObjectLayer\", func() { var err error newObject, err = newObjectLayer(GlobalContext, globalEndpoints) if err != nil { logFatalErrs(err, Endpoint{}, true) } }) storageclass.LookupConfig函数根据环境变量等初始化Standard Storage Class、Reduced Redundancy Storage Class以及Optimized Storage Class、以及 inline data的大小 Standard Storage Class：通过环境变量MINIO_STORAGE_CLASS_STANDARD指定，否则会根据erasure set的大小指定 // DefaultParityBlocks returns default parity blocks for 'drive' count func DefaultParityBlocks(drive int) int { switch drive { case 1: return 0 case 3, 2: return 1 case 4, 5: return 2 case 6, 7: return 3 default: return 4 } } Reduced Redundancy Storage Class: 通过环境变量MINIO_STORAGE_CLASS_RRS指定，否则默认为 1 Optimized Storage Class：通过环境变量MINIO_STORAGE_CLASS_OPTIMIZE指定，默认为\"\" inline block size: 通过环境变量MINIO_STORAGE_CLASS_INLINE_BLOCK指定，默认为128KiB,如果 shard 数据的大小小于inline block size，则会直接将数据和元数据写到同一个文件，即xl.meta ","date":"2025-01-22","objectID":"/posts/minio-get-started/:6:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"MinIO 的存储分层 erasureServerPools // erasureServerPools // minio 服务可以由多个server pool 组成，用来水平拓展 // erasureServerPools是server pool的集合 type erasureServerPools struct { poolMetaMutex sync.RWMutex poolMeta poolMeta rebalMu sync.RWMutex rebalMeta *rebalanceMeta deploymentID [16]byte distributionAlgo string // server pool 由多个erasure set 组成 // 这里的erasureSets结构实际上指单个 server pool serverPools []*erasureSets // Active decommission canceler decommissionCancelers []context.CancelFunc s3Peer *S3PeerSys mpCache *xsync.MapOf[string, MultipartInfo] } erasureSets // erasureSets implements ObjectLayer combining a static list of erasure coded // object sets. NOTE: There is no dynamic scaling allowed or intended in // current design. // server pool 由多个erasure set 组成 // 这里的erasureSets结构实际上指单个 server pool // 上面这段话的意思是不能动态扩展server pool，初始指定后就不能再修改了 type erasureSets struct { sets []*erasureObjects // Reference format. format *formatErasureV3 // erasureDisks mutex to lock erasureDisks. erasureDisksMu sync.RWMutex // Re-ordered list of disks per set. erasureDisks [][]StorageAPI // Distributed locker clients. erasureLockers setsDsyncLockers // Distributed lock owner (constant per running instance). erasureLockOwner string // List of endpoints provided on the command line. endpoints PoolEndpoints // String version of all the endpoints, an optimization // to avoid url.String() conversion taking CPU on // large disk setups. endpointStrings []string // Total number of sets and the number of disks per set. setCount, setDriveCount int defaultParityCount int poolIndex int // Distribution algorithm of choice. distributionAlgo string deploymentID [16]byte lastConnectDisksOpTime time.Time } erasureObjects // erasureObjects - Implements ER object layer. // 表示一个erasure Set type erasureObjects struct { setDriveCount int defaultParityCount int setIndex int poolIndex int // getDisks returns list of storageAPIs. getDisks func() []StorageAPI // getLockers returns list of remote and local lockers. getLockers func() ([]dsync.NetLocker, string) // getEndpoints returns list of endpoint belonging this set. // some may be local and some remote. getEndpoints func() []Endpoint // getEndpoints returns list of endpoint strings belonging this set. // some may be local and some remote. getEndpointStrings func() []string // Locker mutex map. nsMutex *nsLockMap } ","date":"2025-01-22","objectID":"/posts/minio-get-started/:6:1","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"StorageAPI StorageAPI主要有两个实现 xlStorage表示本地存储 storageRESTClient表示远程主机上的存储 // Depending on the disk type network or local, initialize storage API. func newStorageAPI(endpoint Endpoint, opts storageOpts) (storage StorageAPI, err error) { if endpoint.IsLocal { storage, err := newXLStorage(endpoint, opts.cleanUp) if err != nil { return nil, err } return newXLStorageDiskIDCheck(storage, opts.healthCheck), nil } return newStorageRESTClient(endpoint, opts.healthCheck, globalGrid.Load()) } newXLStorage函数调用了getDiskInfo函数，并要求路径不能在rootDrive上。判断磁盘是否支持O_DIRECT，在分布式部署下，如果不支持O_DIRECT，则直接报错。 // Return an error if ODirect is not supported. Single disk will have // oDirect off. // 在类似unix的平台上 disk.ODirectPlatform应该为true if globalIsErasureSD || !disk.ODirectPlatform { s.oDirect = false } else if err := s.checkODirectDiskSupport(info.FSType); err == nil { s.oDirect = true } else { return s, err } // getDiskInfo returns given disk information. func getDiskInfo(drivePath string) (di disk.Info, rootDrive bool, err error) { if err = checkPathLength(drivePath); err == nil { di, err = disk.GetInfo(drivePath, false) if !globalIsCICD \u0026\u0026 !globalIsErasureSD { if globalRootDiskThreshold \u003e 0 { // Use MINIO_ROOTDISK_THRESHOLD_SIZE to figure out if // this disk is a root disk. treat those disks with // size less than or equal to the threshold as rootDrives. rootDrive = di.Total \u003c= globalRootDiskThreshold } else { rootDrive, err = disk.IsRootDisk(drivePath, SlashSeparator) } } } // StorageAPI interface. // 对应磁盘 type StorageAPI interface { // Stringified version of disk. String() string // Storage operations. // Returns true if disk is online and its valid i.e valid format.json. // This has nothing to do with if the drive is hung or not responding. // For that individual storage API calls will fail properly. The purpose // of this function is to know if the \"drive\" has \"format.json\" or not // if it has a \"format.json\" then is it correct \"format.json\" or not. IsOnline() bool // Returns the last time this disk (re)-connected LastConn() time.Time // Indicates if disk is local or not. IsLocal() bool // Returns hostname if disk is remote. Hostname() string // Returns the entire endpoint. Endpoint() Endpoint // Close the disk, mark it purposefully closed, only implemented for remote disks. Close() error // Returns the unique 'uuid' of this disk. GetDiskID() (string, error) // Set a unique 'uuid' for this disk, only used when // disk is replaced and formatted. SetDiskID(id string) // Returns healing information for a newly replaced disk, // returns 'nil' once healing is complete or if the disk // has never been replaced. Healing() *healingTracker DiskInfo(ctx context.Context, opts DiskInfoOptions) (info DiskInfo, err error) NSScanner(ctx context.Context, cache dataUsageCache, updates chan\u003c- dataUsageEntry, scanMode madmin.HealScanMode, shouldSleep func() bool) (dataUsageCache, error) // Volume operations. MakeVol(ctx context.Context, volume string) (err error) MakeVolBulk(ctx context.Context, volumes ...string) (err error) ListVols(ctx context.Context) (vols []VolInfo, err error) StatVol(ctx context.Context, volume string) (vol VolInfo, err error) DeleteVol(ctx context.Context, volume string, forceDelete bool) (err error) // WalkDir will walk a directory on disk and return a metacache stream on wr. WalkDir(ctx context.Context, opts WalkDirOptions, wr io.Writer) error // Metadata operations DeleteVersion(ctx context.Context, volume, path string, fi FileInfo, forceDelMarker bool, opts DeleteOptions) error DeleteVersions(ctx context.Context, volume string, versions []FileInfoVersions, opts DeleteOptions) []error DeleteBulk(ctx context.Context, volume string, paths ...string) error WriteMetadata(ctx context.Context, origvolume, volume, path string, fi FileInfo) error UpdateMetadata(ctx context.Context, volume, path string, fi FileInfo, opts UpdateMetadataOpts) error ReadVersion(ctx context.Context, origvolume, volume, path, versionID string, opts ReadOptions) (FileInfo, err","date":"2025-01-22","objectID":"/posts/minio-get-started/:6:2","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"ObjectLayer 唯一一个实现就是erasureServerPools ObjectLayer 就是 Minio 提供的面向 Object 的接口，而StorageAPI则是具体的本地或者远程存储磁盘。 // ObjectLayer implements primitives for object API layer. // 重要接口 type ObjectLayer interface { // Locking operations on object. NewNSLock(bucket string, objects ...string) RWLocker // Storage operations. Shutdown(context.Context) error NSScanner(ctx context.Context, updates chan\u003c- DataUsageInfo, wantCycle uint32, scanMode madmin.HealScanMode) error BackendInfo() madmin.BackendInfo Legacy() bool // Only returns true for deployments which use CRCMOD as its object distribution algorithm. StorageInfo(ctx context.Context, metrics bool) StorageInfo LocalStorageInfo(ctx context.Context, metrics bool) StorageInfo // Bucket operations. MakeBucket(ctx context.Context, bucket string, opts MakeBucketOptions) error GetBucketInfo(ctx context.Context, bucket string, opts BucketOptions) (bucketInfo BucketInfo, err error) ListBuckets(ctx context.Context, opts BucketOptions) (buckets []BucketInfo, err error) DeleteBucket(ctx context.Context, bucket string, opts DeleteBucketOptions) error ListObjects(ctx context.Context, bucket, prefix, marker, delimiter string, maxKeys int) (result ListObjectsInfo, err error) ListObjectsV2(ctx context.Context, bucket, prefix, continuationToken, delimiter string, maxKeys int, fetchOwner bool, startAfter string) (result ListObjectsV2Info, err error) ListObjectVersions(ctx context.Context, bucket, prefix, marker, versionMarker, delimiter string, maxKeys int) (result ListObjectVersionsInfo, err error) // Walk lists all objects including versions, delete markers. Walk(ctx context.Context, bucket, prefix string, results chan\u003c- itemOrErr[ObjectInfo], opts WalkOptions) error // Object operations. // GetObjectNInfo returns a GetObjectReader that satisfies the // ReadCloser interface. The Close method runs any cleanup // functions, so it must always be called after reading till EOF // // IMPORTANTLY, when implementations return err != nil, this // function MUST NOT return a non-nil ReadCloser. GetObjectNInfo(ctx context.Context, bucket, object string, rs *HTTPRangeSpec, h http.Header, opts ObjectOptions) (reader *GetObjectReader, err error) GetObjectInfo(ctx context.Context, bucket, object string, opts ObjectOptions) (objInfo ObjectInfo, err error) PutObject(ctx context.Context, bucket, object string, data *PutObjReader, opts ObjectOptions) (objInfo ObjectInfo, err error) CopyObject(ctx context.Context, srcBucket, srcObject, destBucket, destObject string, srcInfo ObjectInfo, srcOpts, dstOpts ObjectOptions) (objInfo ObjectInfo, err error) DeleteObject(ctx context.Context, bucket, object string, opts ObjectOptions) (ObjectInfo, error) DeleteObjects(ctx context.Context, bucket string, objects []ObjectToDelete, opts ObjectOptions) ([]DeletedObject, []error) TransitionObject(ctx context.Context, bucket, object string, opts ObjectOptions) error RestoreTransitionedObject(ctx context.Context, bucket, object string, opts ObjectOptions) error // Multipart operations. ListMultipartUploads(ctx context.Context, bucket, prefix, keyMarker, uploadIDMarker, delimiter string, maxUploads int) (result ListMultipartsInfo, err error) NewMultipartUpload(ctx context.Context, bucket, object string, opts ObjectOptions) (result *NewMultipartUploadResult, err error) CopyObjectPart(ctx context.Context, srcBucket, srcObject, destBucket, destObject string, uploadID string, partID int, startOffset int64, length int64, srcInfo ObjectInfo, srcOpts, dstOpts ObjectOptions) (info PartInfo, err error) PutObjectPart(ctx context.Context, bucket, object, uploadID string, partID int, data *PutObjReader, opts ObjectOptions) (info PartInfo, err error) GetMultipartInfo(ctx context.Context, bucket, object, uploadID string, opts ObjectOptions) (info MultipartInfo, err error) ListObjectParts(ctx context.Context, bucket, object, uploadID string, partNumberMarker int, maxParts int, opts ObjectOptions) (result ListPartsInfo, err error) AbortMultipartUpload(ctx contex","date":"2025-01-22","objectID":"/posts/minio-get-started/:6:3","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"重要常量 const ( // Represents Erasure backend. formatBackendErasure = \"xl\" // Represents Erasure backend - single drive formatBackendErasureSingle = \"xl-single\" // formatErasureV1.Erasure.Version - version '1'. formatErasureVersionV1 = \"1\" // formatErasureV2.Erasure.Version - version '2'. formatErasureVersionV2 = \"2\" // formatErasureV3.Erasure.Version - version '3'. formatErasureVersionV3 = \"3\" // Distribution algorithm used, legacy formatErasureVersionV2DistributionAlgoV1 = \"CRCMOD\" // Distributed algorithm used, with N/2 default parity formatErasureVersionV3DistributionAlgoV2 = \"SIPMOD\" // Distributed algorithm used, with EC:4 default parity formatErasureVersionV3DistributionAlgoV3 = \"SIPMOD+PARITY\" ) ","date":"2025-01-22","objectID":"/posts/minio-get-started/:7:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"s3 API: PutObject // PutObject // http处理函数 router.Methods(http.MethodPut).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.PutObjectHandler, traceHdrsS3HFlag)) // objectLayer层接口 putObject = objectAPI.PutObject // Validate storage class metadata if present // x-amz-storage-class minio 只支持 REDUCED_REDUNDANCY 和 Standard if sc := r.Header.Get(xhttp.AmzStorageClass); sc != \"\" { if !storageclass.IsValid(sc) { writeErrorResponse(ctx, w, errorCodes.ToAPIErr(ErrInvalidStorageClass), r.URL) return } } // maximum Upload size for objects in a single operation // 5TiB if isMaxObjectSize(size) { writeErrorResponse(ctx, w, errorCodes.ToAPIErr(ErrEntityTooLarge), r.URL) return } objInfo, err := putObject(ctx, bucket, object, pReader, opts) 同一个对象对应到的erasure set总是同一个，这是通过确定性的 hash 算法得到的，所以 server pool 不能被修改，否则 hash 映射关系可能发生变化。 // Returns always a same erasure coded set for a given input. func (s *erasureSets) getHashedSetIndex(input string) int { // 通过hash得到对应erasure set，以下要素可能影响hash结果 return hashKey(s.distributionAlgo, input, len(s.sets), s.deploymentID) } // PutObject - creates an object upon reading from the input stream // until EOF, erasure codes the data across all disk and additionally // writes `xl.meta` which carries the necessary metadata for future // object operations. func (er erasureObjects) PutObject(ctx context.Context, bucket string, object string, data *PutObjReader, opts ObjectOptions) (objInfo ObjectInfo, err error) { return er.putObject(ctx, bucket, object, data, opts) } ","date":"2025-01-22","objectID":"/posts/minio-get-started/:8:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"block 和 shard block （块） blockSize 代表原始数据在存储时被切分的最小单位。 在 MinIO 中，数据在存储前被分割成多个 block。 这些 block 经过 纠删码（Erasure Coding） 计算后，生成 数据块（data blocks） 和 校验块（parity blocks）。 Shard (分片) shard 是 MinIO 存储在磁盘上的物理单位，包含 数据块 和 校验块。 在 N+M 纠删码（N 个数据块 + M 个校验块）中，每个 shard 对应 一个数据块或一个校验块。 例如， EC: 4+2 （4 个数据块 + 2 个校验块）表示： 数据被分成 4 个 block。 计算出 2 个额外的 parity block（用于恢复数据）。 最终存储 6 个 shard，每个 shard 分别存放在不同的磁盘上。 假设数据为 300MiB，blocksize 为 10MiB, 遵循 EC: 4 + 2, shardsize = ceil(10MiB / 4) =2.5MiB，最终每个 blocksize 存储在磁盘上为 6 个 shard，4 个 data shard，6 个 parity shard ","date":"2025-01-22","objectID":"/posts/minio-get-started/:8:1","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"putObject 的主要流程 创建临时目录，写入分片数据 如果没有加锁，获取名字空间锁，实现原子操作，避免数据竞争 rename 操作，包含将分片移动到目标目录以及写入 xl.meta元数据 最后好像有提交操作，没有看懂 ","date":"2025-01-22","objectID":"/posts/minio-get-started/:8:2","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"名字空间锁的实现原理 （TODO) // func (er erasureObjects) putObject if !opts.NoLock { lk := er.NewNSLock(bucket, object) lkctx, err := lk.GetLock(ctx, globalOperationTimeout) if err != nil { return ObjectInfo{}, err } ctx = lkctx.Context() defer lk.Unlock(lkctx) } erasureObjects中有两个关键的字段getLockers和nsMutex用于名字空间加锁。 type erasureObjects struct { // getLockers returns list of remote and local lockers. getLockers func() ([]dsync.NetLocker, string) // Locker mutex map. nsMutex *nsLockMap } type erasureSets struct { sets []*erasureObjects // Distributed locker clients. erasureLockers setsDsyncLockers // Distributed lock owner (constant per running instance). erasureLockOwner string // setsDsyncLockers is encapsulated type for Close() type setsDsyncLockers [][]dsync.NetLocker func (s *erasureSets) GetLockers(setIndex int) func() ([]dsync.NetLocker, string) { return func() ([]dsync.NetLocker, string) { lockers := make([]dsync.NetLocker, len(s.erasureLockers[setIndex])) copy(lockers, s.erasureLockers[setIndex]) // erasureLockerOwner实际上是globalLocalNodeName // The name of this local node, fetched from arguments // globalLocalNodeName string return lockers, s.erasureLockOwner } } // nsLockMap - namespace lock map, provides primitives to Lock, // Unlock, RLock and RUnlock. type nsLockMap struct { // Indicates if namespace is part of a distributed setup. isDistErasure bool lockMap map[string]*nsLock lockMapMutex sync.Mutex } // newNSLock - return a new name space lock map. func newNSLock(isDistErasure bool) *nsLockMap { nsMutex := nsLockMap{ isDistErasure: isDistErasure, } if isDistErasure { return \u0026nsMutex } nsMutex.lockMap = make(map[string]*nsLock) return \u0026nsMutex } ","date":"2025-01-22","objectID":"/posts/minio-get-started/:9:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"什么是 dsync？ dsync 是 MinIO 实现的分布式锁（distributed locking）库，用于在多节点环境下进行同步锁定，确保数据一致性。 主要作用： 在 MinIO 集群 中，确保多个 MinIO 服务器节点在 并发访问同一资源 时，正确管理读/写锁。 提供 类似 sync.Mutex 和 sync.RWMutex 的分布式版本，但适用于分布式系统，而不是单机环境。 避免数据竞争和不一致性，保证多个 MinIO 服务器不会发生并发冲突。 NetLocker 接口 你提供的 NetLocker 接口定义了一种分布式锁管理机制，与 dsync 兼容，核心方法包括： Lock() / Unlock() —— 写锁 RLock() / RUnlock() —— 读锁 Refresh() —— 续约锁，防止锁过期 ForceUnlock() —— 强制解锁 IsOnline() / IsLocal() —— 检查锁服务是否在线，本地还是远程 String() / Close() —— 返回锁的标识 \u0026 关闭连接 这套机制允许 MinIO 在多个服务器节点间进行分布式锁管理，确保一致性。 ","date":"2025-01-22","objectID":"/posts/minio-get-started/:9:1","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"dsync 是如何工作的？ dsync 采用基于 n/2+1 多数决机制的分布式锁，适用于 MinIO 分布式对象存储集群。 核心特点： 分布式锁（类似 sync.Mutex） Lock() / Unlock() 实现互斥锁，确保多个 MinIO 节点不会同时写入相同数据。 RLock() / RUnlock() 允许多个读取者并发访问，但不能同时有写入者。 基于 Raft 的一致性算法 不存储锁的持久化状态，而是采用 n/2+1 机制 如果大多数（n/2+1）MinIO 节点同意加锁，则锁成功。 如果未达到多数决（如部分节点宕机），加锁失败，防止数据不一致。 这类似于 Paxos/Raft 选举机制，保证数据一致性。 超时 \u0026 续约（避免死锁） 锁会自动超时，防止死锁问题。 Refresh() 允许持有锁的进程 续约，防止锁过期被其他进程获取。 支持本地 \u0026 远程锁 单机模式：类似 sync.Mutex，锁是本地的。 分布式模式（dsync）：锁请求会被发送到多个 MinIO 服务器，确保整个集群同步加锁。 MinIO 为什么需要 dsync？ 在 MinIO 分布式对象存储 中，多个节点可能同时操作同一个对象（如 PUT/DELETE 操作）。 如果没有锁，可能会出现 数据覆盖、损坏或不一致 的问题。 使用 dsync 进行分布式锁管理，MinIO 解决了这些问题： 确保多个节点不会同时写入同一对象，防止数据损坏。 允许多个节点同时读取数据，提高并发性能。 防止死锁 \u0026 允许锁续约，确保锁不会永久占用资源。 ","date":"2025-01-22","objectID":"/posts/minio-get-started/:9:2","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"总结 dsync 是 MinIO 的分布式锁库，用于多节点同步，确保一致性。 采用 n/2+1 多数决机制，防止数据竞争 \u0026 保证锁安全。 提供 读/写锁、强制解锁、锁续约等功能，适用于高并发场景。 MinIO 通过 dsync 确保多个服务器不会并发写入相同对象，保证数据一致性。 ","date":"2025-01-22","objectID":"/posts/minio-get-started/:9:3","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"O_DIRECT 的实际用途 ","date":"2025-01-22","objectID":"/posts/minio-get-started/:10:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"s3 API: GetObject // GetObject router.Methods(http.MethodGet).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.GetObjectHandler, traceHdrsS3HFlag)) 首先加分布式读锁 通过读取xl.meta获取对象的元数据信息，xl.meta保存了part和verison的全部信息，注意可能存在某些磁盘上的xl.meta由于故障而修改落后，所以依然需要读取法定人数的磁盘，从而确定实际的元数据 如果 http 请求通过part或者range要求读取部分数据，最终都会转换成对多个 part 的读取，每个 part 都会划分成不同的block进行操作。 ","date":"2025-01-22","objectID":"/posts/minio-get-started/:11:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"纠删码的基本原理 https://p0kt65jtu2p.feishu.cn/docx/LZ36dMN3LoZCuUxFadccNOXGnKb 假设将数据分成 4 块，采用 EC:2 冗余比例，可以将原来的数据组合成一个输入矩阵 P = [][]byte， 第一维表示不同的数据块，第二维表示数据块的数据，所以这里的 P 的大小为 4 * n，n 为每个数据块的大小 编码矩阵 E 的大小为 6 * 4，要求 编码矩阵的前 4 行组成的矩阵为单位矩阵，保持原来数据块数据不变，后两行用来生成冗余数据。 E * P = C （C 表示生成的数据块和冗余块） 假设有两行数据不慎丢失，此时去掉那两行对应的数据后依然有关系 $E’ * P = C’$ 成立，此时通过求逆可以得到原先的 P，也就从数据丢失中恢复了原来的数据。 ","date":"2025-01-22","objectID":"/posts/minio-get-started/:12:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"分片上传和断点续传 分片下载可以通过前面说过的 http 请求中的range或者partnumber实现。 主要涉及的 s3 API（客户端）: InitiateMultipartUpload UploadPart AbortMultipartUpload CompleteMultipartUpload 在 minio 的客户端代码中实现了分片上传，并且支持并发上传 // PutObject creates an object in a bucket. // // You must have WRITE permissions on a bucket to create an object. // // - For size smaller than 16MiB PutObject automatically does a // single atomic PUT operation. // // - For size larger than 16MiB PutObject automatically does a // multipart upload operation. // // - For size input as -1 PutObject does a multipart Put operation // until input stream reaches EOF. Maximum object size that can // be uploaded through this operation will be 5TiB. // // WARNING: Passing down '-1' will use memory and these cannot // be reused for best outcomes for PutObject(), pass the size always. // // NOTE: Upon errors during upload multipart operation is entirely aborted. func (c *Client) PutObject(ctx context.Context, bucketName, objectName string, reader io.Reader, objectSize int64, opts PutObjectOptions, ) // putObjectMultipartStreamParallel uploads opts.NumThreads parts in parallel. // This is expected to take opts.PartSize * opts.NumThreads * (GOGC / 100) bytes of buffer. func (c *Client) putObjectMultipartStreamParallel(ctx context.Context, bucketName, objectName string, reader io.Reader, opts PutObjectOptions, ) (info UploadInfo, err error) { // PutObjectPart router.Methods(http.MethodPut).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.PutObjectPartHandler, traceHdrsS3HFlag)). Queries(\"partNumber\", \"{partNumber:.*}\", \"uploadId\", \"{uploadId:.*}\") // ListObjectParts router.Methods(http.MethodGet).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.ListObjectPartsHandler)). Queries(\"uploadId\", \"{uploadId:.*}\") // CompleteMultipartUpload router.Methods(http.MethodPost).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.CompleteMultipartUploadHandler)). Queries(\"uploadId\", \"{uploadId:.*}\") // NewMultipartUpload router.Methods(http.MethodPost).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.NewMultipartUploadHandler)). Queries(\"uploads\", \"\") // AbortMultipartUpload router.Methods(http.MethodDelete).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.AbortMultipartUploadHandler)). Queries(\"uploadId\", \"{uploadId:.*}\") NewMultipartUpload 生成 uuid 作为 uploadId 将元数据写入 .minio.sys/multipart uploadId 路径下 PutObjectPart 类似于 PutObejct CompleteMultipartUpload 并没有合并 part，仍然保留每个 part ","date":"2025-01-22","objectID":"/posts/minio-get-started/:13:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":[],"content":"联系方式 github: https://github.com/ShadowUnderMoon ","date":"2025-01-17","objectID":"/about/:0:0","tags":[],"title":"About","uri":"/about/"}]