[{"categories":["Spark"],"content":"运行在不同stage、不同节点上的task见通过shuffle机制传递数据，shuffle解决的问题是如何将数据进行重新组织，使其能够在上游和下游task之间进行传递和计算。如果只是单纯的数据传递，则只需要将数据进行分区、通过网络传输即可，没有太大的难度，但shuffle机制还需要进行各种类型的计算（如聚合、排序），而且数据量一般会很大，如果支持这些不同类型的计算，如果提高shuffle的性能都是shuffle机制设计的难点。 shuffle机制分为shuffle write和shuffle read两个阶段，前者主要解决上游stage输出数据的分区问题，后者主要解决下游stage从上游stage获取数据、重新组织、并为后续操作提供数据的问题。 在shuffle过程中，我们将上游stage称为map stage，下游stage称为reduce stage，相应地，map stage包含多个map task，reduce stage包含多个reduce task。 分区个数和下游stage的task个数一致，分区个数可以通过用户自定义，如groupByKey(numPartitions)中的numPartitions一般被定义为集群中可用cpu个数的1~2倍，即将每个map task的输出数据划分为numPartitions份，相应地，在reduce stage中启动numPartition个task来获取并处理这些数据。如果用户没有自定义，则默认分区个数是parent RDD的分区个数的最大值。 对map task输出的每一个\u003cK, V\u003e recod，根据Key计算其partitionId，具有不同partitionId的record被输出到不同的分区（文件）中。 数据聚合的本质是将相同key的record放在一起，并进行必要的计算，这个过程可以利用HashMap实现。方法是使用两步聚合（two-phase aggregation），先将不同tasks获取到的\u003cK, V\u003e record存放到HashMap中，HashMap中的Key是K, Value是list(V)。然后，对于HashMap中每一个\u003cK, list(V)\u003e record，使用func计算得到\u003cK, func(list(v))\u003e record。两步聚合方案的优点是可以解决数据聚合问题，逻辑清晰、容易实现，缺点是所有shuffle的record都会先被存放到HashMap中，占用内存空间较大。另外，对于包含聚合函数的操作，如reduceByKey(func)，需要先将数据聚合到HashMap中以后再执行func()聚合函数，效率较低。 对于reduceByKey(func)等包含聚合函数的操作来说，我们可以采用一种在线聚合（Online aggregation）的方法来减少内存空间占用。该方法在每个record加入HashMap时，同时进行func()聚合操作，并更新相应的聚合结果。具体地，对于每一个新来的\u003cK, V\u003e record，首先从HashMap中get出已经存在的结果V’ = HashMap.get(K)，然后执行聚合函数得到新的中间结果V’’ = func(V, V’)，最后将V’‘写入HashMap中，即HashMap.put(K, V’’)。一般来说，聚合函数的执行结果会小于原始数据规模，即size(func(list(V))) \u003c Size(list(V))，如sum(), max()等，所以在线聚合可以减少内存消耗。在线聚合将Shuffle Read和聚合函数计算耦合在一起，可以加速计算。但是，对于不包含聚合函数的操作，如groupByKey()等，在线聚合和两步聚合没有差别，因为这些操作不包含聚合函数，无法减少中间数据规模。 Shuffle Writer端的combine操作的目的是减少Shuffle的数据量，只有包含聚合函数的数据操作需要进行map段的combine，对于不包含聚合函数的操作，如groupByKey，我们即使进行了combine操作，也不能减少中间数据的规模。从本质上将，combine和Shuffle Read端的聚合过程没有区别，都是将\u003cK, V\u003e record聚合成\u003cK, func(list(V))\u003e，不同的是，Shuffle Read端聚合的是来自所有map task输出的数据，而combine聚合的是来自单一task输出的数据。因此仍然可以采用Shuffle Read端基于HashMap的解决方案。具体地，首先利用HashMap进行combine，然后对HashMap中每一个record进行分区，输出到对应的分区文件中。 如果需要排序，在Shuffle Read端必须必须执行sort，因为从每个task获取的数据组合起来以后不是全局按Key进行排序的。其次，理论上，在Shuffle Write端不需要进行排序，但如果进行了排序，那么Shuffle Read获取到（来自不同task）的数据是已经部分有序的数据，可以减少Shuffle Read端排序的复杂度。 根据排序和聚合的顺序，有三种方案可供选择： 第一种方案是先排序后聚合，这种方案需要先使用线性数据结果如Array，存在Shuffle Read的\u003cK, V\u003e record，然后对Key进行排序，排序后的数据可以直接从前到后进行扫描聚合，不需要再使用HashMap进行hash-based聚合。这种方案也是Hadoop MapReduce采用的方案，方案优点是既可以满足排序要求又可以满足聚合要求，缺点是需要较大内存来存储线性数据结构，同时排序和聚合过程不能同时进行，即不能使用在线聚合，效率较低。 第二种方案是排序和聚合同时进行，我们可以使用带有排序功能的Map，如TreeMap来对中间数据进行聚合，每次Shuffle Read获取到一个record，就将其放入TreeMap中与现有的record进行聚合，过程与HashMap类似，只有TreeMap自带排序功能。这种方案的优点是排序和聚合可以同时进行，缺点是相比HashMap，TreeMap的排序复杂度较高，TreeMap的插入时间复杂度为O(nlogn)，而且需要不断调整树的结果，不适合数据规模非常大的情况。 第三种方案是先聚合再排序，即维持现有基于HashMap的聚合方案不变，将HashMap中的record或record的引用放入线性数据结构中就行排序。这种方案的优点是聚合和排序过程独立，灵活性较高，而且之间的在线聚合方案不需要改动，缺点是需要复制（copy）数据或者引用，空间占用较大，Spark选择的是第三种方案，设计了特殊的HashMap来高效完成先聚合再排序的任务。 由于我们使用HashMap对数据进行combine和聚合，在数据量大的时候，会出现内存溢出，这个问题既可能出现在Shuffle Write阶段，也可能出现在Shuffle Read阶段。通过使用内存+磁盘混合存储来解决这个问题（吐磁盘），先在内存（如HashMap）中进行数据聚合，如果内存空间不足，则将内存中的数据spill到磁盘上，此时空闲出来的内存可以继续处理新的数据。此过程可以不断重复，直到数据处理完成。然而，问题是spill到磁盘上的数据实际上是部分聚合的结果，并没有和后续的数据进行过聚合。因此，为了得到完成的聚合结果，我们需要再进行下一步数据操作之前对磁盘上和内存中的数据进行再次聚合，这个过程我们称为全局聚合，为了加速全局聚合，我们需要将数据spill到磁盘上时进行排序，这样全局聚合才能够按照顺序读取spill到磁盘上的数据，并减少磁盘I/O。 ","date":"2025-06-01","objectID":"/posts/shuffle/:0:0","tags":["Spark"],"title":"Shuffle","uri":"/posts/shuffle/"},{"categories":["Spark"],"content":"Spark中Shuffle框架的设计 ","date":"2025-06-01","objectID":"/posts/shuffle/:1:0","tags":["Spark"],"title":"Shuffle","uri":"/posts/shuffle/"},{"categories":["Spark"],"content":"Shuffle Write框架设计和实现 在Shuffle Write阶段，数据操作需要分区、聚合和排序3个功能，Spark为了支持所有可能的情况，设计了一个通用的Shuffle Write框架，框架的计算顺序为map()输出 --\u003e 数据聚合 --\u003e 排序 --\u003e 分区输出。map task每计算出一个record及其partitionId，就将record放入类似HashMap的数据结构中进行聚合，聚合完成后，再将HashMap中的数据放入类似Array的数据结构中进行排序，即可按照partitionId，也可以按照partitionId+Key进行排序，最后根据partitionId将数据写入不同的数据分区中，存放到本地磁盘上。其中聚合和排序过程是可选的，如果数据操作不需要聚合或者排序，那么可以去掉相应的聚合或排序过程。 不需要map端聚合和排序 map依次输出\u003cK, V\u003e record并计算其partititionId, Spark根据partitionId，将record依次输出到不同的buffer中，每当buffer填满就将record溢写到磁盘中的分区文件中。分配buffer的原因是map输出record的速度很快，需要进行缓冲来减少磁盘I/O。在实现代码中，Spark将这种Shuffle Write的方式称为BypassMergeSortShuffleWriter，即不需要进行排序的Shuffle Write方式。 该模式的优缺点：优点是速度快，直接将record输出到不同的分区文件中。缺点是资源消耗过高，每个分区都需要一个buffer（大小由spark.Shuffle.file.buffer控制，默认为32KB），且同时需要建立多个分区文件进行溢写。当分区个数太大，如10000，每个map task需要月320MB的内存，会造成内存消耗过大，而且每个task需要同时建立和打开10000个文件，造成资源不足，因此，该shuffle方案适合分区个数较少的情况（\u003c 200）。 该模式适用的操作类型：map端不需要聚合，key不需要排序且分区个数较少（\u003c=spark.Shuffle.sort.bypassMergeThreshold，默认值为200），例如 groupByKey(100)、partitionBy(100)、sortByKey(100)等，注意sortByKey是在Shuffle Rread端进行排序。 不需要map端聚合，但需要排序 在这种情况下需要按照partitionId+Key进行排序。Spark采用的实现方法是建立一个Array来存放map输出的record，并对Array中元素的Key进行精心设计，将每个\u003cK, V\u003e record转化为\u003c(PID, K), V\u003e record存储然后按照partitionId + Key对record进行排序，最后将所有record写入一个文件中，通过建立索引来标示每个分区。 如果Array存放不下，则会先扩容，如果还存放不下，就将Array中的record排序后spill到磁盘上，等待map输出完以后，再将Array中的record与磁盘上已排序的record进行全局排序，得到最终有序的record，并写入文件中。 该Shuffle模式被命名为SortShuffleWriter(KeyOrdering=true)，使用的Array被命名为PartitionedPairBuffer。 该Shuffle模式的优缺点：优点是只需要一个Array结构就可以支持按照partitionId+Key进行排序，Array大小可控，而且具有扩容和spill到磁盘的功能，支持从小规模到大规模数据的排序。同时，输出的数据已经按照partitionId进行排序，因此只需要一个分区文件存储，即可标示不同的分区数据，克服了ByPassMergeSortShuffleWriter中建立文件数过多的问题，适用于分区个数很大的情况，缺点是排序增加计算时延。 该Shuffle模式适用的操作：map端不需要聚合、Key需要排序、分区个数无限制。目前，Spark本身没有提供这种排序类型的数据操作，但不排除用户会自定义，或者系统未来会提供这种类型的操作。sortByKey操作虽然需要按Key进行排序，但这个排序过程在Shuffle Read端完成即可，不需要在Shuffle Write端进行排序。 SortShuffleWriter可以解决BypassMergeSortShuffleWriter模式的缺点，而BypassMergeSortShuffleWriter面向的操作不需要按照Key进行排序。因此，我们只需要将“按PartitionId+key”排序改成“只按PartitionId排序”，就可以支持不需要map端combine、不需要按照key进行排序、分区个数过大的操作，例如，groupByKey(300), partitionBy(300), sortByKey(300)。 需要map段聚合，需要或者不需要按照key进行排序 Spark采用的实现方法是建立一个类似HashMap的数据结构对map输出的record进行聚合。HashMap中的Key是partitionId+Key，HashMap中的Value是经过combine的聚合结果。聚合完成后，Spark对HashMap中的record进行排序，最后将排序后的record写入一个分区文件中。 该Shuffle模式的优缺点：优点是只需要一个HashMap结构就可以支持map端的combine功能，HashMap具有扩容和spill到磁盘的功能，支持小规模到大规模数据的聚合，也适用于分区个数很大的情况。在聚合后使用Array排序，可以灵活支持不同的排序需求。缺点是在内存中进行聚合，内存消耗较大，需要额外的数组进行排序，而且如果有数据spill到磁盘上，还需要再次进行聚合。在实现中，Spark在Shuffle Write端使用一个经过特殊设计和优化的HashMap，命名为PartitionedAppendOnlylMap，可以同时支持聚合和排序操作。相当于HashMap和Array的合体。 该Shuffle模式适用的操作：适合map端聚合、需要或者不需要按照Key进行排序，分区个数无限制的应用，如reduceByKey、aggregateByKey等。 ","date":"2025-06-01","objectID":"/posts/shuffle/:1:1","tags":["Spark"],"title":"Shuffle","uri":"/posts/shuffle/"},{"categories":["Spark"],"content":"Shuffle Read框架设计和实现 在Shuffle Read阶段，数据操作需要3个功能：跨节点数据获取、聚合和排序。Spark为了支持所有的情况，设计了一个通用的Shuffle Read框架，框架的计算顺序为数据获取–\u003e 聚合 —\u003e 排序输出。 不需要聚合，不需要按照Key进行排序 这种情况最简单，只需要实现数据获取功能即可。等待所有的map task结束后, reduce task开始不断从各个map task获取\u003cK, V\u003e record，并将record输出到一个buffer中（大小为spark.reducer.maxSizeInFlight=48MB），下一个操作直接从buffer获取即可。 该Shuffle模式的优缺点：优点是逻辑和实现简单，内存消耗很小。缺点是不支持聚合、排序等复杂功能。 该Shuffle模式适用的操作：适合既不需要聚合也不需要排序的应用，如partitionBy等。 不需要聚合，需要按Key进行排序 获取数据后，将buffer中的record依次输出到一个Array结构（PartitionedPairBuffer）中。由于这里采用了本来用于Shuffle Write端的PartitionedPairBuffer结构，所以还保留了每个record的partitionId。然后，对Array中的record按照Key进行排序，并将排序结果输出或者传递给下一步操作。当内存无法存在所有的record时，PartitionedPairBuffer将record排序后spill到磁盘上，最后将内存中和磁盘上的record进行全局排序，得到最终排序后的record。 该Shuffle模式的优缺点：优点是只需要一个Array结构就可以支持按照Key进行排序，Array大小可控，而且具有扩容和spill到磁盘的功能，不受数据规模限制。缺点是排序增加计算时延。 该Shuffle模式适用的操作：适合reduce端不需要聚合，但需要按照Key进行排序的操作，如sortByKey，sortBy等。 需要聚合，不需要或者需要按Key进行排序 获取record后，Spark建立一个类似HashMap的数据结构（ExternalAppendOnlyMap）对buffer中的record进行聚合，HashMap中的Key是record中的Key，HashMap中的Value是经过聚合函数计算后的结果。之后如果需要按照Key进行排序，则建立一个Array结构，读取HashMap中的record，并对record按Key进行排序，排序完成后，将结果输出或者传递给下一步操作。 该Shuffle模式的优缺点：优点是只需要一个HashMap和一个Array结构就可以支持reduce端的聚合和排序功能，HashMap具有扩容和spill到磁盘上的功能，支持小规模到大规模数据的聚合，边获取数据边聚合，效率较高。缺点是需要在内存中进行聚合，内存消费较大，如果有数据spill到磁盘上，还需要进行再次聚合。另外，经过HashMap聚合后的数据仍然需要拷贝到Array中进行排序，内存消耗较大。在实现中，Spark使用的HashMap是一个经过特殊优化的HashMap，命名为ExternalAppendOnlyMap，可以同时支持聚合和排序操作，相当于HashMap和Array的合体。 该Shuffle模式适用的操作：适合reduce端需要聚合、不需要或需要按Key进行排序的操作，如reduceByKey、aggregateByKey等。 ","date":"2025-06-01","objectID":"/posts/shuffle/:1:2","tags":["Spark"],"title":"Shuffle","uri":"/posts/shuffle/"},{"categories":["Spark"],"content":"支持高效聚合和排序的数据结构 仔细观察Shuffle Write/Read过程，我们会发现Shuffle机制中使用的数据结构的两个特征： 一是只需要支持record的插入和更新操作，不需要支持删除操作，这样我们可以对数据结构进行优化，减少内存消耗 二是只有内存放不下时才需要spill到磁盘上，因此数据结构设计以内存为主，磁盘为辅 ","date":"2025-06-01","objectID":"/posts/shuffle/:2:0","tags":["Spark"],"title":"Shuffle","uri":"/posts/shuffle/"},{"categories":["Spark"],"content":"AppendOnlyMap AppendOnlyMap实际上是一个只支持record添加和对Value进行更新的HashMap。于Java HashMap采用数组 + 链表的实现不同，AppendOnlyMap只使用数组来存储元素，根据元素的Hash值确定存储位置，如果存储元素时发生Hash值冲突，则使用二次地址探测法（Quadratic probing）来解决Hash值的冲突。 AppendOnlyMap将K, V相邻放在数组中，对于每个新来的\u003cK, V\u003e record，先使用Hash(K)计算其存放位置，如果存放位置为空，就把record存放到该位置，如果该位置已经被占用，则根据二次探测法向后指数递增位置，直到发现空位。查找和更新操作也需要根据上面的流程进行寻址。 扩容：AppendOnlyMap使用数组来实现的问题是，如果插入的record太多，则很快就被填满，Spark的解决方案是，如果AppendOnlyMap的利用率达到70%，那么就扩张一倍，扩张意味着原来的Hash失效，因此对所有Key进行rehash，重新排列每个Key的位置。 排序：由于AppendOnlyMap采用了数组作为底层存储结构，可以支持快速排序等排序算法。实现层面，先将数组中所有的\u003cK, V\u003e record转移到数组的前端，用begin和end来表示起始结束位置，然后调用排序算法对[begin, end]中的record进行排序。对于需要按Key进行排序的操作，如sortByKey，可以按照Key进行排序，对于其他操作，只按照Key的Hash值进行排序即可。 输出：迭代AppendOnlyMap数组中的record，从前往后扫描输出即可。 ","date":"2025-06-01","objectID":"/posts/shuffle/:2:1","tags":["Spark"],"title":"Shuffle","uri":"/posts/shuffle/"},{"categories":["Spark"],"content":"ExternalAppendOnlyMap AppendOnlyMap的优点是能够将聚合和排序功能很好地结合在一起，缺点是只能使用内存，难以适用于内存空间不足的问题。为了解决这个问题，Spark基于AppendOnlyMap设计实现了基于内存+磁盘的ExternalAppendOnlyMap，用于Shuffle Read端大规模数据聚合。同时，由于Shuffle Write端聚合需要考虑partitionId，Spark也设计了带有partitionId的ExternalAppendOnlyMap，名为PartitionedAppendOnlyMap。 ExternalAppendOnlyMap的工作原理是，先持有一个AppendOnlyMap来不断接收和聚合新来的record，AppendOnlyMap快被装满时检查一下内存剩余空间是否可以扩展，可以的话直接在内存中扩展，否则对AppendOnlyMap中的record进行排序，然后将record都spill到磁盘上。因为record不断到来，可能会多次填满AppendOnlyMap，所以这个spill过程可以出现多次形成多个spill文件。等record都处理完，此时AppendOnlyMap中可能还留存一些聚合后的record，磁盘上也有多个spill文件。因为这些数据都经过了部分聚合，还需要进行全局聚合（merge）。因此ExternalAppendOnlyMap的最后一步是将内存中的AppendOnlyMap的数据和磁盘上spill文件中的数据进行全局聚合，得到最终结果。 AppendOnlyMap的大小估计 虽然我们知道AppendOnlyMap中持有的数组的长度和大小，但数组里面存放的是Key和Value的引用，并不是它们的实际对象大小，而且Value会不断被更新，实际大小不断变化。想要准备得到AppendOnlyMap的大小比较困难。一种简单的解决方法是在每次插入record或对现有record的Value进行更新后，都扫描一下AppendOnlyMap中存放的record，计算每个record的实际对象大小并相加，但这样会非常耗时。 Spark设计了一个增量式的高效估算算法，在每个record插入或更新时根据历史统计值和当前变化量直接估算当前AppendOnlyMap的大小，算法的复杂度为O(1)，开销很小，在record插入和聚合过程中会定期对当前AppendOnlyMap中的record进行抽样，然后精确计算这些record的总大小、总个数、更新个数及平均值等，并作为历史统计值。进行抽样是因为AppendOnlyMap中的record可能有上万个，难以对每个都精确计算。之后，每当有record插入或更新时，会根据历史统计值和历史平均的变化值，增量估算AppendOnlyMap的总大小，详见SizeTracker.estimateSize方法。抽样也会定期进行，更新统计值以获取更高的精度。 Spill过程与排序 当AppendOnlyMap达到内存限制时，会将record排序后写入磁盘中。排序是为了方便下一步全局聚合（聚合内存和磁盘上的record）时可以采用更高效的merge-sort（外部排序+聚合）。那么，问题是依据什么对record进行排序？自然想要可以根据record的Key进行排序，但是这就要求操作定义Key的排序方法，如sortByKey等操作定义了按照Key进行的排序。大部分操作，如groupByKey，并没有定义Key的排序方法，也不需要输出结果按照Key进行排序。在这种情况下，Spark采用按照Key的Hash值进行排序的方法，这样既可以进行merge-sort，又不要求操作定义Key排序的方法。然而，这种方法的问题是会出现Hash值冲突，也就是不同的Key具有相同的Hash值。为了解决这个问题，Spark在merge-sort的同时会比较Key的Hash值是否相等，以及Key的实际值是否相等。 全局聚合 由于最终的spill文件和内存中的AppendOnlyMap都是经过部分聚合后的结果，其中可能存在相同Key的record，因此还需要一个全局聚合阶段将AppendOnlyMap中的record与spill文件中的record进行聚合，得到最终聚合后的结果。 全局聚合的方法是建立一个最小堆或者最大堆，每次从各个spill文件中读取前几个具有相同Key（或者相同Key的hash值）的record，然后与AppendOnlyMap中的record进行聚合，并输出聚合后的结果。 ","date":"2025-06-01","objectID":"/posts/shuffle/:2:2","tags":["Spark"],"title":"Shuffle","uri":"/posts/shuffle/"},{"categories":["Spark"],"content":"PartitionedAppendOnlyMap PartitionedAppendOnlyMap用于在Shuffle Write端对record进行聚合（combine）。PartitionedAppendOnlyMap的功能和实现与ExternalAppendOnlyMap的功能和实现基本一样，唯一区别是PartitionedAppendOnlyMap中的Key是PartitionId + Key，这样既可以根据partitionId进行排序（面向不需要按key进行排序的操作），也可以根据partitionId + Key进行排序（面向需要按Key进行排序的操作），从而在Shuffle Write阶段可以进行聚合、排序和分区。 ","date":"2025-06-01","objectID":"/posts/shuffle/:2:3","tags":["Spark"],"title":"Shuffle","uri":"/posts/shuffle/"},{"categories":["Spark"],"content":"PartitionedPairBuffer PartitionedPariBuffer本质上是一个基于内存+磁盘的Array，随着数据添加，不断的扩容，但到达内存限制时，就将Array中的数据按照partitionId或者partitionId+Key进行排序，然后spill到磁盘上，该过程可以进行多次，最后对内存中和磁盘上的数据进行全局排序，输出或者提供给下一个操作。 ","date":"2025-06-01","objectID":"/posts/shuffle/:2:4","tags":["Spark"],"title":"Shuffle","uri":"/posts/shuffle/"},{"categories":null,"content":"这篇是对javadoop对concurrentHashMap非常棒的源码解析的学习。 ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:0:0","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"Java7 HashMap HashMap是一个非并发安全的hashmap，使用链表数组实现，逻辑比较简单。 要求容量始终为$2^n$，这样可以利用位运算计算下标，index = hash \u0026 (length - 1) 每次扩容为原先的2倍，这样迁移旧数据时，会将位置table[i]中的链表的所有节点，分拆到新的数组中的newTable[i]和newTable[i + oldLenght]位置。 ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:1:0","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"Java7 ConcurrentHashMap ConcurrentHashMap由一个Segment数组实现，Segment通过继承ReentrantLock来进行加锁，所以每次需要加锁的操作锁住的就是一个Segment，有些地方将Segment称为分段锁，这样只要保证每个Segment都是线程安全的，就实现了全局的线程安全。 concurrencyLevel: 默认为16，也就是说ConcurrentHashMap有16个Segment，所以理论上，最多可以同时支持16个线程并发写，只要它们的操作分别分布在不同的Segment上，这个值可以在初始化的时候设置为其他值，但是一旦初始化后，它是不可以扩容的。 假设concurrentcyLevel为16，那么hash值的高4位被用于找到对应的Segment。 Segment内部是有数组+链表组成的 put操作需要对Segment加独占锁，内部操作类似于HashMap get操作完全没有加锁，完全由代码实现保证不会发生问题。 ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:2:0","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"Java8 HashMap Java8对HashMap进行了一些修改，引入了红黑树，我们可以通过hash快速定位到数组中的具体下表，但之后需要遍历整个链表寻找我们需要的键值对，时间复杂度取决于链表的长度，为O(n)。为了降低这部分的开销，在java8中，当链表中的元素达到了8个时，会将链表转换为红黑树，此时查找的时间复杂度为 O(logn)。 Java7中使用Entry来代表每个HashMap的数据节点，Java8中使用Node，基本没有区别，都是key, value, hash和next这四个属性，不过，Node只能用于链表的情况，红黑树的情况需要使用TreeNode。 ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:3:0","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"Java8 ConcurrentHashMap /** * The array of bins. Lazily initialized upon first insertion. * Size is always a power of two. Accessed directly by iterators. */ transient volatile Node\u003cK,V\u003e[] table; /** * The next table to use; non-null only while resizing. */ // 迁移时使用的临时数组 private transient volatile Node\u003cK,V\u003e[] nextTable; ConcurrentHashMap底层也是一个数组，每个元素要么是链表，要么是红黑树。 ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:0","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"spread函数 static final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash /** * Spreads (XORs) higher bits of hash to lower and also forces top * bit to 0. Because the table uses power-of-two masking, sets of * hashes that vary only in bits above the current mask will * always collide. (Among known examples are sets of Float keys * holding consecutive whole numbers in small tables.) So we * apply a transform that spreads the impact of higher bits * downward. There is a tradeoff between speed, utility, and * quality of bit-spreading. Because many common sets of hashes * are already reasonably distributed (so don't benefit from * spreading), and because we use trees to handle large sets of * collisions in bins, we just XOR some shifted bits in the * cheapest possible way to reduce systematic lossage, as well as * to incorporate impact of the highest bits that would otherwise * never be used in index calculations because of table bounds. */ static final int spread(int h) { return (h ^ (h \u003e\u003e\u003e 16)) \u0026 HASH_BITS; } spread函数将原来的hash值进行处理，获取新的hash值，尽量避免hash碰撞。 ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:1","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"put过程分析 public V put(K key, V value) { return putVal(key, value, false); } final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); // 得到hash值 int hash = spread(key.hashCode()); // 记录相应链表的长度 int binCount = 0; for (Node\u003cK,V\u003e[] tab = table;;) { Node\u003cK,V\u003e f; int n, i, fh; // 如果数组为空，进行数组初始化 if (tab == null || (n = tab.length) == 0) tab = initTable(); // 查找该hash对应的数组位置处的元素 else if ((f = tabAt(tab, i = (n - 1) \u0026 hash)) == null) { // 如果数组该位置为空，用一次cas操作将新值放入其中，如果cas失败，进入下一个循环 if (casTabAt(tab, i, null, new Node\u003cK,V\u003e(hash, key, value, null))) break; // no lock when adding to empty bin } // 当前位置已经扩容完成，MOVED用于标记扩容 // helpTransfer之后会进入下一轮循环 // 这里也能看出，put操作如果遇到对应的hash桶已经被迁移，那么不得已，当前线程需要协助transfer，直到整个table迁移完成 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { // f是该位置的头节点，而且不为空 V oldVal = null; // 获取数组该位置头结点的监视器锁 synchronized (f) { // 获取锁之后重新判断一下当前位置的节点是否已经改变，如果已经改变，进入下一个循环 // 当迁移完成时，头节点改成ForwardingNode，判断失败，会进入下一轮循环，走MOVED分支 if (tabAt(tab, i) == f) { // 头结点的hash值大于0，说明是链表 if (fh \u003e= 0) { // 用于累加，记录链表的长度 binCount = 1; for (Node\u003cK,V\u003e e = f;; ++binCount) { K ek; // 如果发现了相等的key，判断是否需要进行值覆盖，最后跳出循环 if (e.hash == hash \u0026\u0026 ((ek = e.key) == key || (ek != null \u0026\u0026 key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } // 到了链表的最末端，将这个新值放到链表的最后面 Node\u003cK,V\u003e pred = e; if ((e = e.next) == null) { pred.next = new Node\u003cK,V\u003e(hash, key, value, null); break; } } } else if (f instanceof TreeBin) { // 红黑树 Node\u003cK,V\u003e p; binCount = 2; if ((p = ((TreeBin\u003cK,V\u003e)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) { // 判断是否要将链表转换为红黑树，临界值和HashMap一样，也是8 if (binCount \u003e= TREEIFY_THRESHOLD) // 这个方法和HashMap中稍微有一点点不同，那就是它不是一定会进行红黑树转换 // 如果当前数据的长度小于64，那么会选择进行数组扩容，而不是转换为红黑树 treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } addCount(1L, binCount); return null; } 可以看到，ConcurrentHashMap在进行重要操作时，会对数组中的元素进行加锁，这样保证了加锁的粒度适合，避免过粗导致并发性能下降。 ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:2","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"初始化数组 initTable /** * Initializes table, using the size recorded in sizeCtl. */ private final Node\u003cK,V\u003e[] initTable() { Node\u003cK,V\u003e[] tab; int sc; while ((tab = table) == null || tab.length == 0) { // 其他线程已经在初始化数组了，spin等待 if ((sc = sizeCtl) \u003c 0) Thread.yield(); // lost initialization race; just spin // CAS一下，将sizeCtl设置为-1，表示抢到了锁 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if ((tab = table) == null || tab.length == 0) { // DEFAULT_CAPACITY默认初始容量为16 int n = (sc \u003e 0) ? sc : DEFAULT_CAPACITY; // 初始化数组，长度为16或者初始化时提供的长度 @SuppressWarnings(\"unchecked\") Node\u003cK,V\u003e[] nt = (Node\u003cK,V\u003e[])new Node\u003c?,?\u003e[n]; // 将新的数组赋值给table，table是volatile table = tab = nt; sc = n - (n \u003e\u003e\u003e 2); } } finally { // 设置sizeCtl为sc sizeCtl = sc; } break; } } return tab; } 初始化一个合适大小的数组，然后会设置sizeCtl。 初始化方法中的并发问题是通过对sizeCtl进行一个CAS操作来控制的。 ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:3","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"helpTransfer /** * The maximum number of threads that can help resize. * Must fit in 32 - RESIZE_STAMP_BITS bits. */ private static final int MAX_RESIZERS = (1 \u003c\u003c (32 - RESIZE_STAMP_BITS)) - 1; /** * Helps transfer if a resize is in progress. */ final Node\u003cK,V\u003e[] helpTransfer(Node\u003cK,V\u003e[] tab, Node\u003cK,V\u003e f) { Node\u003cK,V\u003e[] nextTab; int sc; if (tab != null \u0026\u0026 (f instanceof ForwardingNode) \u0026\u0026 (nextTab = ((ForwardingNode\u003cK,V\u003e)f).nextTable) != null) { int rs = resizeStamp(tab.length) \u003c\u003c RESIZE_STAMP_SHIFT; while (nextTab == nextTable \u0026\u0026 table == tab \u0026\u0026 (sc = sizeCtl) \u003c 0) { // 这里有三种情况，不帮忙transfer // 1. 帮助迁移的线程数已经达到上限 // 2. // 3. transferIndex小于0，表示数组已经迁移完成 if (sc == rs + MAX_RESIZERS || sc == rs + 1 || transferIndex \u003c= 0) break; // CAS操作将sizeCtrl加一，成功后进行transfer并跳出循环 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) { transfer(tab, nextTab); break; } } return nextTab; } return table; } ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:4","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"链表转红黑树： treeifyBin /** * Replaces all linked nodes in bin at given index unless table is * too small, in which case resizes instead. */ private final void treeifyBin(Node\u003cK,V\u003e[] tab, int index) { Node\u003cK,V\u003e b; int n, sc; if (tab != null) { // MIN_TREEIFY_CAPACITY为64 // 所以，如果数组长度小于64的时候，其实也就是32或者16或者更小的时候，会进行数组扩容 if ((n = tab.length) \u003c MIN_TREEIFY_CAPACITY) // 触发扩容 tryPresize(n \u003c\u003c 1); // 当前位置是链表 else if ((b = tabAt(tab, index)) != null \u0026\u0026 b.hash \u003e= 0) { // 加锁 synchronized (b) { if (tabAt(tab, index) == b) { // 遍历链表，建立一颗红黑树 TreeNode\u003cK,V\u003e hd = null, tl = null; for (Node\u003cK,V\u003e e = b; e != null; e = e.next) { TreeNode\u003cK,V\u003e p = new TreeNode\u003cK,V\u003e(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; } // 将红黑树设置到数组相应位置 setTabAt(tab, index, new TreeBin\u003cK,V\u003e(hd)); } } } } } ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:5","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"扩容： tryPreSize /** * Tries to presize table to accommodate the given number of elements. * * @param size number of elements (doesn't need to be perfectly accurate) */ private final void tryPresize(int size) { // c: size的1.5倍，再加1，再往上去最近的2的n次方 int c = (size \u003e= (MAXIMUM_CAPACITY \u003e\u003e\u003e 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size \u003e\u003e\u003e 1) + 1); int sc; // resize开始后, sizeCtl为负数，所以如果已经开始resize，这段逻辑会被跳过 while ((sc = sizeCtl) \u003e= 0) { Node\u003cK,V\u003e[] tab = table; int n; // 初始化数组，和之前类似 if (tab == null || (n = tab.length) == 0) { n = (sc \u003e c) ? sc : c; if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if (table == tab) { @SuppressWarnings(\"unchecked\") Node\u003cK,V\u003e[] nt = (Node\u003cK,V\u003e[])new Node\u003c?,?\u003e[n]; table = nt; sc = n - (n \u003e\u003e\u003e 2); } } finally { sizeCtl = sc; } } } // 容量足够，跳出循环 else if (c \u003c= sc || n \u003e= MAXIMUM_CAPACITY) break; else if (tab == table) { int rs = resizeStamp(n); // https://bugs.java.com/bugdatabase/view_bug.do?bug_id=8215409 // sc \u003c 0永远不会成立，所以这段代码不起作用，在jdk11中已经去掉 // 复制粘贴是每位程序员的必备技能 if (sc \u003c 0) { Node\u003cK,V\u003e[] nt; if ((sc \u003e\u003e\u003e RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex \u003c= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); } // 这里为什么要加2，没有看懂 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs \u003c\u003c RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); } } } /** * The maximum number of threads that can help resize. * Must fit in 32 - RESIZE_STAMP_BITS bits. */ // 高16位为0，低16位为1 private static final int MAX_RESIZERS = (1 \u003c\u003c (32 - RESIZE_STAMP_BITS)) - 1; /** * The number of bits used for generation stamp in sizeCtl. * Must be at least 6 for 32bit arrays. */ private static int RESIZE_STAMP_BITS = 16; /** * Returns the stamp bits for resizing a table of size n. * Must be negative when shifted left by RESIZE_STAMP_SHIFT. */ // n为数组长度，所以一定是2^n，n的前导零个数实际上用更少的位数编码了n // 从低位起第16位为1（从1开始计数），这样左移16位后一定是一个负数 // 所以 resizeStamp(int n)的效果是将n进行了重新编码，并且添加了resize戳记 static final int resizeStamp(int n) { return Integer.numberOfLeadingZeros(n) | (1 \u003c\u003c (RESIZE_STAMP_BITS - 1)); } ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:6","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"数据迁移： transfer 将原来的tab数组中的元素迁移到新的nextTab数组中。 之前提到的tryPresize方法中调用transfer不涉及多线程，但transfer方法可以在其他地方被调用。典型地，我们之前在说put方法的时候已经说过了，helpTransfer方法中会调用transfer方法。 此方法支持多线程执行，外围调用此方法时，会保证第一个发起数据迁移的线程，nextTab为null，之后再调用此方法的时候，nextTab不会为null。 原数组长度为n，所有我们有n个迁移任务，让每个线程每次负责一个小任务是最简单的，每做完一个任务再检测是否有其他没做完的任务，帮助迁移旧可以了，而Doug Lea使用了一个stride，简单理解就是步长，每个线程每次负责迁移其中的一部分，如每次迁移16个小任务。所以我们就需要一个全局的调度者来安排哪个线程执行哪几个任务，这个就是属性transferIndex的作用。 第一个发起数据迁移的线程会将transferIndex指向原数组最后的位置，然后从后往前的stride个任务属于第一个线程，然后将transferIndex指向新的位置，再往前的stride个任务属于第二个线程，以此类推，当然，这里说的第二个线程不是真的一定指代了第二个线程，也可以是同一个线程，其实就是将一个大的迁移任务分为一个个任务包。 之前提到，原数组i位置的键值对会被分配到新数组i位置和新数组i + oldLength位置，这样每个迁移小任务相互之前不存在资源竞争。 /** * The next table index (plus one) to split while resizing. */ private transient volatile int transferIndex; /** * Moves and/or copies the nodes in each bin to new table. See * above for explanation. */ private final void transfer(Node\u003cK,V\u003e[] tab, Node\u003cK,V\u003e[] nextTab) { int n = tab.length, stride; // stide在单核下直接等于n，多核模式下为 n \u003e\u003e\u003e 3 / NCPU，最小值为16 if ((stride = (NCPU \u003e 1) ? (n \u003e\u003e\u003e 3) / NCPU : n) \u003c MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range // 如果nextTab为null，先进行一次初始化 // 之前提过，外围会保证第一个发起迁移的线程调用此方法时，参数nextTab为null // 之后参与迁移的线程调用此方法时，nextTab不为null if (nextTab == null) { // initiating try { @SuppressWarnings(\"unchecked\") // 容量翻倍 Node\u003cK,V\u003e[] nt = (Node\u003cK,V\u003e[])new Node\u003c?,?\u003e[n \u003c\u003c 1]; nextTab = nt; } catch (Throwable ex) { // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; } // 赋值给nextTable属性 nextTable = nextTab; // transfer属性用于控制迁移的位置，初始为原先数组的长度 transferIndex = n; } int nextn = nextTab.length; // ForwardingNode翻译过来就是正在迁移的Node // 这个构造方法会生成一个Node, key, value和next都是null，关键是hash为MOVED // 后面我们会看到，原数组中位置i处的节点完成迁移工作后，将会将位置i处设置为这个ForwardingNode，用来告诉其他线程该位置已经处理过了 // 所以它其实相当于一个标志 ForwardingNode\u003cK,V\u003e fwd = new ForwardingNode\u003cK,V\u003e(nextTab); // advance指的是做完了一个位置的迁移工作，可以准备做下一个位置的了 boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab // i是位置索引，bound是边界，注意是从后往前 for (int i = 0, bound = 0;;) { Node\u003cK,V\u003e f; int fh; // advance为true表示可以进行下一个位置的迁移了 // 简单理解结局：i指向了transferIndex, bound指向了transferIndex - stride while (advance) { int nextIndex, nextBound; if (--i \u003e= bound || finishing) advance = false; // 将transferIndex赋值给nextIndex // 这里transferIndex一旦小于等于0，说明原数组的所有位置都有相应的线程去处理了 else if ((nextIndex = transferIndex) \u003c= 0) { i = -1; advance = false; } else if (U.compareAndSetInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex \u003e stride ? nextIndex - stride : 0))) { // nextBound是这次迁移任务的边界，注意是从后往前 bound = nextBound; i = nextIndex - 1; advance = false; } } if (i \u003c 0 || i \u003e= n || i + n \u003e= nextn) { int sc; if (finishing) { // 所有迁移操作都已经完成 nextTable = null; // 将新的nextTab赋值给table属性，完成迁移 table = nextTab; // 重新计算sizeCtrl: n为原数组长度，所以sizeCtrl得出的值将是新数组长度的0.75倍 sizeCtl = (n \u003c\u003c 1) - (n \u003e\u003e\u003e 1); return; } // 之前我们说过，sizeCtrl在迁移前会设置为 (rs \u003c\u003c RESIZE_STAMP_SHIFT) + 2 // 然后，每有一个线程参与迁移就会将sizeCtrl + 1 // 这里使用CAS操作对sizeCtrl进行减一，表示做完了属于自己的任务 if (U.compareAndSetInt(this, SIZECTL, sc = sizeCtl, sc - 1)) { // 任务结束，方法退出 if ((sc - 2) != resizeStamp(n) \u003c\u003c RESIZE_STAMP_SHIFT) return; // 所有的迁移任务都已经做完 finishing = advance = true; i = n; // recheck before commit } } // 如果位置i处是空的，没有任何节点，那么放入刚才初始化的ForwardingNode节点 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); // 该位置处是一个ForwardingNode，代表该位置已经迁移过了 else if ((fh = f.hash) == MOVED) advance = true; // already processed else { // 对数组该位置处的节点加锁，开始处理数组该位置处的迁移工作 synchronized (f) { if (tabAt(tab, i) == f) { Node\u003cK,V\u003e ln, hn; // 头节点的hash大于0，说明是链表的Node节点 if (fh \u003e= 0) { // 这里展开解释一下runBits // 假设一个key的hash值为 0010 // table数组的长度总是2^n，这里假定为4,也就是 0100 // 所以这个对象会被放入 i = hash \u0026 (length - 1)位置（效果等价于 hash % length)，当然这是因为length一定为2^n // 还有哪些对象可能被放入这个数组位置呢，需要满足 hash \u0026 0011 = 0010 // 显然低两位要保持一致，不能改变，其余高位可以任意 // 现在发生了两倍扩容，i位置都需要迁移到新的位置 新的位置通过 hash \u0026 0111决定 // 原先数组位置中元素的hash值倒数第三位可能为0，也可能为1，也就是这里的runBits，所以元素会被分布到不同的位置 // 如果为0，和原先一样，分布到i为止 // 如果是1，分布到 i + oldLength位置，oldLength表示原先的数组长度 int runBit = fh \u0026 n; Node\u003cK,V\u003e lastR","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:7","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"get过程分析 public V get(Object key) { Node\u003cK,V\u003e[] tab; Node\u003cK,V\u003e e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null \u0026\u0026 (n = tab.length) \u003e 0 \u0026\u0026 (e = tabAt(tab, (n - 1) \u0026 h)) != null) { // 判断头节点是否就是查找的节点 if ((eh = e.hash) == h) { if ((ek = e.key) == key || (ek != null \u0026\u0026 key.equals(ek))) return e.val; } // 如果头节点的hash小于0，说明正在扩容，或者该位置是红黑树 else if (eh \u003c 0) // 参考 ForwardingNode.find(int h, Object k) 和 TreeBin.find(int h, Object k) return (p = e.find(h, key)) != null ? p.val : null; // 遍历链表 while ((e = e.next) != null) { if (e.hash == h \u0026\u0026 ((ek = e.key) == key || (ek != null \u0026\u0026 key.equals(ek)))) return e.val; } } return null; } 可以看到get操作的实现基本上是无锁的。 /** * A node inserted at head of bins during transfer operations. */ static final class ForwardingNode\u003cK,V\u003e extends Node\u003cK,V\u003e { final Node\u003cK,V\u003e[] nextTable; ForwardingNode(Node\u003cK,V\u003e[] tab) { super(MOVED, null, null, null); this.nextTable = tab; } Node\u003cK,V\u003e find(int h, Object k) { // loop to avoid arbitrarily deep recursion on forwarding nodes // 通过循环来避免对ForwardingNode的递归 outer: for (Node\u003cK,V\u003e[] tab = nextTable;;) { Node\u003cK,V\u003e e; int n; // 没有找到元素，返回null if (k == null || tab == null || (n = tab.length) == 0 || (e = tabAt(tab, (n - 1) \u0026 h)) == null) return null; for (;;) { int eh; K ek; // 头节点就是所需要的节点，直接返回 if ((eh = e.hash) == h \u0026\u0026 ((ek = e.key) == k || (ek != null \u0026\u0026 k.equals(ek)))) return e; if (eh \u003c 0) { if (e instanceof ForwardingNode) { tab = ((ForwardingNode\u003cK,V\u003e)e).nextTable; continue outer; } else return e.find(h, k); } // 搜索到了链表末尾，返回null if ((e = e.next) == null) return null; } } } } ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:8","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"clear过程分析 /** * Removes all of the mappings from this map. */ public void clear() { long delta = 0L; // negative number of deletions int i = 0; Node\u003cK,V\u003e[] tab = table; while (tab != null \u0026\u0026 i \u003c tab.length) { int fh; // 遍历数组中的每个元素 Node\u003cK,V\u003e f = tabAt(tab, i); // 元素为null，继续下一个 if (f == null) ++i; // 如果当前位置的元素已经被移动到新的数组中，帮助transfer，然后restart // restart 跳到新的table上，重新开始 else if ((fh = f.hash) == MOVED) { tab = helpTransfer(tab, f); i = 0; // restart } else { // 获取当前位置的头节点的监视器 synchronized (f) { if (tabAt(tab, i) == f) { Node\u003cK,V\u003e p = (fh \u003e= 0 ? f : (f instanceof TreeBin) ? ((TreeBin\u003cK,V\u003e)f).first : null); // 遍历链表或者红黑树，获取删除的节点个数 while (p != null) { --delta; p = p.next; } // 清空当前位置 setTabAt(tab, i++, null); } } } } if (delta != 0L) addCount(delta, -1); } ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:9","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"remove操作 /** * Removes the key (and its corresponding value) from this map. * This method does nothing if the key is not in the map. * * @param key the key that needs to be removed * @return the previous value associated with {@code key}, or * {@code null} if there was no mapping for {@code key} * @throws NullPointerException if the specified key is null */ public V remove(Object key) { return replaceNode(key, null, null); } /** * Implementation for the four public remove/replace methods: * Replaces node value with v, conditional upon match of cv if * non-null. If resulting value is null, delete. */ // cv是compareValue的缩写，表示期望值 // 如果cv不为null并且匹配当前值，将节点的值替换为v，如果替换后的值为null，则删除该节点 final V replaceNode(Object key, V value, Object cv) { int hash = spread(key.hashCode()); for (Node\u003cK,V\u003e[] tab = table;;) { Node\u003cK,V\u003e f; int n, i, fh; // 通过hash值判断key不存在，直接返回 if (tab == null || (n = tab.length) == 0 || (f = tabAt(tab, i = (n - 1) \u0026 hash)) == null) break; // table正在扩容，帮助扩容，然后进入下一轮循环 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { V oldVal = null; boolean validated = false; // 对数组当前位置的头节点加监视器锁 synchronized (f) { if (tabAt(tab, i) == f) { if (fh \u003e= 0) { validated = true; // 遍历链表 for (Node\u003cK,V\u003e e = f, pred = null;;) { K ek; if (e.hash == hash \u0026\u0026 ((ek = e.key) == key || (ek != null \u0026\u0026 key.equals(ek)))) { V ev = e.val; // 如果cv为null或者cv和当前节点的value相等 if (cv == null || cv == ev || (ev != null \u0026\u0026 cv.equals(ev))) { oldVal = ev; // value不为null，替换当前值 if (value != null) e.val = value; // 删除当前节点，当前节点不是头节点 else if (pred != null) pred.next = e.next; // 删除当前节点，当前节点是头节点 else setTabAt(tab, i, e.next); } break; } pred = e; if ((e = e.next) == null) break; } } // 红黑树逻辑 else if (f instanceof TreeBin) { validated = true; TreeBin\u003cK,V\u003e t = (TreeBin\u003cK,V\u003e)f; TreeNode\u003cK,V\u003e r, p; if ((r = t.root) != null \u0026\u0026 (p = r.findTreeNode(hash, key, null)) != null) { V pv = p.val; if (cv == null || cv == pv || (pv != null \u0026\u0026 cv.equals(pv))) { oldVal = pv; if (value != null) p.val = value; else if (t.removeTreeNode(p)) setTabAt(tab, i, untreeify(t.first)); } } } else if (f instanceof ReservationNode) throw new IllegalStateException(\"Recursive update\"); } } if (validated) { if (oldVal != null) { if (value == null) addCount(-1L, -1); return oldVal; } break; } } } return null; } ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:10","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":null,"content":"size操作 可以看到，size采用类似于LongAdder的方式，将统计mapping数量的工作分散到多个变量上，避免影响性能。 /** * A padded cell for distributing counts. Adapted from LongAdder * and Striped64. See their internal docs for explanation. */ @jdk.internal.vm.annotation.Contended static final class CounterCell { volatile long value; CounterCell(long x) { value = x; } } /** * Base counter value, used mainly when there is no contention, * but also as a fallback during table initialization * races. Updated via CAS. */ private transient volatile long baseCount; /** * Table of counter cells. When non-null, size is a power of 2. */ private transient volatile CounterCell[] counterCells; /** * {@inheritDoc} */ public int size() { long n = sumCount(); return ((n \u003c 0L) ? 0 : (n \u003e (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); } final long sumCount() { CounterCell[] cs = counterCells; long sum = baseCount; if (cs != null) { for (CounterCell c : cs) if (c != null) sum += c.value; } return sum; } /** * Adds to count, and if table is too small and not already * resizing, initiates transfer. If already resizing, helps * perform transfer if work is available. Rechecks occupancy * after a transfer to see if another resize is already needed * because resizings are lagging additions. * * @param x the count to add * @param check if \u003c0, don't check resize, if \u003c= 1 only check if uncontended */ private final void addCount(long x, int check) { CounterCell[] cs; long b, s; if ((cs = counterCells) != null || !U.compareAndSetLong(this, BASECOUNT, b = baseCount, s = b + x)) { CounterCell c; long v; int m; boolean uncontended = true; if (cs == null || (m = cs.length - 1) \u003c 0 || (c = cs[ThreadLocalRandom.getProbe() \u0026 m]) == null || !(uncontended = U.compareAndSetLong(c, CELLVALUE, v = c.value, v + x))) { fullAddCount(x, uncontended); return; } if (check \u003c= 1) return; s = sumCount(); } if (check \u003e= 0) { Node\u003cK,V\u003e[] tab, nt; int n, sc; while (s \u003e= (long)(sc = sizeCtl) \u0026\u0026 (tab = table) != null \u0026\u0026 (n = tab.length) \u003c MAXIMUM_CAPACITY) { int rs = resizeStamp(n) \u003c\u003c RESIZE_STAMP_SHIFT; if (sc \u003c 0) { if (sc == rs + MAX_RESIZERS || sc == rs + 1 || (nt = nextTable) == null || transferIndex \u003c= 0) break; if (U.compareAndSetInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); } else if (U.compareAndSetInt(this, SIZECTL, sc, rs + 2)) transfer(tab, null); s = sumCount(); } } } TODO：后面再看 ","date":"2025-05-28","objectID":"/posts/java-concurrenthashmap/:4:11","tags":null,"title":"Java ConcurrentHashMap","uri":"/posts/java-concurrenthashmap/"},{"categories":["Spark"],"content":"Spark物理执行计划生成方法 Spark具体采用3个步骤来生成物理执行计划，首先根据action操作顺序将应用划分为作业（job），然后根据每个job的逻辑处理流程中的ShuffleDependency依赖关系，将job划分为执行阶段（stage）。最后在每个stage中，根据最后生成的RDD的分区个数生成多个计算任务（task）。 ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:1:0","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"根据action操作将应用划分为作业（job） 当应用程序出现action操作时，如resultRDD.action()，表示应用会生成一个job，该job的逻辑处理流程为从输出数据到resultRDD的逻辑处理流程。 ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:1:1","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"根据ShuffleDependency依赖关系将job划分成执行阶段（stage） 对于每个job，从其最后的RDD往前回溯整个逻辑处理流程，如果遇到NarrowDependency，则将当前RDD的parent RDD纳入，并继续向前追溯，当遇到ShuffleDependency时，停止回溯，将当前已经纳入的所有RDD按照其依赖关系建立一个执行阶段，命名为stage i。 如果将存在ShuffleDependency依赖的RDD也纳入同一个stage，计算每个分区时都需要重复计算ShuffleDependency上游的RDD，这显然没有必要。 ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:1:2","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"根据分区计算将各个stage划分成计算任务（task） 每个分区上的计算逻辑相同，而且是独立的，因此每个分区上的计算可以独立成为一个task。同一个stage中的task可以同时分发到不同的机器并行执行。 ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:1:3","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"job、stage和task的计算顺序 job的提交时间和action被调用的时间有关，当应用程序执行到rdd.action()时，就会立即将rdd.actioin()形成的job提交给spark。job的逻辑处理流程实际上是一个DAG图，经过stage划分后，仍然是DAG图形状。每个stage的输出数据要不是job的输入数据，要不是上游stage的输出结果。因此，计算顺序从包含输入数据的stage开始，从前往后依次执行，仅当上游stage都执行完成后，再执行下游的stage。stage中的每个task因为是独立而且同构的，可以并行执行没有先后之分。 ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:1:4","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"task内部数据的存储的计算问题（流水线计算） 假设一个分区中有三条记录，分别为record1, record2, record3，需要对分区先执行f()操作，再执行g()操作，假设f()操作和g()操作都只依赖于上游分区中的单条记录，则可以采用流水线计算。类似于record1 -\u003e f(record1) -\u003e record1' -\u003e g(record') -\u003e record''，在task计算时只需要再内存中保留当前被处理的单个record即可，没有必要在执行f(record1)之前将record2和record3提前计算出来放入内存中。当然，如果f()操作和g()操作都依赖于上游分区中的多条记录，则流水线计算退化到计算-回收模式，需要一次读取上游分区中的所有数据，每执行完一个操作，回收之前的中间计算结果。 Spark采用流水线式计算来提高task的执行效率，减少内存使用量。这也是Spark可以在有限内存中处理大量大规模数据的原因。然而对于某些需要聚合中间计算结果的操作，还是需要占用一定的内存空间，也会在一定程度上影响流水线计算的效率。 ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:1:5","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"task间的数据传递和计算问题 stage之间存在的依赖关系是ShuffleDependency，而ShuffleDependency是部分依赖的，也就是下游stage中的每个task需要从parent RDD的每个分区中获取部分数据。ShuffleDependency的数据划分方式包括Hash划分、Range划分等，也就是要求上游stage预先将输出数据进行划分，按照分区存在，分区个数和下游task的个数一致，这个过程被称为Shuffle Write。按照分区存放完成后，下游的task将属于自己分区的数据通过网络传输获取，然后将来自上游不同分区的数据聚合在一起进行处理，这个过程被称为Shuffle Read。 ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:1:6","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"stage和task的命名方式 在Spark中，stage也可以有多个，有些stage既包含类似reduce的聚合操作有包含map操作，所以一般不区分是map stage还是reduce stage，而直接使用stage i来命名。 如果task的输出结果需要进行ShuffleWrite，以便传输给下一个stage，那么这些task被称为ShuffleMapTasks，而如果task的输出结果会汇总到Driver端或者直接写入分布式文件系统，那么这些task被称为ResultTasks。 ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:1:7","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"生成物理执行计划的源码分析 ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:0","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"demo程序 // scalastyle:off println package org.apache.spark.examples import java.util.concurrent.TimeUnit import scala.collection.compat.immutable.ArraySeq import org.apache.spark.{HashPartitioner, SparkContext} import org.apache.spark.sql.SparkSession object FilterDemo { def main(args: Array[String]): Unit = { val spark = SparkSession .builder() .appName(\"MapDemo\") .master(\"local\") .getOrCreate() val sc = spark.sparkContext.asInstanceOf[SparkContext] val data1 = Array[(Int, Char)]((1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'e'), (3, 'f'), (2, 'g'), (1, 'h')) val rdd1 = sc.parallelize(ArraySeq.unsafeWrapArray(data1), 3) val partitionedRDD = rdd1.partitionBy(new HashPartitioner(3)) val data2 = Array[(Int, String)]((1, \"A\"), (2, \"B\"), (3, \"C\"), (4, \"D\")) val rdd2 = sc.parallelize(ArraySeq.unsafeWrapArray(data2), 2) .map(x =\u003e (x._1, x._2 + \"\" + x._2)) val data3 = Array[(Int, String)]((3, \"X\"), (5, \"Y\"), (3, \"Z\"), (4, \"Y\")) val rdd3 = sc.parallelize(ArraySeq.unsafeWrapArray(data3), 2) val unionedRDD = rdd2.union(rdd3) val resultRDD = partitionedRDD.join(unionedRDD) resultRDD.count() spark.stop() } } 涉及到以下RDD类别 ParallelCollectionRDD ShuffledRDD CoGroupedRDD MapPartitionsRDD UnionRDD ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:1","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"### runJob def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum def runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] =\u003e U): Array[U] = { runJob(rdd, func, rdd.partitions.indices) } def runJob[T, U: ClassTag]( rdd: RDD[T], func: Iterator[T] =\u003e U, partitions: Seq[Int]): Array[U] = { val cleanedFunc = clean(func) runJob(rdd, (ctx: TaskContext, it: Iterator[T]) =\u003e cleanedFunc(it), partitions) } def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =\u003e U, partitions: Seq[Int]): Array[U] = { val results = new Array[U](partitions.size) runJob[T, U](rdd, func, partitions, (index, res) =\u003e results(index) = res) results } def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =\u003e U, partitions: Seq[Int], resultHandler: (Int, U) =\u003e Unit): Unit = { if (stopped.get()) { throw new IllegalStateException(\"SparkContext has been shutdown\") } val callSite = getCallSite() val cleanedFunc = clean(func) logInfo(log\"Starting job: ${MDC(LogKeys.CALL_SITE_SHORT_FORM, callSite.shortForm)}\") if (conf.getBoolean(\"spark.logLineage\", false)) { logInfo(log\"RDD's recursive dependencies:\\n\" + log\"${MDC(LogKeys.RDD_DEBUG_STRING, rdd.toDebugString)}\") } dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get) progressBar.foreach(_.finishAll()) rdd.doCheckpoint() } count操作会调用runJob创建并执行新的job，getCallSite通过堆栈找到用户调用代码的位置以及调用的spark方法，类似于union at FilterDemo.scala:35。 def runJob[T, U]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =\u003e U, partitions: Seq[Int], callSite: CallSite, resultHandler: (Int, U) =\u003e Unit, properties: Properties): Unit = { val start = System.nanoTime val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties) ThreadUtils.awaitReady(waiter.completionFuture, Duration.Inf) waiter.completionFuture.value.get match { case scala.util.Success(_) =\u003e logInfo(log\"Job ${MDC(LogKeys.JOB_ID, waiter.jobId)} finished: \" + log\"${MDC(LogKeys.CALL_SITE_SHORT_FORM, callSite.shortForm)}, took \" + log\"${MDC(LogKeys.TIME, (System.nanoTime - start) / 1e6)} ms\") case scala.util.Failure(exception) =\u003e logInfo(log\"Job ${MDC(LogKeys.JOB_ID, waiter.jobId)} failed: \" + log\"${MDC(LogKeys.CALL_SITE_SHORT_FORM, callSite.shortForm)}, took \" + log\"${MDC(LogKeys.TIME, (System.nanoTime - start) / 1e6)} ms\") // SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler. val callerStackTrace = Thread.currentThread().getStackTrace.tail exception.setStackTrace(exception.getStackTrace ++ callerStackTrace) throw exception } } runJob会调用submitJob 提交任务，获得JobWaiter句柄，并等待任务结束。 def submitJob[T, U]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =\u003e U, partitions: Seq[Int], callSite: CallSite, resultHandler: (Int, U) =\u003e Unit, properties: Properties): JobWaiter[U] = { // Check to make sure we are not launching a task on a partition that does not exist. val maxPartitions = rdd.partitions.length partitions.find(p =\u003e p \u003e= maxPartitions || p \u003c 0).foreach { p =\u003e throw new IllegalArgumentException( \"Attempting to access a non-existent partition: \" + p + \". \" + \"Total number of partitions: \" + maxPartitions) } // SPARK-23626: `RDD.getPartitions()` can be slow, so we eagerly compute // `.partitions` on every RDD in the DAG to ensure that `getPartitions()` // is evaluated outside of the DAGScheduler's single-threaded event loop: eagerlyComputePartitionsForRddAndAncestors(rdd) val jobId = nextJobId.getAndIncrement() if (partitions.isEmpty) { val clonedProperties = Utils.cloneProperties(properties) if (sc.getLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION) == null) { clonedProperties.setProperty(SparkContext.SPARK_JOB_DESCRIPTION, callSite.shortForm) } val time = clock.getTimeMillis() listenerBus.post( SparkListenerJobStart(jobId, time, Seq.empty, clonedProperties)) listenerBus.post( SparkListenerJobEnd(jobId, time, JobSucceeded)) // Return immediately if the job is running 0 tasks return new JobWaiter[U](this, jobId, 0, resultHandler) } assert(partitions.nonEmpty) val func2 = fun","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:2","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"createResultStage private def createResultStage( rdd: RDD[_], func: (TaskContext, Iterator[_]) =\u003e _, partitions: Array[Int], jobId: Int, callSite: CallSite): ResultStage = { // 获取当前RDD直接依赖的shuffleDependencies val (shuffleDeps, resourceProfiles) = getShuffleDependenciesAndResourceProfiles(rdd) val resourceProfile = mergeResourceProfilesForStage(resourceProfiles) checkBarrierStageWithDynamicAllocation(rdd) checkBarrierStageWithNumSlots(rdd, resourceProfile) checkBarrierStageWithRDDChainPattern(rdd, partitions.toSet.size) // 获取parent stages，这里其他是一个递归过程，内部会调用getShuffleDependenciesAndResourceProfiles val parents = getOrCreateParentStages(shuffleDeps, jobId) // stageId是整个SparkContext范围内唯一的 val id = nextStageId.getAndIncrement() // 创建新的ResultStage，将parent stages传入作为参数 val stage = new ResultStage(id, rdd, func, partitions, parents, jobId, callSite, resourceProfile.id) stageIdToStage(id) = stage updateJobIdStageIdMaps(jobId, stage) stage } createResultStage负责构建整个Job的Stage依赖关系，通过递归地获取ShuffleDependency将job切割成多个stage，并最终返回ResultStage。 private[scheduler] def getShuffleDependenciesAndResourceProfiles( rdd: RDD[_]): (HashSet[ShuffleDependency[_, _, _]], HashSet[ResourceProfile]) = { val parents = new HashSet[ShuffleDependency[_, _, _]] val resourceProfiles = new HashSet[ResourceProfile] val visited = new HashSet[RDD[_]] val waitingForVisit = new ListBuffer[RDD[_]] waitingForVisit += rdd while (waitingForVisit.nonEmpty) { val toVisit = waitingForVisit.remove(0) if (!visited(toVisit)) { visited += toVisit Option(toVisit.getResourceProfile()).foreach(resourceProfiles += _) toVisit.dependencies.foreach { case shuffleDep: ShuffleDependency[_, _, _] =\u003e parents += shuffleDep case dependency =\u003e waitingForVisit.prepend(dependency.rdd) } } } (parents, resourceProfiles) } getShuffleDependenciesAndResourceProfiles返回给定 RDD 直接依赖的ShuffleDependency，以及该stage中与这些 RDD 相关联的ResourceProfiles。 遍历当前RDD的所有依赖，将RDD的ResourceProfile添加到结果resourceProfiles，依赖如果是ShuffleDependency，则将ShuffleDependency添加到结果集中，如果遇到其他类型的依赖，则开始递归遍历父RDD。当然实际实现了为了避免StackOverFlowError，采用了手动维护栈的方法。 private def getOrCreateParentStages(shuffleDeps: HashSet[ShuffleDependency[_, _, _]], firstJobId: Int): List[Stage] = { shuffleDeps.map { shuffleDep =\u003e getOrCreateShuffleMapStage(shuffleDep, firstJobId) }.toList } private def getOrCreateShuffleMapStage( shuffleDep: ShuffleDependency[_, _, _], firstJobId: Int): ShuffleMapStage = { shuffleIdToMapStage.get(shuffleDep.shuffleId) match { case Some(stage) =\u003e stage case None =\u003e // Create stages for all missing ancestor shuffle dependencies. getMissingAncestorShuffleDependencies(shuffleDep.rdd).foreach { dep =\u003e // Even though getMissingAncestorShuffleDependencies only returns shuffle dependencies // that were not already in shuffleIdToMapStage, it's possible that by the time we // get to a particular dependency in the foreach loop, it's been added to // shuffleIdToMapStage by the stage creation process for an earlier dependency. See // SPARK-13902 for more information. if (!shuffleIdToMapStage.contains(dep.shuffleId)) { createShuffleMapStage(dep, firstJobId) } } // Finally, create a stage for the given shuffle dependency. createShuffleMapStage(shuffleDep, firstJobId) } } 对于每个ShuffleDependency，获取对应的ShuffleMapStage。 通过shuffleId查询ShuffleMapStage，如果存在，直接返回。 如果不存在，获取当前ShuffleDependency直接或间接依赖的所有上游缺失的ShuffleDependency，再次检查ShuffleDependency是否已经创建ShuffleMapStage，如果没有创建，则调用createShuffleMapStage创建，最后所有上游的ShuffleMapStage已经创建完毕，创建当前ShuffleDependency的ShuffleMapStage。 /** Find ancestor shuffle dependencies that are not registered in shuffleToMapStage yet */ private def getMissingAncestorShuffleDependencies( rdd: RDD[_]): ListBuffer[ShuffleDependency[_, _, _]] = { val ancestors = new ListBuffer[ShuffleDependency[_, _, _]] val visited = new HashSet[RDD[_]] // We are manually maintaining a stack here to prevent StackOverflowError // caused by recursively visiting val waitingForVisit = new ListBuffer[RDD[_]] waitingForVisit += rdd while (waitingForVisit.nonEmpty) { val toVisit = wa","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:3","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"submitStage /** Submits stage, but first recursively submits any missing parents. */ private def submitStage(stage: Stage): Unit = { val jobId = activeJobForStage(stage) if (jobId.isDefined) { logDebug(s\"submitStage($stage (name=${stage.name};\" + s\"jobs=${stage.jobIds.toSeq.sorted.mkString(\",\")}))\") // waitingStages 正在等待的stage集合 // runningStages 正在执行的stage集合 // failedStages 失败等待手动提交重试的集合 if (!waitingStages(stage) \u0026\u0026 !runningStages(stage) \u0026\u0026 !failedStages(stage)) { // stage尝试次数超过最大限制，abort stage if (stage.getNextAttemptId \u003e= maxStageAttempts) { val reason = s\"$stage (name=${stage.name}) has been resubmitted for the maximum \" + s\"allowable number of times: ${maxStageAttempts}, which is the max value of \" + s\"config `${config.STAGE_MAX_ATTEMPTS.key}` and \" + s\"`${config.STAGE_MAX_CONSECUTIVE_ATTEMPTS.key}`.\" abortStage(stage, reason, None) } else { // 找到stage直接依赖的缺失的stage val missing = getMissingParentStages(stage).sortBy(_.id) logDebug(\"missing: \" + missing) if (missing.isEmpty) { logInfo(log\"Submitting ${MDC(STAGE, stage)} (${MDC(RDD_ID, stage.rdd)}), \" + log\"which has no missing parents\") // 依赖的stage都已经就绪，直接提交当前stage的task submitMissingTasks(stage, jobId.get) } else { // 否则尝试提交依赖的stage，进入递归流程 for (parent \u003c- missing) { submitStage(parent) } // 当前stage加入等待集合 waitingStages += stage } } } } else { abortStage(stage, \"No active job for stage \" + stage.id, None) } } submitStage首先需要查找并提交任何缺失的父stage，如果存在这样的父stage，会递归提交父stage，并将自身加入等待集合中，否则，直接提交当前stage的缺失task。 ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:4","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"getMissingParentStages private def getMissingParentStages(stage: Stage): List[Stage] = { val missing = new HashSet[Stage] val visited = new HashSet[RDD[_]] // We are manually maintaining a stack here to prevent StackOverflowError // caused by recursively visiting val waitingForVisit = new ListBuffer[RDD[_]] waitingForVisit += stage.rdd def visit(rdd: RDD[_]): Unit = { if (!visited(rdd)) { visited += rdd // stage依赖的rdd是否已经计算过并且缓存 val rddHasUncachedPartitions = getCacheLocs(rdd).contains(Nil) if (rddHasUncachedPartitions) { // 如果rdd需要重新计算，遍历rdd的依赖关系 for (dep \u003c- rdd.dependencies) { dep match { // 获取ShuffleDependency对应的ShuffleMapStage，如果mapStage的结果不可得，添加到结果集中 case shufDep: ShuffleDependency[_, _, _] =\u003e val mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId) // Mark mapStage as available with shuffle outputs only after shuffle merge is // finalized with push based shuffle. If not, subsequent ShuffleMapStage won't // read from merged output as the MergeStatuses are not available. if (!mapStage.isAvailable || !mapStage.shuffleDep.shuffleMergeFinalized) { missing += mapStage } else { // Forward the nextAttemptId if skipped and get visited for the first time. // Otherwise, once it gets retried, // 1) the stuffs in stage info become distorting, e.g. task num, input byte, e.t.c // 2) the first attempt starts from 0-idx, it will not be marked as a retry mapStage.increaseAttemptIdOnFirstSkip() } // 如果是窄依赖，则继续回溯 case narrowDep: NarrowDependency[_] =\u003e waitingForVisit.prepend(narrowDep.rdd) } } } } } while (waitingForVisit.nonEmpty) { visit(waitingForVisit.remove(0)) } missing.toList } getMissingParentStages找到当前stage直接依赖的缺失的stage。 ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:5","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"submitMissingTasks 通过findMissingPartitions找到stage对应的所有需要计算的分区的id，调用getPreferredLocs得到每个partition的首选位置。 调用stage.makeNewStageAttempt创建新的stage尝试。记录stage的submissionTime向listenerBus发布SparkListenerStageSubmitted事件 // TODO: Maybe we can keep the taskBinary in Stage to avoid serializing it multiple times. // Broadcasted binary for the task, used to dispatch tasks to executors. Note that we broadcast // the serialized copy of the RDD and for each task we will deserialize it, which means each // task gets a different copy of the RDD. This provides stronger isolation between tasks that // might modify state of objects referenced in their closures. This is necessary in Hadoop // where the JobConf/Configuration object is not thread-safe. var taskBinary: Broadcast[Array[Byte]] = null var partitions: Array[Partition] = null try { // For ShuffleMapTask, serialize and broadcast (rdd, shuffleDep). // For ResultTask, serialize and broadcast (rdd, func). var taskBinaryBytes: Array[Byte] = null // taskBinaryBytes and partitions are both effected by the checkpoint status. We need // this synchronization in case another concurrent job is checkpointing this RDD, so we get a // consistent view of both variables. RDDCheckpointData.synchronized { taskBinaryBytes = stage match { case stage: ShuffleMapStage =\u003e JavaUtils.bufferToArray( closureSerializer.serialize((stage.rdd, stage.shuffleDep): AnyRef)) case stage: ResultStage =\u003e JavaUtils.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): AnyRef)) } partitions = stage.rdd.partitions } if (taskBinaryBytes.length \u003e TaskSetManager.TASK_SIZE_TO_WARN_KIB * 1024) { logWarning(log\"Broadcasting large task binary with size \" + log\"${MDC(NUM_BYTES, Utils.bytesToString(taskBinaryBytes.length))}\") } taskBinary = sc.broadcast(taskBinaryBytes) } catch { // In the case of a failure during serialization, abort the stage. case e: NotSerializableException =\u003e abortStage(stage, \"Task not serializable: \" + e.toString, Some(e)) runningStages -= stage // Abort execution return case e: Throwable =\u003e abortStage(stage, s\"Task serialization failed: $e\\n${Utils.exceptionString(e)}\", Some(e)) runningStages -= stage // Abort execution return } val artifacts = jobIdToActiveJob(jobId).artifacts val tasks: Seq[Task[_]] = try { val serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array() stage match { case stage: ShuffleMapStage =\u003e stage.pendingPartitions.clear() partitionsToCompute.map { id =\u003e val locs = taskIdToLocations(id) val part = partitions(id) stage.pendingPartitions += id new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber(), taskBinary, part, stage.numPartitions, locs, artifacts, properties, serializedTaskMetrics, Option(jobId), Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier()) } case stage: ResultStage =\u003e partitionsToCompute.map { id =\u003e val p: Int = stage.partitions(id) val part = partitions(p) val locs = taskIdToLocations(id) new ResultTask(stage.id, stage.latestInfo.attemptNumber(), taskBinary, part, stage.numPartitions, locs, id, artifacts, properties, serializedTaskMetrics, Option(jobId), Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier()) } } } catch { case NonFatal(e) =\u003e abortStage(stage, s\"Task creation failed: $e\\n${Utils.exceptionString(e)}\", Some(e)) runningStages -= stage return } if (tasks.nonEmpty) { logInfo(log\"Submitting ${MDC(NUM_TASKS, tasks.size)} missing tasks from \" + log\"${MDC(STAGE, stage)} (${MDC(RDD_ID, stage.rdd)}) (first 15 tasks are \" + log\"for partitions ${MDC(PARTITION_IDS, tasks.take(15).map(_.partitionId))})\") val shuffleId = stage match { case s: ShuffleMapStage =\u003e Some(s.shuffleDep.shuffleId) case _: ResultStage =\u003e None } taskScheduler.submitTasks(new TaskSet( tasks.toArray, stage.id, stage.latestInfo.attemptNumber(), jobId, properties, stage.resourceProfileId, shuffleId)) 对于ShuffleMapStage，序列化stage.rdd和stage.shuffleDep，对于ResultStage，序列化stage.rdd和stage.func。调用SparkContext.broadcast将序列化结果广播。 对于需要","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:6","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"resoruceOffers TODO ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:7","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"TaskDescription private[spark] class TaskDescription( val taskId: Long, // taskId val attemptNumber: Int, // task attemp number，唯一标记每次重试 val executorId: String, // 执行task的executor节点 val name: String, val index: Int, // Index within this task's TaskSet val partitionId: Int, // 实际计算的分区id val artifacts: JobArtifactSet, // jar包和文件等 val properties: Properties, // 属性 val cpus: Int, // 需要分配的cpu个数 // resources is the total resources assigned to the task // Eg, Map(\"gpu\" -\u003e Map(\"0\" -\u003e ResourceAmountUtils.toInternalResource(0.7))): // assign 0.7 of the gpu address \"0\" to this task val resources: immutable.Map[String, immutable.Map[String, Long]], // 需要分配的其他资源 val serializedTask: ByteBuffer) { // 序列化的Task assert(cpus \u003e 0, \"CPUs per task should be \u003e 0\") override def toString: String = s\"TaskDescription($name)\" } TaskDescription描述一个将被传到executor上进行执行的task，通常由TaskSetManager.resourceOffer创建，TaskDescription和Task需要被序列化传到executor上，当TaskDescription被executor接收到，executor首先需要得到一系列的jar包和文件，并添加这些到classpath，然后设置属性，再反序列化Task对象（serializedTask)，这也是为什么属性properties被包含在TaskDescription中，尽管它们同样包含在serialized task中。 可以看到，TaskDescription已经确定了task将被发送到的executorId以及对应的RDD分区和资源需求。 ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:8","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"launchTasks // Launch tasks returned by a set of resource offers private def launchTasks(tasks: Seq[Seq[TaskDescription]]): Unit = { for (task \u003c- tasks.flatten) { val serializedTask = TaskDescription.encode(task) if (serializedTask.limit() \u003e= maxRpcMessageSize) { Option(scheduler.taskIdToTaskSetManager.get(task.taskId)).foreach { taskSetMgr =\u003e try { var msg = \"Serialized task %s:%d was %d bytes, which exceeds max allowed: \" + s\"${RPC_MESSAGE_MAX_SIZE.key} (%d bytes). Consider increasing \" + s\"${RPC_MESSAGE_MAX_SIZE.key} or using broadcast variables for large values.\" msg = msg.format(task.taskId, task.index, serializedTask.limit(), maxRpcMessageSize) taskSetMgr.abort(msg) } catch { case e: Exception =\u003e logError(\"Exception in error callback\", e) } } } else { val executorData = executorDataMap(task.executorId) // Do resources allocation here. The allocated resources will get released after the task // finishes. executorData.freeCores -= task.cpus task.resources.foreach { case (rName, addressAmounts) =\u003e executorData.resourcesInfo(rName).acquire(addressAmounts) } logDebug(s\"Launching task ${task.taskId} on executor id: ${task.executorId} hostname: \" + s\"${executorData.executorHost}.\") executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask))) } } } launchTasks批量处理TaskDecription，首先序列化TaskDescription，如果序列化后的长度高于阈值，则放弃当前任务，否则，申请对应的cpu和其他各类资源，最终调用executorEndpoint.send发送RPC请求LaunchTask携带序列化后的TaskDescription。 这样任务就可以被executor接收，并且执行了。 ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:9","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"getPreferredLocs def getPreferredLocs(rdd: RDD[_], partition: Int): Seq[TaskLocation] = { getPreferredLocsInternal(rdd, partition, new HashSet) } private def getPreferredLocsInternal( rdd: RDD[_], partition: Int, visited: HashSet[(RDD[_], Int)]): Seq[TaskLocation] = { // If the partition has already been visited, no need to re-visit. // This avoids exponential path exploration. SPARK-695 if (!visited.add((rdd, partition))) { // Nil has already been returned for previously visited partitions. return Nil } // If the partition is cached, return the cache locations val cached = getCacheLocs(rdd)(partition) if (cached.nonEmpty) { return cached } // If the RDD has some placement preferences (as is the case for input RDDs), get those val rddPrefs = rdd.preferredLocations(rdd.partitions(partition)).toList if (rddPrefs.nonEmpty) { return rddPrefs.filter(_ != null).map(TaskLocation(_)) } // If the RDD has narrow dependencies, pick the first partition of the first narrow dependency // that has any placement preferences. Ideally we would choose based on transfer sizes, // but this will do for now. rdd.dependencies.foreach { case n: NarrowDependency[_] =\u003e for (inPart \u003c- n.getParents(partition)) { val locs = getPreferredLocsInternal(n.rdd, inPart, visited) if (locs != Nil) { return locs } } case _ =\u003e } Nil } getPreferredLocs获取与特定 RDD 的某个分区相关联的位置（locality）信息。首先检查partition是否被cache，如果被cache，直接返回，否则如果RDD自身有位置信息，直接使用，假设RDD是一个input RDD的场景，最后尝试获取RDD第一个窄依赖的第一个分区的位置信息，这里Spark也提到，理想情况下应该基于transfer size进行选择。 ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:10","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"JobWaiter的实现 private[spark] trait JobListener { def taskSucceeded(index: Int, result: Any): Unit def jobFailed(exception: Exception): Unit } JobListerner接口用于监听task完成或者失败的事件，当一个task完成或者整个job失败时被通知。 /** * An object that waits for a DAGScheduler job to complete. As tasks finish, it passes their * results to the given handler function. */ private[spark] class JobWaiter[T]( dagScheduler: DAGScheduler, val jobId: Int, totalTasks: Int, resultHandler: (Int, T) =\u003e Unit) extends JobListener with Logging { private val finishedTasks = new AtomicInteger(0) // If the job is finished, this will be its result. In the case of 0 task jobs (e.g. zero // partition RDDs), we set the jobResult directly to JobSucceeded. private val jobPromise: Promise[Unit] = if (totalTasks == 0) Promise.successful(()) else Promise() def jobFinished: Boolean = jobPromise.isCompleted def completionFuture: Future[Unit] = jobPromise.future /** * Sends a signal to the DAGScheduler to cancel the job with an optional reason. The * cancellation itself is handled asynchronously. After the low level scheduler cancels * all the tasks belonging to this job, it will fail this job with a SparkException. */ def cancel(reason: Option[String]): Unit = { dagScheduler.cancelJob(jobId, reason) } /** * Sends a signal to the DAGScheduler to cancel the job. The cancellation itself is * handled asynchronously. After the low level scheduler cancels all the tasks belonging * to this job, it will fail this job with a SparkException. */ def cancel(): Unit = cancel(None) override def taskSucceeded(index: Int, result: Any): Unit = { // resultHandler call must be synchronized in case resultHandler itself is not thread safe. synchronized { resultHandler(index, result.asInstanceOf[T]) } if (finishedTasks.incrementAndGet() == totalTasks) { jobPromise.success(()) } } override def jobFailed(exception: Exception): Unit = { if (!jobPromise.tryFailure(exception)) { logWarning(\"Ignore failure\", exception) } } } jobPromise字段是一个Promise对象，Promise 是一个表示未来结果的对象，它可以被手动完成（赋值）或失败（抛出异常）。 ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:11","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"getPartitions final def partitions: Array[Partition] = { checkpointRDD.map(_.partitions).getOrElse { if (partitions_ == null) { stateLock.synchronized { if (partitions_ == null) { partitions_ = getPartitions partitions_.zipWithIndex.foreach { case (partition, index) =\u003e require(partition.index == index, s\"partitions($index).partition == ${partition.index}, but it should equal $index\") } } } } partitions_ } } getPartitions是RDD中的虚方法，由RDD子类负责实现。 /** * An identifier for a partition in an RDD. */ trait Partition extends Serializable { /** * Get the partition's index within its parent RDD */ def index: Int // A better default implementation of HashCode override def hashCode(): Int = index override def equals(other: Any): Boolean = super.equals(other) } private[spark] class ParallelCollectionRDD[T: ClassTag]( sc: SparkContext, @transient private val data: Seq[T], numSlices: Int, locationPrefs: Map[Int, Seq[String]]) extends RDD[T](sc, Nil) { // TODO: Right now, each split sends along its full data, even if later down the RDD chain it gets // cached. It might be worthwhile to write the data to a file in the DFS and read it in the split // instead. // UPDATE: A parallel collection can be checkpointed to HDFS, which achieves this goal. override def getPartitions: Array[Partition] = { val slices = ParallelCollectionRDD.slice(data, numSlices).toArray slices.indices.map(i =\u003e new ParallelCollectionPartition(id, i, slices(i))).toArray } ParallelCollectionRDD的getPartitions函数首先将输入的数据分成numSlices份，然后生成对应的分区。 private[spark] class MapPartitionsRDD[U: ClassTag, T: ClassTag]( var prev: RDD[T], f: (TaskContext, Int, Iterator[T]) =\u003e Iterator[U], // (TaskContext, partition index, iterator) preservesPartitioning: Boolean = false, isFromBarrier: Boolean = false, isOrderSensitive: Boolean = false) extends RDD[U](prev) { override def getPartitions: Array[Partition] = firstParent[T].partitions MapPartitionsRDD直接继承父RDD的分区。 @DeveloperApi class UnionRDD[T: ClassTag]( sc: SparkContext, var rdds: Seq[RDD[T]]) extends RDD[T](sc, Nil) { // Nil since we implement getDependencies // visible for testing private[spark] val isPartitionListingParallel: Boolean = rdds.length \u003e conf.get(RDD_PARALLEL_LISTING_THRESHOLD) override def getPartitions: Array[Partition] = { val parRDDs = if (isPartitionListingParallel) { // scalastyle:off parvector val parArray = new ParVector(rdds.toVector) parArray.tasksupport = UnionRDD.partitionEvalTaskSupport // scalastyle:on parvector parArray } else { rdds } val array = new Array[Partition](parRDDs.iterator.map(_.partitions.length).sum) var pos = 0 for ((rdd, rddIndex) \u003c- rdds.zipWithIndex; split \u003c- rdd.partitions) { array(pos) = new UnionPartition(pos, rdd, rddIndex, split.index) pos += 1 } array } UnionRDD类似于MapPartitionsRDD，分区依据父RDD的分区生成。依次遍历每个父RDD的每个分区，生成对应的UnionRDD的分区。 @DeveloperApi class ShuffledRDD[K: ClassTag, V: ClassTag, C: ClassTag]( @transient var prev: RDD[_ \u003c: Product2[K, V]], part: Partitioner) extends RDD[(K, C)](prev.context, Nil) { override def getPartitions: Array[Partition] = { Array.tabulate[Partition](part.numPartitions)(i =\u003e new ShuffledRDDPartition(i)) } private[spark] class ShuffledRDDPartition(val idx: Int) extends Partition { override val index: Int = idx } ShuffledRDD的分区总数通过partitioner.numPartitions得到，生成的分区为ShuffledRDDPartition。 @DeveloperApi class CoGroupedRDD[K: ClassTag]( @transient var rdds: Seq[RDD[_ \u003c: Product2[K, _]]], part: Partitioner) extends RDD[(K, Array[Iterable[_]])](rdds.head.context, Nil) { override def getPartitions: Array[Partition] = { val array = new Array[Partition](part.numPartitions) for (i \u003c- array.indices) { // Each CoGroupPartition will have a dependency per contributing RDD array(i) = new CoGroupPartition(i, rdds.zipWithIndex.map { case (rdd, j) =\u003e // Assume each RDD contributed a single dependency, and get it dependencies(j) match { case s: ShuffleDependency[_, _, _] =\u003e None case _ =\u003e Some(new NarrowCoGroupSplitDep(rdd, i, rdd.partitions(i))) } }.toArray) } array } CoGroupedRDD通过numPar","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:12","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"Stage 一个stage是一组并行的task，这些task都执行相同的函数，并且需要作为一个spark job的一部分来运行，具有相同的shuffle依赖。每一个由调度器执行的任务DAG都会在发生shuffle的边界处分割成多个stage，然后DAGScheduler按照拓扑顺序来依次运行这些stage。 每个stage可以是shuffle map stage，或者是result stage。如果是shuffle map stage，那么他的task结果将作为其他stage的输入；如果是result stage，那么它的task会直接通过在一个RDD上运行某个函数来执行一个spark action。对于shuffle map stage，Spark还会追踪每个输出分区所在的节点位置。 每个Stage还有一个firstJobId用于标识最终提交该stage的job，当使用FIFO调度策略时，这个字段可以让调度器优先计算来自较早job的stages，或者在失败时更快的恢复这些较早的stages。 由于容错恢复（fault recovery）的需要，一个stage可能会被多次重试执行。在这种情况下，stage对象会维护多个StageInfo实例，用于传递给监听器（listeners）或者web ui，最新的一次尝试信息可以通过latestInfo字段访问。 private[scheduler] abstract class Stage( val id: Int, val rdd: RDD[_], val numTasks: Int, val parents: List[Stage], val firstJobId: Int, val callSite: CallSite, val resourceProfileId: Int) extends Logging { val numPartitions = rdd.partitions.length /** Set of jobs that this stage belongs to. */ val jobIds = new HashSet[Int] /** The ID to use for the next new attempt for this stage. */ private var nextAttemptId: Int = 0 private[scheduler] def getNextAttemptId: Int = nextAttemptId val name: String = callSite.shortForm val details: String = callSite.longForm /** * Pointer to the [[StageInfo]] object for the most recent attempt. This needs to be initialized * here, before any attempts have actually been created, because the DAGScheduler uses this * StageInfo to tell SparkListeners when a job starts (which happens before any stage attempts * have been created). */ private var _latestInfo: StageInfo = StageInfo.fromStage(this, nextAttemptId, resourceProfileId = resourceProfileId) /** * Set of stage attempt IDs that have failed. We keep track of these failures in order to avoid * endless retries if a stage keeps failing. * We keep track of each attempt ID that has failed to avoid recording duplicate failures if * multiple tasks from the same stage attempt fail (SPARK-5945). */ val failedAttemptIds = new HashSet[Int] private[scheduler] def clearFailures() : Unit = { failedAttemptIds.clear() } /** Creates a new attempt for this stage by creating a new StageInfo with a new attempt ID. */ def makeNewStageAttempt( numPartitionsToCompute: Int, taskLocalityPreferences: Seq[Seq[TaskLocation]] = Seq.empty): Unit = { val metrics = new TaskMetrics metrics.register(rdd.sparkContext) _latestInfo = StageInfo.fromStage( this, nextAttemptId, Some(numPartitionsToCompute), metrics, taskLocalityPreferences, resourceProfileId = resourceProfileId) nextAttemptId += 1 } /** Forward the nextAttemptId if skipped and get visited for the first time. */ def increaseAttemptIdOnFirstSkip(): Unit = { if (nextAttemptId == 0) { nextAttemptId = 1 } } /** Returns the StageInfo for the most recent attempt for this stage. */ def latestInfo: StageInfo = _latestInfo override final def hashCode(): Int = id override final def equals(other: Any): Boolean = other match { case stage: Stage =\u003e stage != null \u0026\u0026 stage.id == id case _ =\u003e false } /** Returns the sequence of partition ids that are missing (i.e. needs to be computed). */ def findMissingPartitions(): Seq[Int] def isIndeterminate: Boolean = { rdd.outputDeterministicLevel == DeterministicLevel.INDETERMINATE } } 构造参数: id 唯一的stage id rdd 该stage所运行的RDD，如果是shuffle map stage，那么就是我们要在其上运行map任务的rdd，如果是result stage，那么就是我们执行某个action操作所针对的目标rdd numTasks stage中的task总数，特别地result stage可能不会计算rdd的所有分区，比如first, lookup, take等操作 parents 这个stage依赖的stage列表（通过shuffle dependeny依赖） firstJobId 这个stage所属的首个job，用于FIFO 调度 其他字段： jobIds 这个stage所属的所有job nextAttemptId stage每次重试都会获得新的newAttemptId，初始值为0 _latestInfo 最新一次尝试的StageInfo failedAttemptId stage尝试失败的集合 方法： makeNewStageAttempt 创建新的TaskMetrics，并注册到SparkContext中。创建新的StageInfo并递增nextAttemptId findMissingPartitions 返回需要计算的partition id 的序列 ResultStage private[spark] class ResultStage( id: Int, rdd: RDD[_], val func: (TaskContext, Iterator[_]) =\u003e _, val partitions: Array[Int], parents: List[Stage], firstJobId: Int, callSite: CallSite, resourceProfileId: Int) extends Stage(id, rdd, partitions.length, parents, firstJobId, callSi","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:13","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Spark"],"content":"ActiveJob private[spark] class ActiveJob( val jobId: Int, val finalStage: Stage, val callSite: CallSite, val listener: JobListener, val artifacts: JobArtifactSet, val properties: Properties) { /** * Number of partitions we need to compute for this job. Note that result stages may not need * to compute all partitions in their target RDD, for actions like first() and lookup(). */ val numPartitions = finalStage match { case r: ResultStage =\u003e r.partitions.length case m: ShuffleMapStage =\u003e m.numPartitions } /** Which partitions of the stage have finished */ val finished = Array.fill[Boolean](numPartitions)(false) var numFinished = 0 } DAGScheduler中的一个运行中job，job可以有两个逻辑类型，result job通过计算ResultStage执行action，map-stage job在下游stage被提交前计算ShuffleMapStage的map输出。后者被用于自适应查询计划，在提交后续stage之前查看map输出的统计信息，我们通过该类中的finalStage区分这两类job。 只有客户端通过DAGScheduler的submitJob或者submitMapStage方法直接提交的叶子stage，才会被作为job进行追踪，但是，无论是那种类型的job，都可能会触发其依赖的前面stage的执行（这些stage是DAG中所以来的RDD所对应的stage），并且多个job可能会共享其中的一些前置stage。这些依赖关系有DAGScheduler内部进行管理。 一个job起始于一个目标RDD，但最终可能会包含RDD血缘关系中涉及到的其他所有RDD ActiveJob的构造参数包括： JobId job的唯一id finalStage job计算的stage mapPartitions字段表示job中需要计算的分区的个数，注意，ResultStage可能不需要计算RDD中的所有分区，比如对于first或者lookup操作。 finished字段记录stage中的哪些分区已经计算完成。 numFinished字段记录已经计算完成的分区的个数。 ","date":"2025-05-25","objectID":"/posts/spark_physical_plan/:2:14","tags":["Spark"],"title":"Spark物理执行计划","uri":"/posts/spark_physical_plan/"},{"categories":["Grpc"],"content":"OpenTelemetry OpenTelemetry是CNCF孵化的开源观测框架，用于创建和管理遥测数据（telemetry），比如metrics, tracing, logs。这一项目的核心目标是避免依赖于vendor，从而可以更好地集成和拓展。 为什么需要观测？ 更快排查问题 观测然后改进，提升系统性能 持有监测和预警 ","date":"2025-05-13","objectID":"/posts/grpc-metric/:1:0","tags":["Grpc"],"title":"Grpc Metric","uri":"/posts/grpc-metric/"},{"categories":["Spark"],"content":"Spark应用程序需要先转化为逻辑处理流程，逻辑处理流程主要包括： RDD数据模型 数据操作 数据依赖关系 数据操作分为两种，transformation操作并不会触发job的实际执行，action操作创建job并立即执行。类似于java中的stream，采用懒加载的方式。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:0:0","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"常用transformation数据操作 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:0","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"map // scalastyle:off println package org.apache.spark.examples import scala.collection.compat.immutable.ArraySeq import org.apache.spark.SparkContext import org.apache.spark.sql.SparkSession object MapDemo { def main(args: Array[String]): Unit = { val spark = SparkSession .builder() .appName(\"MapDemo\") .master(\"local\") .getOrCreate() val sc = spark.sparkContext.asInstanceOf[SparkContext] val array = Array[(Int, Char)]( (1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (2, 'e'), (3, 'f'), (2, 'g'), (1, 'h')) val inputRDD = sc.parallelize( ArraySeq.unsafeWrapArray(array) , 3) val resultRDD = inputRDD.map(r =\u003e s\"${r._1}_${r._2}\") resultRDD.foreach(println) spark.stop() } } 这里给出了一个简单的例子，通过map函数将key和value拼接起来。 def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]()) } private[spark] class ParallelCollectionRDD[T: ClassTag]( sc: SparkContext, @transient private val data: Seq[T], numSlices: Int, locationPrefs: Map[Int, Seq[String]]) extends RDD[T](sc, Nil) { parallelize将一个局地的scala集合分布式化成RDD，但实际上仅仅是构建ParallelCollectionRDD而已，没有依赖于其他RDD，所以传入的为Nil。 def map[U: ClassTag](f: T =\u003e U): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) =\u003e iter.map(cleanF)) } private[spark] class MapPartitionsRDD[U: ClassTag, T: ClassTag]( var prev: RDD[T], f: (TaskContext, Int, Iterator[T]) =\u003e Iterator[U], // (TaskContext, partition index, iterator) preservesPartitioning: Boolean = false, isFromBarrier: Boolean = false, isOrderSensitive: Boolean = false) extends RDD[U](prev) { map函数对输出的RDD的每条记录应用目标函数，获得新的MapPartitionRDD，依赖于之前的RDD prev。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:1","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"mapValues val resultRDD = inputRDD.mapValues(x =\u003e s\"${x} + 1\") 对之前的map例子稍作调整，调用mapValues而不是map函数，不改变key，只对value进行转换。 def mapValues[U](f: V =\u003e U): RDD[(K, U)] = self.withScope { val cleanF = self.context.clean(f) new MapPartitionsRDD[(K, U), (K, V)](self, (context, pid, iter) =\u003e iter.map { case (k, v) =\u003e (k, cleanF(v)) }, preservesPartitioning = true) } 实际调用了PairRDDFunctions中的mapValue方法，最终生成的依然是MapPartitionsRDD，但有两点不同，一是目标函数只对value进行转换，二是preservePartitioning为true。这里很好理解，map函数会对键值对进行操作，partitioner可能会失效，而mapVlaues只对value进行操作，不影响key，所以partitioner依然保持。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:2","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"filter val resultRDD = inputRDD.filter(r =\u003e r._1 % 2 == 0) filter对输入RDD中的每条记录进行func操作，如果结果为true，则保留这条记录，所有保留的记录形成新的RDD def filter(f: T =\u003e Boolean): RDD[T] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[T, T]( this, (_, _, iter) =\u003e iter.filter(cleanF), preservesPartitioning = true) } filter操作后生成的RDD依然是MapPartitionsRDD，没有修改键值对，preservesPartitioning为true。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:3","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"filterByRange val resultRDD = inputRDD.filterByRange(2, 4) filterByRange对输入RDD中的数据进行过滤，只保留[lower, upper]之间的记录。 def filterByRange(lower: K, upper: K): RDD[P] = self.withScope { def inRange(k: K): Boolean = ordering.gteq(k, lower) \u0026\u0026 ordering.lteq(k, upper) val rddToFilter: RDD[P] = self.partitioner match { case Some(rp: RangePartitioner[_, _]) =\u003e // getPartition获取分区号，partitionIndices表示可能包含目标记录的分区id Range val partitionIndices = (rp.getPartition(lower), rp.getPartition(upper)) match { case (l, u) =\u003e Math.min(l, u) to Math.max(l, u) } PartitionPruningRDD.create(self, partitionIndices.contains) case _ =\u003e self } rddToFilter.filter { case (k, v) =\u003e inRange(k) } } filterByRange操作属于OrderedRDDFunctions，如果RDD通过RangePartitioner分区，这个操作可以执行的更加高效，仅需要对可能包含匹配元素的分区进行扫描，否则需要对所有分区应用filter。 @DeveloperApi object PartitionPruningRDD { def create[T](rdd: RDD[T], partitionFilterFunc: Int =\u003e Boolean): PartitionPruningRDD[T] = { new PartitionPruningRDD[T](rdd, partitionFilterFunc)(rdd.elementClassTag) } } @DeveloperApi class PartitionPruningRDD[T: ClassTag]( prev: RDD[T], partitionFilterFunc: Int =\u003e Boolean) extends RDD[T](prev.context, List(new PruneDependency(prev, partitionFilterFunc))) { PartitionPruningRDD用于RDD分区的剪枝，避免对所有分区进行操作。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:4","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"flatMap val array = Array[String]( \"how do you do\", \"are you ok\", \"thanks\", \"bye bye\", \"I'm ok\" ) val inputRDD = sc.parallelize( ArraySeq.unsafeWrapArray(array) , 3) val resultRDD = inputRDD.flatMap(x =\u003e x.split(\" \")) 对输入RDD中每个元素（如List）执行func操作，得到新元素，然后将所有新元素组合得到新RDD。例如输入RDD中某个分区包含两个元素List(1, 2)和List(3, 4)，func是对List中的每个元素加1，那么最后得到的新RDD中该分区的元素为(2, 3, 4, 5)，实例代码会做分词操作，组成新的RDD。 def flatMap[U: ClassTag](f: T =\u003e IterableOnce[U]): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) =\u003e iter.flatMap(cleanF)) } flatMap最终返回的也是MapPartitionsRDD，对每个分区的iter调用flatMap函数 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:5","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"flatMapValues val array = Array[(Int, String)]( (1, \"how do you do\"), (2, \"are you ok\"), (4, \"thanks\"), (5, \"bye bye\"), (2, \"I'm ok\") ) val inputRDD = sc.parallelize( ArraySeq.unsafeWrapArray(array) , 3) val resultRDD = inputRDD.flatMapValues(x =\u003e x.split(\" \")) 与flatMap类似，但只对RDD中\u003cK, V\u003e record中Value进行操作。 def flatMapValues[U](f: V =\u003e IterableOnce[U]): RDD[(K, U)] = self.withScope { val cleanF = self.context.clean(f) new MapPartitionsRDD[(K, U), (K, V)](self, (context, pid, iter) =\u003e iter.flatMap { case (k, v) =\u003e cleanF(v).iterator.map(x =\u003e (k, x)) }, preservesPartitioning = true) } flatMapValues同样属于PairRDDFunction类，通过flatMap操作\u003cK, V\u003e record中的value，但不改变key，flatMapValues保持原先RDD的分区特性。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:6","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"sample val array = Array[(Int, Char)]( (1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (2, 'e'), (3, 'f'), (2, 'g'), (1, 'h')) val inputRDD = sc.parallelize( ArraySeq.unsafeWrapArray(array) , 3) val sampleRDD = inputRDD.sample(false, 0.5) 对RDD中的数据进行抽样。 def sample( withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] = { require(fraction \u003e= 0, s\"Fraction must be nonnegative, but got ${fraction}\") withScope { if (withReplacement) { new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed) } else { new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed) } } } sample函数用于对RDD抽样，withRepalcement表示抽样是否有放回，fraction表示抽样比例，在有放回抽样中表示每个元素期望被选中的次数，fraction \u003e= 0，使用泊松抽样，在无放回抽样中表示每个元素被选中的概率，fraction 在0到1之间，使用伯努利抽样。不能保证抽样数量精确的等于给定RDD记录总数 * fraction。 private[spark] class PartitionwiseSampledRDD[T: ClassTag, U: ClassTag]( prev: RDD[T], sampler: RandomSampler[T, U], preservesPartitioning: Boolean, @transient private val seed: Long = Utils.random.nextLong) extends RDD[U](prev) { PartitionwiseSampledRDD表示从父 RDD 的各个分区分别进行抽样而生成的 RDD，对于父RDD的每个分区，一个RandomSampler实例被用于获得这个分区中记录的随机抽样结果。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:7","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"sampleByKey val array = Array[(Int, Char)]( (1, 'a'), (2, 'b'), (1, 'c'), (2, 'd'), (2, 'e'), (1, 'f'), (2, 'g'), (1, 'h')) val inputRDD = sc.parallelize( ArraySeq.unsafeWrapArray(array) , 3) val map = Map((1 -\u003e 0.8), (2 -\u003e 0.5)) val sampleRDD = inputRDD.sampleByKey(false, map) 对输入RDD中的数据进行抽样，为每个Key设置抽样比例。 def sampleByKey(withReplacement: Boolean, fractions: Map[K, Double], seed: Long = Utils.random.nextLong): RDD[(K, V)] = self.withScope { require(fractions.values.forall(v =\u003e v \u003e= 0.0), \"Negative sampling rates.\") val samplingFunc = if (withReplacement) { StratifiedSamplingUtils.getPoissonSamplingFunction(self, fractions, false, seed) } else { StratifiedSamplingUtils.getBernoulliSamplingFunction(self, fractions, false, seed) } self.mapPartitionsWithIndex(samplingFunc, preservesPartitioning = true, isOrderSensitive = true) } 使用简单随机抽样并仅遍历一次RDD，根据fractions为不同的键指定不同的采样率，从该RDD创建一个样本，所生成的样本大小大致等于对所有剪枝执行math.ceil(numItems * samplingRate)的总和。 mapPartitionsWithIndex通过对RDD的每个分区应用目标函数得到新的RDD，同时跟踪原来分区的index。应该是将index传入，来保证不同分区获得不同的随机性（只是猜测）。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:8","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"mapPartitions def mapPartitions[U: ClassTag]( f: Iterator[T] =\u003e Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) =\u003e cleanedF(iter), preservesPartitioning) } mapPartitions对输入RDD中的每个分区进行func处理，输出新的一组数据，相较于map操作，具有更大的自由度，可以以任意方式处理整个分区的数据，而不是只能逐条遍历分区中的记录。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:9","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"mapPartitionsWithIndex private[spark] def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) =\u003e Iterator[U], preservesPartitioning: Boolean, isOrderSensitive: Boolean): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) =\u003e cleanedF(index, iter), preservesPartitioning, isOrderSensitive = isOrderSensitive) } mapPartitionsWithIndex和mapPartitions语义类似，只是多传入了partition Id，利用这个id，可以实现对不同分区分别处理，比如之前sampleByKey操作就利用了partition Id。 val list = List(1, 2, 3, 4, 5, 6, 7, 8, 9) val inputRDD = sc.parallelize( list, 3 ) val resultRDD = inputRDD.mapPartitionsWithIndex((pid, iter) =\u003e { iter.map(Value =\u003e s\"Pid: ${pid}, Value: ${Value}\") }) resultRDD.foreach(println) 比如可以利用这个函数打印RDD的内容，了解每个分区中有哪些数据。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:10","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"partitionBy val array = Array[(Int, Char)]( (1, 'a'), (2, 'b'), (1, 'c'), (2, 'd'), (2, 'e'), (1, 'f'), (2, 'g'), (1, 'h')) val inputRDD = sc.parallelize( ArraySeq.unsafeWrapArray(array) , 3) val resultRDD = inputRDD.partitionBy(new HashPartitioner(2)) val resultRDD2 = inputRDD.partitionBy(new RangePartitioner(2, inputRDD)) partitionBy使用新的partitioner对RDD进行分区，要求RDD是\u003cK, V\u003e类型。 def partitionBy(partitioner: Partitioner): RDD[(K, V)] = self.withScope { if (keyClass.isArray \u0026\u0026 partitioner.isInstanceOf[HashPartitioner]) { throw SparkCoreErrors.hashPartitionerCannotPartitionArrayKeyError() } if (self.partitioner == Some(partitioner)) { self } else { new ShuffledRDD[K, V, V](self, partitioner) } } partitionBy如果提供的partitioner和RDD原先的partitioner相同，则返回原来的RDD，否则返回ShuffledRDD。 @DeveloperApi class ShuffledRDD[K: ClassTag, V: ClassTag, C: ClassTag]( @transient var prev: RDD[_ \u003c: Product2[K, V]], part: Partitioner) extends RDD[(K, C)](prev.context, Nil) { ShuffledRDD表示shuffle后的RDD，即重新分区后的数据。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:11","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"groupByKey def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])] = self.withScope { groupByKey(new HashPartitioner(numPartitions)) } def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])] = self.withScope { // groupByKey shouldn't use map side combine because map side combine does not // reduce the amount of data shuffled and requires all map side data be inserted // into a hash table, leading to more objects in the old gen. val createCombiner = (v: V) =\u003e CompactBuffer(v) val mergeValue = (buf: CompactBuffer[V], v: V) =\u003e buf += v val mergeCombiners = (c1: CompactBuffer[V], c2: CompactBuffer[V]) =\u003e c1 ++= c2 val bufs = combineByKeyWithClassTag[CompactBuffer[V]]( createCombiner, mergeValue, mergeCombiners, partitioner, mapSideCombine = false) bufs.asInstanceOf[RDD[(K, Iterable[V])]] } 将RDD1中的\u003cK, V\u003e record按照key聚合在一起，形成K, List\u003cV\u003e，numPartitions表示生成的rdd2的分区个数。groupByKey的行为和父RDD的partitioner有关，如果父RDD和生成的子RDD的partitioiner相同，则不需要shuffle，否则需要进行shuffle。假如在这里指定分区数为3，子RDD的paritioner为HashPartitioner(3)，如果父RDD的partitioner相同，显然没有必要再进行一次shuffle。 def combineByKeyWithClassTag[C]( createCombiner: V =\u003e C, mergeValue: (C, V) =\u003e C, mergeCombiners: (C, C) =\u003e C, partitioner: Partitioner, mapSideCombine: Boolean = true, serializer: Serializer = null)(implicit ct: ClassTag[C]): RDD[(K, C)] = self.withScope { require(mergeCombiners != null, \"mergeCombiners must be defined\") // required as of Spark 0.9.0 // 如果key的类型为数组，则不支持map端聚合以及hash分区 if (keyClass.isArray) { if (mapSideCombine) { throw SparkCoreErrors.cannotUseMapSideCombiningWithArrayKeyError() } if (partitioner.isInstanceOf[HashPartitioner]) { throw SparkCoreErrors.hashPartitionerCannotPartitionArrayKeyError() } } val aggregator = new Aggregator[K, V, C]( self.context.clean(createCombiner), self.context.clean(mergeValue), self.context.clean(mergeCombiners)) // 如果partitioner相同 if (self.partitioner == Some(partitioner)) { self.mapPartitions(iter =\u003e { // 访问ThreadLocal变量，获取当前的taskContext val context = TaskContext.get() // aggregator创建ExternalAppendOnlyMap，用于实现combiner new InterruptibleIterator(context, aggregator.combineValuesByKey(iter, context)) }, preservesPartitioning = true) } else { // parttioner不相同，进行一次shuffle new ShuffledRDD[K, V, C](self, partitioner) .setSerializer(serializer) .setAggregator(aggregator) .setMapSideCombine(mapSideCombine) } } 在paritioner相同的情况下，调用了mapPartitions方法，实际的操作由aggregator.combineValuesByKey实现。 @DeveloperApi case class Aggregator[K, V, C] ( createCombiner: V =\u003e C, mergeValue: (C, V) =\u003e C, mergeCombiners: (C, C) =\u003e C) { def combineValuesByKey( iter: Iterator[_ \u003c: Product2[K, V]], context: TaskContext): Iterator[(K, C)] = { val combiners = new ExternalAppendOnlyMap[K, V, C](createCombiner, mergeValue, mergeCombiners) combiners.insertAll(iter) updateMetrics(context, combiners) combiners.iterator } def combineCombinersByKey( iter: Iterator[_ \u003c: Product2[K, C]], context: TaskContext): Iterator[(K, C)] = { val combiners = new ExternalAppendOnlyMap[K, C, C](identity, mergeCombiners, mergeCombiners) combiners.insertAll(iter) updateMetrics(context, combiners) combiners.iterator } /** Update task metrics after populating the external map. */ private def updateMetrics(context: TaskContext, map: ExternalAppendOnlyMap[_, _, _]): Unit = { Option(context).foreach { c =\u003e c.taskMetrics().incMemoryBytesSpilled(map.memoryBytesSpilled) c.taskMetrics().incDiskBytesSpilled(map.diskBytesSpilled) c.taskMetrics().incPeakExecutionMemory(map.peakMemoryUsedBytes) } } } Aggregator这个类有三个参数： createCombiner 用于从初值创建聚合结果，比如 a -\u003e list[a] mergeValue 将新的值加入聚合结果，比如 b -\u003e list[a, b] mergeCombiners 将两个聚合结果再聚合，比如 [c, d] -\u003e list[a, b, c, d] 可以看到combineValuesByKey操作创建了ExternalAppendOnlyMap，功能类似于hashmap，聚合操作使用传入的聚合函数，将分区中的所有数据插入map中聚合，ExternalAppendOnlyMap实现了吐磁盘，在完成插入后会更新内存的信息，并返回map的迭代器。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:12","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"reduceByKey def reduceByKey(func: (V, V) =\u003e V, numPartitions: Int): RDD[(K, V)] = self.withScope { reduceByKey(new HashPartitioner(numPartitions), func) } def reduceByKey(partitioner: Partitioner, func: (V, V) =\u003e V): RDD[(K, V)] = self.withScope { combineByKeyWithClassTag[V]((v: V) =\u003e v, func, func, partitioner) } reduceByKey使用reduce函数按key聚合，在map端先局地combine然后再在reduce端聚合。 groupByKey没有map端聚合的原因是即使聚合也不能减少传输的数据量和内存用量。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:13","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"aggregateByKey def aggregateByKey[U: ClassTag](zeroValue: U, numPartitions: Int)(seqOp: (U, V) =\u003e U, combOp: (U, U) =\u003e U): RDD[(K, U)] = self.withScope { aggregateByKey(zeroValue, new HashPartitioner(numPartitions))(seqOp, combOp) } def aggregateByKey[U: ClassTag](zeroValue: U, partitioner: Partitioner)(seqOp: (U, V) =\u003e U, combOp: (U, U) =\u003e U): RDD[(K, U)] = self.withScope { // Serialize the zero value to a byte array so that we can get a new clone of it on each key val zeroBuffer = SparkEnv.get.serializer.newInstance().serialize(zeroValue) val zeroArray = new Array[Byte](zeroBuffer.limit) zeroBuffer.get(zeroArray) lazy val cachedSerializer = SparkEnv.get.serializer.newInstance() val createZero = () =\u003e cachedSerializer.deserialize[U](ByteBuffer.wrap(zeroArray)) // We will clean the combiner closure later in `combineByKey` val cleanedSeqOp = self.context.clean(seqOp) combineByKeyWithClassTag[U]((v: V) =\u003e cleanedSeqOp(createZero(), v), cleanedSeqOp, combOp, partitioner) } aggregateByKey底层也是调用了combineByKey，可以看做是一个更加通用的reduceByKey，支持返回类型和value类型不一致，支持map端聚合函数和reduce聚合函数不相同。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:14","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"combineByKey def combineByKeyWithClassTag[C]( createCombiner: V =\u003e C, mergeValue: (C, V) =\u003e C, mergeCombiners: (C, C) =\u003e C, partitioner: Partitioner, mapSideCombine: Boolean = true, serializer: Serializer = null)(implicit ct: ClassTag[C]): RDD[(K, C)] = self.withScope { 前述的聚合函数都是基于combineByKey实现的，所以combineByKey也提供了最大的灵活性，比如aggregateByKey只能指定初始值，然而combineByKey可以通过函数为不同Key指定不同的初始值。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:15","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"foldByKey def foldByKey(zeroValue: V, numPartitions: Int)(func: (V, V) =\u003e V): RDD[(K, V)] = self.withScope { foldByKey(zeroValue, new HashPartitioner(numPartitions))(func) } def foldByKey( zeroValue: V, partitioner: Partitioner)(func: (V, V) =\u003e V): RDD[(K, V)] = self.withScope { // Serialize the zero value to a byte array so that we can get a new clone of it on each key val zeroBuffer = SparkEnv.get.serializer.newInstance().serialize(zeroValue) val zeroArray = new Array[Byte](zeroBuffer.limit) zeroBuffer.get(zeroArray) // When deserializing, use a lazy val to create just one instance of the serializer per task lazy val cachedSerializer = SparkEnv.get.serializer.newInstance() val createZero = () =\u003e cachedSerializer.deserialize[V](ByteBuffer.wrap(zeroArray)) val cleanedFunc = self.context.clean(func) combineByKeyWithClassTag[V]((v: V) =\u003e cleanedFunc(createZero(), v), cleanedFunc, cleanedFunc, partitioner) } foldByKey是一个简化的aggregateByKey，seqOp和combineOp共用一个func。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:16","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"cogroup/groupWith def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))] = self.withScope { cogroup(other, defaultPartitioner(self, other)) } def cogroup[W](other: RDD[(K, W)], partitioner: Partitioner) : RDD[(K, (Iterable[V], Iterable[W]))] = self.withScope { if (partitioner.isInstanceOf[HashPartitioner] \u0026\u0026 keyClass.isArray) { throw SparkCoreErrors.hashPartitionerCannotPartitionArrayKeyError() } val cg = new CoGroupedRDD[K](Seq(self, other), partitioner) cg.mapValues { case Array(vs, w1s) =\u003e (vs.asInstanceOf[Iterable[V]], w1s.asInstanceOf[Iterable[W]]) } } cogroup中文翻译成联合分组，将多个RDD中具有相同Key的Value聚合在一起，假设rdd1包含\u003cK, V\u003e record，rdd2包含\u003cK, W\u003e record，则两者聚合结果为\u003cK, (List\u003cV\u003e, List\u003cW\u003e)。这个操作还有另一个名字groupwith。 cogroup操作实际生成了两个RDD，CoGroupedRDD将数据聚合在一起，MapPartitionsRDD仅对结果的数据类型进行转换。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:17","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"join def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))] = self.withScope { join(other, defaultPartitioner(self, other)) } def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))] = self.withScope { this.cogroup(other, partitioner).flatMapValues( pair =\u003e for (v \u003c- pair._1.iterator; w \u003c- pair._2.iterator) yield (v, w) ) } join和SQL中的join类似，将两个RDD中相同key的value联接起来，假设rdd1中的数据为\u003cK, V\u003e，rdd2中的数据为\u003cK, W\u003e，那么join之后的结果为\u003cK, (V, W)\u003e。在实现中，join首先调用了cogroup生成CoGroupedRDD和MapPartitionedRDD，然后使用flatMapValues计算相同key下value的笛卡尔积。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:18","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"cartesian def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { new CartesianRDD(sc, this, other) } cartesian操作生成两个RDD的笛卡尔积，假设RDD1中的分区个数为m，rdd2中的分区个数为n，cartesian操作会生成m * n个分区，rdd1和rdd2中的分区两两组合，组合后形成CartesianRDD中的一个分区，该分区中的数据是rdd1和rdd2相应的两个分区中数据的笛卡尔积。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:19","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"sortByKey def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length) : RDD[(K, V)] = self.withScope { val part = new RangePartitioner(numPartitions, self, ascending) new ShuffledRDD[K, V, V](self, part) .setKeyOrdering(if (ascending) ordering else ordering.reverse) } sortByKey对rdd1中\u003cK, V\u003e record进行排序，注意只对key进行排序，在相同Key的情况相爱，并不对value进行排序。sortByKey首先通过range划分将数据分布到shuffledRDD的不同分区中，可以保证在生成的RDD中，partition1中的所有record的key小于（或大于）partition2中所有record的key。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:20","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"coalesce def coalesce(numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null) : RDD[T] = withScope { require(numPartitions \u003e 0, s\"Number of partitions ($numPartitions) must be positive.\") if (shuffle) { /** Distributes elements evenly across output partitions, starting from a random partition. */ val distributePartition = (index: Int, items: Iterator[T]) =\u003e { var position = new XORShiftRandom(index).nextInt(numPartitions) items.map { t =\u003e // Note that the hash code of the key will just be the key itself. The HashPartitioner // will mod it with the number of total partitions. position = position + 1 (position, t) } } : Iterator[(Int, T)] // include a shuffle step so that our upstream tasks are still distributed new CoalescedRDD( new ShuffledRDD[Int, T, T]( mapPartitionsWithIndexInternal(distributePartition, isOrderSensitive = true), new HashPartitioner(numPartitions)), numPartitions, partitionCoalescer).values } else { new CoalescedRDD(this, numPartitions, partitionCoalescer) } } private[spark] def mapPartitionsWithIndexInternal[U: ClassTag]( f: (Int, Iterator[T]) =\u003e Iterator[U], preservesPartitioning: Boolean = false, isOrderSensitive: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) =\u003e f(index, iter), preservesPartitioning = preservesPartitioning, isOrderSensitive = isOrderSensitive) } coalesce用于将rdd的分区个数降低或者升高，在不使用shuffle的情况下，会直接生成CoalescedRDD，直接将相邻的分区合并，分区个数只能降低不能升高，当rdd中不同分区中的数据量差别较大时，直接合并容易造成数据倾斜（元素集中于少数分区中）。使用shffule直接解决数据倾斜问题，通过mapPartitionsWithIndex对输出RDD的每个分区进行操作，为原来的记录增加Key，Key是一个Int，对每个分区得到一个随机的起始位置，后续记录的Key是前一条记录的Key + 1，最后使用hash分组时相邻的记录会被分到不同的组。最终生成CoalescedRDD，并丢弃新生成的Key，通过map操作获取原来的记录。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:21","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"repartition def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { coalesce(numPartitions, shuffle = true) } repartition操作底层使用了coalesce的shuffle版本。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:22","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"repartitionAndSortWithinPartitions def repartitionAndSortWithinPartitions(partitioner: Partitioner): RDD[(K, V)] = self.withScope { if (self.partitioner == Some(partitioner)) { self.mapPartitions(iter =\u003e { val context = TaskContext.get() val sorter = new ExternalSorter[K, V, V](context, None, None, Some(ordering)) new InterruptibleIterator(context, sorter.insertAllAndUpdateMetrics(iter).asInstanceOf[Iterator[(K, V)]]) }, preservesPartitioning = true) } else { new ShuffledRDD[K, V, V](self, partitioner).setKeyOrdering(ordering) } } repartitionAndSortWithinPartitions可以灵活使用各种partitioner对数据进行分区，并且可以对输出RDD中的每个分区中的Key进行排序。这样相比于调用repartition然后在每个分区内排序效率更高，因为repartitionAndSortWithinPartitions可以将排序下推到shuffle机制中，注意结果只能保证是分区内有序，不能保证全局有序。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:23","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"intersection def intersection(other: RDD[T]): RDD[T] = withScope { this.map(v =\u003e (v, null)).cogroup(other.map(v =\u003e (v, null))) .filter { case (_, (leftGroup, rightGroup)) =\u003e leftGroup.nonEmpty \u0026\u0026 rightGroup.nonEmpty } .keys } intersection求rdd1和rdd2的交集，输出RDD不包含任何重复的元素。从实现中可以看到，首先通过map函数将record转化为\u003cK, V\u003e类型，V为固定值null，然后通过cogroup将rdd1和rdd2中的record聚合在一起，过滤掉为空的record，最后只保留key，得到交集元素。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:24","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"distinct def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { def removeDuplicatesInPartition(partition: Iterator[T]): Iterator[T] = { // Create an instance of external append only map which ignores values. val map = new ExternalAppendOnlyMap[T, Null, Null]( createCombiner = _ =\u003e null, mergeValue = (a, b) =\u003e a, mergeCombiners = (a, b) =\u003e a) map.insertAll(partition.map(_ -\u003e null)) map.iterator.map(_._1) } partitioner match { case Some(_) if numPartitions == partitions.length =\u003e mapPartitions(removeDuplicatesInPartition, preservesPartitioning = true) case _ =\u003e map(x =\u003e (x, null)).reduceByKey((x, _) =\u003e x, numPartitions).map(_._1) } } distinct是去重操作，对rdd中的数据进行去重，如果rdd已经有partitioner并且分区个数和预期分区个数相同，直接走分区内去重的逻辑，通过创建一个ExternalAppendOnlyMap，得到去重后的数据。其他情况下需要走shuffle逻辑，首先将record映射为\u003cK, V\u003e，V为固定值null，然后调用reduceByKey进行聚合，最终只保留key。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:25","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"union def union(other: RDD[T]): RDD[T] = withScope { sc.union(this, other) } def union[T: ClassTag](first: RDD[T], rest: RDD[T]*): RDD[T] = withScope { union(Seq(first) ++ rest) } def union[T: ClassTag](rdds: Seq[RDD[T]]): RDD[T] = withScope { // 过滤空的RDD val nonEmptyRdds = rdds.filter(!_.partitions.isEmpty) val partitioners = nonEmptyRdds.flatMap(_.partitioner).toSet if (nonEmptyRdds.forall(_.partitioner.isDefined) \u0026\u0026 partitioners.size == 1) { new PartitionerAwareUnionRDD(this, nonEmptyRdds) } else { new UnionRDD(this, nonEmptyRdds) } } union表示将rdd1和rdd2中的元素合并到一起。如果所有RDD的partitioner都相同，则构造PartitionerAwareUnionRDD，分区个数与rdd1和rdd2的分区个数相同，且输出RDD中每个分区中的数据都是rdd1和rdd2对应分区合并的结果。如果rdd1和rdd2的partitioner不同，合并后的RDD为UnionRDD，分区个数是rdd1和rdd2的分区个数之和，输出RDD中的每个分区也一一对应rdd1或者rdd2中的相应的分区。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:26","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"zip def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { zipPartitions(other, preservesPartitioning = false) { (thisIter, otherIter) =\u003e new Iterator[(T, U)] { def hasNext: Boolean = (thisIter.hasNext, otherIter.hasNext) match { case (true, true) =\u003e true case (false, false) =\u003e false case _ =\u003e throw SparkCoreErrors.canOnlyZipRDDsWithSamePartitionSizeError() } def next(): (T, U) = (thisIter.next(), otherIter.next()) } } } 将rdd1和rdd2中的元素按照一一对应关系连接在一起，构成\u003cK, V\u003e record。该操作要求rdd1和rdd2的分区个数相同，而且每个分区包含的元素个数相同。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:27","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"zipParitions def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B]) =\u003e Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD2(sc, sc.clean(f), this, rdd2, preservesPartitioning) } zipPartitions将rdd1和rdd2中的分区按照一一对应关系连接在一起，形成新的rdd。新的rdd中的每个分区的数据都通过对rdd1和rdd2中对应分区执行func函数得到，该操作要求rdd1和rdd2的分区个数相同，但不要求每个分区包含相同的元素个数。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:28","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"zipWithIndex def zipWithIndex(): RDD[(T, Long)] = withScope { new ZippedWithIndexRDD(this) } 对rdd1中的数据进行编号，编号方式是从0开始按序递增，直接返回ZippedWithIndexRDD ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:29","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"zipWtihUniqueId def zipWithUniqueId(): RDD[(T, Long)] = withScope { val n = this.partitions.length.toLong this.mapPartitionsWithIndex { case (k, iter) =\u003e Utils.getIteratorZipWithIndex(iter, 0L).map { case (item, i) =\u003e (item, i * n + k) } } } def getIteratorZipWithIndex[T](iter: Iterator[T], startIndex: Long): Iterator[(T, Long)] = { new Iterator[(T, Long)] { require(startIndex \u003e= 0, \"startIndex should be \u003e= 0.\") var index: Long = startIndex - 1L def hasNext: Boolean = iter.hasNext def next(): (T, Long) = { index += 1L (iter.next(), index) } } } 对rdd1中的数据进行编号，编号方式为round-robin，就像给每个人轮流发扑克牌，如果某些分区比较小，原本应该分给这个分区的编号会轮空，而不是分配给另一个分区。zipWithUniqueId通过mapPartitionsWithIndex实现，返回MapPartitionsRDD ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:30","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"subtractByKey def subtractByKey[W: ClassTag](other: RDD[(K, W)]): RDD[(K, V)] = self.withScope { subtractByKey(other, self.partitioner.getOrElse(new HashPartitioner(self.partitions.length))) } def subtractByKey[W: ClassTag](other: RDD[(K, W)], p: Partitioner): RDD[(K, V)] = self.withScope { new SubtractedRDD[K, V, W](self, other, p) } subtractByKey计算出key在rdd1中而不在rdd2中的record，逻辑类似于cogroup，但实现比CoGroupedRDD更加高效，生成SubtractedRDD。 使用rdd1的paritioner或者分区个数，因为结果集不会大于rdd1 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:31","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"subtract def subtract(other: RDD[T]): RDD[T] = withScope { subtract(other, partitioner.getOrElse(new HashPartitioner(partitions.length))) } def subtract( other: RDD[T], p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { if (partitioner == Some(p)) { // Our partitioner knows how to handle T (which, since we have a partitioner, is // really (K, V)) so make a new Partitioner that will de-tuple our fake tuples val p2 = new Partitioner() { override def numPartitions: Int = p.numPartitions override def getPartition(k: Any): Int = p.getPartition(k.asInstanceOf[(Any, _)]._1) } // Unfortunately, since we're making a new p2, we'll get ShuffleDependencies // anyway, and when calling .keys, will not have a partitioner set, even though // the SubtractedRDD will, thanks to p2's de-tupled partitioning, already be // partitioned by the right/real keys (e.g. p). this.map(x =\u003e (x, null)).subtractByKey(other.map((_, null)), p2).keys } else { this.map(x =\u003e (x, null)).subtractByKey(other.map((_, null)), p).keys } } 将record映射为\u003cK, V\u003e record，V为null，是一个比较常见的思路，这样可以复用代码。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:32","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"sortBy def sortBy[K]( f: (T) =\u003e K, ascending: Boolean = true, numPartitions: Int = this.partitions.length) (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope { this.keyBy[K](f) .sortByKey(ascending, numPartitions) .values } def keyBy[K](f: T =\u003e K): RDD[(K, T)] = withScope { val cleanedF = sc.clean(f) map(x =\u003e (cleanedF(x), x)) } sortBy基于func的计算结果对rdd1中的recorc进行排序，底层使用sortByKey实现。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:33","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"glom def glom(): RDD[Array[T]] = withScope { new MapPartitionsRDD[Array[T], T](this, (_, _, iter) =\u003e Iterator(iter.toArray)) } 将rdd1中的每个分区的record合并到一个list中，底层通过MapPartitionsRDD实现。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:1:34","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"常用action数据操作 action数据操作是用来对计算结果进行后处理，同时提交计算job。可以通过返回值区分一个操作是action还是transformation，transformation操作一般返回RDD类型，而action操作一般返回数值、数据结果（如Map）或者不返回任何值（比如写磁盘）。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:0","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"count def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum def getIteratorSize(iterator: Iterator[_]): Long = { if (iterator.knownSize \u003e= 0) iterator.knownSize.toLong else { var count = 0L while (iterator.hasNext) { count += 1L iterator.next() } count } } count操作首先计算每个分区中record的数目，然后在Driver端进行累加操作，返回rdd中包含的record个数。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:1","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"countByKey def countByKey(): Map[K, Long] = self.withScope { self.mapValues(_ =\u003e 1L).reduceByKey(_ + _).collect().toMap } countByKey统计rdd中每个key出现的次数，返回一个map，要求rdd是\u003cK, V\u003e类型。countByKey首先通过mapValues将\u003cK, V\u003e record中的Value设置为1，然后利用reduceByKey统计每个key出现的次数，最后使用collect方法将结果收集到Driver端。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:2","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"countByValue def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long] = withScope { map(value =\u003e (value, null)).countByKey() } countByValue并不是统计\u003cK, V\u003e record中每个Value出现的次数，而是统计每个record出现的次数。底层首先通过map函数将record转成\u003cK, V\u003e record，Value为null，然后调用countByKey统计Key的次数。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:3","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"collect def collect(): Array[T] = withScope { val results = sc.runJob(this, (iter: Iterator[T]) =\u003e iter.toArray) import org.apache.spark.util.ArrayImplicits._ Array.concat(results.toImmutableArraySeq: _*) } collect操作将rdd中的record收集到Driver端，返回类型为Array[T] ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:4","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"collectAsMap def collectAsMap(): Map[K, V] = self.withScope { val data = self.collect() val map = new mutable.HashMap[K, V] map.sizeHint(data.length) data.foreach { pair =\u003e map.put(pair._1, pair._2) } map } collectAsMap通过collect调用将\u003cK, V\u003e record收集到Driver端。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:5","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"foreach def foreach(f: T =\u003e Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) =\u003e iter.foreach(cleanF)) } 将rdd中的每个record按照func进行处理，底层调用runJob。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:6","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"foreachPartitions def foreachPartition(f: Iterator[T] =\u003e Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) =\u003e cleanF(iter)) } 将rdd中的每个分区中的数据按照func进行处理，底层调用runJob。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:7","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"fold def fold(zeroValue: T)(op: (T, T) =\u003e T): T = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) val cleanOp = sc.clean(op) val foldPartition = (iter: Iterator[T]) =\u003e iter.fold(zeroValue)(cleanOp) val mergeResult = (_: Int, taskResult: T) =\u003e jobResult = op(jobResult, taskResult) sc.runJob(this, foldPartition, mergeResult) jobResult } fold将rdd中的record按照func进行聚合，首先在rdd的每个分区中计算出局部结果即函数foldPartition，然后在Driver段将局部结果聚合成最终结果即函数mergeResult。需要注意的是，fold每次聚合是初始值zeroValue都会参与计算。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:8","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"reduce def reduce(f: (T, T) =\u003e T): T = withScope { val cleanF = sc.clean(f) val reducePartition: Iterator[T] =\u003e Option[T] = iter =\u003e { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } var jobResult: Option[T] = None val mergeResult = (_: Int, taskResult: Option[T]) =\u003e { if (taskResult.isDefined) { jobResult = jobResult match { case Some(value) =\u003e Some(f(value, taskResult.get)) case None =\u003e taskResult } } } sc.runJob(this, reducePartition, mergeResult) // Get the final result out of our Option, or throw an exception if the RDD was empty jobResult.getOrElse(throw SparkCoreErrors.emptyCollectionError()) } 将rdd中的record按照func进行聚合，这里没有提供初始值，所以需要处理空值的情况。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:9","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"aggregate def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =\u003e U, combOp: (U, U) =\u003e U): U = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance()) val cleanSeqOp = sc.clean(seqOp) val aggregatePartition = (it: Iterator[T]) =\u003e it.foldLeft(zeroValue)(cleanSeqOp) val mergeResult = (_: Int, taskResult: U) =\u003e jobResult = combOp(jobResult, taskResult) sc.runJob(this, aggregatePartition, mergeResult) jobResult } 将rdd中的record按照func进行聚合，这里提供了初始值，分区聚合和Driver端聚合都会使用初始值。 为什么已经有了reduceByKey、aggregateByKey等操作，还要定义aggreagte和reduce等操作呢？虽然reduceByKey、aggregateByKey等操作可以对每个分区中的record，以及跨分区且具有相同Key的record进行聚合，但这些聚合都是在部分数据上，类似于\u003cK, func(list(V))，而不是针对所有record进行全局聚合，即func(\u003cK, list(V))。 然而aggregate、reduce等操作存在相同的问题，当需要merge的部分结果很大时，数据传输量很大，而且Driver是单点merge，存在效率和内存空间限制的问题，为了解决这个问题，Spark对这些聚合操作进行了优化，提出了treeAggregate和treeReduce操作。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:10","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"treeAggregate def treeAggregate[U: ClassTag](zeroValue: U)( seqOp: (U, T) =\u003e U, combOp: (U, U) =\u003e U, depth: Int = 2): U = withScope { treeAggregate(zeroValue, seqOp, combOp, depth, finalAggregateOnExecutor = false) } def treeAggregate[U: ClassTag]( zeroValue: U, seqOp: (U, T) =\u003e U, combOp: (U, U) =\u003e U, depth: Int, finalAggregateOnExecutor: Boolean): U = withScope { require(depth \u003e= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") if (partitions.length == 0) { Utils.clone(zeroValue, context.env.closureSerializer.newInstance()) } else { val cleanSeqOp = context.clean(seqOp) val cleanCombOp = context.clean(combOp) val aggregatePartition = (it: Iterator[T]) =\u003e it.foldLeft(zeroValue)(cleanSeqOp) var partiallyAggregated: RDD[U] = mapPartitions(it =\u003e Iterator(aggregatePartition(it))) var numPartitions = partiallyAggregated.partitions.length val scale = math.max(math.ceil(math.pow(numPartitions, 1.0 / depth)).toInt, 2) // If creating an extra level doesn't help reduce // the wall-clock time, we stop tree aggregation. // Don't trigger TreeAggregation when it doesn't save wall-clock time while (numPartitions \u003e scale + math.ceil(numPartitions.toDouble / scale)) { numPartitions /= scale val curNumPartitions = numPartitions partiallyAggregated = partiallyAggregated.mapPartitionsWithIndex { (i, iter) =\u003e iter.map((i % curNumPartitions, _)) }.foldByKey(zeroValue, new HashPartitioner(curNumPartitions))(cleanCombOp).values } if (finalAggregateOnExecutor \u0026\u0026 partiallyAggregated.partitions.length \u003e 1) { // map the partially aggregated rdd into a key-value rdd // do the computation in the single executor with one partition // get the new RDD[U] partiallyAggregated = partiallyAggregated .map(v =\u003e (0.toByte, v)) .foldByKey(zeroValue, new ConstantPartitioner)(cleanCombOp) .values } val copiedZeroValue = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) partiallyAggregated.fold(copiedZeroValue)(cleanCombOp) } } treeAggreagte是为了解决aggregate在Driver端聚合导致的数据传输量大、单点merge、内存空间限制等问题，思路类似于归并排序的层次归并，每层都将分区数目降低为原来的1/scale，也就是一颗近似完美的平衡树，让每层每个节点的负载都相对合理。我们可以在参数中指定depth，假设分区数量为N，则近似有N / (scale^depth) = 1。当然Spark在何时停止局部聚合做了优化，平衡效率和开销，选择在numPartitions \u003e scale + math.ceil(numPartitions.toDouble / scale时停止局部聚合，numPartitions表示当前分区数，numParttions/scale表示如果继续局部聚合下一层的分区数，为什么会有一个额外的scale，我认为应该是为了避免极端情况，比如分区数为2，scale为2， 那么如果没有额外的scale作为成本，这里会继续局部聚合，然后有了额外的scale。 实现上局部聚合使用了foldByKey，尽管形式上使用了ShuffleDependency，但是由于每个分区中只有一条记录，实际数据传输时类似于多对一的NarrowDependency。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:11","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"treeReduce treeReduce是reduce的优化版本。底层实际上调用了treeAggregate。 def treeReduce(f: (T, T) =\u003e T, depth: Int = 2): T = withScope { require(depth \u003e= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") val cleanF = context.clean(f) val reducePartition: Iterator[T] =\u003e Option[T] = iter =\u003e { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } val partiallyReduced = mapPartitions(it =\u003e Iterator(reducePartition(it))) val op: (Option[T], Option[T]) =\u003e Option[T] = (c, x) =\u003e { if (c.isDefined \u0026\u0026 x.isDefined) { Some(cleanF(c.get, x.get)) } else if (c.isDefined) { c } else if (x.isDefined) { x } else { None } } partiallyReduced.treeAggregate(Option.empty[T])(op, op, depth) .getOrElse(throw SparkCoreErrors.emptyCollectionError()) } ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:12","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"reduceByKeyLocally def reduceByKeyLocally(func: (V, V) =\u003e V): Map[K, V] = self.withScope { val cleanedF = self.sparkContext.clean(func) if (keyClass.isArray) { throw SparkCoreErrors.reduceByKeyLocallyNotSupportArrayKeysError() } val reducePartition = (iter: Iterator[(K, V)]) =\u003e { val map = new JHashMap[K, V] iter.foreach { pair =\u003e val old = map.get(pair._1) map.put(pair._1, if (old == null) pair._2 else cleanedF(old, pair._2)) } Iterator(map) } : Iterator[JHashMap[K, V]] val mergeMaps = (m1: JHashMap[K, V], m2: JHashMap[K, V]) =\u003e { m2.asScala.foreach { pair =\u003e val old = m1.get(pair._1) m1.put(pair._1, if (old == null) pair._2 else cleanedF(old, pair._2)) } m1 } : JHashMap[K, V] self.mapPartitions(reducePartition).reduce(mergeMaps).asScala } reduceByKeyLocally首先在rdd的各个分区中进行聚合，并使用HashMap来存储聚合结果，然后将数据汇总到Driver端进行全局聚合，仍然是将聚合结果存在到HashMap。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:13","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"take def take(num: Int): Array[T] = withScope { val scaleUpFactor = Math.max(conf.get(RDD_LIMIT_SCALE_UP_FACTOR), 2) if (num == 0) { new Array[T](0) } else { val buf = new ArrayBuffer[T] val totalParts = this.partitions.length var partsScanned = 0 while (buf.size \u003c num \u0026\u0026 partsScanned \u003c totalParts) { // The number of partitions to try in this iteration. It is ok for this number to be // greater than totalParts because we actually cap it at totalParts in runJob. var numPartsToTry = conf.get(RDD_LIMIT_INITIAL_NUM_PARTITIONS) val left = num - buf.size if (partsScanned \u003e 0) { // If we didn't find any rows after the previous iteration, multiply by // limitScaleUpFactor and retry. Otherwise, interpolate the number of partitions we need // to try, but overestimate it by 50%. We also cap the estimation in the end. if (buf.isEmpty) { numPartsToTry = partsScanned * scaleUpFactor } else { // As left \u003e 0, numPartsToTry is always \u003e= 1 numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor) } } val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts)) val res = sc.runJob(this, (it: Iterator[T]) =\u003e it.take(left).toArray, p) res.foreach(buf ++= _.take(num - buf.size)) partsScanned += p.size } buf.toArray } } take表示从rdd中取出前num个record。take操作首先取出rdd中第一个分区的前num个record，如果num大于partition1中record的总数，则take会继续从后续的分区中取出record，为了提高效率，spark会根据前面分区分区的平均大小估计后续需要取几个分区来满足take的需求。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:14","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"first def first(): T = withScope { take(1) match { case Array(t) =\u003e t case _ =\u003e throw SparkCoreErrors.emptyCollectionError() } } 只取出rdd中的第一个record。底层通过take(1)实现。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:15","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"takeOrdered def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { if (num == 0 || this.getNumPartitions == 0) { Array.empty } else { this.mapPartitionsWithIndex { case (pid, iter) =\u003e if (iter.nonEmpty) { // Priority keeps the largest elements, so let's reverse the ordering. Iterator.single(collectionUtils.takeOrdered(iter, num)(ord).toArray) } else if (pid == 0) { // make sure partition 0 always returns an array to avoid reduce on empty RDD Iterator.single(Array.empty[T]) } else { Iterator.empty } }.reduce { (array1, array2) =\u003e val size = math.min(num, array1.length + array2.length) val array = Array.ofDim[T](size) collectionUtils.mergeOrdered[T](Seq(array1, array2))(ord).copyToArray(array, 0, size) array } } } 取出rdd中最小的num个record。首先使用mapPartitionsWithIndex在每个分区中找出最小的num个record，因为全局最小的n个元素一定是每个分区中最小的n个元素的子集，然后通过reduce操作将这些record收集到Driver段，进行排序，然后取出前num个record。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:16","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"top def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { takeOrdered(num)(ord.reverse) } 取出rdd中最大的num个record。底层通过takeOrdered实现。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:17","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"max/min def max()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.max) } def min()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.min) } 返回rdd中的最大、最小值。底层基于reduce实现。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:18","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"isEmpty def isEmpty(): Boolean = withScope { partitions.length == 0 || take(1).length == 0 } 判断rdd是否为空，如果rdd不包含任何record，则返回true。如果分区数为0，则rdd一定为空，分区数大于0并不意味着rdd一定不为空，需要通过take(1)判断是否有数据。如果对rdd执行一些数据操作，比如过滤、求交集等，rdd为空的话，那么执行其他操作也一定为空，因此，提前判断rdd是否为空，可以避免提交冗余的job。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:19","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"lookup def lookup(key: K): Seq[V] = self.withScope { self.partitioner match { case Some(p) =\u003e val index = p.getPartition(key) val process = (it: Iterator[(K, V)]) =\u003e { val buf = new ArrayBuffer[V] for (pair \u003c- it if pair._1 == key) { buf += pair._2 } buf.toSeq } : Seq[V] val res = self.context.runJob(self, process, Array(index).toImmutableArraySeq) res(0) case None =\u003e self.filter(_._1 == key).map(_._2).collect().toImmutableArraySeq } } loopup函数找出rdd中包含特定key的value，将这些value形成List。loopup首先过滤出给定key的record，然后使用map得到相应的value，最后使用collect将这些value收集到Driver端形成list。如果rdd的partitioner已经确定，那么在过滤前，通过getPartition确定key所在的分区，减少操作的数据量。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:20","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"saveAsTextFile def saveAsTextFile(path: String): Unit = withScope { saveAsTextFile(path, null) } def saveAsTextFile(path: String, codec: Class[_ \u003c: CompressionCodec]): Unit = withScope { this.mapPartitions { iter =\u003e val text = new Text() iter.map { x =\u003e require(x != null, \"text files do not allow null rows\") text.set(x.toString) (NullWritable.get(), text) } }.saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path, codec) } saveAsTextFile将rdd保存成文本文件，通过toString操作获取record的字符串形式，然后将record转化为\u003cNullWriter, Text\u003e类型，NullWriter的意思是控血，也就是每条输出数据只包含类似为文本的Value。底层调用saveAsHadoopFile。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:21","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"saveAsObjectFile def saveAsObjectFile(path: String): Unit = withScope { this.mapPartitions(iter =\u003e iter.grouped(10).map(_.toArray)) .map(x =\u003e (NullWritable.get(), new BytesWritable(Utils.serialize(x)))) .saveAsSequenceFile(path) } saveAsObjectFile将rdd保存为序列化对象形式的SequenceFile，针对普通对象类型，将record惊醒序列化，并且以每10个record为1组转化为SequenceFile\u003cNullableWritable, Array[Object]\u003e，调用saveAsSequenceFile将文件写入HDFS中。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:22","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"saveAsSequenceFile def saveAsSequenceFile( path: String, codec: Option[Class[_ \u003c: CompressionCodec]] = None): Unit = self.withScope { def anyToWritable[U: IsWritable](u: U): Writable = u // TODO We cannot force the return type of `anyToWritable` be same as keyWritableClass and // valueWritableClass at the compile time. To implement that, we need to add type parameters to // SequenceFileRDDFunctions. however, SequenceFileRDDFunctions is a public class so it will be a // breaking change. val convertKey = self.keyClass != _keyWritableClass val convertValue = self.valueClass != _valueWritableClass logInfo(log\"Saving as sequence file of type \" + log\"(${MDC(LogKeys.KEY, _keyWritableClass.getSimpleName)},\" + log\"${MDC(LogKeys.VALUE, _valueWritableClass.getSimpleName)})\") val format = classOf[SequenceFileOutputFormat[Writable, Writable]] val jobConf = new JobConf(self.context.hadoopConfiguration) if (!convertKey \u0026\u0026 !convertValue) { self.saveAsHadoopFile(path, _keyWritableClass, _valueWritableClass, format, jobConf, codec) } else if (!convertKey \u0026\u0026 convertValue) { self.map(x =\u003e (x._1, anyToWritable(x._2))).saveAsHadoopFile( path, _keyWritableClass, _valueWritableClass, format, jobConf, codec) } else if (convertKey \u0026\u0026 !convertValue) { self.map(x =\u003e (anyToWritable(x._1), x._2)).saveAsHadoopFile( path, _keyWritableClass, _valueWritableClass, format, jobConf, codec) } else if (convertKey \u0026\u0026 convertValue) { self.map(x =\u003e (anyToWritable(x._1), anyToWritable(x._2))).saveAsHadoopFile( path, _keyWritableClass, _valueWritableClass, format, jobConf, codec) } } saveAsSequenceFile将rdd保存为SequenceFile形式的文件，针对\u003cK, V\u003e 类型的record，将record进行序列化后，以SequenceFile形式写入分布式文件系统中，底层调用saveAsHadoopFile实现。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:23","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"saveAsHadoopFile def saveAsHadoopFile( path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ \u003c: OutputFormat[_, _]], conf: JobConf = new JobConf(self.context.hadoopConfiguration), codec: Option[Class[_ \u003c: CompressionCodec]] = None): Unit = self.withScope { // Rename this as hadoopConf internally to avoid shadowing (see SPARK-2038). val hadoopConf = conf hadoopConf.setOutputKeyClass(keyClass) hadoopConf.setOutputValueClass(valueClass) conf.setOutputFormat(outputFormatClass) for (c \u003c- codec) { hadoopConf.setCompressMapOutput(true) hadoopConf.set(\"mapreduce.output.fileoutputformat.compress\", \"true\") hadoopConf.setMapOutputCompressorClass(c) hadoopConf.set(\"mapreduce.output.fileoutputformat.compress.codec\", c.getCanonicalName) hadoopConf.set(\"mapreduce.output.fileoutputformat.compress.type\", CompressionType.BLOCK.toString) } // Use configured output committer if already set if (conf.getOutputCommitter == null) { hadoopConf.setOutputCommitter(classOf[FileOutputCommitter]) } // When speculation is on and output committer class name contains \"Direct\", we should warn // users that they may loss data if they are using a direct output committer. val speculationEnabled = self.conf.get(SPECULATION_ENABLED) val outputCommitterClass = hadoopConf.get(\"mapred.output.committer.class\", \"\") if (speculationEnabled \u0026\u0026 outputCommitterClass.contains(\"Direct\")) { val warningMessage = log\"${MDC(CLASS_NAME, outputCommitterClass)} \" + log\"may be an output committer that writes data directly to \" + log\"the final location. Because speculation is enabled, this output committer may \" + log\"cause data loss (see the case in SPARK-10063). If possible, please use an output \" + log\"committer that does not have this behavior (e.g. FileOutputCommitter).\" logWarning(warningMessage) } FileOutputFormat.setOutputPath(hadoopConf, SparkHadoopWriterUtils.createPathFromString(path, hadoopConf)) saveAsHadoopDataset(hadoopConf) } def saveAsHadoopDataset(conf: JobConf): Unit = self.withScope { val config = new HadoopMapRedWriteConfigUtil[K, V](new SerializableJobConf(conf)) SparkHadoopWriter.write( rdd = self, config = config) } saveAsHadoopFile将rdd保存为Haddop HDFS文件系统支持的文件，进行必要的初始化和配置后，通过SparkHadoopWriter将rdd写入hadoop中。 ","date":"2025-05-11","objectID":"/posts/spark_logical_plan/:2:24","tags":["Spark"],"title":"Spark逻辑处理流程","uri":"/posts/spark_logical_plan/"},{"categories":["Spark"],"content":"RDD数据模型 RDD （Resilient Distributed DataSet)是spark对计算过程中输入输出数据以及中间数据的抽象，表示不可变、分区的集合数据，可以被并行处理。 abstract class RDD[T: ClassTag]( @transient private var _sc: SparkContext, @transient private var deps: Seq[Dependency[_]] ) extends Serializable with Logging { RDD类包含一些基础操作，比如map、filter和persist，另外 PairRDDFunctions包含专门处理键值对RDD的操作，比如groupByKey和join DoubleRDDFunctions包含数据为Double类型的RDD的操作 SequenceFileRDDFunction包含可以被保存为SequenceFiels的RDD的操作 OrderedRDDFunctions键值对RDD，key通过隐式转换后支持排序 RDD主要有5种属性： 分区列表 计算每个分区的函数 对其他RDD的依赖组成的依赖链表 可选，键值对RDD进行分区的Partitioner 比如某个RDD是hash分区的 可选，计算每个分区的本地化偏好列表，比如依据hdfs文件的block位置给定偏好，降低网络传输开销 ","date":"2025-05-10","objectID":"/posts/spark-base/:1:0","tags":["Spark"],"title":"Spark基础知识","uri":"/posts/spark-base/"},{"categories":["Spark"],"content":"RDD常用属性 SparkContext RDD所属的上下文 Seq[Dependency[_]] 当前RDD依赖的RDD列表 Option[Partitioner] partitioner，可以被子类重写，表示RDD是如何分区的 Array[Partition] RDD拥有的所有分区 ","date":"2025-05-10","objectID":"/posts/spark-base/:1:1","tags":["Spark"],"title":"Spark基础知识","uri":"/posts/spark-base/"},{"categories":["Spark"],"content":"Partition /** * An identifier for a partition in an RDD. */ trait Partition extends Serializable { /** * Get the partition's index within its parent RDD */ def index: Int // A better default implementation of HashCode override def hashCode(): Int = index override def equals(other: Any): Boolean = super.equals(other) } Partition表示RDD中的一个分区 private[spark] class PartitionPruningRDDPartition(idx: Int, val parentSplit: Partition) extends Partition { override val index = idx } PartitionPruningRDDPartition表示父RDD被剪枝后生成的子RDD中的分区。idx表示子RDD中分区的partition Id，parentsplit表示对应的父RDD中的分区。 ","date":"2025-05-10","objectID":"/posts/spark-base/:1:2","tags":["Spark"],"title":"Spark基础知识","uri":"/posts/spark-base/"},{"categories":["Spark"],"content":"Partitioner abstract class Partitioner extends Serializable { def numPartitions: Int def getPartition(key: Any): Int } Partitioner定义了键值对RDD中的元素如何通过key进行分区，映射每个key到一个partition ID，从0到 numPartitions - 1。注意partitioner必须是确定性的，给定相同的partition key必须返回相同的分区。 HashPartitioner class HashPartitioner(partitions: Int) extends Partitioner { require(partitions \u003e= 0, s\"Number of partitions ($partitions) cannot be negative.\") def numPartitions: Int = partitions def getPartition(key: Any): Int = key match { case null =\u003e 0 case _ =\u003e Utils.nonNegativeMod(key.hashCode, numPartitions) } override def equals(other: Any): Boolean = other match { case h: HashPartitioner =\u003e h.numPartitions == numPartitions case _ =\u003e false } override def hashCode: Int = numPartitions } HashPartitioner使用java的Object.hashCode实现了基于hash的分区，java数据的hashCode基于数据的identity而不是他们的内容，所以尝试对RDD[Array[_]]或者RDD[(Array[_], _)]使用HashPartitioner将产生非预期效果。 RangePartitioner class RangePartitioner[K : Ordering : ClassTag, V]( partitions: Int, rdd: RDD[_ \u003c: Product2[K, V]], private var ascending: Boolean = true, val samplePointsPerPartitionHint: Int = 20) extends Partitioner { RangePartitioner将可排序的几率按范围划分成大致相等的区间，范围是通过对传入的RDD进行采样确定的。分区的实际数量可能和partitions参数不一致，比如当采样的记录少于partitions时。 def getPartition(key: Any): Int = { val k = key.asInstanceOf[K] var partition = 0 if (rangeBounds.length \u003c= 128) { // 分区个数很少，没有必要走二分查找 while (partition \u003c rangeBounds.length \u0026\u0026 ordering.gt(k, rangeBounds(partition))) { partition += 1 } } else { // Determine which binary search method to use only once. partition = binarySearch(rangeBounds, k) // binarySearch either returns the match location or -[insertion point]-1 if (partition \u003c 0) { partition = -partition-1 } if (partition \u003e rangeBounds.length) { partition = rangeBounds.length } } if (ascending) { partition } else { rangeBounds.length - partition } } partitioner最重要的函数getPartition，用于确定某个\u003cK, V\u003e record应该分到哪个partition。 // 前partitions - 1个分区的上边界 private var rangeBounds: Array[K] = { if (partitions \u003c= 1) { Array.empty } else { // 为了使输出分区大致平衡所需要的采样数据量，最大上限为100万 val sampleSize = math.min(samplePointsPerPartitionHint.toDouble * partitions, 1e6) // 假设输出的分区大致平衡，这里超采样一部分 val sampleSizePerPartition = math.ceil(3.0 * sampleSize / rdd.partitions.length).toInt val (numItems, sketched) = RangePartitioner.sketch(rdd.map(_._1), sampleSizePerPartition) if (numItems == 0L) { Array.empty } else { // 如果某个分区包含的元素数量远多余平均值，将对该分区重新采样，以确保从该分区中收集到足够的样本 // fraction表示样本数量和数据总量的比值 val fraction = math.min(sampleSize / math.max(numItems, 1L), 1.0) val candidates = ArrayBuffer.empty[(K, Float)] val imbalancedPartitions = mutable.Set.empty[Int] sketched.foreach { case (idx, n, sample) =\u003e // 按照比例当前分区应该抽样的平均数量高于实际采样数量，认为当前分区需要重采样 if (fraction * n \u003e sampleSizePerPartition) { imbalancedPartitions += idx } else { // weight是采样概率的倒数，举个例子，假设有两个分区，都采样了30个样本 // 但a分区大小为300，b分区大小为60，显然a和b分区采样的每个样本应该占的权重不同 // weight的作用就在于此 val weight = (n.toDouble / sample.length).toFloat for (key \u003c- sample) { candidates += ((key, weight)) } } } if (imbalancedPartitions.nonEmpty) { // 仅对需要重新抽样的分区进行操作 val imbalanced = new PartitionPruningRDD(rdd.map(_._1), imbalancedPartitions.contains) val seed = byteswap32(-rdd.id - 1) // 使用sample进行抽样, 抽样的比例为fraction // 假设第一次抽样，总数为3000，抽样大小为30，平均抽样比例为0.1，所以进行重抽样，这次抽样占比为0.1，也就是300 val reSampled = imbalanced.sample(withReplacement = false, fraction, seed).collect() val weight = (1.0 / fraction).toFloat candidates ++= reSampled.map(x =\u003e (x, weight)) } // 如果采样的记录少于partitions，则最终的分区数量也会少于partitions RangePartitioner.determineBounds(candidates, math.min(partitions, candidates.size)) } } } def sketch[K : ClassTag]( rdd: RDD[K], sampleSizePerPartition: Int): (Long, Array[(Int, Long, Array[K])]) = { val shift = rdd.id // val classTagK = classTag[K] // to avoid serializing the entire partitioner object val sketched = rdd.mapPartitionsWithIndex { (idx, iter) =\u003e val seed = byteswap32(idx ^ (shift \u003c\u003c 16)) val (sample, n) = SamplingUtils.reservoirSampleAndCount( iter, sampleSizePerPartition, seed) Iterator((idx, n,","date":"2025-05-10","objectID":"/posts/spark-base/:1:3","tags":["Spark"],"title":"Spark基础知识","uri":"/posts/spark-base/"},{"categories":["Spark"],"content":"Dependency @DeveloperApi abstract class Dependency[T] extends Serializable { def rdd: RDD[T] } RDD依赖的基础类。 NarrowDependency @DeveloperApi abstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] { /** * Get the parent partitions for a child partition. * @param partitionId a partition of the child RDD * @return the partitions of the parent RDD that the child partition depends upon */ def getParents(partitionId: Int): Seq[Int] override def rdd: RDD[T] = _rdd } 窄依赖NarrowDependency，子RDD的每个分区依赖于父RDD的一小部分分区，窄依赖允许流水线执行，getParenets返回子RDD分区依赖的所有父RDD分区。 PruneDependency private[spark] class PruneDependency[T](rdd: RDD[T], partitionFilterFunc: Int =\u003e Boolean) extends NarrowDependency[T](rdd) { @transient val partitions: Array[Partition] = rdd.partitions .filter(s =\u003e partitionFilterFunc(s.index)).zipWithIndex // idx是子RDD的partition Id，从0开始 // split是对应的父RDD中的分区 .map { case(split, idx) =\u003e new PartitionPruningRDDPartition(idx, split) : Partition } override def getParents(partitionId: Int): List[Int] = { List(partitions(partitionId).asInstanceOf[PartitionPruningRDDPartition].parentSplit.index) } } PruneDependency是窄依赖的一种，子RDD中的分区是父RDD中分区剪枝后的子集，子RDD中的每个分区唯一依赖于父RDD的对应分区。 OneToOneDependency @DeveloperApi class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) { override def getParents(partitionId: Int): List[Int] = List(partitionId) } OneToOneDependency表示父rdd和子rdd的分区之间是一一映射关系。 RangeDependency /** * :: DeveloperApi :: * Represents a one-to-one dependency between ranges of partitions in the parent and child RDDs. * @param rdd the parent RDD * @param inStart the start of the range in the parent RDD * @param outStart the start of the range in the child RDD * @param length the length of the range */ @DeveloperApi class RangeDependency[T](rdd: RDD[T], inStart: Int, outStart: Int, length: Int) extends NarrowDependency[T](rdd) { override def getParents(partitionId: Int): List[Int] = { if (partitionId \u003e= outStart \u0026\u0026 partitionId \u003c outStart + length) { List(partitionId - outStart + inStart) } else { Nil } } } RangeDependency表示父 RDD 和子 RDD 中分区范围之间的一对一依赖关系。依然是一一对应关系，但分区号可能不相同。 ShuffleDependency 先通过一个例子来说明ShuffleDependency的用途 def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))] = self.withScope { this.cogroup(other, partitioner).flatMapValues( pair =\u003e for (v \u003c- pair._1.iterator; w \u003c- pair._2.iterator) yield (v, w) ) } def cogroup[W](other: RDD[(K, W)], partitioner: Partitioner) : RDD[(K, (Iterable[V], Iterable[W]))] = self.withScope { if (partitioner.isInstanceOf[HashPartitioner] \u0026\u0026 keyClass.isArray) { throw SparkCoreErrors.hashPartitionerCannotPartitionArrayKeyError() } val cg = new CoGroupedRDD[K](Seq(self, other), partitioner) cg.mapValues { case Array(vs, w1s) =\u003e (vs.asInstanceOf[Iterable[V]], w1s.asInstanceOf[Iterable[W]]) } } // CoGroupedRDD override def getDependencies: Seq[Dependency[_]] = { rdds.map { rdd: RDD[_] =\u003e if (rdd.partitioner == Some(part)) { logDebug(\"Adding one-to-one dependency with \" + rdd) new OneToOneDependency(rdd) } else { logDebug(\"Adding shuffle dependency with \" + rdd) new ShuffleDependency[K, Any, CoGroupCombiner]( rdd.asInstanceOf[RDD[_ \u003c: Product2[K, _]]], part, serializer) } } } 假设有一个join操作，指定了结果RDD的Partitioner，内部调用了cogroup生成了CoGroupedRDD，并且将依赖的RDD都作为参数传入，如果依赖的RDD和指定的Partitioner相同，则是窄依赖，否则是宽依赖，生成ShufflDependency。 @DeveloperApi class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag]( @transient private val _rdd: RDD[_ \u003c: Product2[K, V]], val partitioner: Partitioner, val serializer: Serializer = SparkEnv.get.serializer, val keyOrdering: Option[Ordering[K]] = None, val aggregator: Option[Aggregator[K, V, C]] = None, val mapSideCombine: Boolean = false, val shuffleWriterProcessor: ShuffleWriteProcessor = new ShuffleWriteProcessor) extends Dependency[Product2[K, V]] with Logging { if (mapSideCombine) { require(aggregator.isDefined, \"Map-side combine without Aggregator specified!\") } override def rdd: RDD[Product2[K, V]] = _rdd.asInstanceOf[RDD[Product2[K, V]]] private[spar","date":"2025-05-10","objectID":"/posts/spark-base/:1:4","tags":["Spark"],"title":"Spark基础知识","uri":"/posts/spark-base/"},{"categories":["grpc"],"content":"gprc流程概括 grpc的流程可以大致分成两个阶段，分别为grpc连接阶段和grpc交互阶段，如图所示（此图来自后面的参考文献）。 在RPC连接阶段，client和server之间建立起TCP连接，grpc底层依赖于HTTP2，因此client和server还需要协调frame的相关设置，例如frame的大小，滑动窗口的大小等。 在RPC交互阶段，client将数据发送给server，并等待server执行执行method之后返回结果。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:1:0","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"Client的流程 在RPC连接阶段，client接收到一个目标地址和一系列的DialOptions，然后 配置连接参数，interceptor等，启动resolver Rosovler根据目标地址获取server的地址列表（比如一个DNS name可能会指向多个server ip，dnsResolver是grpc内置的resolver之一），启动balancer Balancer根据平衡策略，从诸多server地址中选择一个或者多个建立TCP连接 client在TCP连接建立完成之后，等待server发来的HTTP2 Setting frame，并调整自身的HTTP2相关配置，随后向server发送HTTP2 Setting frame 在RPC交互阶段，某个rpc方法被调用后 Client创建一个stream对象用来管理整个交互流程 Client将service name, method name等信息放到header frame中并发送给server client将method的参数信息放到data frame中并发送给server client等待server传回的header frame和data frame，一次rpc call的result status会被包含在header frame中，而method的返回值被包含在data frame中 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:1:1","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"Server流程 在rpc连接阶段，server在完成一些初始化的配置之后，开始监听某个tcp端口，在和某个client建立了tcp连接之后完成http2 settting frame的交互。 在rpc交互阶段： server等待client发来的header frame，从而创建出一个stream对象来管理真个交互流程，根据header frame中的信息，server知道client请求的是哪一个service的那一个method server接受到client发来的data frame，并执行method server将执行是否成功等信息放在header frame中发送给client server将method执行的结果（返回值）放在data frame中发送给client ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:1:2","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc Server的rpc连接阶段 func main() { flag.Parse() lis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port)) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } s := grpc.NewServer() pb.RegisterGreeterServer(s, \u0026server{}) log.Printf(\"server listening at %v\", lis.Addr()) if err := s.Serve(lis); err != nil { log.Fatalf(\"failed to serve: %v\", err) } } 如上是一个简单的服务端程序，流程如下 首先通过net.Listener监听tcp端口，毕竟grpc服务是基于tcp的 创建grpc server，并注册服务，\u0026server{}实际上就是服务的实现 阻塞等待来自client的访问 func (s *Server) Serve(lis net.Listener) error { s.serve = true for { rawConn, err := lis.Accept() s.serveWG.Add(1) go func() { s.handleRawConn(lis.Addr().String(), rawConn) s.serveWG.Done() }() } grpc在一个for循环中等待来自client的访问，每次有新的client端访问，创建一个net.Conn，并创建一个新的goroutine处理这个net.Conn，所以这个连接上的请求，无论客户端调用哪一个远程访问或者调用几次，都会由这个goroutine处理。 func (s *Server) handleRawConn(lisAddr string, rawConn net.Conn) { // 如果grpc server已经关闭，那么同样关闭这个tcp连接 if s.quit.HasFired() { rawConn.Close() return } // 设置tcp超时时间 rawConn.SetDeadline(time.Now().Add(s.opts.connectionTimeout)) // Finish handshaking (HTTP2) st := s.newHTTP2Transport(rawConn) // 清理tcp超时时间 rawConn.SetDeadline(time.Time{}) // rpc交互阶段，创建新的goroutine处理来自client的数据 go func() { s.serveStreams(context.Background(), st, rawConn) s.removeConn(lisAddr, st) }() } 在这里，首先判断gprc server是否关闭，如果关闭，则同样关闭tcp连接。然后进行HTTP2的握手，这里专门设置了tcp超时时间，避免握手迟迟不结束，导致资源占用，在握手结束后，清理tcp超时时间，避免对后面请求的影响。最后新启动一个goroutine，用来处理实际的请求。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:2:0","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc服务端HTTP2握手 func (s *Server) newHTTP2Transport(c net.Conn) transport.ServerTransport { // 组装 serverconfig config := \u0026transport.ServerConfig{ MaxStreams: s.opts.maxConcurrentStreams, ConnectionTimeout: s.opts.connectionTimeout, ... } // 根据config的配置项，和client进行http2的握手 st, err := transport.NewServerTransport(c, config) 根据使用者在启动grpc server时的配置项，或者默认的配置项，调用transport.NewServerTransport完成和client的http2握手。 writeBufSize := config.WriteBufferSize readBufSize := config.ReadBufferSize maxHeaderListSize := defaultServerMaxHeaderListSize if config.MaxHeaderListSize != nil { maxHeaderListSize = *config.MaxHeaderListSize } framer := newFramer(conn, writeBufSize, readBufSize, config.SharedWriteBuffer, maxHeaderListSize) 首先创建framer，用来负责接受和发送HTTP2 frame，是server和client交流的实际接口。 // Send initial settings as connection preface to client. isettings := []http2.Setting{{ ID: http2.SettingMaxFrameSize, Val: http2MaxFrameLen, }} if config.MaxStreams != math.MaxUint32 { isettings = append(isettings, http2.Setting{ ID: http2.SettingMaxConcurrentStreams, Val: config.MaxStreams, }) } ... if err := framer.fr.WriteSettings(isettings...); err != nil { return nil, connectionErrorf(false, err, \"transport: %v\", err) } grpc server端首先明确自己的HTTP2的初始配置，比如MaxFrameSize等，并将这些配置信息通过frame.fr发送给client，frame.fr实际上就是golang原生的http2.Framer，在底层，这些配置信息会被包装成一个Setting Frame发送给client。 client在收到Setting Frame后，根据自身情况调整参数，同样发送一个Setting Frame给sever。 t := \u0026http2Server{ done: done, conn: conn, peer: peer, framer: framer, readerDone: make(chan struct{}), loopyWriterDone: make(chan struct{}), maxStreams: config.MaxStreams, inTapHandle: config.InTapHandle, fc: \u0026trInFlow{limit: uint32(icwz)}, state: reachable, activeStreams: make(map[uint32]*ServerStream), stats: config.StatsHandlers, kp: kp, idle: time.Now(), kep: kep, initialWindowSize: iwz, bufferPool: config.BufferPool, } // controlbuf用来缓存Setting Frame等和设置相关的Frame的缓存 t.controlBuf = newControlBuffer(t.done) // 自增连接id t.connectionID = atomic.AddUint64(\u0026serverConnectionCounter, 1) // flush framer，确保向client发送了setting frame t.framer.writer.Flush() grpc server在发送了setting frame之后，创建好http2Server对象，并等待client的后续消息。 // Check the validity of client preface. preface := make([]byte, len(clientPreface)) // 读取客户端发来的client preface，并验证是否和预期一致 if _, err := io.ReadFull(t.conn, preface); err != nil { // In deployments where a gRPC server runs behind a cloud load balancer // which performs regular TCP level health checks, the connection is // closed immediately by the latter. Returning io.EOF here allows the // grpc server implementation to recognize this scenario and suppress // logging to reduce spam. if err == io.EOF { return nil, io.EOF } return nil, connectionErrorf(false, err, \"transport: http2Server.HandleStreams failed to receive the preface from client: %v\", err) } if !bytes.Equal(preface, clientPreface) { return nil, connectionErrorf(false, nil, \"transport: http2Server.HandleStreams received bogus greeting from client: %q\", preface) } // 读取client端发来的frame frame, err := t.framer.fr.ReadFrame() if err == io.EOF || err == io.ErrUnexpectedEOF { return nil, err } if err != nil { return nil, connectionErrorf(false, err, \"transport: http2Server.HandleStreams failed to read initial settings frame: %v\", err) } atomic.StoreInt64(\u0026t.lastRead, time.Now().UnixNano()) // 转成SettingFrame sf, ok := frame.(*http2.SettingsFrame) if !ok { return nil, connectionErrorf(false, nil, \"transport: http2Server.HandleStreams saw invalid preface type %T from client\", frame) } // 处理SettingFrame t.handleSettings(sf) func (t *http2Server) handleSettings(f *http2.SettingsFrame) { // 如果是ack frame，则直接返回 if f.IsAck() { return } var ss []http2.Setting var updateFuncs []func() f.ForeachSetting(func(s http2.Setting) error { switch s.ID { // 更新http2Server中的配置信息 case http2.SettingMaxHeaderListSize: updateFuncs = append(updateFuncs, func() { t.maxSendHeaderListSize = new(uint32) *t.maxSendHeaderListSize = s.Val }) default: ss = append(ss, s) } return nil }) // 这里又遇到了controlBuf // 执行updateFunc","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:2:1","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc server的rpc交互阶段 HTTP2中定义了很多类型的frame，包括data, headers等，具体如下，对于不同的frame类型，HTTP2 server应该有不同的处理逻辑。在grpc中，对frame类型的分类和处理，被包含在func (s *Server) serveStreams中。 // FrameType represents the type of an HTTP/2 Frame. // See [Frame Type]. // // [Frame Type]: https://httpwg.org/specs/rfc7540.html#FrameType type FrameType uint8 // Frame types defined in the HTTP/2 Spec. const ( FrameTypeData FrameType = 0x0 FrameTypeHeaders FrameType = 0x1 FrameTypeRSTStream FrameType = 0x3 FrameTypeSettings FrameType = 0x4 FrameTypePing FrameType = 0x6 FrameTypeGoAway FrameType = 0x7 FrameTypeWindowUpdate FrameType = 0x8 FrameTypeContinuation FrameType = 0x9 ) func (s *Server) serveStreams(ctx context.Context, st transport.ServerTransport, rawConn net.Conn) { streamQuota := newHandlerQuota(s.opts.maxConcurrentStreams) // 阻塞并接受来自client的frame st.HandleStreams(ctx, func(stream *transport.ServerStream) { s.handlersWG.Add(1) streamQuota.acquire() f := func() { defer streamQuota.release() defer s.handlersWG.Done() // 当一个新的stream被创建之后，进行一些配置 s.handleStream(st, stream) } if s.opts.numServerWorkers \u003e 0 { select { case s.serverWorkerChannel \u003c- f: return default: // If all stream workers are busy, fallback to the default code path. } } go f() }) } st.HandleStreams会阻塞当前goroutine，并等待来自client的frame，在一个for循环中等待并读取来自client的frame，并采取不同的处理方式。 grpc服务端使用一个goroutine向外发送数据loopyWriter，使用另一个goroutine读取数据serverStreams。 func (t *http2Server) HandleStreams(ctx context.Context, handle func(*ServerStream)) { defer func() { close(t.readerDone) \u003c-t.loopyWriterDone }() // for循环，持续处理一个连接的上请求 for { // 限流 t.controlBuf.throttle() // 读取frame frame, err := t.framer.fr.ReadFrame() atomic.StoreInt64(\u0026t.lastRead, time.Now().UnixNano()) // 根据frame的类型分别处理 switch frame := frame.(type) { // MetaHeaderFrame并不是http2的frame类型，而是经过包装的类型 // headers frame + zero or more continuation frame + hspack编码内容的解码 case *http2.MetaHeadersFrame: if err := t.operateHeaders(ctx, frame, handle); err != nil { // Any error processing client headers, e.g. invalid stream ID, // is considered a protocol violation. t.controlBuf.put(\u0026goAway{ code: http2.ErrCodeProtocol, debugData: []byte(err.Error()), closeConn: err, }) continue } case *http2.DataFrame: t.handleData(frame) case *http2.RSTStreamFrame: t.handleRSTStream(frame) case *http2.SettingsFrame: t.handleSettings(frame) case *http2.PingFrame: t.handlePing(frame) case *http2.WindowUpdateFrame: t.handleWindowUpdate(frame) case *http2.GoAwayFrame: // TODO: Handle GoAway from the client appropriately. default: if t.logger.V(logLevel) { t.logger.Infof(\"Received unsupported frame type %T\", frame) } } } } ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:3:0","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"Headers Frame的处理 func (t *http2Server) operateHeaders(ctx context.Context, frame *http2.MetaHeadersFrame, handle func(*ServerStream)) error { // Acquire max stream ID lock for entire duration t.maxStreamMu.Lock() defer t.maxStreamMu.Unlock() // 从客户端frame中获取streamID streamID := frame.Header().StreamID // 校验stream id if streamID%2 != 1 || streamID \u003c= t.maxStreamID { // illegal gRPC stream id. return fmt.Errorf(\"received an illegal stream id: %v. headers frame: %+v\", streamID, frame) } // 将获得的streamID设置到http2Server t.maxStreamID = streamID // 无界message缓冲 buf := newRecvBuffer() // 创建stream s := \u0026ServerStream{ Stream: \u0026Stream{ id: streamID, buf: buf, fc: \u0026inFlow{limit: uint32(t.initialWindowSize)}, }, st: t, headerWireLength: int(frame.Header().Length), } 在grpc server和client端，存在这一个stream的概念，用来表征一次grpc call。一个grpc call总是以一个来自client的headers frame开始，因此server会在operateHeaders中创建一个Stream对象，stream有一个client和server端一致的id，也有一个buf缓存。 for _, hf := range frame.Fields { switch hf.Name { case \"grpc-encoding\": s.recvCompress = hf.Value case \":method\": // POST, GET这些 httpMethod = hf.Value case \":path\": // 使用grpc那个服务的那个方法 s.method = hf.Value case \"grpc-timeout\": timeoutSet = true var err error if timeout, err = decodeTimeout(hf.Value); err != nil { headerError = status.Newf(codes.Internal, \"malformed grpc-timeout: %v\", err) } } } grpc server会遍历frame中的field，并将filed中的信息记录在stream中。:method和:path这两个field需要特别注意，client端需要填写好这两个field来明确地指定要调用server端提供的那一个方法，也就是说，调用哪一个server方法的信息是和调用方法的参数分开在不同的frame中的。 if frame.StreamEnded() { // s is just created by the caller. No lock needed. s.state = streamReadDone } // 超时设置 if timeoutSet { s.ctx, s.cancel = context.WithTimeout(ctx, timeout) } else { s.ctx, s.cancel = context.WithCancel(ctx) } if uint32(len(t.activeStreams)) \u003e= t.maxStreams { t.mu.Unlock() t.controlBuf.put(\u0026cleanupStream{ streamID: streamID, rst: true, rstCode: http2.ErrCodeRefusedStream, onWrite: func() {}, }) s.cancel() return nil } // 将stream加入activeStreams map t.activeStreams[streamID] = s if len(t.activeStreams) == 1 { t.idle = time.Time{} } // Start a timer to close the stream on reaching the deadline. if timeoutSet { // We need to wait for s.cancel to be updated before calling // t.closeStream to avoid data races. cancelUpdated := make(chan struct{}) timer := internal.TimeAfterFunc(timeout, func() { \u003c-cancelUpdated t.closeStream(s, true, http2.ErrCodeCancel, false) }) oldCancel := s.cancel s.cancel = func() { oldCancel() timer.Stop() } close(cancelUpdated) } s.trReader = \u0026transportReader{ reader: \u0026recvBufferReader{ ctx: s.ctx, ctxDone: s.ctxDone, recv: s.buf, }, windowHandler: func(n int) { t.updateWindow(s, uint32(n)) }, } // Register the stream with loopy. t.controlBuf.put(\u0026registerStream{ streamID: s.id, wq: s.wq, }) handle(s) 这个新建的stream对象会被放到server的activeStreams map中，并调用回调函数handle(s)来进一步处理这个stream，其中最重要的是调用s.handleStream。 st.HandleStreams(ctx, func(stream *transport.ServerStream) { s.handlersWG.Add(1) streamQuota.acquire() f := func() { defer streamQuota.release() defer s.handlersWG.Done() s.handleStream(st, stream) } // 如果设置了worker池，则先尝试提交任务到worker池中，如果不行，新起goroutine执行 if s.opts.numServerWorkers \u003e 0 { select { case s.serverWorkerChannel \u003c- f: return default: // If all stream workers are busy, fallback to the default code path. } } go f() }) // initServerWorkers creates worker goroutines and a channel to process incoming // connections to reduce the time spent overall on runtime.morestack. func (s *Server) initServerWorkers() { s.serverWorkerChannel = make(chan func()) s.serverWorkerChannelClose = sync.OnceFunc(func() { close(s.serverWorkerChannel) }) for i := uint32(0); i \u003c s.opts.numServerWorkers; i++ { go s.serverWorker() } } 回调函数中会将处理stream的任务提交到其他goroutine中，如果可用的worker，则由worker执行，否则另起goroutine来执行。 func (s *Server) handleStream(t transport.ServerTransport, stream *transport.ServerStream) { // 获取grpc路径 sm := stream.Method() pos := strings.LastIndex(sm, \"/\") // 调用的grpc service name service := sm[:pos] // 调用的grpc method name method := sm[pos","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:3:1","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"Data Frame的处理 func (t *http2Server) handleData(f *http2.DataFrame) { size := f.Header().Length // Select the right stream to dispatch. s, ok := t.getStream(f) if !ok { return } if s.getState() == streamReadDone { t.closeStream(s, true, http2.ErrCodeStreamClosed, false) return } if size \u003e 0 { if len(f.Data()) \u003e 0 { pool := t.bufferPool s.write(recvMsg{buffer: mem.Copy(f.Data(), pool)}) } } if f.StreamEnded() { // Received the end of stream from the client. s.compareAndSwapState(streamActive, streamReadDone) s.write(recvMsg{err: io.EOF}) } } 在处理data frame时 根据stream ID，从server的activeStreams map中找到stream对象 从bufferPool中拿到一块buffer，并将frame的数据写入到buffer 将这块buffer保存到stream的recvBuffer中 如果读取结束，修改流状态为streamReadDone，并且写入io.EOF标记 recvBuffer中缓存的数据，最终会被前面提到的recvAndDecompress函数读取，从而在server端重建rpc的参数。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:3:2","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"Setting Frame的处理 func (t *http2Server) handleSettings(f *http2.SettingsFrame) { if f.IsAck() { return } var ss []http2.Setting var updateFuncs []func() f.ForeachSetting(func(s http2.Setting) error { switch s.ID { case http2.SettingMaxHeaderListSize: updateFuncs = append(updateFuncs, func() { t.maxSendHeaderListSize = new(uint32) *t.maxSendHeaderListSize = s.Val }) default: ss = append(ss, s) } return nil }) t.controlBuf.executeAndPut(func() bool { for _, f := range updateFuncs { f() } return true }, \u0026incomingSettings{ ss: ss, }) } handleSettings并没有直接将settting frame的参数应用在server上，而是将其放到了controlBuf中。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:3:3","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"server如何发送frame grpc server在每次收到一个新的来自client的连接后，会创建一个Framer，这个Framer就是实际上负责发送和接收HTTP2 frame的接口，每一个client都对应一个Framer来处理来自该client的所有frame，不管这些frame是否属于同一个stream。 type framer struct { // 一个包含了buffer的net.Conn的writer writer *bufWriter // 原生的http2.Framer，负责数据读写 fr *http2.Framer } framer其实就是对golang原生http2.Framer的封装。 type bufWriter struct { pool *sync.Pool buf []byte offset int batchSize int conn net.Conn err error } func newBufWriter(conn net.Conn, batchSize int, pool *sync.Pool) *bufWriter { w := \u0026bufWriter{ batchSize: batchSize, conn: conn, pool: pool, } // this indicates that we should use non shared buf if pool == nil { w.buf = make([]byte, batchSize) } return w } func (w *bufWriter) Write(b []byte) (int, error) { // 在write之间检查上一次write是否发生了错误 if w.err != nil { return 0, w.err } // 如果batchsize为0，说明不需要写缓存，直接向net.Conn写数据 if w.batchSize == 0 { // Buffer has been disabled. n, err := w.conn.Write(b) return n, toIOError(err) } if w.buf == nil { b := w.pool.Get().(*[]byte) w.buf = *b } written := 0 // 如果写入的数据少于batchSize，则缓存，暂时不写入conn // 如果写入的数据多余batchSize，则调用flushKeepBuffer不断写数据 for len(b) \u003e 0 { copied := copy(w.buf[w.offset:], b) b = b[copied:] written += copied w.offset += copied if w.offset \u003c w.batchSize { continue } if err := w.flushKeepBuffer(); err != nil { return written, err } } return written, nil } func (w *bufWriter) Flush() error { // 刷新数据到conn err := w.flushKeepBuffer() // Only release the buffer if we are in a \"shared\" mode if w.buf != nil \u0026\u0026 w.pool != nil { b := w.buf w.pool.Put(\u0026b) w.buf = nil } return err } func (w *bufWriter) flushKeepBuffer() error { if w.err != nil { return w.err } if w.offset == 0 { return nil } _, w.err = w.conn.Write(w.buf[:w.offset]) w.err = toIOError(w.err) w.offset = 0 return w.err } grpc server实现了一个简单的缓存写给http2.framer作为io.Writer。 // 全局writeBufferPool var writeBufferPoolMap = make(map[int]*sync.Pool) var writeBufferMutex sync.Mutex func newFramer(conn net.Conn, writeBufferSize, readBufferSize int, sharedWriteBuffer bool, maxHeaderListSize uint32) *framer { if writeBufferSize \u003c 0 { writeBufferSize = 0 } var r io.Reader = conn if readBufferSize \u003e 0 { // 设置io.Reader r = bufio.NewReaderSize(r, readBufferSize) } var pool *sync.Pool if sharedWriteBuffer { pool = getWriteBufferPool(writeBufferSize) } // 设置io.Writer w := newBufWriter(conn, writeBufferSize, pool) // 创建framer f := \u0026framer{ writer: w, fr: http2.NewFramer(w, r), } f.fr.SetMaxReadFrameSize(http2MaxFrameLen) // Opt-in to Frame reuse API on framer to reduce garbage. // Frames aren't safe to read from after a subsequent call to ReadFrame. f.fr.SetReuseFrames() f.fr.MaxHeaderListSize = maxHeaderListSize f.fr.ReadMetaHeaders = hpack.NewDecoder(http2InitHeaderTableSize, nil) return f } // writeBuffer 使用sync.Pool func getWriteBufferPool(size int) *sync.Pool { writeBufferMutex.Lock() defer writeBufferMutex.Unlock() pool, ok := writeBufferPoolMap[size] if ok { return pool } pool = \u0026sync.Pool{ New: func() any { b := make([]byte, size) return \u0026b }, } writeBufferPoolMap[size] = pool return pool } 传递给http2.framer的io.Reader使用了bifio package。 writeBuffer使用了go标准库中的sync.Pool，根据需要的size获取对应的sync.Pool，如果池中有对应的byte[]，获取然后返回，如果没有，创建新的byte[]并返回。池中元素的回收时机，go允许在任何时候自动回收池中的元素（gc）。 grpc server为每一个client创建一个loopyWriter，有这个loopyWriter负责发送数据。 type loopyWriter struct { // 客户端还是服务端 side side // controlBuffer cbuf *controlBuffer // 发送配额 sendQuota uint32 // 发送端初始窗口大小 outbound initial window size oiws uint32 // 已经建立未清理的stream，在客户端，指所有已经将Headers发送出去的stream， // 在服务端，指所有已经接收到Headers的stream estdStreams map[uint32]*outStream // 活跃stream列表，有数据需要发送且包含stream-level流控，里面的每个stream内部都有一个数据列表用来存放发送的数据 activeStreams *outStreamList // http2.Framer的包装，用来实际读写数 framer *framer hBuf *bytes.Buffer // The buffer for HPACK encoding. hEnc *hpack.Encoder // HPACK encoder. bdpEst *bdpEstimator draining bool // 底层tcp连接 conn net.Conn logger *grpclog.PrefixLogger bufferPool mem.BufferPool // Side-specific handlers ssGoAwayHandler func(*goAway) (bool, error) } loopyWriter从control buffer中接收frame，每个frame被单独","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:4:0","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc server的流量控制 grpc在应用层实现了自己的流量控制，并将流量控制分成了三个层级 sample level 流量控制 connection level 流量控制 stream level 流量控制 流量控制可以说是grpc高性能的关键，通过动态地控制数据发送和接收的速率，grpc保证在任何网络情况下都能发挥最大的性能，尽量提高传输带宽并降低传输延迟。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:5:0","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"采样流量控制 BDP估算和动态流量控制窗口 BDP和动态流量控制窗口缩小了grpc和http1.1在高延迟网络中的性能表现。带宽延迟积（BDP，Bandwidth Delay Product）是网络连接的带宽和数据往返延迟的乘积，能够有效地表示在网络被完全利用时网络上有多少字节数据。 BDP算法基本思路如下： 每次接收方收到一个data frame时，它就会发送一个BDP ping（一个带有唯一数据、仅用于BDP估算的ping）。在这之后，接收方开始统计它接收到的字节数（包括触发该BDP ping的那部分数据），直到它收到该ping的ack为止。这个在大约1.5个RTT（round-trip time）内接收到的字节总数，约为BDP的1.5倍。如果这个总字节数接近当前的窗口（比如超过窗口的2/3），那么我们必须增大窗口。我们将窗口大小（包括stremaing和connection窗口）设为采样得到的BDP的两倍（也就是接收到的字节总数的两倍）。 在grpc server端定义了一个bdpEstimator，是用来计算BDP的核心。 const ( // bdpLimit is the maximum value the flow control windows will be increased // to. TCP typically limits this to 4MB, but some systems go up to 16MB. // Since this is only a limit, it is safe to make it optimistic. bdpLimit = (1 \u003c\u003c 20) * 16 // alpha is a constant factor used to keep a moving average // of RTTs. alpha = 0.9 // If the current bdp sample is greater than or equal to // our beta * our estimated bdp and the current bandwidth // sample is the maximum bandwidth observed so far, we // increase our bbp estimate by a factor of gamma. beta = 0.66 // To put our bdp to be smaller than or equal to twice the real BDP, // we should multiply our current sample with 4/3, however to round things out // we use 2 as the multiplication factor. gamma = 2 ) // Adding arbitrary data to ping so that its ack can be identified. // Easter-egg: what does the ping message say? var bdpPing = \u0026ping{data: [8]byte{2, 4, 16, 16, 9, 14, 7, 7}} type bdpEstimator struct { // sentAt is the time when the ping was sent. sentAt time.Time mu sync.Mutex // bdp is the current bdp estimate. bdp uint32 // sample is the number of bytes received in one measurement cycle. sample uint32 // bwMax is the maximum bandwidth noted so far (bytes/sec). bwMax float64 // bool to keep track of the beginning of a new measurement cycle. isSent bool // Callback to update the window sizes. updateFlowControl func(n uint32) // sampleCount is the number of samples taken so far. sampleCount uint64 // round trip time (seconds) rtt float64 } bdpEstimator有两个主要的方法add和calculate // add的返回值指示loopyWriter是否发送BDP ping frame给client func (b *bdpEstimator) add(n uint32) bool { b.mu.Lock() defer b.mu.Unlock() // 如果bdp已经达到上限，就不再发送bdp ping进行采样 if b.bdp == bdpLimit { return false } // 如果在当前时间点没有bdp ping frame发送出去，就应该发送，来进行采样 if !b.isSent { b.isSent = true b.sample = n b.sentAt = time.Time{} b.sampleCount++ return true } // 已经有bdp ping frame发送出去了，但是还没有收到ack，累加收到的字节数 b.sample += n return false } add函数有两个作用： 告知loopyWriter是否开始采样 记录采样开始的时间和初始数据量 func (t *http2Server) handleData(f *http2.DataFrame) { size := f.Header().Length var sendBDPPing bool if t.bdpEst != nil { sendBDPPing = t.bdpEst.add(size) } if w := t.fc.onData(size); w \u003e 0 { t.controlBuf.put(\u0026outgoingWindowUpdate{ streamID: 0, increment: w, }) } if sendBDPPing { // Avoid excessive ping detection (e.g. in an L7 proxy) // by sending a window update prior to the BDP ping. if w := t.fc.reset(); w \u003e 0 { t.controlBuf.put(\u0026outgoingWindowUpdate{ streamID: 0, increment: w, }) } t.controlBuf.put(bdpPing) } // Select the right stream to dispatch. s, ok := t.getStream(f) } handleData函数是grpc serve收到来自client的http2 data frame之后执行的函数，可以看到，grpc server和每个client之间都维护着一个bdpEstimator，每次收到一个data frame，grpc server都会判断是否需要进行采样，如果需要采样，就向client发送一个bdpPing frame，这个frame也是加入controlBuffer，异步处理的。 这里也将连接的流量控制和应用程序读取数据的行为解耦，也就是说，连接级别的窗口更新不应该依赖于应用是否读取了数据。stream-level流控已经有这个限制（必须等待应用读取后才能更新窗口），所以如果某个stream很慢，发送方已经被阻塞（因为窗口耗尽）。解耦可以避免下面的情况发生，当某些strema很慢（或者压根没有读取数据）时，导致其他活跃的stream由于没有connection-level流控窗口而被阻塞。 func (l *loopyWriter) pingHandler(p *ping) error { if !p.ack { l.bdpEst.timesnap(p.data) } return l.framer.fr.WritePing(p.ack, p.data) } func (b *bdpEstimator) timesnap(d [8]byte) { if bdpPing.data != d { return } b.sentAt = time.Now() } 前面提到bdp ping frame是通过control framer异步发送出去的，这个时间点可能和之前决定发送ping的时间点有一定的距离，为了更准确的计算RTT，所以在使用http2.framer实际发送数据前，重新更新了bdp ping frame的发送时间。 Client端在收到一个bdp ping frame之后，会立刻返回一个ack，server会捕捉到这个ack。 func (t *http2Server) handlePing(f *http2.PingFrame) { if f.IsAck() { if f.Dat","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:5:1","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"controlBuffer数据结构 先介绍一个重要的数据结果controlBuffer，这个在之前已经提到过了，在向外发送数据前，其实都会加入controlBuffer中，然后再进行处理。 type controlBuffer struct { // wakeupch的作用是在阻塞读取缓存中的内容时，当有新的frame加入itemList，可以解决阻塞并返回itemList中的frame wakeupCh chan struct{} // Unblocks readers waiting for something to read. // done \u003c-chan struct{} // Closed when the transport is done. // Mutex guards all the fields below, except trfChan which can be read // atomically without holding mu. mu sync.Mutex // 和wakeupCh配置使用，确保不向wakeupCh中放入多余的struct，保证阻塞读取缓存不会因为wakeupCh中的多余元素错误解除阻塞 consumerWaiting bool // True when readers are blocked waiting for new data. closed bool // True when the controlbuf is finished. list *itemList // List of queued control frames. // 记录排队的响应帧数量 transportResponseFrames int // 当transportResponseFrames \u003e= maxQueuedTransportResponseFrames时， // 创建trfChan，用于控制是否继续从client读取frame trfChan atomic.Pointer[chan struct{}] } controlBuffer中的数据被称为control frame，一个control frame不止表示向外发送的data、message、headers，也被用来指示loopyWriter更新自身的内部状态。control frame和http2 frame没有直接关系，尽管有些control frame，比如说 dataFrame和headerFrame确实作为http2 frame向外传输。 controlBuffer维护了一个itemList（单向链表），本质上是一块缓存区，这块缓存区主要有两个作用： 缓存需要发送的frame 根据缓存中transportResponseFrame的数量，决定是否暂时停止读取从client发来的frame 下面看controlBuffer中的一些主要函数，加深理解 func newControlBuffer(done \u003c-chan struct{}) *controlBuffer { return \u0026controlBuffer{ wakeupCh: make(chan struct{}, 1), list: \u0026itemList{}, done: done, } } newControlBuffer用于创建controlBuffer实例，其中wakeupCh是缓冲区为1的channel。 func (c *controlBuffer) throttle() { if ch := c.trfChan.Load(); ch != nil { select { case \u003c-(*ch): case \u003c-c.done: } } } throttle函数会被阻塞，如果controlBuffer中存在太多的响应帧，比如incommingSettings、cleanupStrema等。在grpc server的代码中，throttle函数通常出现在grpc server接收client frame的开头，也就是说，当transportResponseFrames数量过多时，grpc server会暂停接受来自client的frame，maxQueuedTransportResponseFrames为50。 func (c *controlBuffer) executeAndPut(f func() bool, it cbItem) (bool, error) { c.mu.Lock() defer c.mu.Unlock() if c.closed { return false, ErrConnClosing } if f != nil { if !f() { // f wasn't successful return false, nil } } if it == nil { return true, nil } var wakeUp bool if c.consumerWaiting { wakeUp = true c.consumerWaiting = false } // 将item加入到buffer中 c.list.enqueue(it) if it.isTransportResponseFrame() { c.transportResponseFrames++ if c.transportResponseFrames == maxQueuedTransportResponseFrames { // We are adding the frame that puts us over the threshold; create // a throttling channel. ch := make(chan struct{}) c.trfChan.Store(\u0026ch) } } if wakeUp { select { case c.wakeupCh \u003c- struct{}{}: default: } } return true, nil } executeAndPut运行f函数，如果f函数返回true，添加给定的item到controlBuf。如果consumerWaiting为true，也就是loopyWriter发现没有消息可供处理，所以阻塞获取control frame，这里会向wakeupCh中放入一个元素，来通知消费者可以读取frame了。在这里也会检查响应帧的数量，如果超过阈值，则创建trfChan。 func (c *controlBuffer) get(block bool) (any, error) { // for循环 for { c.mu.Lock() frame, err := c.getOnceLocked() if frame != nil || err != nil || !block { c.mu.Unlock() return frame, err } // 设置状态为consumerWaiting c.consumerWaiting = true c.mu.Unlock() // Release the lock above and wait to be woken up. select { // control buffer中没有control frame，阻塞等待 case \u003c-c.wakeupCh: case \u003c-c.done: return nil, errors.New(\"transport closed by client\") } } } func (c *controlBuffer) getOnceLocked() (any, error) { if c.closed { return false, ErrConnClosing } if c.list.isEmpty() { return nil, nil } h := c.list.dequeue().(cbItem) // 将controlframe移除响应帧，可能会解封对client请求的读取 if h.isTransportResponseFrame() { if c.transportResponseFrames == maxQueuedTransportResponseFrames { // We are removing the frame that put us over the // threshold; close and clear the throttling channel. ch := c.trfChan.Swap(nil) close(*ch) } c.transportResponseFrames-- } return h, nil } get从control buffer中获取下一个control frame，如果block参数为true并且control buffer中没有control frame，调用被阻塞直到有control frame或者buffer被关闭。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:5:2","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"connection level流量控制 connection level流量控制会控制对于某个client某一时刻能够发送的数据总量。 type loopyWriter struct { ...... sendQuota uint32 ...... } 控制的方式就是在loopyWriter中用一个sendQuota来标记该client目前可发送数据的配额。 func (l *loopyWriter) processData() (bool, error) { ...... l.sendQuota -= uint32(size) ...... } sendQuota会被初始化为65535，并且每当有数据被grpc server发送给client的时候，sendQuota都会减少和被发送数据相等的大小。 为了配合server端的流量控制，client端在连接初始化时被分配了一个limit，默认为65536字节，client端会记录收到的数据量的总和unacked，当unacked超过了limit的1/4后，client就会向server段发送一个window update（数值为unacked）,通知server可以将quota加回来，同时将unacked置零。 可以看到为了避免频繁的发送window update占用网络带宽，client并不会在每次接收到数据之后就发送window update，而是等待接收的数据量达到某一阈值后再发送。 // trInFlow 是 client 端决定是否发送 window update 给 server 的核心 type trInFlow struct { // server 端能够发送数据的上限, 会被 server 端根据采用控制的结果更新 limit uint32 // client 端已经接收到的数据 unacked uint32 // 用于 metric 记录, 不影响流量控制 effectiveWindowSize uint32 } // 参数 n 是 client 接收到的数据大小, 返回值表示需要向 server 发送的 window update 中的数值大小. // 返回 0 代表不需要发送 window update func (f *trInFlow) onData(n uint32) uint32 { f.unacked += n // 超过 1/4 * limit 才会发送 window update, 且数值为已经接收到的数据总量 if f.unacked \u003e= f.limit/4 { w := f.unacked f.unacked = 0 f.updateEffectiveWindowSize() return w } f.updateEffectiveWindowSize() return 0 } trInFlow是client端控制是否发送window update的核心，limit会随server端发来的window update而改变。 type outgoingWindowUpdate struct { streamID uint32 increment uint32 } 最终向对端发送的是WindowUpdate Frame，其中streamID为0，表示作用于整个连接，increment表示quota的增量。 func (t *http2Server) handleWindowUpdate(f *http2.WindowUpdateFrame) { t.controlBuf.put(\u0026incomingWindowUpdate{ streamID: f.Header().StreamID, increment: f.Increment, }) } 服务端收到WindowUpdateFrame后，会将消息包装成incomingWindowUpdate放入controlBuf中 func (l *loopyWriter) incomingWindowUpdateHandler(w *incomingWindowUpdate) error { // Otherwise update the quota. if w.streamID == 0 { l.sendQuota += w.increment return nil } ...... } 当grpc server收到来自client的http2 FrameWindowUpdate frame时，才会将这一quota增加，也就是说sendQuota会在server发出数据时减少，在收到来自client的FrameWindowUpdate frame时增加，connection level的流量控制是server和client相互交互的结果，由双方共同决定窗口大小。 func (l *loopyWriter) processData() (bool, error) { if l.sendQuota == 0 { return true, nil } if maxSize \u003e int(l.sendQuota) { // connection-level flow control. maxSize = int(l.sendQuota) } 当loopyWriter打算向外发送数据时，如果sendQuota为零，就停止向外发送数据，如果打算向外发送的数据超过sendquota，则只发送sendQuota大小的数据。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:5:3","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"stream level流量控制 一个stream的流量控制有三种状态，分别是 active: stream中有数据且数据可以被发送 empty: stream中没有数据 waitingOnStreamQuota: stream的quota不足，等待有quota时再发送数据 一个stream一开始的状态为empty，因为一个stream在被创建出来时还没有待发送的数据。 func (l *loopyWriter) preprocessData(df *dataFrame) error { str, ok := l.estdStreams[df.streamID] if !ok { return nil } // If we got data for a stream it means that // stream was originated and the headers were sent out. str.itl.enqueue(df) if str.state == empty { str.state = active l.activeStreams.enqueue(str) } return nil } 当server处理controlBuffer时遇到某个stream的frame时，会将该stream转成active状态，active状态的stream可以发送数据。 func (l *loopyWriter) processData() (bool, error) { ...... if strQuota := int(l.oiws) - str.bytesOutStanding; strQuota \u003c= 0 { // stream-level flow control. str.state = waitingOnStreamQuota return false, nil } ...... str.bytesOutStanding += size ...... } 发送数据之后，byteOutStanding会增加相应的数据大小，表明该stream有这些数据被发送给client，还没有收到回应。而当byteOutStanding的大小超过loopyWriter.oiws，也就是65535后，会拒绝为该stream继续发送数据，这种策略避免了不断向一个失去回应的client发送数据，避免浪费网络带宽。 stream level的流量控制和connenction level的流量控制原理基本上一直，主要的区别有两点： stream level的流量控制中的quota只针对单个stream，每个stream既受限于stream level流量控制，又受限于conection level流量控制 client端决定反馈给server windowUpdate frame的时机更负责一些 // 入站流量控制（inbound flow control type inFlow struct { mu sync.Mutex // stream能接受的数据上限，初始为65535字节，受到采样流量控制的影响 limit uint32 // 收到但未被应用消费（未被读取）的数据量 pendingData uint32 // 应用已经消费但还未发送windowUpdate frame的数据量，用于减低windowUpdate frame的发送频率 pendingUpdate uint32 // 是在limit基础上额外增加的数据量，当应用试着读取超过limit大小的数据是，会临时在limit上增加delta，来允许应用读取数据 delta uint32 } steam level的流量控制不光要记录已经收到的数据量，还需要记录被stream消费掉的数据量，以达到更精准的流量控制，对应的数据结构为inFlow。 // 当data frame被接收时，调用onData更新pendingData func (f *inFlow) onData(n uint32) error { f.mu.Lock() f.pendingData += n if f.pendingData+f.pendingUpdate \u003e f.limit+f.delta { limit := f.limit rcvd := f.pendingData + f.pendingUpdate f.mu.Unlock() return fmt.Errorf(\"received %d-bytes data exceeding the limit %d bytes\", rcvd, limit) } f.mu.Unlock() return nil } 当client接收到来自server的data frame的时候，pendingData增加接收到的数据量。 // 当应用读取数据时调用onRead，返回增加的窗口大小 func (f *inFlow) onRead(n uint32) uint32 { f.mu.Lock() if f.pendingData == 0 { f.mu.Unlock() return 0 } f.pendingData -= n if n \u003e f.delta { n -= f.delta f.delta = 0 } else { f.delta -= n n = 0 } f.pendingUpdate += n if f.pendingUpdate \u003e= f.limit/4 { wu := f.pendingUpdate f.pendingUpdate = 0 f.mu.Unlock() return wu } f.mu.Unlock() return 0 } 当应用读取n字节数据时，pendingData减去n，pendingUpdate增加n，如果存在delta，则需要先还清之前delta的欠债，然后才能将余额增加到pengingUpdate，如果pendignUpdate超过1/4 limit，返回pendingUpdate作为增加的窗口大小，对端可以继续在stream上发送数据，这一切都是为了渐渐消除之前为了允许server发送大量数据而临时增加的额度。 func (f *inFlow) maybeAdjust(n uint32) uint32 { if n \u003e uint32(math.MaxInt32) { n = uint32(math.MaxInt32) } f.mu.Lock() defer f.mu.Unlock() // 接收者的视角下发送者可以继续发送的最大字节数 estSenderQuota := int32(f.limit - (f.pendingData + f.pendingUpdate)) // 假设要读取n字节长度的grpc message，estUntransmittedData表示发送者可能还没有发送的最大字节数 estUntransmittedData := int32(n - f.pendingData) // 这意味着除非我们发送一个window update frame，否则发送者可能无法发送message的所有字节 // 由于有来自应用的活跃读请求，因此我们需要发送window update frame，允许超过原先的limit if estUntransmittedData \u003e estSenderQuota { if f.limit+n \u003e maxWindowSize { f.delta = maxWindowSize - f.limit } else { // 这里更新窗口到可以接受message，主要考虑到message可能存在padding f.delta = n } return f.delta } return 0 } maybeAdjust的核心逻辑是保证grpc message一定有足够的窗口能够被发送，避免陷入停滞，如果由于message需要临时增加窗口大小，则增加delta，而不是limit。最终向对端发送window update frame，提示对端可以继续发送数据。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:5:4","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc流量控制小结 流量控制，一般是指在网络传输过程中，发送者主动限制自身发送数据的速率或者发送的数据量，以适应接收者处理数据的速度，当接收者的处理速度较慢是，来不及处理的数据会被存放在内存中，而当内存中的数据缓存区被填满后，新收到的数据就会被扔掉，导致发送者不得不重新发送，造成网络带宽的浪费。 流量控制是一个网络组件的基本功能，我们熟知的TCP协议就规定了流量控制算法，grpc建立在TCP之上，也依赖于http2 WindowUupdate Frame实现了自己在应用层的流量控制。 在grpc中，流量控制体现在三个维度： 采样流量控制：grpc接收者检测一段时间内收到的数据量，从而推测出bdp，并指导发送者调整流量控制窗口 connection level流量控制：发送者在初始化时被分配一定的quota，quota随数据发送而降低，并在收到接收者的反馈之后增加，发送者在耗尽quota之后不能再发送数据 stream level流量控制：和connection level的流量控制类似，只不过connection level管理的是一个连接的所有流量，而stream level管理的是connection中诸多stream中的一个。 grpc中的流量控制仅针对HTTP2 data frame。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:5:5","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc timeout实现 Deadline对于一个网络服务来说很重要，client可以指定一个deadline，从而当时间超过后，可以及时放弃请求，返回DEADLINE_EXCEEDED。可以解决类似于 尾部延迟，某些请求相比于其他请求花费太多时间才返回 避免客户端无意义阻塞等待，比如服务器已经挂掉了，等待已经没有意义了 避免资源的不合理占用，rpc请求可能会持有一些资源，通过及时中断，可以释放这些资源 怎么得到一个合理的deadlines，需要考虑多方面因素，包括整个系统的端到端延迟，哪些RPC是串行的，哪些是并行的，然后尝试估算每一阶段的耗时，最终得到一个粗略的估计。 在grpc中，client和server会分别独立和局地的判断rpc调用是否成功，这意味着client和server得到的结论可能不一致，一个在server端成功的rpc调用可能在client端被认为是失败的，比如服务器可以发送响应，但响应达到是client的超时已经触发，client最终会终止当前rpc调用调用并返回DEADLINE_EXCEEDED。 clientDeadline := time.Now().Add(time.Duration(*deadlineMs) * time.Millisecond) ctx, cancel := context.WithDeadline(ctx, clientDeadline) 在go中通过对ctx指定超时时间来设置grpc超时。 response = blockingStub.withDeadlineAfter(deadlineMs, TimeUnit.MILLISECONDS).sayHello(request); 在java中通过调用client stub的方法withDeadLineAfter来设置超时时间。 在服务端，server可以查询某个rpc是否已经超时，在server可以处理rpc请求时，检查是否client还在等待非常重要，特别是在做一些很费时间的处理时。 if ctx.Err() == context.Canceled { return status.New(codes.Canceled, \"Client cancelled, abandoning.\") } if (Context.current().isCancelled()) { responseObserver.onError(Status.CANCELLED.withDescription(\"Cancelled by client\").asRuntimeException()); return; } grpc over http2规定了deadline是通过在请求的Headers frame中指定grpc-timeout字段实现的，其值的格式包含两部分： TimeoutValue ascii形式的正整数字符串，最多8位 TimeoutUnit 可以为Hour -\u003e H / Minute -\u003e M / Second -\u003e S / Millisecond -\u003e m / Microsecond -\u003e u / Nanosecond -\u003e n func (t *http2Server) operateHeaders(ctx context.Context, frame *http2.MetaHeadersFrame, handle func(*ServerStream)) error { streamID := frame.Header().StreamID s := \u0026ServerStream{ Stream: \u0026Stream{ id: streamID, buf: buf, fc: \u0026inFlow{limit: uint32(t.initialWindowSize)}, }, st: t, headerWireLength: int(frame.Header().Length), } // 找到grpc-timeout字段 for _, hf := range frame.Fields { case \"grpc-timeout\": timeoutSet = true var err error if timeout, err = decodeTimeout(hf.Value); err != nil { headerError = status.Newf(codes.Internal, \"malformed grpc-timeout: %v\", err) } // 为stream设置deadline if timeoutSet { s.ctx, s.cancel = context.WithTimeout(ctx, timeout) } else { s.ctx, s.cancel = context.WithCancel(ctx) } // 启动一个timer在超时的情况下关闭stream if timeoutSet { // We need to wait for s.cancel to be updated before calling // t.closeStream to avoid data races. cancelUpdated := make(chan struct{}) timer := internal.TimeAfterFunc(timeout, func() { \u003c-cancelUpdated // 最终会发送http2 RST frame关闭stream t.closeStream(s, true, http2.ErrCodeCancel, false) }) oldCancel := s.cancel s.cancel = func() { oldCancel() timer.Stop() } close(cancelUpdated) } server端获得headers frame后，在operateHeaders中进行处理。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:6:0","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc deadline在java中的实现 private static \u003cV\u003e V getUnchecked(Future\u003cV\u003e future) { try { return future.get(); } catch (InterruptedException e) { // 恢复中断 Thread.currentThread().interrupt(); // 抛出StatusRuntimeException throw Status.CANCELLED .withDescription(\"Thread interrupted\") .withCause(e) .asRuntimeException(); } catch (ExecutionException e) { throw toStatusRuntimeException(e.getCause()); } } 返回future.get()的结果，而且是可中断的，适用于不会抛出受检异常的任务，如果发生中断，线程在抛出异常前会先恢复中断。 如果get抛出CancellationException，原样抛出异常 如果get抛出ExecutionException或者InterruptedException，则抛出StatusRuntimeException public class StatusRuntimeException extends RuntimeException { private static final long serialVersionUID = 1950934672280720624L; private final Status status; private final Metadata trailers; public StatusRuntimeException(Status status) { this(status, (Metadata)null); } public StatusRuntimeException(Status status, @Nullable Metadata trailers) { super(Status.formatThrowableMessage(status), status.getCause()); this.status = status; this.trailers = trailers; } public final Status getStatus() { return this.status; } @Nullable public final Metadata getTrailers() { return this.trailers; } } StatusRuntimeExceptioni是Status的RuntimeException形式，为了能够通过异常传播Status。 public final class Status { private final Code code; private final String description; private final Throwable cause; Status定义了操作的状态通过提供标准的Code和可选的描述。对于客户端，每个远程调用调用都会在完成时返回一个status，如果发生错误status会被传播到blocking stub作为StatusRuntimeExcpetion，或者作为listener的显式参数，类似的，服务端可以抛出StatusRuntimeException或者将status传递给callback函数。 Code是一个enum类型，这里列出一些值得更多关注的code： OK 操作成功结束 CALCELLED 操作被取消，一般是被调用者取消 UNKNOWN 未知错误 INVALID_ARGUMENT 客户单提供了无效的参数 DEADLINE_EXCEEDED 在操作完成前超时 UNIMPLEMENTED 服务中的的操作未实现 INTERNAL 内部错误，表示底层系统所期望的一些不变量遭到了破坏 UNAVAILABLE 服务当前不可用，可能是瞬时错误，可以通过backoff重试纠正，然后对于非幂等操作重试不一定安全 ListenableFuture public interface ListenableFuture\u003cV extends @Nullable Object\u003e extends Future\u003cV\u003e { void addListener(Runnable listener, Executor executor); } google在ListenableFutureExplained文章中推荐总是使用ListenableFuture而不是Future，并给出了以下原因： 大多数Futures方法需要ListenableFutures 省去后续更改为ListenableFuture的麻烦 提供工具方法时不再需要提供Future和ListenableFuture两种变体 传统的Future表示异步计算的结果，一个计算可能也可能还没有产生结果，Future可以作为正在进行中的计算的句柄，服务承诺未来提供结果给我们。 ListenableFuture允许注册回调函数，一旦计算完成，这些回调函数将被执行，或者如果注册回调函数时计算已经开始，则立即开始执行。 addListenr方法表示当future完成时，注册的回调函数将在提供的线程池中被执行。 推荐通过Futures.addCallback(ListenableFuture\u003cV\u003e, FutureCallback\u003cV\u003e, Executor)添加回调函数，FutureCallback\u003cV\u003e实现了两个方法 onSuccess(V) 当future成功后基于执行结果执行行动 onFailure(Throwable) 在future失败时基于失败原因执行行动 对应于JDK通过ExecutorService.submit(Callable)初始化一个异步计算，guava提供了ListerningExecutorService接口，在任何ExecutorService返回Future的地方都改成返回ListenableFuture。可以通过使用MoreExecutors.listerningDecorator(ExecutorSerivce)将ExecutorService转换成ListerningExecutorService。 如果你打算转换FutureTask，guava提供了ListenableFutureTask.create(Callable\u003cV\u003e)和ListenableFutureTask.create(Runnable, V)，不同于jdk，ListenableFutureTask不希望被直接继承。 如果你需要的future抽象希望直接设置future的值而不是实现一个方法去计算这个值，可以考虑拓展AbstractFuture\u003cV\u003e或者直接使用SettableFuture。 如果你必须将其他API提供的future转换成ListenableFuture，那么你可能别无选择，只能使用较为重量级的JdkFutureAdapters.listenInPoolThread(Future) 方法来完成转换。但在可能的情况下，建议你修改原始代码，使其直接返回 ListenableFuture。 推荐使用ListenableFuture的最重要原因为它使得构建复杂的异步操作链变得可行，类似于JDK中提供了CompletableFuture。 GrpcFuture private static final class GrpcFuture\u003cRespT\u003e extends AbstractFuture\u003cRespT\u003e { private final ClientCall\u003c?, RespT\u003e call; // Non private to avoid synthetic class GrpcFuture(ClientCall\u003c?, RespT\u003e call) { this.call = call; } @Override protected void interruptTask() { call.cancel(\"GrpcFuture was cancelled\", null); } @Override protected boolean set(@Nullable RespT resp) { return super.set(resp); } @Override protected boolean setException(Throwable throwable) { return super.setException(throwable); } @SuppressWarnings(\"MissingOverride\") // Add @Override once Java 6 support is dropped protected String pendingToString() { return MoreObjects.toStringHelper(this).add(\"clientCall\", call).toString(); } } GrpcFuture继承了AbstractFuture，可以通过interruptTask取消grpc调用，通过set或者setException方法直接设置future的","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:6:1","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"grpc keepalive实现 [grpc Keepalive是一种在http2连接空闲（没有数据传输）是保持连接活动状态的技术，通过定期发送ping帧来实现。http2保活机制能够提升http2连接的性能和可靠性，但需要仔细配置保活间隔时间。 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:7:0","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":["grpc"],"content":"参考文献 gprc源码分析 zhengxinzx 一系列grpc源码分析，主要介绍了grpc的原理和流量控制，强烈推荐 grpc over http2 基于http2实现grpc协议规范 ","date":"2025-05-02","objectID":"/posts/grpc-in-practice/:8:0","tags":["grpc"],"title":"Grpc源码分析","uri":"/posts/grpc-in-practice/"},{"categories":null,"content":"阅前须知 ","date":"2025-04-28","objectID":"/posts/java-concurrency-jmm/:1:0","tags":null,"title":"Java内存模型","uri":"/posts/java-concurrency-jmm/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"阅前提示 参考文献中的文章非常的好，基本看完了就能理解很多东西，推荐阅读 源码中也提供了很多注释文本，推荐对照源码学习。 ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:1:0","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"重要概念和接口 ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:2:0","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"Runnable 接口 @FunctionalInterface public interface Runnable { public abstract void run(); } 线程可以接受一个实现 Runnable 接口的对象，并执行对应的逻辑。 ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:2:1","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"Callable 接口 @FunctionalInterface public interface Callable\u003cV\u003e { V call() throws Exception; } 类似Runnalbe接口，但可以返回结果和抛出异常 ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:2:2","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"Future 接口 表示异步执行的结果，提供了获取结果以及取消计算执行等方法。 public interface Future\u003cV\u003e { // 取消任务执行，mayInterruptIfRunning参数为true时将中断正在执行任务的线程，否则正在执行的任务将继续执行 boolean cancel(boolean mayInterruptIfRunning); // 是否任务在执行完成前被取消 boolean isCancelled(); // 任务是否完成，不管任务正常结束、抛出异常还是被取消都认为任务完成 boolean isDone(); // 等待任务完成并获得结果 // 计算被取消时抛出CalcellationException // 计算抛出异常时抛出ExecutionException // 当前线程等待时被中断抛出InteruptedException V get() throws InterruptedException, ExecutionException; // 超时版本的get，如果等待超时抛出TimeoutException V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; } ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:2:3","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"Executor 接口 public interface Executor { void execute(Runnable command); } 线程池基础接口，提交一个任务到线程池执行。 Memory consistency effects: Actions in a thread prior to submitting a Runnable object to an Executor happen-before its execution begins, perhaps in another thread. ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:2:4","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"ExecutorSerivce 接口 ExecutorService继承了Executor接口，通常我们使用ExecutorService作为线程池接口，它提供了丰富的功能，一般能够满足需求。 public interface ExecutorService extends Executor { // 已提交的任务继续执行，但不再接收新任务，等待正在执行任务终止请使用awaitTermination void shutdown(); // 尝试停止所有正在执行的任务，终止所有等待任务的处理，返回等待任务列表，等待正在执行任务终止请使用awaitTermination // 无法保证所以任务都能终止，经典的实现会通过`interrupt`取消任务，但如果任务不响应中断，则可能永远都不会停止 List\u003cRunnable\u003e shutdownNow(); // 线程池是否关闭 boolean isShutdown(); // shutdown后所有任务是否已经结束 // 这个方法只有在调用shutdown或者shutdownNow后才可能返回true boolean isTerminated(); // 在shutdown后调用，阻塞直到所有任务执行完成或者超时发生或者当前线程被中断 boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; // 提交一个有返回值的任务，通过Future对象获取返回值 // task不能为null，否则抛出NullPointerException // 如果任务不能被调用执行，抛出 RejectedExecutionException \u003cT\u003e Future\u003cT\u003e submit(Callable\u003cT\u003e task); // 类似 submit(Callable)，返回值对应传入的result参数 \u003cT\u003e Future\u003cT\u003e submit(Runnable task, T result); // 类似 submit(Callable)，返回值为null Future\u003c?\u003e submit(Runnable task); // 执行给定的任务列表，当全部完成时返回Futrue列表，在操作执行时修改给定的集合会导致未定义行为 // 在等待时发生中断，抛出InterruptedException，取消未完成的任务 // 任务列表和其中的任务都不能为null，否则抛出NullPointerException // 如果任何任务不能被调度执行，抛出RejectedExecutionException \u003cT\u003e List\u003cFuture\u003cT\u003e\u003e invokeAll(Collection\u003c? extends Callable\u003cT\u003e\u003e tasks) throws InterruptedException; // 超时版本的invokeAll，如果超时发生，取消未完成的任务 \u003cT\u003e List\u003cFuture\u003cT\u003e\u003e invokeAll(Collection\u003c? extends Callable\u003cT\u003e\u003e tasks, long timeout, TimeUnit unit) throws InterruptedException; // 类似invokeAll，执行给定的任务列表，返回一个成功执行任务的结果，指没有抛出异常，取消未执行完成的任务 // tasks为空时抛出IllegalArgumentException // 如果没有任务成功完成，抛出ExecutionException \u003cT\u003e T invokeAny(Collection\u003c? extends Callable\u003cT\u003e\u003e tasks) throws InterruptedException, ExecutionException; // 超时版本的invokeAny // 超时发生抛出TimeoutException \u003cT\u003e T invokeAny(Collection\u003c? extends Callable\u003cT\u003e\u003e tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; } Memory consistency effects: Actions in a thread prior to the submission of a Runnable or Callable task to an ExecutorService happen-before any actions taken by that task, which in turn happen-before the result is retrieved via Future. get(). Doug Lea 给了一个终止线程池的例子，首先调用shutdown拒绝接受新任务，然后调用shutdowNow，取消逗留的任务，这里特别处理了当前线程遇到interrupt的情况。 void shutdownAndAwaitTermination(ExecutorService pool) { pool.shutdown(); // Disable new tasks from being submitted try { // Wait a while for existing tasks to terminate if (!pool.awaitTermination(60, TimeUnit.SECONDS)) { pool.shutdownNow(); // Cancel currently executing tasks // Wait a while for tasks to respond to being cancelled if (!pool.awaitTermination(60, TimeUnit.SECONDS)) System.err.println(\"Pool did not terminate\"); } } catch (InterruptedException ie) { // (Re-)Cancel if current thread also interrupted pool.shutdownNow(); // Preserve interrupt status Thread.currentThread().interrupt(); } } ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:2:5","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"FutureTask 类源码解析 接口RunnableFuture是Runnalbe的Future，run方法的成功执行对应Future的完成，并允许获取结果。 public interface RunnableFuture\u003cV\u003e extends Runnable, Future\u003cV\u003e { void run(); } FutureTask类实现了RunnableFuture，FutureTask的具体实现原理留在后面再讲。(todo) ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:3:0","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"AbstractExecutorService 源码解析 AbstractExecutorService抽象类派生自ExecutorService接口，然后在其基础上实现了几个实用的方法，这些方法提供给子类进行调用。 抽象类实现了 invokeAny 和 invokeAll 方法（这两个方法先不看 todo），方法newTaskFor用于将Runnable或者Callable包装成FutureTask。提交任务到线程池中有两类方法，submit用于需要返回值的场景，execute用于不需要返回值的场景，当然可以都只用submit方法，当不需要返回值时返回 null 即可。 public abstract class AbstractExecutorService implements ExecutorService { // 将runnable包装成FutureTask protected \u003cT\u003e RunnableFuture\u003cT\u003e newTaskFor(Runnable runnable, T value) { return new FutureTask\u003cT\u003e(runnable, value); } // 将Callable包装成FutureTask protected \u003cT\u003e RunnableFuture\u003cT\u003e newTaskFor(Callable\u003cT\u003e callable) { return new FutureTask\u003cT\u003e(callable); } // 包装成FutureTask，并交给底层execute方法执行 public Future\u003c?\u003e submit(Runnable task) { if (task == null) throw new NullPointerException(); RunnableFuture\u003cVoid\u003e ftask = newTaskFor(task, null); execute(ftask); return ftask; } public \u003cT\u003e Future\u003cT\u003e submit(Runnable task, T result) { if (task == null) throw new NullPointerException(); RunnableFuture\u003cT\u003e ftask = newTaskFor(task, result); execute(ftask); return ftask; } public \u003cT\u003e Future\u003cT\u003e submit(Callable\u003cT\u003e task) { if (task == null) throw new NullPointerException(); RunnableFuture\u003cT\u003e ftask = newTaskFor(task); execute(ftask); return ftask; } ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:4:0","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"ThreadPoolExecutor ThreadPoolExecutor是 JDK 中的线程池实现，实现了任务提交、线程管理、监控等方法。 通过构造函数，介绍一些重要的属性： corePoolSize 核心线程数，注意有时将核心线程数内的线程称为核心线程，但核心线程本身和其他线程一样 maximumPoolSize 最大线程数 workQueue 任务队列，BlockingQueue 接口的某个实现（常用 ArrayBlockingQueue 和 LinkedBlockingQueue） keepAliveTime 空闲线程的保活线程，默认只对非核心线程生效，可以通过设置allowCoreThreadTimeout(true)使核心线程数内的线程可以被回收 threadFactory 用于生成线程，比如设置线程的名字 handler 设置线程池的拒绝策略 Doug Lea 采用一个 32 为的整数来存放线程池状态和线程池中的线程数，其中高 3 为用于存放线程池状态，低 29 位表示线程数。 private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); private static final int COUNT_BITS = Integer.SIZE - 3; private static final int CAPACITY = (1 \u003c\u003c COUNT_BITS) - 1; // runState is stored in the high-order bits private static final int RUNNING = -1 \u003c\u003c COUNT_BITS; private static final int SHUTDOWN = 0 \u003c\u003c COUNT_BITS; private static final int STOP = 1 \u003c\u003c COUNT_BITS; private static final int TIDYING = 2 \u003c\u003c COUNT_BITS; private static final int TERMINATED = 3 \u003c\u003c COUNT_BITS; // Packing and unpacking ctl private static int runStateOf(int c) { return c \u0026 ~CAPACITY; } private static int workerCountOf(int c) { return c \u0026 CAPACITY; } private static int ctlOf(int rs, int wc) { return rs | wc; } /* * Bit field accessors that don't require unpacking ctl. * These depend on the bit layout and on workerCount being never negative. */ private static boolean runStateLessThan(int c, int s) { return c \u003c s; } private static boolean runStateAtLeast(int c, int s) { return c \u003e= s; } private static boolean isRunning(int c) { return c \u003c SHUTDOWN; } 线程池各种状态的介绍： RUNNING: 接受新的任务，处理等待队列中的任务 SHUTDWON: 不接受新的任务，但会继续处理等待队列中的任务 STOP: 不接受新的任务提交，不再处理等待队列中的任务，中断正在执行的线程 TIDYING: 所有的任务都销毁了，workCount 为 0，执行钩子方法 terminated() TERMINATED: terminated()方法调用结束后，线程池的状态切换为此 RUNNING 定义为-1，SHUTDOWN 定义为 0，其他都比 0 大，所以等于 0 时不能提交任务，大于 0 的话，连正在执行的任务也要中断 状态迁移过程： RUNNING -\u003e SHUTDOWN，调用 shutdown() (RUNNING or SHUTDOWN) -\u003e STOP: 调用 shutdownNow() SHUTDOWN -\u003e TIDYING: 当任务队列和线程池都清空后，有 SHUTDOWN 转换为 TIDYING STOP -\u003e TIDYING: 任务队列清空后 TIDYING -\u003e TERMINATED: terminated()方法结束后 Doug Lea 将线程池中的线程包装成内部类 Worker，所以任务是 Runnable （内部变量名叫 task 或者 command)，线程是 worker AQS: todo worker 的实现包含复杂的并发控制，这些暂时不考虑 private final class Worker extends AbstractQueuedSynchronizer implements Runnable { /** Thread this worker is running in. Null if factory fails. */ final Thread thread; /** Initial task to run. Possibly null. */ Runnable firstTask; /** Per-thread task counter */ volatile long completedTasks; /** * Creates with given first task and thread from ThreadFactory. * @param firstTask the first task (null if none) */ Worker(Runnable firstTask) { setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this); } /** Delegates main run loop to outer runWorker */ public void run() { runWorker(this); } execute是一个非常重要的方法，所有submit方法底层都会调用execute方法提交任务。可以看到尽管这段代码非常短小，但由于并发问题实现逻辑比较绕。 public void execute(Runnable command) { if (command == null) throw new NullPointerException(); int c = ctl.get(); // 如果当前线程数少于核心线程数，直接添加一个worker来执行任务，将当前任务作为它的第一个任务 // addWorker调用会原子的检查runState和workerCount，避免错误添加新的线程 if (workerCountOf(c) \u003c corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } // 如果线程池处于RUNNING状态，将这个任务添加到任务队列workQueue中 if (isRunning(c) \u0026\u0026 workQueue.offer(command)) { // double-check int recheck = ctl.get(); // 如果线程不处于RUNNING状态，移除已经入队的任务，并执行拒绝策略 if (! isRunning(recheck) \u0026\u0026 remove(command)) reject(command); // 如果线程池还是RUNNING状态，并且线程数为0，那么开启新的线程 else if (workerCountOf(recheck) == 0) addWorker(null, false); } // 如果队列满了，尝试创建新的线程，如果已经达到最大线程数，执行拒绝策略 else if (!addWorker(command, false)) reject(command); } addWorker 方法用来创建新的线程 private boolean addWorker(Runnable firstTask, boolean core) { retry: for (;;) { int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. // 线程池非RUNNINKG状态，则关闭 // 需要排除一种特殊情况，线程池处于SHUTDOWN状态，且等待队列非空，这种情况下应该允许进一步判断是否创建新的worker if (rs \u003e= SHUTDOWN \u0026\u0026 ! (rs == SHUTDOWN \u0026\u0026 firstTask == null \u0026\u0026 ! workQueue.isEmpty())) return false; for (;;) { int wc = ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:5:0","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"Executors工具类 生成一个固定大小的线程池 public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u003cRunnable\u003e()); } 最大线程数设置为和核心线程数相等，此时keepAliveTime设置为0（线程池默认不会不会corePoolSize内的线程），任务队列采用LinkedBlockingQueue，无界队列。 单线程线程池，类似上面，核心线程数为1 public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u003cRunnable\u003e())); } 缓存线程池 public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue\u003cRunnable\u003e()); } 核心线程数为0，最大线程数为Integer.MAX_VALUE，keepAliveTime为60s，任务队列采用SynchronousQueue 线程数不设上限，任务队列为同步队列，60s超时后空闲线程会被回收 ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:6:0","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["JVM","Java","JDK11","线程池"],"content":"参考文献 深度解读 java 线程池设计思想及源码实现 javadoop ","date":"2025-04-26","objectID":"/posts/java_thread_pool/:7:0","tags":["JVM","Java","JDK11","线程池"],"title":"Java线程池源码分析","uri":"/posts/java_thread_pool/"},{"categories":["Spark"],"content":"通过如下的方法在idea中配置spark开发环境，最后和一般的java项目一样，使用maven面板的 clean和package进行编译。 我实际使用的编译器为java17，idea会提示配置scala编译器。 The Maven-based build is the build of reference for Apache Spark. Building Spark using Maven requires Maven 3.9.6 and Java 8/11/17. Spark requires Scala 2.12/2.13; support for Scala 2.11 was removed in Spark 3.0.0. While many of the Spark developers use SBT or Maven on the command line, the most common IDE we use is IntelliJ IDEA. You can get the community edition for free (Apache committers can get free IntelliJ Ultimate Edition licenses) and install the JetBrains Scala plugin from Preferences \u003e Plugins. To create a Spark project for IntelliJ: Download IntelliJ and install the Scala plug-in for IntelliJ. Go to File -\u003e Import Project, locate the spark source directory, and select “Maven Project”. In the Import wizard, it’s fine to leave settings at their default. However it is usually useful to enable “Import Maven projects automatically”, since changes to the project structure will automatically update the IntelliJ project. As documented in Building Spark, some build configurations require specific profiles to be enabled. The same profiles that are enabled with -P[profile name] above may be enabled on the Profiles screen in the Import wizard. For example, if developing for Hadoop 2.7 with YARN support, enable profiles yarn and hadoop-2.7. These selections can be changed later by accessing the “Maven Projects” tool window from the View menu, and expanding the Profiles section. ","date":"2025-04-05","objectID":"/posts/spark_developement_envrionment/:0:0","tags":["Spark"],"title":"Spark开发环境搭建","uri":"/posts/spark_developement_envrionment/"},{"categories":["Spark"],"content":"遇到的问题 Exception in thread \"main\" java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/Filter at java.base/java.lang.Class.forName0(Native Method) at java.base/java.lang.Class.forName(Class.java:578) at java.base/java.lang.Class.forName(Class.java:557) at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41) at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36) at org.apache.spark.util.SparkClassUtils$.classForName(SparkClassUtils.scala:141) at org.apache.spark.sql.SparkSession$.lookupCompanion(SparkSession.scala:826) at org.apache.spark.sql.SparkSession$.CLASSIC_COMPANION$lzycompute(SparkSession.scala:816) at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$CLASSIC_COMPANION(SparkSession.scala:815) at org.apache.spark.sql.SparkSession$.$anonfun$DEFAULT_COMPANION$1(SparkSession.scala:820) at scala.util.Try$.apply(Try.scala:217) at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$DEFAULT_COMPANION(SparkSession.scala:820) at org.apache.spark.sql.SparkSession$Builder.\u003cinit\u003e(SparkSession.scala:854) at org.apache.spark.sql.SparkSession$.builder(SparkSession.scala:833) at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:28) at org.apache.spark.examples.SparkPi.main(SparkPi.scala) Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.Filter at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:528) ... 16 more 在idea Run/Debug Configuration中添加Add dependencies with provided scope to classpath org.apache.spark.SparkException: A master URL must be set in your configuration at org.apache.spark.SparkContext.\u003cinit\u003e(SparkContext.scala:421) at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:3062) at org.apache.spark.sql.classic.SparkSession$Builder.$anonfun$build$2(SparkSession.scala:911) at scala.Option.getOrElse(Option.scala:201) at org.apache.spark.sql.classic.SparkSession$Builder.build(SparkSession.scala:902) at org.apache.spark.sql.classic.SparkSession$Builder.getOrCreate(SparkSession.scala:931) at org.apache.spark.sql.classic.SparkSession$Builder.getOrCreate(SparkSession.scala:804) at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:923) at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:30) at org.apache.spark.examples.SparkPi.main(SparkPi.scala) 添加jvm参数-Dspark.master=local，本地运行 不知道为什么idea不能直接找到parallelize的定义而飘红，这里直接导入SparkContext并且通过asInstanceOf[SparkContext]明示idea。 // scalastyle:off println package org.apache.spark.examples import scala.math.random import org.apache.spark.SparkContext import org.apache.spark.sql.SparkSession /** Computes an approximation to pi */ object SparkPi { def main(args: Array[String]): Unit = { val spark = SparkSession .builder() .appName(\"Spark Pi\") .getOrCreate() val slices = if (args.length \u003e 0) args(0).toInt else 2 val n = math.min(100000L * slices, Int.MaxValue).toInt // avoid overflow val count = spark.sparkContext.asInstanceOf[SparkContext].parallelize( 1 until n, slices).map { i =\u003e val x = random() * 2 - 1 val y = random() * 2 - 1 if (x*x + y*y \u003c= 1) 1 else 0 }.reduce(_ + _) println(s\"Pi is roughly ${4.0 * count / (n - 1)}\") spark.stop() } } ","date":"2025-04-05","objectID":"/posts/spark_developement_envrionment/:1:0","tags":["Spark"],"title":"Spark开发环境搭建","uri":"/posts/spark_developement_envrionment/"},{"categories":["Spark","内存管理"],"content":"关键问题 内存被分成哪些区域，各分区之间的关系是什么，通过什么参数控制 内存上报和释放的单位是什么，上报和释放是如何实现的 如何避免内存没有释放导致资源泄露 如何避免重复上报和漏上报问题 对象的生命周期和内存上报释放之间的关系 哪些对象会被上报，为什么选择这些对象上报 内存上报是否持有对象引用 ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:1:0","tags":["Spark","内存管理"],"title":"Spark内存管理","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","内存管理"],"content":"源码分析 ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:2:0","tags":["Spark","内存管理"],"title":"Spark内存管理","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","内存管理"],"content":"MemoryBlock MemoryBlock表示一段连续的内存空间，类似于操作系统中page的概念。 MemoryBlock继承自MemoryLocation，当追踪堆外分配时，obj为空，offset表示堆外内存地址，当追踪堆内内存分配时，obj为对象引用，offset为对象内偏移量，可以看到MemoryLocation只是记录了对象的位置信息，没有记录对象内存占用的信息。 public class MemoryLocation { @Nullable Object obj; long offset; public class MemoryBlock extends MemoryLocation { private final long length; public int pageNumber = NO_PAGE_NUMBER; MemoryBlock新增两个字段，length表示page的大小，pageNumber很好理解，TaskMemoryManager会给每个页分配一个页号，有以下几种特殊情况 NO_PAGE_NUMBER 表示没有被TaskMemoryManager分配，初始值 FREED_IN_TMM_PAGE_NUMBER 表示被TaskMemoryManager释放，TaskMemoryManager.free操作中会将页号设置为此值，MemoryAllocator.free遇到没有被TaskMemoryMananger释放的页时，会报错 FREED_IN_ALLOCATOR_PAGE_NUMBER 被MemoryAllocator释放，可以检测多次释放 ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:2:1","tags":["Spark","内存管理"],"title":"Spark内存管理","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","内存管理"],"content":"MemoryAllocator MemoryAllocator接口定义了申请和释放MemoryBlock的方法，HeapMemoryAllocator和UnsafeMemoryAllocator分别实现了堆内和堆外的内存分配器。 public interface MemoryAllocator { /** * Allocates a contiguous block of memory. Note that the allocated memory is not guaranteed * to be zeroed out (call `fill(0)` on the result if this is necessary). */ MemoryBlock allocate(long size) throws OutOfMemoryError; void free(MemoryBlock memory); MemoryAllocator UNSAFE = new UnsafeMemoryAllocator(); MemoryAllocator HEAP = new HeapMemoryAllocator(); } HeapMemoryAllocator public class HeapMemoryAllocator implements MemoryAllocator { @GuardedBy(\"this\") private final Map\u003cLong, LinkedList\u003cWeakReference\u003clong[]\u003e\u003e\u003e bufferPoolsBySize = new HashMap\u003c\u003e(); private static final int POOLING_THRESHOLD_BYTES = 1024 * 1024; 可以看到实际分配的对象就是long数组，并且做了池化，对于1MB以上的内存尝试放入池中，这里没有限制池的大小，持有的是long数组的弱引用，减少频繁申请和释放大内存造成的开销。 如果申请不到内存，会抛出OutOfMemoryError UnsafeMemoryAllocator 实现没有什么特殊的地方，直接调用Spark包装过的Unsafe API，直接调用Unsafe包中的API，所以不受MaxDirectMemorySize的控制 public long allocateMemory(long bytes) { beforeMemoryAccess(); return theInternalUnsafe.allocateMemory(bytes); } ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:2:2","tags":["Spark","内存管理"],"title":"Spark内存管理","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","内存管理"],"content":"MemoryManager MemoryManager抽象类负责管理内存，在计算和存储之间共享内存，计算内存指在shuffles, joins, sorts and aggregations 中计算过程所使用的内存，而存储内存指被用于缓存或者在集群中传播内部数据所占用的内存，每个JVM只有一个MemoryManager。 Spark内存参数 spark.memory.offHeap.enabled 如果开启，某些计算将使用堆外内存，要求spark.memory.offHeap.size必须为正数，默认关闭 spark.memory.fraction (堆内存 - 300MB)被用于计算和存储的比例，这个值越低，吐磁盘以及缓存驱逐发生的越频繁，这个设置的主要目的是留出空间给用户数据结构以及比如稀疏、不寻常的大内存记录导致的内存估算不准确。默认值为0.6 spark.memory.offHeap.size指定了spark堆外使用的内存大小 saprk.memory.storageFraction免于驱逐的存储内存占用内存大小，这里表示为spark.memory.fraction留出的内存的百分比。默认为0.5 堆外内存由spark.memory.offHeap.size规定，堆外存储内存为$spark.memory.offHeap.size * spark.memory.storageFractioin$，剩余的内存为堆外计算内存。 主要字段和方法 @GuardedBy(\"this\") protected val onHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(\"this\") protected val offHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.OFF_HEAP) @GuardedBy(\"this\") protected val onHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(\"this\") protected val offHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.OFF_HEAP) StorageMemoryPool实际管理存储内存，ExecutionMemoryPool实际管理计算内存，这两者在处理关键操作是都需要持有MemoryManager对象锁，从而实现在存储和计算之间共享内存的操作。 acquireStorageMemory:获得存储内存用来缓存block等 acquireUnrollMemory: 获取展开内存用来展开给定的block acquireExecutionMemory: 获得计算内存，调用可能阻塞，确保每个任务至少有机会获得$1/ 2N$内存池大小，N表示当前活跃任务数量，比如老的任务已经占用了很多内存而任务数增加 releaseExecutionMemory 释放计算内存 releaseAllExecutionMemoryForTask 释放当前任务的所有计算内存 releaseStorageMemory 释放存储内存 releaseAllStorageMemory 释放所有存储内存 releaseUnrollMemory 释放展开内存 ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:2:3","tags":["Spark","内存管理"],"title":"Spark内存管理","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","内存管理"],"content":"UnifiedMemoryManager 堆内的内存总量由spark.testing.memory指定，默认为jvm堆大小，保留内存为300MB $$ 可用于存储或者计算的内存 = (spark.testing.memory - reserved\\ memory) * spark.memory.fraction $$ 初始存储内存大小占比由spark.memory.storageFraction指定，存储和计算可以相互借用对方的内存，遵循以下规则： 如果计算内存不足，可最多可以让存储将占用超过初始存储内存大小的空间返还给计算内存 如果存储空间不足，可以借用计算内存的多余空间 acquireExecutionMemory: 实际调用executionPool.acquireMemory，依赖于回调函数maybeGrowExecutionPool和computeMaxExecutionPoolSize，前者可能将部分存储内存转移到计算内存，后者计算当前情况下最大计算内存，等于可用内存减去存储内存当前占用和存储内存初始大小的最小值。 acquiredStorageMemory: 如果存储内存空间不足，则尝试借用部分计算内存空间，最后调用storagePool.acquireMemory实际执行操作 acquireUnrollMemory: 实际调用acquiredStorageMemory ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:2:4","tags":["Spark","内存管理"],"title":"Spark内存管理","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","内存管理"],"content":"MemoryPool 管理一块可以调整大小的内存区域的内部状态和使用记录。 ExecutionMemoryPool 字段memoryForTask记录了每个task id (long)对应的内存消耗(long)。 每个任务最少可以占用 $1 / 2N * poolSize$，而每个任务最多占用$1 / N * maxPoolsize$ acquireMemory: 如果是新的任务，加入memoryForTask，并且通知所有等待获取计算内存的任务，当前任务数增加 循环，直到任务占用超过了上限1/N，或者有空闲内存，以下步骤均在循环体来 调用maybeGrowPool尝试从存储空间获取内存 计算每个任务的最少内存占用和最高内存占用 如果获得的计算内存加上当前内存占用低于最少内存占用，则等待通知 否则更新状态，并返回获取到的内存大小 releaseMemory: 释放内存，如果释放后当前内存为0，则移除当前任务，只要释放内存，则通知在acquiredMemory等待的任务内存已经释放 StorageMemoryPool acquireMemory: 如果存储空间不足，则调用memoryStore.evictBlocksToFreeSpace释放部分空间，判断需要的内存大小是否小于等于当前空闲内存 releaseMemory: 释放内存 freeSpaceToShrinkPool: 释放内存来减少存储空间的占用，必要时调用memoryStore.evictBlocksToFreeSpace驱逐block ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:2:5","tags":["Spark","内存管理"],"title":"Spark内存管理","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","内存管理"],"content":"TaskMemoryManager 内存地址编码 当需要将一个int或者long之类的元素插入到数组或者堆外的指定位置时 对于堆内，需要知道数据的引用以及偏移量，在TaskMemoryManager中保存了pageNumber和MemoryBlock的映射，而MemoryBlock保存了对象的引用，所以使用64位编码内存地址时，前13位用来储存pageNumber，后51位用来存储数组中的偏移量。（对象的地址会由于gc的原因而变动，所以不能直接使用对象地址） 对于堆外，需要知道申请到堆外内存的起始地址和偏移量，依然使用前13位存储pageNumber，使用后51位存储偏移量。这里如果直接使用内存地址，则不能知道对应的page是那个，当使用前13位储存pageNumber后，后51位显然不能储存内存的绝对地址，而应该存储内存相对于起始地址的偏移量。 主要字段作用 pageTable: 页表，保存pageNumber到MemoryBlock的映射，MemoryBlock[PAGE_TABLE_SIZE] memoryManager: TaskMemoryManager共享MemoryManager的内存资源 taskAttemptId: task Id tungtenMemoryMode: 使用堆内还是堆外内存，和MemoryManger保持一致 consumers：内存消费者，支持吐磁盘，HashSet\u003cMemoryConsumer\u003e acquiredButNotUsed: 向内存管理框架申请内存成功，但实际申请内存时发生OOM，认为MemoryManager可能高估了实际的可用内存，将这部分内存配额保存在此字段，方便后续触发吐磁盘，long currentOffHeapMemory: 任务当前堆外内存占用，long currentOnHeapMemory：任务当前堆内内存占用，long peakOffHeapMemory：任务最高堆外内存占用，long peakOnHeapMemory：任务最高堆内内存占用，long 主要方法 acquireExecutionMemory为指定的MemoryConsumer获取内存，如果没有足够的内存，触发吐磁盘释放内存，返回成功获得的计算内存(\u003c=N)。 public long acquireExecutionMemory(long required, MemoryConsumer requestingConsumer) { 首先调用MemoryManager.acquireExecutionMemory尝试获取计算内存 如果获取到足够的内存，则跳过吐磁盘逻辑 如果没有获取到足够的内存，尝试吐磁盘释放内存，并尝试获取计算内存 吐磁盘有两个优化的目标： 最小化吐磁盘调用的次数，减少吐磁盘文件的数量并且避免小的吐磁盘文件 避免吐磁盘释放内存超过所需，如果我们只是想要一丁点内存，不希望尽可能多的吐磁盘，很多内存消费者吐磁盘时会释放比请求多的内存 所以这里采用一种启发式的算法，选择内存占用超过所需内存的MemoryConsumer中最小的MemoryConsumer来平衡这些因素，当只有少量大内存请求时，这种方法效率很好，但如果场景中有大量小内存请求，这种方法会导致产生大量小的spill文件 具体实现，将所有的MemoryConsumer放入一个TreeMap中，根据内存占用排序，如果是当前MemoryConsumer，则认为内存占用为0，这样当前MemoryConsumer被spill的优先级最低。 然后选择内存占用超过所需内存的MemoryConsumer中最小的MemoryConsumer进行吐磁盘操作并且尝试获取计算内存，如果没有符合这一条件的MemoryConsumer，则直接选择内存占用最大的MemoryCosumer进行吐磁盘并尝试获取计算内存trySpillAndAcquire。 如果获取到的内存依然不满足需求，则继续吐磁盘流程，选择下一个MemoryConsumer，重复上述流程。 最终不管是否获取到了所需的内存，都将MemoryConsumer加入consumers中，并更新当前和最高的任务内存占用 trySpillAndAcquire对选中的MemoryConsumer执行吐磁盘操作释放内存，并尝试获取所需的计算内存 * @return number of bytes acquired (\u003c= requested) * @throws RuntimeException if task is interrupted * @throws SparkOutOfMemoryError if an IOException occurs during spilling */ private long trySpillAndAcquire(MemoryConsumer requestingConsumer, long requested, List\u003cMemoryConsumer\u003e cList, int idx) 首先调用MemoryConsumer#spill方法尝试释放内存，如果释放内存为0，则直接返回0 如果释放内存大于0，调用MemoryManager#acquireExecutionMemory尝试获取计算内存，这里需要注意，吐磁盘释放的内存会被所有任务公平竞争，所以可能无法获取到这次吐磁盘释放的所有内存，需要在下一次循环中继续尝试吐磁盘 两种异常场景，当任务被中断时，抛出RuntimeException，吐磁盘遇到IOException时，抛出SparkOutOfMemoryError releaseExecutionMemory 为一个MemoryConsumer释放N字节的计算内存，实际调用了MemoryManager#releaseExecutionMemory，并更新当前内存占用 showMemoryUsage dump所有Consumer的内存占用 allocatePage 分配内存，并更新页表，该操作旨在为多个算子之间共享的大块内存分配空间 public MemoryBlock allocatePage(long size, MemoryConsumer consumer) 首先调用TaskMemoryManager#acquiredExectionMemory获取计算内存，如果没有获取到内存，则返回null 然后通过MemoryManager#tungstenMemoryAllocator#allocate实际申请内存，如果遇到OutOfMemoryError，则认为实际上没有足够多的内存，实际的空闲内存要比MemoryManager认为的少一些，所以将从内存管理框架中获得的内存配额添加到acquiredButNotUsed字段中，并再次调用当前函数，这次将触发吐磁盘操作释放内存（p.s. 感觉处理OutOfMemoryError的意义不大，OutOfMeomryError发生时应该直接结束程序，因为程序已经进入了异常状态，无法预料OutOfMemoryError对程序的影响） 如果成功获取到内存，则需要更新页表，并返回对应的页，其实就是MemoryBlock freePage释放页占用的内存，更新pageNumber为FREED_IN_TMM_PAGE_NUMBER，清理页表，调用MemoryManager.tunstenMemoryAllocator#free实际释放内存，调用releaseExecutionMemory释放内存管理框架对应的内存配额。 似乎用逻辑内存指代内存管理框架中的内存配额，而用物理内存指代实际的内存更加好一些 public void freePage(MemoryBlock page, MemoryConsumer consumer) { cleanUpAllAllocatedMemory清理所有申请的内存和页 调用MemoryManager#tungstenMemoryAllocator#free释放每个页的内存 调用MemoryManager#releaseExectionMemory释放acquiredButNotUsed内存 调用MemoryManager#ReleaseAllExecutionMemoryForTask释放任务的所有计算内存，并返回释放的内存大小，非0值可以用来检测内存泄露 ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:2:6","tags":["Spark","内存管理"],"title":"Spark内存管理","uri":"/posts/spark_memory_manager/"},{"categories":["Spark","内存管理"],"content":"参考资料 Deep Dive into Spark Memory Management Apache Spark Memory Management: Deep Dive ","date":"2025-03-09","objectID":"/posts/spark_memory_manager/:3:0","tags":["Spark","内存管理"],"title":"Spark内存管理","uri":"/posts/spark_memory_manager/"},{"categories":["kubernetes"],"content":"Pods Pod 是 Kubernetes 中最小的可部署计算单元，你可以创建和管理它们。 Pod（类似于一群鲸鱼的“pod”或豌豆荚“pea pod”） 是一组一个或多个容器，这些容器共享存储和网络资源，并且有一个规范来定义如何运行它们。Pod 内部的内容始终是 共同调度（co-scheduled）并在相同的上下文中运行 的。Pod 充当一个特定应用的“逻辑主机”（logical host）：它包含一个或多个 相对紧密耦合的应用容器。在非云环境下，运行在同一台物理机或虚拟机上的应用程序，可以类比于在 Kubernetes 中运行在同一逻辑主机上的应用。 除了应用容器之外，Pod 还可以包含 Init 容器（init containers），这些容器在 Pod 启动时运行。此外，你还可以 注入临时容器（ephemeral containers） 来调试正在运行的 Pod。 ","date":"2025-02-07","objectID":"/posts/k8s_storage/:1:0","tags":null,"title":"k8s 存储","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"Volumes k8s Volumes 为pod中的容器提供了通过文件系统访问和共享数据的方式，数据共享可以在一个容器中的不同进程或者容器间甚至不同的pod。 volume能够解决数据的持久化以及共享存储的问题。 k8s支持多种volumes，pod可以同时使用任意数量的不同类型volume，Ephemeral volume 的生命周期和pod相同，persistent volumes 可以超出一个pod的生命周期。当pod挂掉时，K8s会摧毁 ephemeral volume 但不会摧毁 persistent volume 。对于在给定pod中的任意类型的volume，数据在容器重启时都会被保留。 本质上，卷（Volume）是一个目录，其中可能包含一些数据，并且可供 Pod 内的容器访问。该目录如何创建、由何种存储介质支持以及其内容，取决于所使用的特定卷类型。 为了使用一个卷，声明要被提供给pod的卷在.spec.volumes下，声明在容器的哪里挂载这些卷在spec.containers[*].volumeMounts中。 当一个pod被启动时，容器中的进程看到的文件系统视图有两部分组成，一部分是容器镜像的初始内容，另一部分是挂载到容器中的卷。对于pod中的每个容器，需要独立的声明不同容器的挂载点。 ","date":"2025-02-07","objectID":"/posts/k8s_storage/:2:0","tags":null,"title":"k8s 存储","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"Storage Classes StorageClass 为管理员提供了一种描述其提供的存储类别的方法。不同的存储类别可能对应不同的 服务质量（QoS）级别、备份策略，或者由集群管理员自定义的其他策略。Kubernetes 本身并不对这些存储类别的具体含义做任何规定。 每个StorageClass 包含字段 provisioner, parameters和raclaimPolicy，当一个属于某个storage class 的persistent volume需要被动态提供给 persistent volume claim时被使用。 Storage Class的名字非常重要，用户通过名字请求某类存储，管理员在创建storage class对象是设置名字以及类别的其他参数。 作为管理员，你可以声明一个默认的storage class用于没有指定类别的任何PVC。 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: low-latency annotations: storageclass.kubernetes.io/is-default-class: \"false\" provisioner: csi-driver.example-vendor.example reclaimPolicy: Retain # default value is Delete allowVolumeExpansion: true mountOptions: - discard # this might enable UNMAP / TRIM at the block storage layer volumeBindingMode: WaitForFirstConsumer parameters: guaranteedReadWriteLatency: \"true\" # provider-specific ","date":"2025-02-07","objectID":"/posts/k8s_storage/:3:0","tags":null,"title":"k8s 存储","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"Persistent Volumes PersistentVolume (PV) 是集群中的一块存储，可以由管理员提供或者通过 Storage Classes动态提供。 **PersistentVolumeClaim (PVC)**是用户对存储的青秀区，类似于pod，pod消费节点资源而PVCs消费PV资源 有两种方式提供PVs： 静态：管理员直接创建PV 动态：当没有静态PV满足PVC，集群可能尝试动态的提供卷，PVC必须要求Storage Class，管理员必须创建并且配置storage class，Storage Class \"\"关闭动态获取卷 Pod使用PVC作为卷，集群检查PVC得到对应的卷并将卷绑定到pod。 ","date":"2025-02-07","objectID":"/posts/k8s_storage/:4:0","tags":null,"title":"k8s 存储","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"回收策略 当用户使用完卷后，可以将PVC对象删除从而允许资源的回收，PV的回收策略告诉集群当卷被释放后应该怎么样处理卷，目前有三种策略：Retained, Recycled和 Deleted。 这里只介绍Retain策略： Retain回收策略允许手动的资源回收，当PVC被删除时， PV依然存在，卷被认为是释放状态，但它并不能够被另一个PVC请求直接使用因为前任的数据还在上面。管理员可以通过以下方式手动回收卷： 删除PV 手动清理数据 手动删除对应的storage asset 如果想要重用相同的storage asset，使用相同的storage asset definition 创建一个新的PV ","date":"2025-02-07","objectID":"/posts/k8s_storage/:4:1","tags":null,"title":"k8s 存储","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"PV的类型 PV类型作为插件实现，这里给出k8s支持的一些插件： csi : Container Storage Interface local: 挂载在节点上的本地存储设备 每个PV包含一个规范(spec) 和状态 (status)，PV对象的名字必须是一个有效的 DNS subdomain name，这意味着 包含不超过253个字符 只包含小写字符、数字、-或者. 字符或者数字开头 字符或者数字结尾 apiVersion: v1 kind: PersistentVolume metadata: name: pv0003 spec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: slow mountOptions: - hard - nfsvers=4.1 nfs: path: /tmp server: 172.17.0.2 访问模式 ReadWriteOnce，卷可以被挂载为单个节点可读写，ReadWriteOnce仍然允许多个pod访问，只要这些pod在相同的节点上。对于单pod访问，可以使用ReadWriteOncePod。 节点亲和度 Node Affinity，对于大多数卷类型，不需要设置这个字段，对于local卷需要显示设置这个字段。 一个PV可以声明节点亲和度来限制在那个节点上这个卷可以被访问，使用某个PV的Pod将只会被调度到满足节点亲和度的节点上。 ","date":"2025-02-07","objectID":"/posts/k8s_storage/:4:2","tags":null,"title":"k8s 存储","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"PersistentVolumeClaims apiVersion: v1 kind: PersistentVolumeClaim metadata: name: myclaim spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: slow selector: matchLabels: release: \"stable\" matchExpressions: - {key: environment, operator: In, values: [dev]} ","date":"2025-02-07","objectID":"/posts/k8s_storage/:4:3","tags":null,"title":"k8s 存储","uri":"/posts/k8s_storage/"},{"categories":["kubernetes"],"content":"参考文献 https://kubernetes.io/docs/concepts/storage/persistent-volumes/ ","date":"2025-02-07","objectID":"/posts/k8s_storage/:5:0","tags":null,"title":"k8s 存储","uri":"/posts/k8s_storage/"},{"categories":null,"content":"Prometheus https://www.youtube.com/watch?v=h4Sl21AKiDg Prometheus Server, Pushgateway, Alertmanager https://prometheus.io/docs/concepts/metric_types/ https://itnext.io/prometheus-for-beginners-5f20c2e89b6c Prometheus is essentially just another metrics collection and analysis tool, and at its core it is made up of 3 components: A time series database that will store all our metrics data A data retrieval worker that is responsible for pulling/scraping metrics from external sources and pushing them into the database A web server that provides a simple web interface for configuration and querying of the data stored. https://prometheus.io/docs/practices/naming/#metric-names https://prometheus.io/docs/practices/naming/#base-units ","date":"2025-02-01","objectID":"/posts/prometheus_and_grafana/:1:0","tags":["prometheus","grafana"],"title":"Prometheus_and_grafana","uri":"/posts/prometheus_and_grafana/"},{"categories":["MinIO"],"content":"xl.meta 数据结构 当对象大小超过 128KiB 后，比如a.txt，数据和元数据分开存储 MinIO 提供了命令行工具xl-meta用来查看xl.meta文件 { \"Versions\": [ { \"Header\": { \"EcM\": 1, \"EcN\": 0, \"Flags\": 2, \"ModTime\": \"2025-01-23T15:27:45.311572+08:00\", \"Signature\": \"d0c2b58b\", \"Type\": 1, \"VersionID\": \"00000000000000000000000000000000\" }, \"Idx\": 0, \"Metadata\": { \"Type\": 1, \"V2Obj\": { \"CSumAlgo\": 1, \"DDir\": \"74hQxU7FTrq56ShK8pjqAA==\", \"EcAlgo\": 1, \"EcBSize\": 1048576, \"EcDist\": [1], \"EcIndex\": 1, \"EcM\": 1, \"EcN\": 0, \"ID\": \"AAAAAAAAAAAAAAAAAAAAAA==\", \"MTime\": 1737617265311572000, \"MetaSys\": {}, \"MetaUsr\": { \"content-type\": \"text/plain\", \"etag\": \"90a1a2b65a4e40d55d758f2a59fe33b4\" }, \"PartASizes\": [2097152], \"PartETags\": null, \"PartNums\": [1], \"PartSizes\": [2097152], \"Size\": 2097152 }, \"v\": 1734527744 } } ] } . ├── a.txt │ ├── ef8850c5-4ec5-4eba-b9e9-284af298ea00 │ │ └── part.1 │ └── xl.meta └── b.txt └── xl.meta ","date":"2025-01-22","objectID":"/posts/minio-get-started/:1:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"minio 的启动流程 minio 启动核心的核心命令为 minio server https://minio{1...4}.example.net:9000/mnt/disk{1...4}/minio，表示 minio 服务分布部署在 4 台服务器上总共 16 块磁盘上，...这种写法称之为拓展表达式，比如 http://minio{1…4}.example.net:9000实际上表示http://minio1.example.net:9000到http://minio4.example.net:9000`的4台主机。 go 程序的入口为main#main()函数，直接调用了cmd#Main,其中做了一些命令行程序的相关操作，包括注册命令，其中registerCommand(serverCmd)注册服务相关命令，cmd#ServerMain是主要启动流程函数。 // Run the app - exit on error. if err := newApp(appName).Run(args); err != nil { os.Exit(1) //nolint:gocritic } var serverCmd = cli.Command{ Name: \"server\", Usage: \"start object storage server\", Flags: append(ServerFlags, GlobalFlags...), Action: serverMain, ","date":"2025-01-22","objectID":"/posts/minio-get-started/:2:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"ServerMain server http://127.0.0.1:/Users/hanjing/mnt/minio0{1...3} http://127.0.0.1:/Users/hanjing/mnt/minio0{4...6} 处理系统终止或者重启相关的信号等 signal.Notify(globalOSSignalCh, os.Interrupt, syscall.SIGTERM, syscall.SIGQUIT) go handleSignals() buildServerCtxt决定磁盘布局以及是否使用 legacy 方式，调用函数cmd#mergeDisksLayoutFromArgs判断是否使用了拓展表达式，如果没有，legacy = true，否则legacy =false, legacy参数的作用我们在后面就能看到了。 serverHandleCmdArgs函数中调用 createServerEndpoints， // Handle all server command args and build the disks layout bootstrapTrace(\"serverHandleCmdArgs\", func() { // 这里确定了erasure set size的大小 err := buildServerCtxt(ctx, \u0026globalServerCtxt) logger.FatalIf(err, \"Unable to prepare the list of endpoints\") serverHandleCmdArgs(globalServerCtxt) }) ","date":"2025-01-22","objectID":"/posts/minio-get-started/:2:1","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"MinIO 的 DNS 缓存 MinIO 为了避免向外发送过多的 DNS 查询，所以实现了 DNS 缓存，默认使用net.DefaultResolver实际执行 DNS 查询，设置的 DNS 查询超时时间为5s，缓存的刷新时间在容器环境下默认为30s，在其他环境下为10min，可以通过dns-cache-ttl指定。 type Resolver struct { // Timeout defines the maximum allowed time allowed for a lookup. Timeout time.Duration // Resolver is used to perform actual DNS lookup. If nil, // net.DefaultResolver is used instead. Resolver DNSResolver once sync.Once mu sync.RWMutex cache map[string]*cacheEntry } globalDNSCache = \u0026dnscache.Resolver{ Timeout: 5 * time.Second, } func runDNSCache(ctx *cli.Context) { dnsTTL := ctx.Duration(\"dns-cache-ttl\") // Check if we have configured a custom DNS cache TTL. if dnsTTL \u003c= 0 { if orchestrated { dnsTTL = 30 * time.Second } else { dnsTTL = 10 * time.Minute } } // Call to refresh will refresh names in cache. go func() { // Baremetal setups set DNS refresh window up to dnsTTL duration. t := time.NewTicker(dnsTTL) defer t.Stop() for { select { case \u003c-t.C: globalDNSCache.Refresh() case \u003c-GlobalContext.Done(): return } } }() } ","date":"2025-01-22","objectID":"/posts/minio-get-started/:3:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"构造拓扑关系 (buildServerCtxt) // serverCtxt保存了磁盘布局 type disksLayout struct { // 是否使用拓展表达式 legacy bool // server pool的集合 pools []poolDisksLayout } type poolDisksLayout struct { // server pool对应的命令行命令 cmdline string // layout的第一位表示不同的erasure set，第二维表示同一个erasure set中不同的磁盘路径 layout [][]string } 构造拓扑关系的主要函数实现是mergeDisksLayoutFromArgs，判断环境变量MINIO_ERASURE_SET_DRIVE_COUNT是否存在，环境变量MINIO_ERASURE_SET_DRIVE_COUNT表示 erasure set 中指定的磁盘数量，否则默认为 0，表示自动设置最优结果。根据是否使用拓展表达式会走不同的逻辑。这里我们主要关心使用拓展表达式的场景GetAllSets(setDriveCount, arg)。（顺带一提，legacy style 会走GetAllSets(setDriveCount, args...)，可以看到 legacy style 只能指定一个server pool） // mergeDisksLayoutFromArgs supports with and without ellipses transparently. // 构造网络拓扑 func mergeDisksLayoutFromArgs(args []string, ctxt *serverCtxt) (err error) { if len(args) == 0 { return errInvalidArgument } ok := true // ok 表示是否使用拓展表达式，true表示不使用拓展表达式 // 只要在其中一个arg中使用拓展表达式，结果均为false for _, arg := range args { ok = ok \u0026\u0026 !ellipses.HasEllipses(arg) } var setArgs [][]string // 通过环境变量得到erasure set的大小，默认为0 v, err := env.GetInt(EnvErasureSetDriveCount, 0) if err != nil { return err } setDriveCount := uint64(v) // None of the args have ellipses use the old style. if ok { setArgs, err = GetAllSets(setDriveCount, args...) if err != nil { return err } // 所有的参数组成一个server pool ctxt.Layout = disksLayout{ legacy: true, pools: []poolDisksLayout{{layout: setArgs, cmdline: strings.Join(args, \" \")}}, } return } for _, arg := range args { if !ellipses.HasEllipses(arg) \u0026\u0026 len(args) \u003e 1 { // TODO: support SNSD deployments to be decommissioned in future return fmt.Errorf(\"all args must have ellipses for pool expansion (%w) args: %s\", errInvalidArgument, args) } setArgs, err = GetAllSets(setDriveCount, arg) if err != nil { return err } ctxt.Layout.pools = append(ctxt.Layout.pools, poolDisksLayout{cmdline: arg, layout: setArgs}) } return } GetAllSets主要调用了parseEndpointSet，通过正则表达式解析带有拓展表达式的输入参数，并返回一个[][]string，表示不同 erasure set 中的磁盘路径。这里主要对应的数据结构是endpointSet，主要实现两件事情，第一确定 setSize，第二确定如何将 endpoints 分布到不同的 erasure set 中。 // Endpoint set represents parsed ellipses values, also provides // methods to get the sets of endpoints. type endpointSet struct { // 解析终端字符串得到的arg pattern，如果有多个ellipses，对应多个`Pattern` argPatterns []ellipses.ArgPattern endpoints []string // Endpoints saved from previous GetEndpoints(). // 对于ellipses-style的参数 // setIndexes对应一行，记录了server pool size /setSize 个 setSize值 setIndexes [][]uint64 // All the sets. } type ArgPattern []Pattern // Pattern - ellipses pattern, describes the range and also the // associated prefix and suffixes. type Pattern struct { Prefix string Suffix string Seq []string } 函数getSetIndexes的目的是找到合适的setSize，MinIO 规定分布式部署 setSize 的取值必须属于var setSizes = []uint64{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}，首先从SetSizes中找到能够被server pool size整除的setCounts集合，如果自定义了setSize则判断自定义的setSize是否属于setCounts集合，如果属于则setSize设置成功，否则返回错误。如果没有设置自定义的setSize，函数possibleSetCountsWithSymmetry从setCounts集合中找到具有symmetry属性的值，MinIO 中输入带拓展表达式的参数对应的 pattern 列表和参数中的顺序是相反的，symmetry过滤出能够被 pattern 中最后一个 pattern 对应的数量整除或者被整除的setCounts中的值，这里举一个例子http://127.0.0.{1...4}:9000/Users/hanjing/mnt/minio{1...32}，显然symmetry函数会判断 4 和setCounts中值的关系，而不是 32 和setCounts中值的关系，这可能与 MinIO 希望尽可能将 erasure set 的中不同磁盘分布到不同的节点上有关。最后取出剩余候选值中最大的值作为最终的setSize。 func (s endpointSet) Get() (sets [][]string) { k := uint64(0) endpoints := s.getEndpoints() for i := range s.setIndexes { for j := range s.setIndexes[i] { sets = append(sets, endpoints[k:s.setIndexes[i][j]+k]) k = s.setIndexes[i][j] + k } } return sets } endpointSet#Get方法返回一个二维数据，第一维表示 不同的 erasure set，第二位表示 erasure set 中的不同磁盘。这里getEndpoints多重循环迭代 ellipses-style 对应的 pattern，如果还记得的话，pattern 的顺序和实际在参数中出现的顺序相反，这样得到的endpoints列表将不同节点上的磁盘均匀分布，后面连续取列表中的一段组成erasure set时，得到的erasure set中的磁盘也分布在不同的节点上。 ","date":"2025-01-22","objectID":"/posts/minio-get-started/:4:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"serverHandleCmdArgs 函数 globalEndpoints, setupType, err = createServerEndpoints(globalMinioAddr, ctxt.Layout.pools, ctxt.Layout.legacy) logger.FatalIf(err, \"Invalid command line arguments\") globalNodes = globalEndpoints.GetNodes() globalIsErasure = (setupType == ErasureSetupType) globalIsDistErasure = (setupType == DistErasureSetupType) if globalIsDistErasure { globalIsErasure = true } globalIsErasureSD = (setupType == ErasureSDSetupType) if globalDynamicAPIPort \u0026\u0026 globalIsDistErasure { logger.FatalIf(errInvalidArgument, \"Invalid --address=\\\"%s\\\", port '0' is not allowed in a distributed erasure coded setup\", ctxt.Addr) } globalLocalNodeName = GetLocalPeer(globalEndpoints, globalMinioHost, globalMinioPort) nodeNameSum := sha256.Sum256([]byte(globalLocalNodeName)) globalLocalNodeNameHex = hex.EncodeToString(nodeNameSum[:]) // Initialize, see which NIC the service is running on, and save it as global value setGlobalInternodeInterface(ctxt.Interface) 里面有一个比较重要的工具函数isLocalHost，通过 DNS 查询 host 对应的 ip，和所有网卡对应的所有本地 ip 取交集,如果交集为空，说明不是本地服务器，否则是本地服务器。 函数createServerEndpoints将数据结构[]poolDisksLayout转换成EndpointServerPools，并指定对应的SetupType 对于单磁盘部署，要求使用目录路径指定输入参数，IsLocal一定为true，SetupType为ErasureSDSetupType。其他情况下根据，根据本地 ip 和给定的 host，判断IsLocal，如果 host 为空（MinIO 称为PathEndpointType)，则setupType = ErasureSetupType，否则为URLEndpointType情况，如果不同host:port的数量等于 1，则是ErasureSetupType，否则对应DistErasureSetupType，根据得到的SetType设置全局参数。 EndpointServerPools实际上是[][]EndPoint，第一位 // EndpointServerPools是 PoolEndpoints的集合，实际上描述整个部署的拓扑结构 type EndpointServerPools []PoolEndpoints // PoolEndpoints represent endpoints in a given pool // along with its setCount and setDriveCount. // PoolEndpoints表示一个server pool的结构 type PoolEndpoints struct { // indicates if endpoints are provided in non-ellipses style // legacy 表示 是否使用遗留的方法表示终端，而不使用省略号表达式 Legacy bool // SetCount表示 server pool中的 erasure set的数量 SetCount int // DrivesPerSet 表示一个erasure set中的磁盘数量 DrivesPerSet int // type Endpoints []Endpoint // 表示一个server pool中的所有disk Endpoints Endpoints // server pool对应的命令行指令 CmdLine string // 操作系统信息 Platform string } type Endpoint struct { *url.URL // 如果是单个目录的输入，则 IsLocal为true // 如果输入参数ip是本地ip，IsLocal也为true // 其他情况下为false IsLocal bool PoolIdx, SetIdx, DiskIdx int } // SetupType - enum for setup type. type SetupType int const ( // UnknownSetupType - starts with unknown setup type. UnknownSetupType SetupType = iota // FSSetupType - FS setup type enum. FSSetupType // ErasureSDSetupType - Erasure single drive setup enum. ErasureSDSetupType // ErasureSetupType - Erasure setup type enum. ErasureSetupType // DistErasureSetupType - Distributed Erasure setup type enum. DistErasureSetupType ) 以下函数列出了 Minio 支持的不同模式，和上面的SetType之间存在对应关系。 // Returns the mode in which MinIO is running func getMinioMode() string { switch { case globalIsDistErasure: return globalMinioModeDistErasure case globalIsErasure: return globalMinioModeErasure case globalIsErasureSD: return globalMinioModeErasureSD default: return globalMinioModeFS } } ","date":"2025-01-22","objectID":"/posts/minio-get-started/:4:1","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"HTTP 服务器注册 API 注册分布式命名空间锁 registerAPIRouter注册 s3 相关的主要 api // Configure server. bootstrapTrace(\"configureServer\", func() { handler, err := configureServerHandler(globalEndpoints) if err != nil { logger.Fatal(config.ErrUnexpectedError(err), \"Unable to configure one of server's RPC services\") } // Allow grid to start after registering all services. close(globalGridStart) close(globalLockGridStart) httpServer := xhttp.NewServer(getServerListenAddrs()). UseHandler(setCriticalErrorHandler(corsHandler(handler))). UseTLSConfig(newTLSConfig(getCert)). UseIdleTimeout(globalServerCtxt.IdleTimeout). UseReadTimeout(globalServerCtxt.IdleTimeout). UseWriteTimeout(globalServerCtxt.IdleTimeout). UseReadHeaderTimeout(globalServerCtxt.ReadHeaderTimeout). UseBaseContext(GlobalContext). UseCustomLogger(log.New(io.Discard, \"\", 0)). // Turn-off random logging by Go stdlib UseTCPOptions(globalTCPOptions) httpServer.TCPOptions.Trace = bootstrapTraceMsg go func() { serveFn, err := httpServer.Init(GlobalContext, func(listenAddr string, err error) { bootLogIf(GlobalContext, fmt.Errorf(\"Unable to listen on `%s`: %v\", listenAddr, err)) }) if err != nil { globalHTTPServerErrorCh \u003c- err return } globalHTTPServerErrorCh \u003c- serveFn() }() setHTTPServer(httpServer) }) // configureServer handler returns final handler for the http server. func configureServerHandler(endpointServerPools EndpointServerPools) (http.Handler, error) { // Initialize router. `SkipClean(true)` stops minio/mux from // normalizing URL path minio/minio#3256 router := mux.NewRouter().SkipClean(true).UseEncodedPath() // Initialize distributed NS lock. if globalIsDistErasure { registerDistErasureRouters(router, endpointServerPools) } // Add Admin router, all APIs are enabled in server mode. registerAdminRouter(router, true) // Add healthCheck router registerHealthCheckRouter(router) // Add server metrics router registerMetricsRouter(router) // Add STS router always. registerSTSRouter(router) // Add KMS router registerKMSRouter(router) // Add API router registerAPIRouter(router) router.Use(globalMiddlewares...) return router, nil } registerAPIRouter会注册主要的 s3 API，这里举GetObject操作为例进行说明，当 http method 为GET时，如果没有命中其他的路由，则认为是GetObject操作，从 Path 中获取object名字，并使用api.GetObjectHandler进行处理和响应，s3APIMiddleware作为中间件，可以做一些额外的操作，比如监控和记录日志。 api对象中保存了一个函数引用，通过这个函数引用，能够得到全局的ObjectLayer对象，ObjectLayer实现了对象 API 层的基本操作。 // GetObject router.Methods(http.MethodGet).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.GetObjectHandler, traceHdrsS3HFlag)) // Initialize API. api := objectAPIHandlers{ ObjectAPI: newObjectLayerFn, } // objectAPIHandlers implements and provides http handlers for S3 API. type objectAPIHandlers struct { ObjectAPI func() ObjectLayer } func newObjectLayerFn() ObjectLayer { globalObjLayerMutex.RLock() defer globalObjLayerMutex.RUnlock() return globalObjectAPI } ","date":"2025-01-22","objectID":"/posts/minio-get-started/:5:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"ObjectLayer 的初始化流程 var newObject ObjectLayer bootstrapTrace(\"newObjectLayer\", func() { var err error newObject, err = newObjectLayer(GlobalContext, globalEndpoints) if err != nil { logFatalErrs(err, Endpoint{}, true) } }) storageclass.LookupConfig函数根据环境变量等初始化Standard Storage Class、Reduced Redundancy Storage Class以及Optimized Storage Class、以及 inline data的大小 Standard Storage Class：通过环境变量MINIO_STORAGE_CLASS_STANDARD指定，否则会根据erasure set的大小指定 // DefaultParityBlocks returns default parity blocks for 'drive' count func DefaultParityBlocks(drive int) int { switch drive { case 1: return 0 case 3, 2: return 1 case 4, 5: return 2 case 6, 7: return 3 default: return 4 } } Reduced Redundancy Storage Class: 通过环境变量MINIO_STORAGE_CLASS_RRS指定，否则默认为 1 Optimized Storage Class：通过环境变量MINIO_STORAGE_CLASS_OPTIMIZE指定，默认为\"\" inline block size: 通过环境变量MINIO_STORAGE_CLASS_INLINE_BLOCK指定，默认为128KiB,如果 shard 数据的大小小于inline block size，则会直接将数据和元数据写到同一个文件，即xl.meta ","date":"2025-01-22","objectID":"/posts/minio-get-started/:6:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"MinIO 的存储分层 erasureServerPools // erasureServerPools // minio 服务可以由多个server pool 组成，用来水平拓展 // erasureServerPools是server pool的集合 type erasureServerPools struct { poolMetaMutex sync.RWMutex poolMeta poolMeta rebalMu sync.RWMutex rebalMeta *rebalanceMeta deploymentID [16]byte distributionAlgo string // server pool 由多个erasure set 组成 // 这里的erasureSets结构实际上指单个 server pool serverPools []*erasureSets // Active decommission canceler decommissionCancelers []context.CancelFunc s3Peer *S3PeerSys mpCache *xsync.MapOf[string, MultipartInfo] } erasureSets // erasureSets implements ObjectLayer combining a static list of erasure coded // object sets. NOTE: There is no dynamic scaling allowed or intended in // current design. // server pool 由多个erasure set 组成 // 这里的erasureSets结构实际上指单个 server pool // 上面这段话的意思是不能动态扩展server pool，初始指定后就不能再修改了 type erasureSets struct { sets []*erasureObjects // Reference format. format *formatErasureV3 // erasureDisks mutex to lock erasureDisks. erasureDisksMu sync.RWMutex // Re-ordered list of disks per set. erasureDisks [][]StorageAPI // Distributed locker clients. erasureLockers setsDsyncLockers // Distributed lock owner (constant per running instance). erasureLockOwner string // List of endpoints provided on the command line. endpoints PoolEndpoints // String version of all the endpoints, an optimization // to avoid url.String() conversion taking CPU on // large disk setups. endpointStrings []string // Total number of sets and the number of disks per set. setCount, setDriveCount int defaultParityCount int poolIndex int // Distribution algorithm of choice. distributionAlgo string deploymentID [16]byte lastConnectDisksOpTime time.Time } erasureObjects // erasureObjects - Implements ER object layer. // 表示一个erasure Set type erasureObjects struct { setDriveCount int defaultParityCount int setIndex int poolIndex int // getDisks returns list of storageAPIs. getDisks func() []StorageAPI // getLockers returns list of remote and local lockers. getLockers func() ([]dsync.NetLocker, string) // getEndpoints returns list of endpoint belonging this set. // some may be local and some remote. getEndpoints func() []Endpoint // getEndpoints returns list of endpoint strings belonging this set. // some may be local and some remote. getEndpointStrings func() []string // Locker mutex map. nsMutex *nsLockMap } ","date":"2025-01-22","objectID":"/posts/minio-get-started/:6:1","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"StorageAPI StorageAPI主要有两个实现 xlStorage表示本地存储 storageRESTClient表示远程主机上的存储 // Depending on the disk type network or local, initialize storage API. func newStorageAPI(endpoint Endpoint, opts storageOpts) (storage StorageAPI, err error) { if endpoint.IsLocal { storage, err := newXLStorage(endpoint, opts.cleanUp) if err != nil { return nil, err } return newXLStorageDiskIDCheck(storage, opts.healthCheck), nil } return newStorageRESTClient(endpoint, opts.healthCheck, globalGrid.Load()) } newXLStorage函数调用了getDiskInfo函数，并要求路径不能在rootDrive上。判断磁盘是否支持O_DIRECT，在分布式部署下，如果不支持O_DIRECT，则直接报错。 // Return an error if ODirect is not supported. Single disk will have // oDirect off. // 在类似unix的平台上 disk.ODirectPlatform应该为true if globalIsErasureSD || !disk.ODirectPlatform { s.oDirect = false } else if err := s.checkODirectDiskSupport(info.FSType); err == nil { s.oDirect = true } else { return s, err } // getDiskInfo returns given disk information. func getDiskInfo(drivePath string) (di disk.Info, rootDrive bool, err error) { if err = checkPathLength(drivePath); err == nil { di, err = disk.GetInfo(drivePath, false) if !globalIsCICD \u0026\u0026 !globalIsErasureSD { if globalRootDiskThreshold \u003e 0 { // Use MINIO_ROOTDISK_THRESHOLD_SIZE to figure out if // this disk is a root disk. treat those disks with // size less than or equal to the threshold as rootDrives. rootDrive = di.Total \u003c= globalRootDiskThreshold } else { rootDrive, err = disk.IsRootDisk(drivePath, SlashSeparator) } } } // StorageAPI interface. // 对应磁盘 type StorageAPI interface { // Stringified version of disk. String() string // Storage operations. // Returns true if disk is online and its valid i.e valid format.json. // This has nothing to do with if the drive is hung or not responding. // For that individual storage API calls will fail properly. The purpose // of this function is to know if the \"drive\" has \"format.json\" or not // if it has a \"format.json\" then is it correct \"format.json\" or not. IsOnline() bool // Returns the last time this disk (re)-connected LastConn() time.Time // Indicates if disk is local or not. IsLocal() bool // Returns hostname if disk is remote. Hostname() string // Returns the entire endpoint. Endpoint() Endpoint // Close the disk, mark it purposefully closed, only implemented for remote disks. Close() error // Returns the unique 'uuid' of this disk. GetDiskID() (string, error) // Set a unique 'uuid' for this disk, only used when // disk is replaced and formatted. SetDiskID(id string) // Returns healing information for a newly replaced disk, // returns 'nil' once healing is complete or if the disk // has never been replaced. Healing() *healingTracker DiskInfo(ctx context.Context, opts DiskInfoOptions) (info DiskInfo, err error) NSScanner(ctx context.Context, cache dataUsageCache, updates chan\u003c- dataUsageEntry, scanMode madmin.HealScanMode, shouldSleep func() bool) (dataUsageCache, error) // Volume operations. MakeVol(ctx context.Context, volume string) (err error) MakeVolBulk(ctx context.Context, volumes ...string) (err error) ListVols(ctx context.Context) (vols []VolInfo, err error) StatVol(ctx context.Context, volume string) (vol VolInfo, err error) DeleteVol(ctx context.Context, volume string, forceDelete bool) (err error) // WalkDir will walk a directory on disk and return a metacache stream on wr. WalkDir(ctx context.Context, opts WalkDirOptions, wr io.Writer) error // Metadata operations DeleteVersion(ctx context.Context, volume, path string, fi FileInfo, forceDelMarker bool, opts DeleteOptions) error DeleteVersions(ctx context.Context, volume string, versions []FileInfoVersions, opts DeleteOptions) []error DeleteBulk(ctx context.Context, volume string, paths ...string) error WriteMetadata(ctx context.Context, origvolume, volume, path string, fi FileInfo) error UpdateMetadata(ctx context.Context, volume, path string, fi FileInfo, opts UpdateMetadataOpts) error ReadVersion(ctx context.Context, origvolume, volume, path, versionID string, opts ReadOptions) (FileInfo, err","date":"2025-01-22","objectID":"/posts/minio-get-started/:6:2","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"ObjectLayer 唯一一个实现就是erasureServerPools ObjectLayer 就是 Minio 提供的面向 Object 的接口，而StorageAPI则是具体的本地或者远程存储磁盘。 // ObjectLayer implements primitives for object API layer. // 重要接口 type ObjectLayer interface { // Locking operations on object. NewNSLock(bucket string, objects ...string) RWLocker // Storage operations. Shutdown(context.Context) error NSScanner(ctx context.Context, updates chan\u003c- DataUsageInfo, wantCycle uint32, scanMode madmin.HealScanMode) error BackendInfo() madmin.BackendInfo Legacy() bool // Only returns true for deployments which use CRCMOD as its object distribution algorithm. StorageInfo(ctx context.Context, metrics bool) StorageInfo LocalStorageInfo(ctx context.Context, metrics bool) StorageInfo // Bucket operations. MakeBucket(ctx context.Context, bucket string, opts MakeBucketOptions) error GetBucketInfo(ctx context.Context, bucket string, opts BucketOptions) (bucketInfo BucketInfo, err error) ListBuckets(ctx context.Context, opts BucketOptions) (buckets []BucketInfo, err error) DeleteBucket(ctx context.Context, bucket string, opts DeleteBucketOptions) error ListObjects(ctx context.Context, bucket, prefix, marker, delimiter string, maxKeys int) (result ListObjectsInfo, err error) ListObjectsV2(ctx context.Context, bucket, prefix, continuationToken, delimiter string, maxKeys int, fetchOwner bool, startAfter string) (result ListObjectsV2Info, err error) ListObjectVersions(ctx context.Context, bucket, prefix, marker, versionMarker, delimiter string, maxKeys int) (result ListObjectVersionsInfo, err error) // Walk lists all objects including versions, delete markers. Walk(ctx context.Context, bucket, prefix string, results chan\u003c- itemOrErr[ObjectInfo], opts WalkOptions) error // Object operations. // GetObjectNInfo returns a GetObjectReader that satisfies the // ReadCloser interface. The Close method runs any cleanup // functions, so it must always be called after reading till EOF // // IMPORTANTLY, when implementations return err != nil, this // function MUST NOT return a non-nil ReadCloser. GetObjectNInfo(ctx context.Context, bucket, object string, rs *HTTPRangeSpec, h http.Header, opts ObjectOptions) (reader *GetObjectReader, err error) GetObjectInfo(ctx context.Context, bucket, object string, opts ObjectOptions) (objInfo ObjectInfo, err error) PutObject(ctx context.Context, bucket, object string, data *PutObjReader, opts ObjectOptions) (objInfo ObjectInfo, err error) CopyObject(ctx context.Context, srcBucket, srcObject, destBucket, destObject string, srcInfo ObjectInfo, srcOpts, dstOpts ObjectOptions) (objInfo ObjectInfo, err error) DeleteObject(ctx context.Context, bucket, object string, opts ObjectOptions) (ObjectInfo, error) DeleteObjects(ctx context.Context, bucket string, objects []ObjectToDelete, opts ObjectOptions) ([]DeletedObject, []error) TransitionObject(ctx context.Context, bucket, object string, opts ObjectOptions) error RestoreTransitionedObject(ctx context.Context, bucket, object string, opts ObjectOptions) error // Multipart operations. ListMultipartUploads(ctx context.Context, bucket, prefix, keyMarker, uploadIDMarker, delimiter string, maxUploads int) (result ListMultipartsInfo, err error) NewMultipartUpload(ctx context.Context, bucket, object string, opts ObjectOptions) (result *NewMultipartUploadResult, err error) CopyObjectPart(ctx context.Context, srcBucket, srcObject, destBucket, destObject string, uploadID string, partID int, startOffset int64, length int64, srcInfo ObjectInfo, srcOpts, dstOpts ObjectOptions) (info PartInfo, err error) PutObjectPart(ctx context.Context, bucket, object, uploadID string, partID int, data *PutObjReader, opts ObjectOptions) (info PartInfo, err error) GetMultipartInfo(ctx context.Context, bucket, object, uploadID string, opts ObjectOptions) (info MultipartInfo, err error) ListObjectParts(ctx context.Context, bucket, object, uploadID string, partNumberMarker int, maxParts int, opts ObjectOptions) (result ListPartsInfo, err error) AbortMultipartUpload(ctx contex","date":"2025-01-22","objectID":"/posts/minio-get-started/:6:3","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"重要常量 const ( // Represents Erasure backend. formatBackendErasure = \"xl\" // Represents Erasure backend - single drive formatBackendErasureSingle = \"xl-single\" // formatErasureV1.Erasure.Version - version '1'. formatErasureVersionV1 = \"1\" // formatErasureV2.Erasure.Version - version '2'. formatErasureVersionV2 = \"2\" // formatErasureV3.Erasure.Version - version '3'. formatErasureVersionV3 = \"3\" // Distribution algorithm used, legacy formatErasureVersionV2DistributionAlgoV1 = \"CRCMOD\" // Distributed algorithm used, with N/2 default parity formatErasureVersionV3DistributionAlgoV2 = \"SIPMOD\" // Distributed algorithm used, with EC:4 default parity formatErasureVersionV3DistributionAlgoV3 = \"SIPMOD+PARITY\" ) ","date":"2025-01-22","objectID":"/posts/minio-get-started/:7:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"s3 API: PutObject // PutObject // http处理函数 router.Methods(http.MethodPut).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.PutObjectHandler, traceHdrsS3HFlag)) // objectLayer层接口 putObject = objectAPI.PutObject // Validate storage class metadata if present // x-amz-storage-class minio 只支持 REDUCED_REDUNDANCY 和 Standard if sc := r.Header.Get(xhttp.AmzStorageClass); sc != \"\" { if !storageclass.IsValid(sc) { writeErrorResponse(ctx, w, errorCodes.ToAPIErr(ErrInvalidStorageClass), r.URL) return } } // maximum Upload size for objects in a single operation // 5TiB if isMaxObjectSize(size) { writeErrorResponse(ctx, w, errorCodes.ToAPIErr(ErrEntityTooLarge), r.URL) return } objInfo, err := putObject(ctx, bucket, object, pReader, opts) 同一个对象对应到的erasure set总是同一个，这是通过确定性的 hash 算法得到的，所以 server pool 不能被修改，否则 hash 映射关系可能发生变化。 // Returns always a same erasure coded set for a given input. func (s *erasureSets) getHashedSetIndex(input string) int { // 通过hash得到对应erasure set，以下要素可能影响hash结果 return hashKey(s.distributionAlgo, input, len(s.sets), s.deploymentID) } // PutObject - creates an object upon reading from the input stream // until EOF, erasure codes the data across all disk and additionally // writes `xl.meta` which carries the necessary metadata for future // object operations. func (er erasureObjects) PutObject(ctx context.Context, bucket string, object string, data *PutObjReader, opts ObjectOptions) (objInfo ObjectInfo, err error) { return er.putObject(ctx, bucket, object, data, opts) } ","date":"2025-01-22","objectID":"/posts/minio-get-started/:8:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"block 和 shard block （块） blockSize 代表原始数据在存储时被切分的最小单位。 在 MinIO 中，数据在存储前被分割成多个 block。 这些 block 经过 纠删码（Erasure Coding） 计算后，生成 数据块（data blocks） 和 校验块（parity blocks）。 Shard (分片) shard 是 MinIO 存储在磁盘上的物理单位，包含 数据块 和 校验块。 在 N+M 纠删码（N 个数据块 + M 个校验块）中，每个 shard 对应 一个数据块或一个校验块。 例如， EC: 4+2 （4 个数据块 + 2 个校验块）表示： 数据被分成 4 个 block。 计算出 2 个额外的 parity block（用于恢复数据）。 最终存储 6 个 shard，每个 shard 分别存放在不同的磁盘上。 假设数据为 300MiB，blocksize 为 10MiB, 遵循 EC: 4 + 2, shardsize = ceil(10MiB / 4) =2.5MiB，最终每个 blocksize 存储在磁盘上为 6 个 shard，4 个 data shard，6 个 parity shard ","date":"2025-01-22","objectID":"/posts/minio-get-started/:8:1","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"putObject 的主要流程 创建临时目录，写入分片数据 如果没有加锁，获取名字空间锁，实现原子操作，避免数据竞争 rename 操作，包含将分片移动到目标目录以及写入 xl.meta元数据 最后好像有提交操作，没有看懂 ","date":"2025-01-22","objectID":"/posts/minio-get-started/:8:2","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"putObject选择serverPool // PutObject - writes an object to least used erasure pool. func (z *erasureServerPools) PutObject(ctx context.Context, bucket string, object string, data *PutObjReader, opts ObjectOptions) (ObjectInfo, error) { // Validate put object input args. if err := checkPutObjectArgs(ctx, bucket, object); err != nil { return ObjectInfo{}, err } object = encodeDirObject(object) if z.SinglePool() { return z.serverPools[0].PutObject(ctx, bucket, object, data, opts) } idx, err := z.getPoolIdx(ctx, bucket, object, data.Size()) if err != nil { return ObjectInfo{}, err } if opts.DataMovement \u0026\u0026 idx == opts.SrcPoolIdx { return ObjectInfo{}, DataMovementOverwriteErr{ Bucket: bucket, Object: object, VersionID: opts.VersionID, Err: errDataMovementSrcDstPoolSame, } } return z.serverPools[idx].PutObject(ctx, bucket, object, data, opts) } 如果只有一个server pool，那只能使用当前的server pool。 // getPoolIdx returns the found previous object and its corresponding pool idx, // if none are found falls back to most available space pool, this function is // designed to be only used by PutObject, CopyObject (newObject creation) and NewMultipartUpload. func (z *erasureServerPools) getPoolIdx(ctx context.Context, bucket, object string, size int64) (idx int, err error) { idx, err = z.getPoolIdxExistingWithOpts(ctx, bucket, object, ObjectOptions{ SkipDecommissioned: true, SkipRebalancing: true, }) if err != nil \u0026\u0026 !isErrObjectNotFound(err) { return idx, err } if isErrObjectNotFound(err) { idx = z.getAvailablePoolIdx(ctx, bucket, object, size) if idx \u003c 0 { return -1, toObjectErr(errDiskFull) } } return idx, nil } 如果对象在某个server pool中已经存在，则返回对应的server pool，否则选择空闲容量最多的server pool。 func (z *erasureServerPools) getPoolInfoExistingWithOpts(ctx context.Context, bucket, object string, opts ObjectOptions) (PoolObjInfo, []poolErrs, error) { var noReadQuorumPools []poolErrs poolObjInfos := make([]PoolObjInfo, len(z.serverPools)) poolOpts := make([]ObjectOptions, len(z.serverPools)) for i := range z.serverPools { poolOpts[i] = opts } var wg sync.WaitGroup for i, pool := range z.serverPools { wg.Add(1) go func(i int, pool *erasureSets, opts ObjectOptions) { defer wg.Done() // remember the pool index, we may sort the slice original index might be lost. pinfo := PoolObjInfo{ Index: i, } // do not remove this check as it can lead to inconsistencies // for all callers of bucket replication. if !opts.MetadataChg { opts.VersionID = \"\" } pinfo.ObjInfo, pinfo.Err = pool.GetObjectInfo(ctx, bucket, object, opts) poolObjInfos[i] = pinfo }(i, pool, poolOpts[i]) } wg.Wait() // Sort the objInfos such that we always serve latest // this is a defensive change to handle any duplicate // content that may have been created, we always serve // the latest object. sort.Slice(poolObjInfos, func(i, j int) bool { mtime1 := poolObjInfos[i].ObjInfo.ModTime mtime2 := poolObjInfos[j].ObjInfo.ModTime return mtime1.After(mtime2) }) defPool := PoolObjInfo{Index: -1} for _, pinfo := range poolObjInfos { // skip all objects from suspended pools if asked by the // caller. if opts.SkipDecommissioned \u0026\u0026 z.IsSuspended(pinfo.Index) { continue } // Skip object if it's from pools participating in a rebalance operation. if opts.SkipRebalancing \u0026\u0026 z.IsPoolRebalancing(pinfo.Index) { continue } if pinfo.Err == nil { // found a pool return pinfo, z.poolsWithObject(poolObjInfos, opts), nil } if isErrReadQuorum(pinfo.Err) \u0026\u0026 !opts.MetadataChg { // read quorum is returned when the object is visibly // present but its unreadable, we simply ask the writes to // schedule to this pool instead. If there is no quorum // it will fail anyways, however if there is quorum available // with enough disks online but sufficiently inconsistent to // break parity threshold, allow them to be overwritten // or allow new versions to be added. return pinfo, z.poolsWithObject(poolObjInfos, opts), nil } defPool = pinfo if !isErrObjectNotFound(pinfo.Err) \u0026\u0026 !isErrVersionNotFound(pinfo.Err) { return pinfo, noReadQuorumPools, pinfo.Err } //","date":"2025-01-22","objectID":"/posts/minio-get-started/:8:3","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"名字空间锁的实现原理 （TODO) // func (er erasureObjects) putObject if !opts.NoLock { lk := er.NewNSLock(bucket, object) lkctx, err := lk.GetLock(ctx, globalOperationTimeout) if err != nil { return ObjectInfo{}, err } ctx = lkctx.Context() defer lk.Unlock(lkctx) } erasureObjects中有两个关键的字段getLockers和nsMutex用于名字空间加锁。 type erasureObjects struct { // getLockers returns list of remote and local lockers. getLockers func() ([]dsync.NetLocker, string) // Locker mutex map. nsMutex *nsLockMap } type erasureSets struct { sets []*erasureObjects // Distributed locker clients. erasureLockers setsDsyncLockers // Distributed lock owner (constant per running instance). erasureLockOwner string // setsDsyncLockers is encapsulated type for Close() type setsDsyncLockers [][]dsync.NetLocker func (s *erasureSets) GetLockers(setIndex int) func() ([]dsync.NetLocker, string) { return func() ([]dsync.NetLocker, string) { lockers := make([]dsync.NetLocker, len(s.erasureLockers[setIndex])) copy(lockers, s.erasureLockers[setIndex]) // erasureLockerOwner实际上是globalLocalNodeName // The name of this local node, fetched from arguments // globalLocalNodeName string return lockers, s.erasureLockOwner } } // nsLockMap - namespace lock map, provides primitives to Lock, // Unlock, RLock and RUnlock. type nsLockMap struct { // Indicates if namespace is part of a distributed setup. isDistErasure bool lockMap map[string]*nsLock lockMapMutex sync.Mutex } // newNSLock - return a new name space lock map. func newNSLock(isDistErasure bool) *nsLockMap { nsMutex := nsLockMap{ isDistErasure: isDistErasure, } if isDistErasure { return \u0026nsMutex } nsMutex.lockMap = make(map[string]*nsLock) return \u0026nsMutex } ","date":"2025-01-22","objectID":"/posts/minio-get-started/:9:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"什么是 dsync？ dsync 是 MinIO 实现的分布式锁（distributed locking）库，用于在多节点环境下进行同步锁定，确保数据一致性。 主要作用： 在 MinIO 集群 中，确保多个 MinIO 服务器节点在 并发访问同一资源 时，正确管理读/写锁。 提供 类似 sync.Mutex 和 sync.RWMutex 的分布式版本，但适用于分布式系统，而不是单机环境。 避免数据竞争和不一致性，保证多个 MinIO 服务器不会发生并发冲突。 NetLocker 接口 你提供的 NetLocker 接口定义了一种分布式锁管理机制，与 dsync 兼容，核心方法包括： Lock() / Unlock() —— 写锁 RLock() / RUnlock() —— 读锁 Refresh() —— 续约锁，防止锁过期 ForceUnlock() —— 强制解锁 IsOnline() / IsLocal() —— 检查锁服务是否在线，本地还是远程 String() / Close() —— 返回锁的标识 \u0026 关闭连接 这套机制允许 MinIO 在多个服务器节点间进行分布式锁管理，确保一致性。 ","date":"2025-01-22","objectID":"/posts/minio-get-started/:9:1","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"dsync 是如何工作的？ dsync 采用基于 n/2+1 多数决机制的分布式锁，适用于 MinIO 分布式对象存储集群。 核心特点： 分布式锁（类似 sync.Mutex） Lock() / Unlock() 实现互斥锁，确保多个 MinIO 节点不会同时写入相同数据。 RLock() / RUnlock() 允许多个读取者并发访问，但不能同时有写入者。 基于 Raft 的一致性算法 不存储锁的持久化状态，而是采用 n/2+1 机制 如果大多数（n/2+1）MinIO 节点同意加锁，则锁成功。 如果未达到多数决（如部分节点宕机），加锁失败，防止数据不一致。 这类似于 Paxos/Raft 选举机制，保证数据一致性。 超时 \u0026 续约（避免死锁） 锁会自动超时，防止死锁问题。 Refresh() 允许持有锁的进程 续约，防止锁过期被其他进程获取。 支持本地 \u0026 远程锁 单机模式：类似 sync.Mutex，锁是本地的。 分布式模式（dsync）：锁请求会被发送到多个 MinIO 服务器，确保整个集群同步加锁。 MinIO 为什么需要 dsync？ 在 MinIO 分布式对象存储 中，多个节点可能同时操作同一个对象（如 PUT/DELETE 操作）。 如果没有锁，可能会出现 数据覆盖、损坏或不一致 的问题。 使用 dsync 进行分布式锁管理，MinIO 解决了这些问题： 确保多个节点不会同时写入同一对象，防止数据损坏。 允许多个节点同时读取数据，提高并发性能。 防止死锁 \u0026 允许锁续约，确保锁不会永久占用资源。 ","date":"2025-01-22","objectID":"/posts/minio-get-started/:9:2","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"总结 dsync 是 MinIO 的分布式锁库，用于多节点同步，确保一致性。 采用 n/2+1 多数决机制，防止数据竞争 \u0026 保证锁安全。 提供 读/写锁、强制解锁、锁续约等功能，适用于高并发场景。 MinIO 通过 dsync 确保多个服务器不会并发写入相同对象，保证数据一致性。 ","date":"2025-01-22","objectID":"/posts/minio-get-started/:9:3","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"O_DIRECT 的实际用途 ","date":"2025-01-22","objectID":"/posts/minio-get-started/:10:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"s3 API: GetObject // GetObject router.Methods(http.MethodGet).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.GetObjectHandler, traceHdrsS3HFlag)) 首先加分布式读锁 通过读取xl.meta获取对象的元数据信息，xl.meta保存了part和verison的全部信息，注意可能存在某些磁盘上的xl.meta由于故障而修改落后，所以依然需要读取法定人数的磁盘，从而确定实际的元数据 如果 http 请求通过part或者range要求读取部分数据，最终都会转换成对多个 part 的读取，每个 part 都会划分成不同的block进行操作。 func (z *erasureServerPools) GetObjectNInfo(ctx context.Context, bucket, object string, rs *HTTPRangeSpec, h http.Header, opts ObjectOptions) (gr *GetObjectReader, err error) { if err = checkGetObjArgs(ctx, bucket, object); err != nil { return nil, err } // This is a special call attempted first to check for SOS-API calls. gr, err = veeamSOSAPIGetObject(ctx, bucket, object, rs, opts) if err == nil { return gr, nil } // reset any error to 'nil' and any reader to be 'nil' gr = nil err = nil object = encodeDirObject(object) if z.SinglePool() { return z.serverPools[0].GetObjectNInfo(ctx, bucket, object, rs, h, opts) } var unlockOnDefer bool nsUnlocker := func() {} defer func() { if unlockOnDefer { nsUnlocker() } }() // Acquire lock if !opts.NoLock { lock := z.NewNSLock(bucket, object) lkctx, err := lock.GetRLock(ctx, globalOperationTimeout) if err != nil { return nil, err } ctx = lkctx.Context() nsUnlocker = func() { lock.RUnlock(lkctx) } unlockOnDefer = true } checkPrecondFn := opts.CheckPrecondFn opts.CheckPrecondFn = nil // do not need to apply pre-conditions at lower layer. opts.NoLock = true // no locks needed at lower levels for getObjectInfo() objInfo, zIdx, err := z.getLatestObjectInfoWithIdx(ctx, bucket, object, opts) if err != nil { if objInfo.DeleteMarker { if opts.VersionID == \"\" { return \u0026GetObjectReader{ ObjInfo: objInfo, }, toObjectErr(errFileNotFound, bucket, object) } // Make sure to return object info to provide extra information. return \u0026GetObjectReader{ ObjInfo: objInfo, }, toObjectErr(errMethodNotAllowed, bucket, object) } return nil, err } // check preconditions before reading the stream. if checkPrecondFn != nil \u0026\u0026 checkPrecondFn(objInfo) { return nil, PreConditionFailed{} } opts.NoLock = true gr, err = z.serverPools[zIdx].GetObjectNInfo(ctx, bucket, object, rs, h, opts) if err != nil { return nil, err } if unlockOnDefer { unlockOnDefer = gr.ObjInfo.Inlined } if !unlockOnDefer { return gr.WithCleanupFuncs(nsUnlocker), nil } return gr, nil } // getLatestObjectInfoWithIdx returns the objectInfo of the latest object from multiple pools (this function // is present in-case there were duplicate writes to both pools, this function also returns the // additional index where the latest object exists, that is used to start the GetObject stream. func (z *erasureServerPools) getLatestObjectInfoWithIdx(ctx context.Context, bucket, object string, opts ObjectOptions) (ObjectInfo, int, error) { object = encodeDirObject(object) results := make([]struct { zIdx int oi ObjectInfo err error }, len(z.serverPools)) var wg sync.WaitGroup for i, pool := range z.serverPools { wg.Add(1) go func(i int, pool *erasureSets) { defer wg.Done() results[i].zIdx = i results[i].oi, results[i].err = pool.GetObjectInfo(ctx, bucket, object, opts) }(i, pool) } wg.Wait() // Sort the objInfos such that we always serve latest // this is a defensive change to handle any duplicate // content that may have been created, we always serve // the latest object. sort.Slice(results, func(i, j int) bool { a, b := results[i], results[j] if a.oi.ModTime.Equal(b.oi.ModTime) { // On tiebreak, select the lowest pool index. return a.zIdx \u003c b.zIdx } return a.oi.ModTime.After(b.oi.ModTime) }) for _, res := range results { err := res.err if err == nil { return res.oi, res.zIdx, nil } if !isErrObjectNotFound(err) \u0026\u0026 !isErrVersionNotFound(err) { // some errors such as MethodNotAllowed for delete marker // should be returned upwards. return res.oi, res.zIdx, err } // When its a delete marker and versionID is empty // we should simply return the error right away. if res.oi.DeleteMarker \u0026\u0026 opts.VersionID == \"\" { return res.oi, res.zI","date":"2025-01-22","objectID":"/posts/minio-get-started/:11:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"纠删码的基本原理 https://p0kt65jtu2p.feishu.cn/docx/LZ36dMN3LoZCuUxFadccNOXGnKb 假设将数据分成 4 块，采用 EC:2 冗余比例，可以将原来的数据组合成一个输入矩阵 P = [][]byte， 第一维表示不同的数据块，第二维表示数据块的数据，所以这里的 P 的大小为 4 * n，n 为每个数据块的大小 编码矩阵 E 的大小为 6 * 4，要求 编码矩阵的前 4 行组成的矩阵为单位矩阵，保持原来数据块数据不变，后两行用来生成冗余数据。 E * P = C （C 表示生成的数据块和冗余块） 假设有两行数据不慎丢失，此时去掉那两行对应的数据后依然有关系 $E’ * P = C’$ 成立，此时通过求逆可以得到原先的 P，也就从数据丢失中恢复了原来的数据。 ","date":"2025-01-22","objectID":"/posts/minio-get-started/:12:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":["MinIO"],"content":"分片上传和断点续传 分片下载可以通过前面说过的 http 请求中的range或者partnumber实现。 主要涉及的 s3 API（客户端）: InitiateMultipartUpload UploadPart AbortMultipartUpload CompleteMultipartUpload 在 minio 的客户端代码中实现了分片上传，并且支持并发上传 // PutObject creates an object in a bucket. // // You must have WRITE permissions on a bucket to create an object. // // - For size smaller than 16MiB PutObject automatically does a // single atomic PUT operation. // // - For size larger than 16MiB PutObject automatically does a // multipart upload operation. // // - For size input as -1 PutObject does a multipart Put operation // until input stream reaches EOF. Maximum object size that can // be uploaded through this operation will be 5TiB. // // WARNING: Passing down '-1' will use memory and these cannot // be reused for best outcomes for PutObject(), pass the size always. // // NOTE: Upon errors during upload multipart operation is entirely aborted. func (c *Client) PutObject(ctx context.Context, bucketName, objectName string, reader io.Reader, objectSize int64, opts PutObjectOptions, ) // putObjectMultipartStreamParallel uploads opts.NumThreads parts in parallel. // This is expected to take opts.PartSize * opts.NumThreads * (GOGC / 100) bytes of buffer. func (c *Client) putObjectMultipartStreamParallel(ctx context.Context, bucketName, objectName string, reader io.Reader, opts PutObjectOptions, ) (info UploadInfo, err error) { // PutObjectPart router.Methods(http.MethodPut).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.PutObjectPartHandler, traceHdrsS3HFlag)). Queries(\"partNumber\", \"{partNumber:.*}\", \"uploadId\", \"{uploadId:.*}\") // ListObjectParts router.Methods(http.MethodGet).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.ListObjectPartsHandler)). Queries(\"uploadId\", \"{uploadId:.*}\") // CompleteMultipartUpload router.Methods(http.MethodPost).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.CompleteMultipartUploadHandler)). Queries(\"uploadId\", \"{uploadId:.*}\") // NewMultipartUpload router.Methods(http.MethodPost).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.NewMultipartUploadHandler)). Queries(\"uploads\", \"\") // AbortMultipartUpload router.Methods(http.MethodDelete).Path(\"/{object:.+}\"). HandlerFunc(s3APIMiddleware(api.AbortMultipartUploadHandler)). Queries(\"uploadId\", \"{uploadId:.*}\") NewMultipartUpload 生成 uuid 作为 uploadId 将元数据写入 .minio.sys/multipart uploadId 路径下 PutObjectPart 类似于 PutObejct CompleteMultipartUpload 并没有合并 part，仍然保留每个 part ","date":"2025-01-22","objectID":"/posts/minio-get-started/:13:0","tags":["MinIO"],"title":"MinIO大杂烩","uri":"/posts/minio-get-started/"},{"categories":[],"content":"联系方式 github: https://github.com/ShadowUnderMoon ","date":"2025-01-17","objectID":"/about/:0:0","tags":[],"title":"About","uri":"/about/"}]